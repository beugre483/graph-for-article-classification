title,abstract,category,label
End-to-End Speaker Diarization as Post-Processing,"This paper investigates the utilization of an end-to-end diarization model as post-processing of conventional clustering-based diarization. Clustering-based diarization methods partition frames into clusters of the number of speakers; thus, they typically cannot handle overlapping speech because each frame is assigned to one speaker. On the other hand, some end-to-end diarization methods can handle overlapping speech by treating the problem as multi-label classification. Although some methods can treat a flexible number of speakers, they do not perform well when the number of speakers is large. To compensate for each other's weakness, we propose to use a two-speaker end-to-end diarization method as post-processing of the results obtained by a clustering-based method. We iteratively select two speakers from the results and update the results of the two speakers to improve the overlapped region. Experimental results show that the proposed algorithm consistently improved the performance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II datasets.",cs.CL,NLP
Regularized Attentive Capsule Network for Overlapped Relation Extraction,"Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves significant improvements in relation extraction.",cs.CL,NLP
Should I visit this place? Inclusion and Exclusion Phrase Mining from Reviews,"Although several automatic itinerary generation services have made travel planning easy, often times travellers find themselves in unique situations where they cannot make the best out of their trip. Visitors differ in terms of many factors such as suffering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowledge this is the first work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classifier provides a binary overlap F1 of $\sim$80 and $\sim$82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classifier provides an F1 of $\sim$98 and $\sim$97 for 11-class inclusion and exclusion classification respectively. We believe that our work can significantly improve the quality of an automatic itinerary generation service.",cs.CL,NLP
Speech Synthesis as Augmentation for Low-Resource ASR,"Speech synthesis might hold the key to low-resource speech recognition. Data augmentation techniques have become an essential part of modern speech recognition training. Yet, they are simple, naive, and rarely reflect real-world conditions. Meanwhile, speech synthesis techniques have been rapidly getting closer to the goal of achieving human-like speech. In this paper, we investigate the possibility of using synthesized speech as a form of data augmentation to lower the resources necessary to build a speech recognizer. We experiment with three different kinds of synthesizers: statistical parametric, neural, and adversarial. Our findings are interesting and point to new research directions for the future.",cs.CL,NLP
QUACKIE: A NLP Classification Task With Ground Truth Explanations,"NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP Interpretability, but their dependence on human provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a specific classification task by diverting question-answering datasets. For this custom classification task, the interpretability ground-truth arises directly from the definition of the classification problem. We use this method to propose a benchmark and lay the groundwork for future research in NLP interpretability by evaluating a wide range of current state of the art methods.",cs.CL,NLP
"I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling","To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradiction detection with the typical unstructured approach. Results reveal that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) the structured utterance-based approach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.",cs.CL,NLP
Panarchy: ripples of a boundary concept,"How do social-ecological systems change over time? In 2002 Holling and colleagues proposed the concept of Panarchy, which presented social-ecological systems as an interacting set of adaptive cycles, each of which is produced by the dynamic tensions between novelty and efficiency at multiple scales. Initially introduced as a conceptual framework and set of metaphors, panarchy has gained the attention of scholars across many disciplines and its ideas continue to inspire further conceptual developments. Almost twenty years after this concept was introduced we review how it has been used, tested, extended and revised. We do this by combining qualitative methods and machine learning. Document analysis was used to code panarchy features that are commonly used in the scientific literature (N = 42), a qualitative analysis that was complemented with topic modeling of 2177 documents. We find that the adaptive cycle is the feature of panarchy that has attracted the most attention. Challenges remain in empirically grounding the metaphor, but recent theoretical and empirical work offers some avenues for future research.",cs.CL,NLP
My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism,"Significant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. However, little research has been put to understand and interpret the black-box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capability raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi-modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretability to find the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We find that the systems tested consider essays not as a piece of prose having the characteristics of natural flow of speech and grammatical structure, but as `word-soups' where a few words are much more important than the other words. Removing the context surrounding those few important words causes the prose to lose the flow of speech and grammar, however has little impact on the predicted score. We also find that since the models are not semantically grounded with world-knowledge and common sense, adding false facts such as ``the world is flat'' actually increases the score instead of decreasing it.",cs.CL,NLP
Measuring University Impact: Wikipedia approach,"The impact of Universities on the social, economic and political landscape is one of the key directions in contemporary educational evaluation. In this paper, we discuss the new methodological technique that evaluates the impact of university based on popularity (number of page-views) of their alumni's pages on Wikipedia. It allows revealing the alumni popularity dynamics and tracking its state. Preliminary analysis shows that the number of page-views is higher for the contemporary persons that prove the perspectives of this approach. Then, universities were ranked based on the methodology and compared to the famous international university rankings ARWU and QS based only on alumni scales: for the top 10 universities, there is an intersection of two universities (Columbia University, Stanford University). The correlation coefficients between different university rankings are provided in the paper. Finally, the ranking based on the alumni popularity was compared with the ranking of universities based on the popularity of their webpages on Wikipedia: there is a strong connection between these indicators.",cs.CL,NLP
Neural document expansion for ad-hoc information retrieval,"Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing significant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be effectively adapted to standard IR tasks, where labels are scarce and many long documents are present.",cs.CL,NLP
Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation,"Spoken question answering (SQA) is challenging due to complex reasoning on top of the spoken documents. The recent studies have also shown the catastrophic impact of automatic speech recognition (ASR) errors on SQA. Therefore, this work proposes to mitigate the ASR errors by aligning the mismatch between ASR hypotheses and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively utilize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score.",cs.CL,NLP
Query Expansion for Cross-Language Question Re-Ranking,"Community question-answering (CQA) platforms have become very popular forums for asking and answering questions daily. While these forums are rich repositories of community knowledge, they present challenges for finding relevant answers and similar questions, due to the open-ended nature of informal discussions. Further, if the platform allows questions and answers in multiple languages, we are faced with the additional challenge of matching cross-lingual information. In this work, we focus on the cross-language question re-ranking shared task, which aims to find existing questions that may be written in different languages. Our contribution is an exploration of query expansion techniques for this problem. We investigate expansions based on Word Embeddings, DBpedia concepts linking, and Hypernym, and show that they outperform existing state-of-the-art methods.",cs.CL,NLP
Posterior-regularized REINFORCE for Instance Selection in Distant Supervision,"This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.",cs.CL,NLP
End-to-End Speech Translation with Knowledge Distillation,"End-to-end speech translation (ST), which directly translates from source language speech into target language text, has attracted intensive attentions in recent years. Compared to conventional pipeline systems, end-to-end ST models have advantages of lower latency, smaller model size and less error propagation. However, the combination of speech recognition and text translation in one model is more difficult than each of these two tasks. In this paper, we propose a knowledge distillation approach to improve ST model by transferring the knowledge from text translation model. Specifically, we first train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation. Experiments on English- French Augmented LibriSpeech and English-Chinese TED corpus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the instruction of teacher model, end-to-end ST model can gain significant improvements by over 3.5 BLEU points.",cs.CL,NLP
FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance,"Frequently Asked Question (FAQ) retrieval is an important task where the objective is to retrieve an appropriate Question-Answer (QA) pair from a database based on a user's query. We propose a FAQ retrieval system that considers the similarity between a user's query and a question as well as the relevance between the query and an answer. Although a common approach to FAQ retrieval is to construct labeled data for training, it takes annotation costs. Therefore, we use a traditional unsupervised information retrieval system to calculate the similarity between the query and question. On the other hand, the relevance between the query and answer can be learned by using QA pairs in a FAQ database. The recently-proposed BERT model is used for the relevance calculation. Since the number of QA pairs in FAQ page is not enough to train a model, we cope with this issue by leveraging FAQ sets that are similar to the one in question. We evaluate our approach on two datasets. The first one is localgovFAQ, a dataset we construct in a Japanese administrative municipality domain. The second is StackExchange dataset, which is the public dataset in English. We demonstrate that our proposed method outperforms baseline methods on these datasets.",cs.CL,NLP
Automatic Inference of Minimalist Grammars using an SMT-Solver,"We introduce (1) a novel parser for Minimalist Grammars (MG), encoded as a system of first-order logic formulae that may be evaluated using an SMT-solver, and (2) a novel procedure for inferring Minimalist Grammars using this parser. The input to this procedure is a sequence of sentences that have been annotated with syntactic relations such as semantic role labels (connecting arguments to predicates) and subject-verb agreement. The output of this procedure is a set of minimalist grammars, each of which is able to parse the sentences in the input sequence such that the parse for a sentence has the same syntactic relations as those specified in the annotation for that sentence. We applied this procedure to a set of sentences annotated with syntactic relations and evaluated the inferred grammars using cost functions inspired by the Minimum Description Length principle and the Subset principle. Inferred grammars that were optimal with respect to certain combinations of these cost functions were found to align with contemporary theories of syntax.",cs.CL,NLP
ShapeGlot: Learning Language for Shape Differentiation,"In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We first build a large scale, carefully controlled dataset of human utterances that each refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",cs.CL,NLP
Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy,"Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of different coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classification are unscalable and cannot accommodate large scale EHR datasets.
  In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). The system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: first, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and finally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We confirmed the robustness and generalisation properties of Text2Node by mapping ICD-9-CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy.
  This system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in healthcare. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models.",cs.CL,NLP
Who wrote this book? A challenge for e-commerce,"Modern e-commerce catalogs contain millions of references, associated with textual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular significance is the book category, where the author name(s) field poses a significant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different authors' names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author's name using sequence-to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and find that the top proposal of the system is the normalized author name with 72% accuracy.",cs.CL,NLP
AI-Powered Text Generation for Harmonious Human-Machine Interaction: Current State and Future Directions,"In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation.",cs.CL,NLP
A Persona-based Multi-turn Conversation Model in an Adversarial Learning Framework,"In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN ($phredGAN$) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n-gram scores.",cs.CL,NLP
Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System,"Applying neural-networks on Question Answering has gained increasing popularity in recent years. In this paper, I implemented a model with Bi-directional attention flow layer, connected with a Multi-layer LSTM encoder, connected with one start-index decoder and one conditioning end-index decoder. I introduce a new end-index decoder layer, conditioning on start-index output. The Experiment shows this has increased model performance by 15.16%. For prediction, I proposed a new smart-span equation, rewarding both short answer length and high probability in start-index and end-index, which further improved the prediction accuracy. The best single model achieves an F1 score of 73.97% and EM score of 64.95% on test set.",cs.CL,NLP
Towards Recognizing Phrase Translation Processes: Experiments on English-French,"When translating phrases (words or group of words), human translators, consciously or not, resort to different translation processes apart from the literal translation, such as Idiom Equivalence, Generalization, Particularization, Semantic Modulation, etc. Translators and linguists (such as Vinay and Darbelnet, Newmark, etc.) have proposed several typologies to characterize the different translation processes. However, to the best of our knowledge, there has not been effort to automatically classify these fine-grained translation processes. Recently, an English-French parallel corpus of TED Talks has been manually annotated with translation process categories, along with established annotation guidelines. Based on these annotated examples, we propose an automatic classification of translation processes at subsentential level. Experimental results show that we can distinguish non-literal translation from literal translation with an accuracy of 87.09%, and 55.20% for classifying among five non-literal translation processes. This work demonstrates that it is possible to automatically classify translation processes. Even with a small amount of annotated examples, our experiments show the directions that we can follow in future work. One of our long term objectives is leveraging this automatic classification to better control paraphrase extraction from bilingual parallel corpora.",cs.CL,NLP
"Short Text Topic Modeling Techniques, Applications, and Performance: A Survey","Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.",cs.CL,NLP
Contextual Aware Joint Probability Model Towards Question Answering System,"In this paper, we address the question answering challenge with the SQuAD 2.0 dataset. We design a model architecture which leverages BERT's capability of context-aware word embeddings and BiDAF's context interactive exploration mechanism. By integrating these two state-of-the-art architectures, our system tries to extract the contextual word representation at word and character levels, for better comprehension of both question and context and their correlations. We also propose our original joint posterior probability predictor module and its associated loss functions. Our best model so far obtains F1 score of 75.842% and EM score of 72.24% on the test PCE leaderboad.",cs.CL,NLP
Time-series Insights into the Process of Passing or Failing Online University Courses using Neural-Induced Interpretable Student States,"This paper addresses a key challenge in Educational Data Mining, namely to model student behavioral trajectories in order to provide a means for identifying students most at-risk, with the goal of providing supportive interventions. While many forms of data including clickstream data or data from sensors have been used extensively in time series models for such purposes, in this paper we explore the use of textual data, which is sometimes available in the records of students at large, online universities. We propose a time series model that constructs an evolving student state representation using both clickstream data and a signal extracted from the textual notes recorded by human mentors assigned to each student. We explore how the addition of this textual data improves both the predictive power of student states for the purpose of identifying students at risk for course failure as well as for providing interpretable insights about student course engagement processes.",cs.CL,NLP
Unsupervised Data Augmentation for Consistency Training,"Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",cs.CL,NLP
Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.",cs.CL,NLP
Multi-task Pairwise Neural Ranking for Hashtag Segmentation,"Hashtags are often employed on social media and beyond to add metadata to a textual utterance with the goal of increasing discoverability, aiding search, or providing additional semantics. However, the semantic content of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined together and can include abbreviations and unorthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hashtag segmentation by framing it as a pairwise ranking problem between candidate segmentations. Our novel neural approaches demonstrate 24.6% error reduction in hashtag segmentation accuracy compared to the current state-of-the-art method. Finally, we demonstrate that a deeper understanding of hashtag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the SemEval 2017 sentiment analysis dataset.",cs.CL,NLP
From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots,"The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behavior, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image-pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach significantly outperforms other state-of-the-art methods.",cs.CL,NLP
A Surprising Density of Illusionable Natural Speech,"Recent work on adversarial examples has demonstrated that most natural inputs can be perturbed to fool even state-of-the-art machine learning systems. But does this happen for humans as well? In this work, we investigate: what fraction of natural instances of speech can be turned into ""illusions"" which either alter humans' perception or result in different people having significantly different perceptions? We first consider the McGurk effect, the phenomenon by which adding a carefully chosen video clip to the audio channel affects the viewer's perception of what is said (McGurk and MacDonald, 1976). We obtain empirical estimates that a significant fraction of both words and sentences occurring in natural speech have some susceptibility to this effect. We also learn models for predicting McGurk illusionability. Finally we demonstrate that the Yanny or Laurel auditory illusion (Pressnitzer et al., 2018) is not an isolated occurrence by generating several very different new instances. We believe that the surprising density of illusionable natural speech warrants further investigation, from the perspectives of both security and cognitive science. Supplementary videos are available at: https://www.youtube.com/playlist?list=PLaX7t1K-e_fF2iaenoKznCatm0RC37B_k.",cs.CL,NLP
A Review of Automated Speech and Language Features for Assessment of Cognitive and Thought Disorders,"It is widely accepted that information derived from analyzing speech (the acoustic signal) and language production (words and sentences) serves as a useful window into the health of an individual's cognitive ability. In fact, most neuropsychological testing batteries have a component related to speech and language where clinicians elicit speech from patients for subjective evaluation across a broad set of dimensions. With advances in speech signal processing and natural language processing, there has been recent interest in developing tools to detect more subtle changes in cognitive-linguistic function. This work relies on extracting a set of features from recorded and transcribed speech for objective assessments of speech and language, early diagnosis of neurological disease, and tracking of disease after diagnosis. With an emphasis on cognitive and thought disorders, in this paper we provide a review of existing speech and language features used in this domain, discuss their clinical application, and highlight their advantages and disadvantages. Broadly speaking, the review is split into two categories: language features based on natural language processing and speech features based on speech signal processing. Within each category, we consider features that aim to measure complementary dimensions of cognitive-linguistics, including language diversity, syntactic complexity, semantic coherence, and timing. We conclude the review with a proposal of new research directions to further advance the field.",cs.CL,NLP
Resolving Gendered Ambiguous Pronouns with BERT,"Pronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a necessary component of machine translation systems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP researchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun resolution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team.",cs.CL,NLP
Joint Effects of Context and User History for Predicting Online Conversation Re-entries,"As the online world continues its exponential growth, interpersonal communication has come to play an increasingly central role in opinion formation and change. In order to help users better engage with each other online, we study a challenging problem of re-entry prediction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users' previous chatting history will affect their continued interests in future engagement. Specifically, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting history jointly result in their re-entry behavior. We experiment with two large-scale datasets collected from Twitter and Reddit. Results show that our proposed framework with bi-attention achieves an F1 score of 61.1 on Twitter conversations, outperforming the state-of-the-art methods from previous work.",cs.CL,NLP
Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs,"The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial information, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several recent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embeddings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighborhood surrounding a triple. To this effect, our paper proposes a novel attention based feature embedding that captures both entity and relation features in any given entity's neighborhood. Additionally, we also encapsulate relation clusters and multihop relations in our model. Our empirical study offers insights into the efficacy of our attention based model and we show marked performance gains in comparison to state of the art methods on all datasets.",cs.CL,NLP
Learning to Explain: Answering Why-Questions via Rephrasing,"Providing plausible responses to why questions is a challenging but critical goal for language based human-machine interaction. Explanations are challenging in that they require many different forms of abstract knowledge and reasoning. Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations. They are also often limited to ranking pre-existing explanation choices. In our work, we contribute to the under-explored area of generating natural language explanations for general phenomena. We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to generate natural language explanations. We compare different training strategies and evaluate their performance using both automatic scores and human ratings. We demonstrate that our strategy is sufficient to generate highly plausible explanations for general open-domain phenomena compared to other models trained on different datasets.",cs.CL,NLP
Exploiting Sentential Context for Neural Machine Translation,"In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Specifically, we first show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all the internal layers of the encoder to form a more comprehensive context representation. Experimental results on the WMT14 English-to-German and English-to-French benchmarks show that our model consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT.",cs.CL,NLP
Lattice-Based Transformer Encoder for Neural Machine Translation,"Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word-level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder.",cs.CL,NLP
NNE: A Dataset for Nested Named Entity Recognition in English Newswire,"Named entity recognition (NER) is widely used in natural language processing applications and downstream tasks. However, most NER tools target flat annotation from popular datasets, eschewing the semantic information available in nested entity mentions. We describe NNE---a fine-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our annotation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER.",cs.CL,NLP
Self-Attentional Models for Lattice Inputs,"Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference.",cs.CL,NLP
An Introduction to a New Text Classification and Visualization for Natural Language Processing Using Topological Data Analysis,"Topological Data Analysis (TDA) is a novel new and fast growing field of data science providing a set of new topological and geometric tools to derive relevant features out of complex high-dimensional data. In this paper we apply two of best methods in topological data analysis, ""Persistent Homology"" and ""Mapper"", in order to classify persian poems which has been composed by two of the best Iranian poets namely ""Ferdowsi"" and ""Hafez"". This article has two main parts, in the first part we explain the mathematics behind these two methods which is easy to understand for general audience and in the second part we describe our models and the results of applying TDA tools to NLP.",cs.CL,NLP
Listening while Speaking and Visualizing: Improving ASR through Multimodal Chain,"Previously, a machine speech chain, which is based on sequence-to-sequence deep learning, was proposed to mimic speech perception and production behavior. Such chains separately processed listening and speaking by automatic speech recognition (ASR) and text-to-speech synthesis (TTS) and simultaneously enabled them to teach each other in semi-supervised learning when they received unpaired data. Unfortunately, this speech chain study is limited to speech and textual modalities. In fact, natural communication is actually multimodal and involves both auditory and visual sensory systems. Although the said speech chain reduces the requirement of having a full amount of paired data, in this case we still need a large amount of unpaired data. In this research, we take a further step and construct a multimodal chain and design a closely knit chain architecture that combines ASR, TTS, image captioning, and image production models into a single framework. The framework allows the training of each component without requiring a large number of parallel multimodal data. Our experimental results also show that an ASR can be further trained without speech and text data and cross-modal data augmentation remains possible through our proposed chain, which improves the ASR performance.",cs.CL,NLP
Hierarchical Decision Making by Generating and Following Natural Language Instructions,"We explore using latent natural language instructions as an expressive and compositional representation of complex actions for hierarchical decision making. Rather than directly selecting micro-actions, our agent first generates a latent plan in natural language, which is then executed by a separate model. We introduce a challenging real-time strategy game environment in which the actions of a large number of units must be coordinated across long time scales. We gather a dataset of 76 thousand pairs of instructions and executions from human play, and train instructor and executor models. Experiments show that models using natural language as a latent variable significantly outperform models that directly imitate human actions. The compositional structure of language proves crucial to its effectiveness for action representation. We also release our code, models and data.",cs.CL,NLP
Evaluating Discourse in Structured Text Representations,"Discourse structure is integral to understanding a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expensive labeled discourse data. Liu and Lapata (2018) propose a structured attention mechanism for text classification that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evaluate on additional discourse-relevant tasks and datasets, in order to assess whether the structured attention improves performance on the end task and whether it captures a text's discourse structure. We find the learned latent trees have little to no structure and instead focus on lexical cues; even after obtaining more structured trees with proposed model modifications, the trees are still far from capturing discourse structure when compared to discourse dependency trees from an existing discourse parser. Finally, ablation studies show the structured attention provides little benefit, sometimes even hurting performance.",cs.CL,NLP
A Study of Feature Extraction techniques for Sentiment Analysis,"Sentiment Analysis refers to the study of systematically extracting the meaning of subjective text . When analysing sentiments from the subjective text using Machine Learning techniques,feature extraction becomes a significant part. We perform a study on the performance of feature extraction techniques TF-IDF(Term Frequency-Inverse Document Frequency) and Doc2vec (Document to Vector) using Cornell movie review datasets, UCI sentiment labeled datasets, stanford movie review datasets,effectively classifying the text into positive and negative polarities by using various pre-processing methods like eliminating StopWords and Tokenization which increases the performance of sentiment analysis in terms of accuracy and time taken by the classifier.The features obtained after applying feature extraction techniques on the text sentences are trained and tested using the classifiers Logistic Regression,Support Vector Machines,K-Nearest Neighbours , Decision Tree and Bernoulli Nave Bayes",cs.CL,NLP
Thirty Musts for Meaning Banking,"Meaning banking--creating a semantically annotated corpus for the purpose of semantic parsing or generation--is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper's format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4-11), and design of meaning representations (Section 12-30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future.",cs.CL,NLP
Detection of Bangla Fake News using MNB and SVM Classifier,"Fake news has been coming into sight in significant numbers for numerous business and political reasons and has become frequent in the online world. People can get contaminated easily by these fake news for its fabricated words which have enormous effects on the offline community. Thus, interest in research in this area has risen. Significant research has been conducted on the detection of fake news from English texts and other languages but a few in Bangla Language. Our work reflects the experimental analysis on the detection of Bangla fake news from social media as this field still requires much focus. In this research work, we have used two supervised machine learning algorithms, Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to detect Bangla fake news with CountVectorizer and Term Frequency - Inverse Document Frequency Vectorizer as feature extraction. Our proposed framework detects fake news depending on the polarity of the corresponding article. Finally, our analysis shows SVM with the linear kernel with an accuracy of 96.64% outperform MNB with an accuracy of 93.32%.",cs.CL,NLP
SANA : Sentiment Analysis on Newspapers comments in Algeria,"It is very current in today life to seek for tracking the people opinion from their interaction with occurring events. A very common way to do that is comments in articles published in newspapers web sites dealing with contemporary events. Sentiment analysis or opinion mining is an emergent field who is the purpose is finding the behind phenomenon masked in opinionated texts. We are interested in our work by comments in Algerian newspaper websites. For this end, two corpora were used SANA and OCA. SANA corpus is created by collection of comments from three Algerian newspapers, and annotated by two Algerian Arabic native speakers, while OCA is a freely available corpus for sentiment analysis. For the classification we adopt Supports vector machines, naive Bayes and knearest neighbors. Obtained results are very promising and show the different effects of stemming in such domain, also knearest neighbors give important improvement comparing to other classifiers unlike similar works where SVM is the most dominant. From this study we observe the importance of dedicated resources and methods the newspaper comments sentiment analysis which we look forward in future works.",cs.CL,NLP
Data-driven Detection and Analysis of the Patterns of Creaky Voice,"This paper investigates the temporal excitation patterns of creaky voice. Creaky voice is a voice quality frequently used as a phrase-boundary marker, but also as a means of portraying attitude, affective states and even social status. Consequently, the automatic detection and modelling of creaky voice may have implications for speech technology applications. The acoustic characteristics of creaky voice are, however, rather distinct from modal phonation. Further, several acoustic patterns can bring about the perception of creaky voice, thereby complicating the strategies used for its automatic detection, analysis and modelling. The present study is carried out using a variety of languages, speakers, and on both read and conversational data and involves a mutual information-based assessment of the various acoustic features proposed in the literature for detecting creaky voice. These features are then exploited in classification experiments where we achieve an appreciable improvement in detection accuracy compared to the state of the art. Both experiments clearly highlight the presence of several creaky patterns. A subsequent qualitative and quantitative analysis of the identified patterns is provided, which reveals a considerable speaker-dependent variability in the usage of these creaky patterns. We also investigate how creaky voice detection systems perform across creaky patterns.",cs.CL,NLP
Benchmarking BioRelEx for Entity Tagging and Relation Extraction,"Extracting relationships and interactions between different biological entities is still an extremely challenging problem but has not received much attention as much as extraction in other generic domains. In addition to the lack of annotated data, low benchmarking is still a major reason for slow progress. In order to fill this gap, we compare multiple existing entity and relation extraction models over a recently introduced public dataset, BioRelEx of sentences annotated with biological entities and relations. Our straightforward benchmarking shows that span-based multi-task architectures like DYGIE show 4.9% and 6% absolute improvements in entity tagging and relation extraction respectively over the previous state-of-art and that incorporating domain-specific information like embeddings pre-trained over related domains boosts performance.",cs.CL,NLP
CNRL at SemEval-2020 Task 5: Modelling Causal Reasoning in Language with Multi-Head Self-Attention Weights based Counterfactual Detection,"In this paper, we describe an approach for modelling causal reasoning in natural language by detecting counterfactuals in text using multi-head self-attention weights. We use pre-trained transformer models to extract contextual embeddings and self-attention weights from the text. We show the use of convolutional layers to extract task-specific features from these self-attention weights. Further, we describe a fine-tuning approach with a common base model for knowledge sharing between the two closely related sub-tasks for counterfactual detection. We analyze and compare the performance of various transformer models in our experiments. Finally, we perform a qualitative analysis with the multi-head self-attention weights to interpret our models' dynamics.",cs.CL,NLP
Neural Unsupervised Domain Adaptation in NLP---A Survey,"Deep neural networks excel at learning from labeled data and achieve state-of-the-art resultson a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.",cs.CL,NLP
Stance Prediction for Contemporary Issues: Data and Experiments,"We investigate whether pre-trained bidirectional transformers with sentiment and emotion information improve stance detection in long discussions of contemporary issues. As a part of this work, we create a novel stance detection dataset covering 419 different controversial issues and their related pros and cons collected by procon.org in nonpartisan format. Experimental results show that a shallow recurrent neural network with sentiment or emotion information can reach competitive results compared to fine-tuned BERT with 20x fewer parameters. We also use a simple approach that explains which input phrases contribute to stance detection.",cs.CL,NLP
On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition,"Recently, there has been a strong push to transition from hybrid models to end-to-end (E2E) models for automatic speech recognition. Currently, there are three promising E2E methods: recurrent neural network transducer (RNN-T), RNN attention-based encoder-decoder (AED), and Transformer-AED. In this study, we conduct an empirical comparison of RNN-T, RNN-AED, and Transformer-AED models, in both non-streaming and streaming modes. We use 65 thousand hours of Microsoft anonymized training data to train these models. As E2E models are more data hungry, it is better to compare their effectiveness with large amount of training data. To the best of our knowledge, no such comprehensive study has been conducted yet. We show that although AED models are stronger than RNN-T in the non-streaming mode, RNN-T is very competitive in streaming mode if its encoder can be properly initialized. Among all three E2E models, transformer-AED achieved the best accuracy in both streaming and non-streaming mode. We show that both streaming RNN-T and transformer-AED models can obtain better accuracy than a highly-optimized hybrid model.",cs.CL,NLP
Efficient Deployment of Conversational Natural Language Interfaces over Databases,"Many users communicate with chatbots and AI assistants in order to help them with various tasks. A key component of the assistant is the ability to understand and answer a user's natural language questions for question-answering (QA). Because data can be usually stored in a structured manner, an essential step involves turning a natural language question into its corresponding query language. However, in order to train most natural language-to-query-language state-of-the-art models, a large amount of training data is needed first. In most domains, this data is not available and collecting such datasets for various domains can be tedious and time-consuming. In this work, we propose a novel method for accelerating the training dataset collection for developing the natural language-to-query-language machine learning models. Our system allows one to generate conversational multi-term data, where multiple turns define a dialogue session, enabling one to better utilize chatbot interfaces. We train two current state-of-the-art NL-to-QL models, on both an SQL and SPARQL-based datasets in order to showcase the adaptability and efficacy of our created data.",cs.CL,NLP
Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation,"Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end automatic speech recognition (ASR) for low-resource languages. Pre-trained or jointly trained encoder-decoder models, however, do not share the language modeling (decoder) for the same language, which is likely to be inefficient for distant target languages. We introduce speech-to-text translation (ST) as an auxiliary task to incorporate additional knowledge of the target language and enable transferring from that target language. Specifically, we first translate high-resource ASR transcripts into a target low-resource language, with which a ST model is trained. Both ST and target ASR share the same attention-based encoder-decoder architecture and vocabulary. The former task then provides a fully pre-trained model for the latter, bringing up to 24.6% word error rate (WER) reduction to the baseline (direct transfer from high-resource ASR). We show that training ST with human translations is not necessary. ST trained with machine translation (MT) pseudo-labels brings consistent gains. It can even outperform those using human labels when transferred to target ASR by leveraging only 500K MT examples. Even with pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer brings up to 8.9% WER reduction to direct transfer.",cs.CL,NLP
Unsupervised Paraphrase Generation using Pre-trained Language Models,"Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \cite{radford2019language} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation.",cs.CL,NLP
Human brain activity for machine attention,"Cognitively inspired NLP leverages human-derived data to teach machines about language processing mechanisms. Recently, neural networks have been augmented with behavioral data to solve a range of NLP tasks spanning syntax and semantics. We are the first to exploit neuroscientific data, namely electroencephalography (EEG), to inform a neural attention model about language processing of the human brain. The challenge in working with EEG data is that features are exceptionally rich and need extensive pre-processing to isolate signals specific to text processing. We devise a method for finding such EEG features to supervise machine attention through combining theoretically motivated cropping with random forest tree splits. After this dimensionality reduction, the pre-processed EEG features are capable of distinguishing two reading tasks retrieved from a publicly available EEG corpus. We apply these features to regularise attention on relation classification and show that EEG is more informative than strong baselines. This improvement depends on both the cognitive load of the task and the EEG frequency domain. Hence, informing neural attention models with EEG signals is beneficial but requires further investigation to understand which dimensions are the most useful across NLP tasks.",cs.CL,NLP
Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation,"Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process. However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing. To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is predicted token-by-token. By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors. Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4$\times$ speedup while maintaining comparable performance compared with the corresponding autoregressive model.",cs.CL,NLP
Input-independent Attention Weights Are Expressive Enough: A Study of Attention in Self-supervised Audio Transformers,"In this paper, we seek solutions for reducing the computation complexity of transformer-based models for speech representation learning. We evaluate 10 attention algorithms; then, we pre-train the transformer-based model with those attention algorithms in a self-supervised fashion and treat them as feature extractors on downstream tasks, including phoneme classification and speaker classification. With the assistance of t-SNE, PCA and some observation, the attention weights in self-supervised audio transformers can be categorized into four general cases. Based on these cases and some analyses, we are able to use a specific set of attention weights to initialize the model. Our approach shows comparable performance to the typical self-attention yet requires 20% less time in both training and inference.",cs.CL,NLP
CycleGT: Unsupervised Graph-to-Text and Text-to-Graph Generation via Cycle Training,"Two important tasks at the intersection of knowledge graphs and natural language processing are graph-to-text (G2T) and text-to-graph (T2G) conversion. Due to the difficulty and high cost of data collection, the supervised data available in the two fields are usually on the magnitude of tens of thousands, for example, 18K in the WebNLG~2017 dataset after preprocessing, which is far fewer than the millions of data for other tasks such as machine translation. Consequently, deep learning models for G2T and T2G suffer largely from scarce training data. We present CycleGT, an unsupervised training method that can bootstrap from fully non-parallel graph and text data, and iteratively back translate between the two forms. Experiments on WebNLG datasets show that our unsupervised model trained on the same number of data achieves performance on par with several fully supervised models. Further experiments on the non-parallel GenWiki dataset verify that our method performs the best among unsupervised baselines. This validates our framework as an effective approach to overcome the data scarcity problem in the fields of G2T and T2G. Our code is available at https://github.com/QipengGuo/CycleGT.",cs.CL,NLP
Analysis and Synthesis of Hypo and Hyperarticulated Speech,"This paper focuses on the analysis and synthesis of hypo and hyperarticulated speech in the framework of HMM-based speech synthesis. First of all, a new French database matching our needs was created, which contains three identical sets, pronounced with three different degrees of articulation: neutral, hypo and hyperarticulated speech. On that basis, acoustic and phonetic analyses were performed. It is shown that the degrees of articulation significantly influence, on one hand, both vocal tract and glottal characteristics, and on the other hand, speech rate, phone durations, phone variations and the presence of glottal stops. Finally, neutral, hypo and hyperarticulated speech are synthesized using HMM-based speech synthesis and both objective and subjective tests aiming at assessing the generated speech quality are performed. These tests show that synthesized hypoarticulated speech seems to be less naturally rendered than neutral and hyperarticulated speech.",cs.CL,NLP
Maximum Phase Modeling for Sparse Linear Prediction of Speech,"Linear prediction (LP) is an ubiquitous analysis method in speech processing. Various studies have focused on sparse LP algorithms by introducing sparsity constraints into the LP framework. Sparse LP has been shown to be effective in several issues related to speech modeling and coding. However, all existing approaches assume the speech signal to be minimum-phase. Because speech is known to be mixed-phase, the resulting residual signal contains a persistent maximum-phase component. The aim of this paper is to propose a novel technique which incorporates a modeling of the maximum-phase contribution of speech, and can be applied to any filter representation. The proposed method is shown to significantly increase the sparsity of the LP residual signal and to be effective in two illustrative applications: speech polarity detection and excitation modeling.",cs.CL,NLP
The Discussion Tracker Corpus of Collaborative Argumentation,"Although Natural Language Processing (NLP) research on argument mining has advanced considerably in recent years, most studies draw on corpora of asynchronous and written texts, often produced by individuals. Few published corpora of synchronous, multi-party argumentation are available. The Discussion Tracker corpus, collected in American high school English classes, is an annotated dataset of transcripts of spoken, multi-party argumentation. The corpus consists of 29 multi-party discussions of English literature transcribed from 985 minutes of audio. The transcripts were annotated for three dimensions of collaborative argumentation: argument moves (claims, evidence, and explanations), specificity (low, medium, high) and collaboration (e.g., extensions of and disagreements about others' ideas). In addition to providing descriptive statistics on the corpus, we provide performance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research.",cs.CL,NLP
Open-Retrieval Conversational Question Answering,"Conversational search is one of the ultimate goals of information retrieval. Recent research approaches conversational search by simplified settings of response ranking and conversational question answering, where an answer is either selected from a given candidate set or extracted from a given passage. These simplifications neglect the fundamental role of retrieval in conversational search. To address this limitation, we introduce an open-retrieval conversational question answering (ORConvQA) setting, where we learn to retrieve evidence from a large collection before extracting answers, as a further step towards building functional conversational search systems. We create a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an end-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader that are all based on Transformers. Our extensive experiments on OR-QuAC demonstrate that a learnable retriever is crucial for ORConvQA. We further show that our system can make a substantial improvement when we enable history modeling in all system components. Moreover, we show that the reranker component contributes to the model performance by providing a regularization effect. Finally, further in-depth analyses are performed to provide new insights into ORConvQA.",cs.CL,NLP
GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning,"A chatbot that converses like a human should be goal-oriented (i.e., be purposeful in conversation), which is beyond language generation. However, existing dialogue systems often heavily rely on cumbersome hand-crafted rules or costly labelled datasets to reach the goals. In this paper, we propose Goal-oriented Chatbots (GoChat), a framework for end-to-end training chatbots to maximize the longterm return from offline multi-turn dialogue datasets. Our framework utilizes hierarchical reinforcement learning (HRL), where the high-level policy guides the conversation towards the final goal by determining some sub-goals, and the low-level policy fulfills the sub-goals by generating the corresponding utterance for response. In our experiments on a real-world dialogue dataset for anti-fraud in financial, our approach outperforms previous methods on both the quality of response generation as well as the success rate of accomplishing the goal.",cs.CL,NLP
Acoustic Word Embedding System for Code-Switching Query-by-example Spoken Term Detection,"In this paper, we propose a deep convolutional neural network-based acoustic word embedding system on code-switching query by example spoken term detection. Different from previous configurations, we combine audio data in two languages for training instead of only using one single language. We transform the acoustic features of keyword templates and searching content to fixed-dimensional vectors and calculate the distances between keyword segments and searching content segments obtained in a sliding manner. An auxiliary variability-invariant loss is also applied to training data within the same word but different speakers. This strategy is used to prevent the extractor from encoding undesired speaker- or accent-related information into the acoustic word embeddings. Experimental results show that our proposed system produces promising searching results in the code-switching test scenario. With the increased number of templates and the employment of variability-invariant loss, the searching performance is further enhanced.",cs.CL,NLP
Chat as Expected: Learning to Manipulate Black-box Neural Dialogue Models,"Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.",cs.CL,NLP
Contextual Dialogue Act Classification for Open-Domain Conversational Agents,"Classifying the general intent of the user utterance in a conversation, also known as Dialogue Act (DA), e.g., open-ended question, statement of opinion, or request for an opinion, is a key step in Natural Language Understanding (NLU) for conversational agents. While DA classification has been extensively studied in human-human conversations, it has not been sufficiently explored for the emerging open-domain automated conversational agents. Moreover, despite significant advances in utterance-level DA classification, full understanding of dialogue utterances requires conversational context. Another challenge is the lack of available labeled data for open-domain human-machine conversations. To address these problems, we propose a novel method, CDAC (Contextual Dialogue Act Classifier), a simple yet effective deep learning approach for contextual dialogue act classification. Specifically, we use transfer learning to adapt models trained on human-human conversations to predict dialogue acts in human-machine dialogues. To investigate the effectiveness of our method, we train our model on the well-known Switchboard human-human dialogue dataset, and fine-tune it for predicting dialogue acts in human-machine conversation data, collected as part of the Amazon Alexa Prize 2018 competition. The results show that the CDAC model outperforms an utterance-level state of the art baseline by 8.0% on the Switchboard dataset, and is comparable to the latest reported state-of-the-art contextual DA classification results. Furthermore, our results show that fine-tuning the CDAC model on a small sample of manually labeled human-machine conversations allows CDAC to more accurately predict dialogue acts in real users' conversations, suggesting a promising direction for future improvements.",cs.CL,NLP
Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing,"One daunting problem for semantic parsing is the scarcity of annotation. Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance. The downstream naive semantic parser accepts the intermediate output and returns the target logical form. Furthermore, the entire training process is split into two phases: pre-training and cycle learning. Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model. Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.",cs.CL,NLP
Towards Large-Scale Data Mining for Data-Driven Analysis of Sign Languages,"Access to sign language data is far from adequate. We show that it is possible to collect the data from social networking services such as TikTok, Instagram, and YouTube by applying data filtering to enforce quality standards and by discovering patterns in the filtered data, making it easier to analyse and model. Using our data collection pipeline, we collect and examine the interpretation of songs in both the American Sign Language (ASL) and the Brazilian Sign Language (Libras). We explore their differences and similarities by looking at the co-dependence of the orientation and location phonological parameters",cs.CL,NLP
Event Arguments Extraction via Dilate Gated Convolutional Neural Network with Enhanced Local Features,"Event Extraction plays an important role in information-extraction to understand the world. Event extraction could be split into two subtasks: one is event trigger extraction, the other is event arguments extraction. However, the F-Score of event arguments extraction is much lower than that of event trigger extraction, i.e. in the most recent work, event trigger extraction achieves 80.7%, while event arguments extraction achieves only 58%. In pipelined structures, the difficulty of event arguments extraction lies in its lack of classification feature, and the much higher computation consumption. In this work, we proposed a novel Event Extraction approach based on multi-layer Dilate Gated Convolutional Neural Network (EE-DGCNN) which has fewer parameters. In addition, enhanced local information is incorporated into word features, to assign event arguments roles for triggers predicted by the first subtask. The numerical experiments demonstrated significant performance improvement beyond state-of-art event extraction approaches on real-world datasets. Further analysis of extraction procedure is presented, as well as experiments are conducted to analyze impact factors related to the performance improvement.",cs.CL,NLP
On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior,"Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context. However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading. Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora. We find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is (near-)linear. We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations, the better its psychometric predictive power. However, we find nontrivial differences across model architectures. For any given perplexity, deep Transformer models and n-gram models generally show superior psychometric predictive power over LSTM or structurally supervised neural models, especially for eye movement data. Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.",cs.CL,NLP
Automatic Text Summarization of COVID-19 Medical Research Articles using BERT and GPT-2,"With the COVID-19 pandemic, there is a growing urgency for medical community to keep up with the accelerating growth in the new coronavirus-related literature. As a result, the COVID-19 Open Research Dataset Challenge has released a corpus of scholarly articles and is calling for machine learning approaches to help bridging the gap between the researchers and the rapidly growing publications. Here, we take advantage of the recent advances in pre-trained NLP models, BERT and OpenAI GPT-2, to solve this challenge by performing text summarization on this dataset. We evaluate the results using ROUGE scores and visual inspection. Our model provides abstractive and comprehensive information based on keywords extracted from the original articles. Our work can help the the medical community, by providing succinct summaries of articles for which the abstract are not already available.",cs.CL,NLP
Cascaded Text Generation with Markov Transformers,"The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autoregressive model with sub-linear parallel time generation. Noting that conditional random fields with bounded context can be decoded in parallel, we propose an efficient cascaded decoding approach for generating high-quality output. To parameterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with specific autoregressive context cutoffs. This approach requires only a small modification from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on five machine translation datasets.",cs.CL,NLP
Hybrid Improved Document-level Embedding (HIDE),"In recent times, word embeddings are taking a significant role in sentiment analysis. As the generation of word embeddings needs huge corpora, many applications use pretrained embeddings. In spite of the success, word embeddings suffers from certain drawbacks such as it does not capture sentiment information of a word, contextual information in terms of parts of speech tags and domain-specific information. In this work we propose HIDE a Hybrid Improved Document level Embedding which incorporates domain information, parts of speech information and sentiment information into existing word embeddings such as GloVe and Word2Vec. It combine improved word embeddings into document level embeddings. Further, Latent Semantic Analysis (LSA) has been used to represent documents as a vectors. HIDE is generated, combining LSA and document level embeddings, which is computed from improved word embeddings. We test HIDE with six different datasets and shown considerable improvement over the accuracy of existing pretrained word vectors such as GloVe and Word2Vec. We further compare our work with two existing document level sentiment analysis approaches. HIDE performs better than existing systems.",cs.CL,NLP
Embeddings of Label Components for Sequence Labeling: A Case Study of Fine-grained Named Entity Recognition,"In general, the labels used in sequence labeling consist of different types of elements. For example, IOB-format entity labels, such as B-Person and I-Person, can be decomposed into span (B and I) and type information (Person). However, while most sequence labeling models do not consider such label components, the shared components across labels, such as Person, can be beneficial for label prediction. In this work, we propose to integrate label component information as embeddings into models. Through experiments on English and Japanese fine-grained named entity recognition, we demonstrate that the proposed method improves performance, especially for instances with low-frequency labels.",cs.CL,NLP
Training Multilingual Machine Translation by Alternately Freezing Language-Specific Encoders-Decoders,"We propose a modular architecture of language-specific encoder-decoders that constitutes a multilingual machine translation system that can be incrementally extended to new languages without the need for retraining the existing system when adding new languages. Differently from previous works, we simultaneously train $N$ languages in all translation directions by alternately freezing encoder or decoder modules, which indirectly forces the system to train in a common intermediate representation for all languages. Experimental results from multilingual machine translation show that we can successfully train this modular architecture improving on the initial languages while falling slightly behind when adding new languages or doing zero-shot translation. Additional comparison of the quality of sentence representation in the task of natural language inference shows that the alternately freezing training is also beneficial in this direction.",cs.CL,NLP
Learning Constraints for Structured Prediction Using Rectifier Networks,"Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions. Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy. However, designing good constraints often relies on domain expertise. In this paper, we study the problem of learning such constraints. We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables. Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.",cs.CL,NLP
The 'Letter' Distribution in the Chinese Language,"Corpus-based statistical analysis plays a significant role in linguistic research, and ample evidence has shown that different languages exhibit some common laws. Studies have found that letters in some alphabetic writing languages have strikingly similar statistical usage frequency distributions. Does this hold for Chinese, which employs ideogram writing? We obtained letter frequency data of some alphabetic writing languages and found the common law of the letter distributions. In addition, we collected Chinese literature corpora for different historical periods from the Tang Dynasty to the present, and we dismantled the Chinese written language into three kinds of basic particles: characters, strokes and constructive parts. The results of the statistical analysis showed that, in different historical periods, the intensity of the use of basic particles in Chinese writing varied, but the form of the distribution was consistent. In particular, the distributions of the Chinese constructive parts are certainly consistent with those alphabetic writing languages. This study provides new evidence of the consistency of human languages.",cs.CL,NLP
Computational linguistic assessment of textbook and online learning media by means of threshold concepts in business education,"Threshold concepts are key terms in domain-based knowledge acquisition. They are regarded as building blocks of the conceptual development of domain knowledge within particular learners. From a linguistic perspective, however, threshold concepts are instances of specialized vocabularies, exhibiting particular linguistic features. Threshold concepts are typically used in specialized texts such as textbooks -- that is, within a formal learning environment. However, they also occur in informal learning environments like newspapers. In this article, a first approach is taken to combine both lines into an overarching research program - that is, to provide a computational linguistic assessment of different resources, including in particular online resources, by means of threshold concepts. To this end, the distributive profiles of 63 threshold concepts from business education (which have been collected from threshold concept research) has been investigated in three kinds of (German) resources, namely textbooks, newspapers, and Wikipedia. Wikipedia is (one of) the largest and most widely used online resources. We looked at the threshold concepts' frequency distribution, their compound distribution, and their network structure within the three kind of resources. The two main findings can be summarized as follows: Firstly, the three kinds of resources can indeed be distinguished in terms of their threshold concepts' profiles. Secondly, Wikipedia definitely appears to be a formal learning resource.",cs.CL,NLP
DeText: A Deep Text Ranking Framework with BERT,"Ranking is the most important component in a search system. Mostsearch systems deal with large amounts of natural language data,hence an effective ranking system requires a deep understandingof text semantics. Recently, deep learning based natural languageprocessing (deep NLP) models have generated promising results onranking systems. BERT is one of the most successful models thatlearn contextual embedding, which has been applied to capturecomplex query-document relations for search ranking. However,this is generally done by exhaustively interacting each query wordwith each document word, which is inefficient for online servingin search product systems. In this paper, we investigate how tobuild an efficient BERT-based ranking model for industry use cases.The solution is further extended to a general ranking framework,DeText, that is open sourced and can be applied to various rankingproductions. Offline and online experiments of DeText on threereal-world search systems present significant improvement overstate-of-the-art approaches.",cs.CL,NLP
Data balancing for boosting performance of low-frequency classes in Spoken Language Understanding,"Despite the fact that data imbalance is becoming more and more common in real-world Spoken Language Understanding (SLU) applications, it has not been studied extensively in the literature. To the best of our knowledge, this paper presents the first systematic study on handling data imbalance for SLU. In particular, we discuss the application of existing data balancing techniques for SLU and propose a multi-task SLU model for intent classification and slot filling. Aiming to avoid over-fitting, in our model methods for data balancing are leveraged indirectly via an auxiliary task which makes use of a class-balanced batch generator and (possibly) synthetic data. Our results on a real-world dataset indicate that i) our proposed model can boost performance on low frequency intents significantly while avoiding a potential performance decrease on the head intents, ii) synthetic data are beneficial for bootstrapping new intents when realistic data are not available, but iii) once a certain amount of realistic data becomes available, using synthetic data in the auxiliary task only yields better performance than adding them to the primary task training data, and iv) in a joint training scenario, balancing the intent distribution individually improves not only intent classification but also slot filling performance.",cs.CL,NLP
An exploration of the encoding of grammatical gender in word embeddings,"The vector representation of words, known as word embeddings, has opened a new research approach in linguistic studies. These representations can capture different types of information about words. The grammatical gender of nouns is a typical classification of nouns based on their formal and semantic properties. The study of grammatical gender based on word embeddings can give insight into discussions on how grammatical genders are determined. In this study, we compare different sets of word embeddings according to the accuracy of a neural classifier determining the grammatical gender of nouns. It is found that there is an overlap in how grammatical gender is encoded in Swedish, Danish, and Dutch embeddings. Our experimental results on the contextualized embeddings pointed out that adding more contextual information to embeddings is detrimental to the classifier's performance. We also observed that removing morpho-syntactic features such as articles from the training corpora of embeddings decreases the classification performance dramatically, indicating a large portion of the information is encoded in the relationship between nouns and articles.",cs.CL,NLP
Improving End-to-End Speech-to-Intent Classification with Reptile,"End-to-end spoken language understanding (SLU) systems have many advantages over conventional pipeline systems, but collecting in-domain speech data to train an end-to-end system is costly and time consuming. One question arises from this: how to train an end-to-end SLU with limited amounts of data? Many researchers have explored approaches that make use of other related data resources, typically by pre-training parts of the model on high-resource speech recognition. In this paper, we suggest improving the generalization performance of SLU models with a non-standard learning algorithm, Reptile. Though Reptile was originally proposed for model-agnostic meta learning, we argue that it can also be used to directly learn a target task and result in better generalization than conventional gradient descent. In this work, we employ Reptile to the task of end-to-end spoken intent classification. Experiments on four datasets of different languages and domains show improvement of intent prediction accuracy, both when Reptile is used alone and used in addition to pre-training.",cs.CL,NLP
Data Weighted Training Strategies for Grammatical Error Correction,"Recent progress in the task of Grammatical Error Correction (GEC) has been driven by addressing data sparsity, both through new methods for generating large and noisy pretraining data and through the publication of small and higher-quality finetuning data in the BEA-2019 shared task. Building upon recent work in Neural Machine Translation (NMT), we make use of both kinds of data by deriving example-level scores on our large pretraining data based on a smaller, higher-quality dataset. In this work, we perform an empirical study to discover how to best incorporate delta-log-perplexity, a type of example scoring, into a training schedule for GEC. In doing so, we perform experiments that shed light on the function and applicability of delta-log-perplexity. Models trained on scored data achieve state-of-the-art results on common GEC test sets.",cs.CL,NLP
Peking Opera Synthesis via Duration Informed Attention Network,"Peking Opera has been the most dominant form of Chinese performing art since around 200 years ago. A Peking Opera singer usually exhibits a very strong personal style via introducing improvisation and expressiveness on stage which leads the actual rhythm and pitch contour to deviate significantly from the original music score. This inconsistency poses a great challenge in Peking Opera singing voice synthesis from a music score. In this work, we propose to deal with this issue and synthesize expressive Peking Opera singing from the music score based on the Duration Informed Attention Network (DurIAN) framework. To tackle the rhythm mismatch, Lagrange multiplier is used to find the optimal output phoneme duration sequence with the constraint of the given note duration from music score. As for the pitch contour mismatch, instead of directly inferring from music score, we adopt a pseudo music score generated from the real singing and feed it as input during training. The experiments demonstrate that with the proposed system we can synthesize Peking Opera singing voice with high-quality timbre, pitch and expressiveness.",cs.CL,NLP
A general solution to the preferential selection model,"We provide a general analytic solution to Herbert Simon's 1955 model for time-evolving novelty functions. This has far-reaching consequences: Simon's is a pre-cursor model for Barabasi's 1999 preferential attachment model for growing social networks, and our general abstraction of it more considers attachment to be a form of link selection. We show that any system which can be modeled as instances of types---i.e., occurrence data (frequencies)---can be generatively modeled (and simulated) from a distributional perspective with an exceptionally high-degree of accuracy.",cs.CL,NLP
DQI: A Guide to Benchmark Evaluation,"A `state of the art' model A surpasses humans in a benchmark B, but fails on similar benchmarks C, D, and E. What does B have that the other benchmarks do not? Recent research provides the answer: spurious bias. However, developing A to solve benchmarks B through E does not guarantee that it will solve future benchmarks. To progress towards a model that `truly learns' an underlying task, we need to quantify the differences between successive benchmarks, as opposed to existing binary and black-box approaches. We propose a novel approach to solve this underexplored task of quantifying benchmark quality by debuting a data quality metric: DQI.",cs.CL,NLP
LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for Multi-Granular Propaganda Span Identification,"In this paper we describe our submission for the task of Propaganda Span Identification in news articles. We introduce a BERT-BiLSTM based span-level propaganda classification model that identifies which token spans within the sentence are indicative of propaganda. The ""multi-granular"" model incorporates linguistic knowledge at various levels of text granularity, including word, sentence and document level syntactic, semantic and pragmatic affect features, which significantly improve model performance, compared to its language-agnostic variant. To facilitate better representation learning, we also collect a corpus of 10k news articles, and use it for fine-tuning the model. The final model is a majority-voting ensemble which learns different propaganda class boundaries by leveraging different subsets of incorporated knowledge and attains $4^{th}$ position on the test leaderboard. Our final model and code is released at https://github.com/sopu/PropagandaSemEval2020.",cs.CL,NLP
FireBERT: Hardening BERT-based classifiers against adversarial attack,"We present FireBERT, a set of three proof-of-concept NLP classifiers hardened against TextFooler-style word-perturbation by producing diverse alternatives to original samples. In one approach, we co-tune BERT against the training data and synthetic adversarial samples. In a second approach, we generate the synthetic samples at evaluation time through substitution of words and perturbation of embedding vectors. The diversified evaluation results are then combined by voting. A third approach replaces evaluation-time word substitution with perturbation of embedding vectors. We evaluate FireBERT for MNLI and IMDB Movie Review datasets, in the original and on adversarial examples generated by TextFooler. We also test whether TextFooler is less successful in creating new adversarial samples when manipulating FireBERT, compared to working on unhardened classifiers. We show that it is possible to improve the accuracy of BERT-based models in the face of adversarial attacks without significantly reducing the accuracy for regular benchmark samples. We present co-tuning with a synthetic data generator as a highly effective method to protect against 95% of pre-manufactured adversarial samples while maintaining 98% of original benchmark performance. We also demonstrate evaluation-time perturbation as a promising direction for further research, restoring accuracy up to 75% of benchmark performance for pre-made adversarials, and up to 65% (from a baseline of 75% orig. / 12% attack) under active attack by TextFooler.",cs.CL,NLP
Question Identification in Arabic Language Using Emotional Based Features,"With the growth of content on social media networks, enterprises and services providers have become interested in identifying the questions of their customers. Tracking these questions become very challenging with the growth of text that grows directly proportional to the increase of Arabic users thus making it very difficult to be tracked manually. By automatic identifying the questions seeking answers on the social media networks and defining their category, we can automatically answer them by finding an existing answer or even routing them to those responsible for answering those questions in the customer service. This will result in saving the time and the effort and enhancing the customer feedback and improving the business. In this paper, we have implemented a binary classifier to classify Arabic text to either question seeking answer or not. We have added emotional based features to the state of the art features. Experimental evaluation has done and showed that these emotional features have improved the accuracy of the classifier.",cs.CL,NLP
Neural PLDA Modeling for End-to-End Speaker Verification,"While deep learning models have made significant advances in supervised classification problems, the application of these models for out-of-set verification tasks like speaker recognition has been limited to deriving feature embeddings. The state-of-the-art x-vector PLDA based speaker verification systems use a generative model based on probabilistic linear discriminant analysis (PLDA) for computing the verification score. Recently, we had proposed a neural network approach for backend modeling in speaker verification called the neural PLDA (NPLDA) where the likelihood ratio score of the generative PLDA model is posed as a discriminative similarity function and the learnable parameters of the score function are optimized using a verification cost. In this paper, we extend this work to achieve joint optimization of the embedding neural network (x-vector network) with the NPLDA network in an end-to-end (E2E) fashion. This proposed end-to-end model is optimized directly from the acoustic features with a verification cost function and during testing, the model directly outputs the likelihood ratio score. With various experiments using the NIST speaker recognition evaluation (SRE) 2018 and 2019 datasets, we show that the proposed E2E model improves significantly over the x-vector PLDA baseline speaker verification system.",cs.CL,NLP
A Parallel Evaluation Data Set of Software Documentation with Document Structure Annotation,"This paper accompanies the software documentation data set for machine translation, a parallel evaluation data set of data originating from the SAP Help Portal, that we released to the machine translation community for research purposes. It offers the possibility to tune and evaluate machine translation systems in the domain of corporate software documentation and contributes to the availability of a wider range of evaluation scenarios. The data set comprises of the language pairs English to Hindi, Indonesian, Malay and Thai, and thus also increases the test coverage for the many low-resource language pairs. Unlike most evaluation data sets that consist of plain parallel text, the segments in this data set come with additional metadata that describes structural information of the document context. We provide insights into the origin and creation, the particularities and characteristics of the data set as well as machine translation results.",cs.CL,NLP
Paraphrase Generation as Zero-Shot Multilingual Translation: Disentangling Semantic Similarity from Lexical and Syntactic Diversity,"Recent work has shown that a multilingual neural machine translation (NMT) model can be used to judge how well a sentence paraphrases another sentence in the same language (Thompson and Post, 2020); however, attempting to generate paraphrases from such a model using standard beam search produces trivial copies or near copies. We introduce a simple paraphrase generation algorithm which discourages the production of n-grams that are present in the input. Our approach enables paraphrase generation in many languages from a single multilingual NMT model. Furthermore, the amount of lexical diversity between the input and output can be controlled at generation time. We conduct a human evaluation to compare our method to a paraphraser trained on the large English synthetic paraphrase database ParaBank 2 (Hu et al., 2019c) and find that our method produces paraphrases that better preserve meaning and are more gramatical, for the same level of lexical diversity. Additional smaller human assessments demonstrate our approach also works in two non-English languages.",cs.CL,NLP
Connecting Embeddings for Knowledge Graph Entity Typing,"Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion. In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge from KGs. Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference. Accordingly, we build two novel embedding models to realize the mechanisms. Afterward, a joint model with them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs. Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing. The source code and data of this paper can be obtained from: https://github.com/ Adam1679/ConnectE",cs.CL,NLP
problemConquero at SemEval-2020 Task 12: Transformer and Soft label-based approaches,"In this paper, we present various systems submitted by our team problemConquero for SemEval-2020 Shared Task 12 Multilingual Offensive Language Identification in Social Media. We participated in all the three sub-tasks of OffensEval-2020, and our final submissions during the evaluation phase included transformer-based approaches and a soft label-based approach. BERT based fine-tuned models were submitted for each language of sub-task A (offensive tweet identification). RoBERTa based fine-tuned model for sub-task B (automatic categorization of offense types) was submitted. We submitted two models for sub-task C (offense target identification), one using soft labels and the other using BERT based fine-tuned model. Our ranks for sub-task A were Greek-19 out of 37, Turkish-22 out of 46, Danish-26 out of 39, Arabic-39 out of 53, and English-20 out of 85. We achieved a rank of 28 out of 43 for sub-task B. Our best rank for sub-task C was 20 out of 39 using BERT based fine-tuned model.",cs.CL,NLP
Training with reduced precision of a support vector machine model for text classification,"This paper presents the impact of using quantization on the efficiency of multi-class text classification in the training process of a support vector machine (SVM). This work is focused on comparing the efficiency of SVM model trained using reduced precision with its original form. The main advantage of using quantization is decrease in computation time and in memory footprint on the dedicated hardware platform which supports low precision computation like GPU (16-bit) or FPGA (any bit-width). The paper presents the impact of a precision reduction of the SVM training process on text classification accuracy. The implementation of the CPU was performed using the OpenMP library. Additionally, the results of the implementation of the GPU using double, single and half precision are presented.",cs.CL,NLP
Towards an Automated SOAP Note: Classifying Utterances from Medical Conversations,"Summaries generated from medical conversations can improve recall and understanding of care plans for patients and reduce documentation burden for doctors. Recent advancements in automatic speech recognition (ASR) and natural language understanding (NLU) offer potential solutions to generate these summaries automatically, but rigorous quantitative baselines for benchmarking research in this domain are lacking. In this paper, we bridge this gap for two tasks: classifying utterances from medical conversations according to (i) the SOAP section and (ii) the speaker role. Both are fundamental building blocks along the path towards an end-to-end, automated SOAP note for medical conversations. We provide details on a dataset that contains human and ASR transcriptions of medical conversations and corresponding machine learning optimized SOAP notes. We then present a systematic analysis in which we adapt an existing deep learning architecture to the two aforementioned tasks. The results suggest that modelling context in a hierarchical manner, which captures both word and utterance level context, yields substantial improvements on both classification tasks. Additionally, we develop and analyze a modular method for adapting our model to ASR output.",cs.CL,NLP
Morphological Skip-Gram: Using morphological knowledge to improve word representation,"Natural language processing models have attracted much interest in the deep learning community. This branch of study is composed of some applications such as machine translation, sentiment analysis, named entity recognition, question and answer, and others. Word embeddings are continuous word representations, they are an essential module for those applications and are generally used as input word representation to the deep learning models. Word2Vec and GloVe are two popular methods to learn word embeddings. They achieve good word representations, however, they learn representations with limited information because they ignore the morphological information of the words and consider only one representation vector for each word. This approach implies that Word2Vec and GloVe are unaware of the word inner structure. To mitigate this problem, the FastText model represents each word as a bag of characters n-grams. Hence, each n-gram has a continuous vector representation, and the final word representation is the sum of its characters n-grams vectors. Nevertheless, the use of all n-grams character of a word is a poor approach since some n-grams have no semantic relation with their words and increase the amount of potentially useless information. This approach also increases the training phase time. In this work, we propose a new method for training word embeddings, and its goal is to replace the FastText bag of character n-grams for a bag of word morphemes through the morphological analysis of the word. Thus, words with similar context and morphemes are represented by vectors close to each other. To evaluate our new approach, we performed intrinsic evaluations considering 15 different tasks, and the results show a competitive performance compared to FastText.",cs.CL,NLP
Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding,"Mining a set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massive text corpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical Topic Mining, which takes a category tree described by category names only, and aims to mine a set of representative terms for each category from a text corpus to help a user comprehend his/her interested topics. We develop a novel joint tree and text embedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH, mines a high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchical text classification tasks.",cs.CL,NLP
CoVoST 2 and Massively Multilingual Speech-to-Text Translation,"Speech translation has recently become an increasingly popular topic of research, partly due to the development of benchmark datasets. Nevertheless, current datasets cover a limited number of languages. With the aim to foster research in massive multilingual speech translation and speech translation for low resource language pairs, we release CoVoST 2, a large-scale multilingual speech translation corpus covering translations from 21 languages into English and from English into 15 languages. This represents the largest open dataset available to date from total volume and language coverage perspective. Data sanity checks provide evidence about the quality of the data, which is released under CC0 license. We also provide extensive speech recognition, bilingual and multilingual machine translation and speech translation baselines with open-source implementation.",cs.CL,NLP
One-Shot Learning for Language Modelling,"Humans can infer a great deal about the meaning of a word, using the syntax and semantics of surrounding words even if it is their first time reading or hearing it. We can also generalise the learned concept of the word to new tasks. Despite great progress in achieving human-level performance in certain tasks (Silver et al., 2016), learning from one or few examples remains a key challenge in machine learning, and has not thoroughly been explored in Natural Language Processing (NLP).
  In this work we tackle the problem of oneshot learning for an NLP task by employing ideas from recent developments in machine learning: embeddings, attention mechanisms (softmax) and similarity measures (cosine, Euclidean, Poincare, and Minkowski). We adapt the framework suggested in matching networks (Vinyals et al., 2016), and explore the effectiveness of the aforementioned methods in one, two and three-shot learning problems on the task of predicting missing word explored in (Vinyals et al., 2016) by using the WikiText-2 dataset. Our work contributes in two ways: Our first contribution is that we explore the effectiveness of different distance metrics on k-shot learning, and show that there is no single best distance metric for k-shot learning, which challenges common belief. We found that the performance of a distance metric depends on the number of shots used during training. The second contribution of our work is that we establish a benchmark for one, two, and three-shot learning on a language task with a publicly available dataset that can be used to benchmark against in future research.",cs.CL,NLP
Check_square at CheckThat! 2020: Claim Detection in Social Media via Fusion of Transformer and Syntactic Features,"In this digital age of news consumption, a news reader has the ability to react, express and share opinions with others in a highly interactive and fast manner. As a consequence, fake news has made its way into our daily life because of very limited capacity to verify news on the Internet by large companies as well as individuals. In this paper, we focus on solving two problems which are part of the fact-checking ecosystem that can help to automate fact-checking of claims in an ever increasing stream of content on social media. For the first problem, claim check-worthiness prediction, we explore the fusion of syntactic features and deep transformer Bidirectional Encoder Representations from Transformers (BERT) embeddings, to classify check-worthiness of a tweet, i.e. whether it includes a claim or not. We conduct a detailed feature analysis and present our best performing models for English and Arabic tweets. For the second problem, claim retrieval, we explore the pre-trained embeddings from a Siamese network transformer model (sentence-transformers) specifically trained for semantic textual similarity, and perform KD-search to retrieve verified claims with respect to a query tweet.",cs.CL,NLP
Narrative Interpolation for Generating and Understanding Stories,"We propose a method for controlled narrative/story generation where we are able to guide the model to produce coherent narratives with user-specified target endings by interpolation: for example, we are told that Jim went hiking and at the end Jim needed to be rescued, and we want the model to incrementally generate steps along the way. The core of our method is an interpolation model based on GPT-2 which conditions on a previous sentence and a next sentence in a narrative and fills in the gap. Additionally, a reranker helps control for coherence of the generated text. With human evaluation, we show that ending-guided generation results in narratives which are coherent, faithful to the given ending guide, and require less manual effort on the part of the human guide writer than past approaches.",cs.CL,NLP
"SGG: Spinbot, Grammarly and GloVe based Fake News Detection","Recently, news consumption using online news portals has increased exponentially due to several reasons, such as low cost and easy accessibility. However, such online platforms inadvertently also become the cause of spreading false information across the web. They are being misused quite frequently as a medium to disseminate misinformation and hoaxes. Such malpractices call for a robust automatic fake news detection system that can keep us at bay from such misinformation and hoaxes. We propose a robust yet simple fake news detection system, leveraging the tools for paraphrasing, grammar-checking, and word-embedding. In this paper, we try to the potential of these tools in jointly unearthing the authenticity of a news article. Notably, we leverage Spinbot (for paraphrasing), Grammarly (for grammar-checking), and GloVe (for word-embedding) tools for this purpose. Using these tools, we were able to extract novel features that could yield state-of-the-art results on the Fake News AMT dataset and comparable results on Celebrity datasets when combined with some of the essential features. More importantly, the proposed method is found to be more robust empirically than the existing ones, as revealed in our cross-domain analysis and multi-domain analysis.",cs.CL,NLP
Very Deep Transformers for Neural Machine Translation,"We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformer-based models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-of-the-art benchmark results on WMT14 English-French (43.8 BLEU and 46.4 BLEU with back-translation) and WMT14 English-German (30.1 BLEU).The code and trained models will be publicly available at: https://github.com/namisan/exdeep-nmt.",cs.CL,NLP
IITK at the FinSim Task: Hypernym Detection in Financial Domain via Context-Free and Contextualized Word Embeddings,"In this paper, we present our approaches for the FinSim 2020 shared task on ""Learning Semantic Representations for the Financial Domain"". The goal of this task is to classify financial terms into the most relevant hypernym (or top-level) concept in an external ontology. We leverage both context-dependent and context-independent word embeddings in our analysis. Our systems deploy Word2vec embeddings trained from scratch on the corpus (Financial Prospectus in English) along with pre-trained BERT embeddings. We divide the test dataset into two subsets based on a domain rule. For one subset, we use unsupervised distance measures to classify the term. For the second subset, we use simple supervised classifiers like Naive Bayes, on top of the embeddings, to arrive at a final prediction. Finally, we combine both the results. Our system ranks 1st based on both the metrics, i.e., mean rank and accuracy.",cs.CL,NLP
Applying GPGPU to Recurrent Neural Network Language Model based Fast Network Search in the Real-Time LVCSR,"Recurrent Neural Network Language Models (RNNLMs) have started to be used in various fields of speech recognition due to their outstanding performance. However, the high computational complexity of RNNLMs has been a hurdle in applying the RNNLM to a real-time Large Vocabulary Continuous Speech Recognition (LVCSR). In order to accelerate the speed of RNNLM-based network searches during decoding, we apply the General Purpose Graphic Processing Units (GPGPUs). This paper proposes a novel method of applying GPGPUs to RNNLM-based graph traversals. We have achieved our goal by reducing redundant computations on CPUs and amount of transfer between GPGPUs and CPUs. The proposed approach was evaluated on both WSJ corpus and in-house data. Experiments shows that the proposed approach achieves the real-time speed in various circumstances while maintaining the Word Error Rate (WER) to be relatively 10% lower than that of n-gram models.",cs.CL,NLP
SBAT: Video Captioning with Sparse Boundary-Aware Transformer,"In this paper, we focus on the problem of applying the transformer structure to video captioning effectively. The vanilla transformer is proposed for uni-modal language generation task such as machine translation. However, video captioning is a multimodal learning problem, and the video features have much redundancy between different time steps. Based on these concerns, we propose a novel method called sparse boundary-aware transformer (SBAT) to reduce the redundancy in video representation. SBAT employs boundary-aware pooling operation for scores from multihead attention and selects diverse features from different scenarios. Also, SBAT includes a local correlation scheme to compensate for the local information loss brought by sparse operation. Based on SBAT, we further propose an aligned cross-modal encoding scheme to boost the multimodal interaction. Experimental results on two benchmark datasets show that SBAT outperforms the state-of-the-art methods under most of the metrics.",cs.CL,NLP
HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for Code-Mixed Texts,"Problems involving code-mixed language are often plagued by a lack of resources and an absence of materials to perform sophisticated transfer learning with. In this paper we describe our submission to the Sentimix Hindi-English task involving sentiment classification of code-mixed texts, and with an F1 score of 67.1%, we demonstrate that simple convolution and attention may well produce reasonable results.",cs.CL,NLP
The Annotation Guideline of LST20 Corpus,"This report presents the annotation guideline for LST20, a large-scale corpus with multiple layers of linguistic annotation for Thai language processing. Our guideline consists of five layers of linguistic annotation: word segmentation, POS tagging, named entities, clause boundaries, and sentence boundaries. The dataset complies to the CoNLL-2003-style format for ease of use. LST20 Corpus offers five layers of linguistic annotation as aforementioned. At a large scale, it consists of 3,164,864 words, 288,020 named entities, 248,962 clauses, and 74,180 sentences, while it is annotated with 16 distinct POS tags. All 3,745 documents are also annotated with 15 news genres. Regarding its sheer size, this dataset is considered large enough for developing joint neural models for NLP. With the existence of this publicly available corpus, Thai has become a linguistically rich language for the first time.",cs.CL,NLP
OCoR: An Overlapping-Aware Code Retriever,"Code retrieval helps developers reuse the code snippet in the open-source projects. Given a natural language description, code retrieval aims to search for the most relevant code among a set of code. Existing state-of-the-art approaches apply neural networks to code retrieval. However, these approaches still fail to capture an important feature: overlaps. The overlaps between different names used by different people indicate that two different names may be potentially related (e.g., ""message"" and ""msg""), and the overlaps between identifiers in code and words in natural language descriptions indicate that the code snippet and the description may potentially be related. To address these problems, we propose a novel neural architecture named OCoR, where we introduce two specifically-designed components to capture overlaps: the first embeds identifiers by character to capture the overlaps between identifiers, and the second introduces a novel overlap matrix to represent the degrees of overlaps between each natural language word and each identifier.
  The evaluation was conducted on two established datasets. The experimental results show that OCoR significantly outperforms the existing state-of-the-art approaches and achieves 13.1% to 22.3% improvements. Moreover, we also conducted several in-depth experiments to help understand the performance of different components in OCoR.",cs.CL,NLP
Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering,"There are many existing retrieval and question answering datasets. However, most of them either focus on ranked list evaluation or single-candidate question answering. This divide makes it challenging to properly evaluate approaches concerned with ranking documents and providing snippets or answers for a given query. In this work, we present FiRA: a novel dataset of Fine-Grained Relevance Annotations. We extend the ranked retrieval annotations of the Deep Learning track of TREC 2019 with passage and word level graded relevance annotations for all relevant documents. We use our newly created data to study the distribution of relevance in long documents, as well as the attention of annotators to specific positions of the text. As an example, we evaluate the recently introduced TKL document ranking model. We find that although TKL exhibits state-of-the-art retrieval results for long documents, it misses many relevant passages.",cs.CL,NLP
Prosody Learning Mechanism for Speech Synthesis System Without Text Length Limit,"Recent neural speech synthesis systems have gradually focused on the control of prosody to improve the quality of synthesized speech, but they rarely consider the variability of prosody and the correlation between prosody and semantics together. In this paper, a prosody learning mechanism is proposed to model the prosody of speech based on TTS system, where the prosody information of speech is extracted from the melspectrum by a prosody learner and combined with the phoneme sequence to reconstruct the mel-spectrum. Meanwhile, the sematic features of text from the pre-trained language model is introduced to improve the prosody prediction results. In addition, a novel self-attention structure, named as local attention, is proposed to lift this restriction of input text length, where the relative position information of the sequence is modeled by the relative position matrices so that the position encodings is no longer needed. Experiments on English and Mandarin show that speech with more satisfactory prosody has obtained in our model. Especially in Mandarin synthesis, our proposed model outperforms baseline model with a MOS gap of 0.08, and the overall naturalness of the synthesized speech has been significantly improved.",cs.CL,NLP
Exploration of Gender Differences in COVID-19 Discourse on Reddit,"Decades of research on differences in the language of men and women have established postulates about preferences in lexical, topical, and emotional expression between the two genders, along with their sociological underpinnings. Using a novel dataset of male and female linguistic productions collected from the Reddit discussion platform, we further confirm existing assumptions about gender-linked affective distinctions, and demonstrate that these distinctions are amplified in social media postings involving emotionally-charged discourse related to COVID-19. Our analysis also confirms considerable differences in topical preferences between male and female authors in spontaneous pandemic-related discussions.",cs.CL,NLP
Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems,"Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication for future work.",cs.CL,NLP
Probing Neural Dialog Models for Conversational Understanding,"The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog.",cs.CL,NLP
Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph,"Most of the existing text generative steganographic methods are based on coding the conditional probability distribution of each word during the generation process, and then selecting specific words according to the secret information, so as to achieve information hiding. Such methods have their limitations which may bring potential security risks. Firstly, with the increase of embedding rate, these models will choose words with lower conditional probability, which will reduce the quality of the generated steganographic texts; secondly, they can not control the semantic expression of the final generated steganographic text. This paper proposes a new text generative steganography method which is quietly different from the existing models. We use a Knowledge Graph (KG) to guide the generation of steganographic sentences. On the one hand, we hide the secret information by coding the path in the knowledge graph, but not the conditional probability of each generated word; on the other hand, we can control the semantic expression of the generated steganographic text to a certain extent. The experimental results show that the proposed model can guarantee both the quality of the generated text and its semantic expression, which is a supplement and improvement to the current text generation steganography.",cs.CL,NLP
EPIC30M: An Epidemics Corpus Of Over 30 Million Relevant Tweets,"Since the start of COVID-19, several relevant corpora from various sources are presented in the literature that contain millions of data points. While these corpora are valuable in supporting many analyses on this specific pandemic, researchers require additional benchmark corpora that contain other epidemics to facilitate cross-epidemic pattern recognition and trend analysis tasks. During our other efforts on COVID-19 related work, we discover very little disease related corpora in the literature that are sizable and rich enough to support such cross-epidemic analysis tasks. In this paper, we present EPIC30M, a large-scale epidemic corpus that contains 30 millions micro-blog posts, i.e., tweets crawled from Twitter, from year 2006 to 2020. EPIC30M contains a subset of 26.2 millions tweets related to three general diseases, namely Ebola, Cholera and Swine Flu, and another subset of 4.7 millions tweets of six global epidemic outbreaks, including 2009 H1N1 Swine Flu, 2010 Haiti Cholera, 2012 Middle-East Respiratory Syndrome (MERS), 2013 West African Ebola, 2016 Yemen Cholera and 2018 Kivu Ebola. Furthermore, we explore and discuss the properties of the corpus with statistics of key terms and hashtags and trends analysis for each subset. Finally, we demonstrate the value and impact that EPIC30M could create through a discussion of multiple use cases of cross-epidemic research topics that attract growing interest in recent years. These use cases span multiple research areas, such as epidemiological modeling, pattern recognition, natural language understanding and economical modeling.",cs.CL,NLP
ClarQ: A large-scale and diverse dataset for Clarification Question Generation,"Question answering and conversational systems are often baffled and need help clarifying certain ambiguities. However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions. In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange. The framework utilises a neural network based architecture for classifying clarification questions. It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall. We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering. The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange. We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.",cs.CL,NLP
Leveraging Multimodal Behavioral Analytics for Automated Job Interview Performance Assessment and Feedback,"Behavioral cues play a significant part in human communication and cognitive perception. In most professional domains, employee recruitment policies are framed such that both professional skills and personality traits are adequately assessed. Hiring interviews are structured to evaluate expansively a potential employee's suitability for the position - their professional qualifications, interpersonal skills, ability to perform in critical and stressful situations, in the presence of time and resource constraints, etc. Therefore, candidates need to be aware of their positive and negative attributes and be mindful of behavioral cues that might have adverse effects on their success. We propose a multimodal analytical framework that analyzes the candidate in an interview scenario and provides feedback for predefined labels such as engagement, speaking rate, eye contact, etc. We perform a comprehensive analysis that includes the interviewee's facial expressions, speech, and prosodic information, using the video, audio, and text transcripts obtained from the recorded interview. We use these multimodal data sources to construct a composite representation, which is used for training machine learning classifiers to predict the class labels. Such analysis is then used to provide constructive feedback to the interviewee for their behavioral cues and body language. Experimental validation showed that the proposed methodology achieved promising results.",cs.CL,NLP
Exploration of End-to-End ASR for OpenSTT -- Russian Open Speech-to-Text Dataset,"This paper presents an exploration of end-to-end automatic speech recognition systems (ASR) for the largest open-source Russian language data set -- OpenSTT. We evaluate different existing end-to-end approaches such as joint CTC/Attention, RNN-Transducer, and Transformer. All of them are compared with the strong hybrid ASR system based on LF-MMI TDNN-F acoustic model. For the three available validation sets (phone calls, YouTube, and books), our best end-to-end model achieves word error rate (WER) of 34.8%, 19.1%, and 18.1%, respectively. Under the same conditions, the hybridASR system demonstrates 33.5%, 20.9%, and 18.6% WER.",cs.CL,NLP
Data Augmentation for Training Dialog Models Robust to Speech Recognition Errors,"Speech-based virtual assistants, such as Amazon Alexa, Google assistant, and Apple Siri, typically convert users' audio signals to text data through automatic speech recognition (ASR) and feed the text to downstream dialog models for natural language understanding and response generation. The ASR output is error-prone; however, the downstream dialog models are often trained on error-free text data, making them sensitive to ASR errors during inference time. To bridge the gap and make dialog models more robust to ASR errors, we leverage an ASR error simulator to inject noise into the error-free text data, and subsequently train the dialog models with the augmented data. Compared to other approaches for handling ASR errors, such as using ASR lattice or end-to-end methods, our data augmentation approach does not require any modification to the ASR or downstream dialog models; our approach also does not introduce any additional latency during inference time. We perform extensive experiments on benchmark data and show that our approach improves the performance of downstream dialog models in the presence of ASR errors, and it is particularly effective in the low-resource situations where there are constraints on model size or the training data is scarce.",cs.CL,NLP
Position Masking for Language Models,Masked language modeling (MLM) pre-training models such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. This is an effective technique which has led to good results on all NLP benchmarks. We propose to expand upon this idea by masking the positions of some tokens along with the masked input token ids. We follow the same standard approach as BERT masking a percentage of the tokens positions and then predicting their original values using an additional fully connected classifier stage. This approach has shown good performance gains (.3\% improvement) for the SQUAD additional improvement in convergence times. For the Graphcore IPU the convergence of BERT Base with position masking requires only 50\% of the tokens from the original BERT paper.,cs.CL,NLP
Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network,"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other few-shot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word's similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model -- TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.",cs.CL,NLP
Advances of Transformer-Based Models for News Headline Generation,"Pretrained language models based on Transformer architecture are the reason for recent breakthroughs in many areas of NLP, including sentiment analysis, question answering, named entity recognition. Headline generation is a special kind of text summarization task. Models need to have strong natural language understanding that goes beyond the meaning of individual words and sentences and an ability to distinguish essential information to succeed in it. In this paper, we fine-tune two pretrained Transformer-based models (mBART and BertSumAbs) for that task and achieve new state-of-the-art results on the RIA and Lenta datasets of Russian news. BertSumAbs increases ROUGE on average by 2.9 and 2.0 points respectively over previous best score achieved by Phrase-Based Attentional Transformer and CopyNet.",cs.CL,NLP
Deep or Simple Models for Semantic Tagging? It Depends on your Data [Experiments],"Semantic tagging, which has extensive applications in text mining, predicts whether a given piece of text conveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.
  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ""simple models"" over datasets with varying characteristics. Specifically, we select three prevalent deep models (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and compare their performance on the semantic tagging task over 21 datasets. Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task.",cs.CL,NLP
Topic Modeling on User Stories using Word Mover's Distance,"Requirements elicitation has recently been complemented with crowd-based techniques, which continuously involve large, heterogeneous groups of users who express their feedback through a variety of media. Crowd-based elicitation has great potential for engaging with (potential) users early on but also results in large sets of raw and unstructured feedback. Consolidating and analyzing this feedback is a key challenge for turning it into sensible user requirements. In this paper, we focus on topic modeling as a means to identify topics within a large set of crowd-generated user stories and compare three approaches: (1) a traditional approach based on Latent Dirichlet Allocation, (2) a combination of word embeddings and principal component analysis, and (3) a combination of word embeddings and Word Mover's Distance. We evaluate the approaches on a publicly available set of 2,966 user stories written and categorized by crowd workers. We found that a combination of word embeddings and Word Mover's Distance is most promising. Depending on the word embeddings we use in our approaches, we manage to cluster the user stories in two ways: one that is closer to the original categorization and another that allows new insights into the dataset, e.g. to find potentially new categories. Unfortunately, no measure exists to rate the quality of our results objectively. Still, our findings provide a basis for future work towards analyzing crowd-sourced user stories.",cs.CL,NLP
SacreROUGE: An Open-Source Library for Using and Developing Summarization Evaluation Metrics,"We present SacreROUGE, an open-source library for using and developing summarization evaluation metrics. SacreROUGE removes many obstacles that researchers face when using or developing metrics: (1) The library provides Python wrappers around the official implementations of existing evaluation metrics so they share a common, easy-to-use interface; (2) it provides functionality to evaluate how well any metric implemented in the library correlates to human-annotated judgments, so no additional code needs to be written for a new evaluation metric; and (3) it includes scripts for loading datasets that contain human judgments so they can easily be used for evaluation. This work describes the design of the library, including the core Metric interface, the command-line API for evaluating summarization models and metrics, and the scripts to load and reformat publicly available datasets. The development of SacreROUGE is ongoing and open to contributions from the community.",cs.CL,NLP
GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines,"The lack of publicly accessible text corpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely distributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for German text, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.",cs.CL,NLP
Fine-grained Language Identification with Multilingual CapsNet Model,"Due to a drastic improvement in the quality of internet services worldwide, there is an explosion of multilingual content generation and consumption. This is especially prevalent in countries with large multilingual audience, who are increasingly consuming media outside their linguistic familiarity/preference. Hence, there is an increasing need for real-time and fine-grained content analysis services, including language identification, content transcription, and analysis. Accurate and fine-grained spoken language detection is an essential first step for all the subsequent content analysis algorithms. Current techniques in spoken language detection may lack on one of these fronts: accuracy, fine-grained detection, data requirements, manual effort in data collection \& pre-processing. Hence in this work, a real-time language detection approach to detect spoken language from 5 seconds' audio clips with an accuracy of 91.8\% is presented with exiguous data requirements and minimal pre-processing. Novel architectures for Capsule Networks is proposed which operates on spectrogram images of the provided audio snippets. We use previous approaches based on Recurrent Neural Networks and iVectors to present the results. Finally we show a ``Non-Class'' analysis to further stress on why CapsNet architecture works for LID task.",cs.CL,NLP
LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,"Machine reading is a fundamental task for testing the capability of natural language understanding, which is closely related to human cognition in many aspects. With the rising of deep learning techniques, algorithmic models rival human performances on simple QA, and thus increasingly challenging machine reading datasets have been proposed. Though various challenges such as evidence integration and commonsense knowledge have been integrated, one of the fundamental capabilities in human reading, namely logical reasoning, is not fully investigated. We build a comprehensive dataset, named LogiQA, which is sourced from expert-written questions for testing human Logical reasoning. It consists of 8,678 QA instances, covering multiple types of deductive reasoning. Results show that state-of-the-art neural models perform by far worse than human ceiling. Our dataset can also serve as a benchmark for reinvestigating logical AI under the deep learning NLP setting. The dataset is freely available at https://github.com/lgw863/LogiQA-dataset",cs.CL,NLP
UniTrans: Unifying Model Transfer and Data Transfer for Cross-Lingual Named Entity Recognition with Unlabeled Data,"Prior works in cross-lingual named entity recognition (NER) with no/little labeled data fall into two primary categories: model transfer based and data transfer based methods. In this paper we find that both method types can complement each other, in the sense that, the former can exploit context information via language-independent features but sees no task-specific information in the target language; while the latter generally generates pseudo target-language training data via translation but its exploitation of context information is weakened by inaccurate translations. Moreover, prior works rarely leverage unlabeled data in the target language, which can be effortlessly collected and potentially contains valuable information for improved results. To handle both problems, we propose a novel approach termed UniTrans to Unify both model and data Transfer for cross-lingual NER, and furthermore, to leverage the available information from unlabeled target-language data via enhanced knowledge distillation. We evaluate our proposed UniTrans over 4 target languages on benchmark datasets. Our experimental results show that it substantially outperforms the existing state-of-the-art methods.",cs.CL,NLP
Intelligent requirements engineering from natural language and their chaining toward CAD models,"This paper assumes that design language plays an important role in how designers design and on the creativity of designers. Designers use and develop models as an aid to thinking, a focus for discussion and decision-making and a means of evaluating the reliability of the proposals. This paper proposes an intelligent method for requirements engineering from natural language and their chaining toward CAD models. The transition from linguistic analysis to the representation of engineering requirements consists of the translation of the syntactic structure into semantic form represented by conceptual graphs. Based on the isomorphism between conceptual graphs and predicate logic, a formal language of the specification is proposed. The outcome of this language is chained and translated in Computer Aided Three-Dimensional Interactive Application (CATIA) models. The tool (EGEON: Engineering desiGn sEmantics elabOration and applicatioN) is developed to represent the semantic network of engineering requirements. A case study on the design of a car door hinge is presented to illustrates the proposed method.",cs.CL,NLP
Unsupervised Text Generation by Learning from Search,"In this work, we present TGLS, a novel framework to unsupervised Text Generation by Learning from Search. We start by applying a strong search algorithm (in particular, simulated annealing) towards a heuristically defined objective that (roughly) estimates the quality of sentences. Then, a conditional generative model learns from the search results, and meanwhile smooth out the noise of search. The alternation between search and learning can be repeated for performance bootstrapping. We demonstrate the effectiveness of TGLS on two real-world natural language generation tasks, paraphrase generation and text formalization. Our model significantly outperforms unsupervised baseline methods in both tasks. Especially, it achieves comparable performance with the state-of-the-art supervised methods in paraphrase generation.",cs.CL,NLP
NLPDove at SemEval-2020 Task 12: Improving Offensive Language Detection with Cross-lingual Transfer,"This paper describes our approach to the task of identifying offensive languages in a multilingual setting. We investigate two data augmentation strategies: using additional semi-supervised labels with different thresholds and cross-lingual transfer with data selection. Leveraging the semi-supervised dataset resulted in performance improvements compared to the baseline trained solely with the manually-annotated dataset. We propose a new metric, Translation Embedding Distance, to measure the transferability of instances for cross-lingual data selection. We also introduce various preprocessing steps tailored for social media text along with methods to fine-tune the pre-trained multilingual BERT (mBERT) for offensive language identification. Our multilingual systems achieved competitive results in Greek, Danish, and Turkish at OffensEval 2020.",cs.CL,NLP
"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech","We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron 2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder.
  We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate: (1) stability and performance when training on low amounts of data, (2) pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines.",cs.CL,NLP
Characterizing COVID-19 Misinformation Communities Using a Novel Twitter Dataset,"From conspiracy theories to fake cures and fake treatments, COVID-19 has become a hot-bed for the spread of misinformation online. It is more important than ever to identify methods to debunk and correct false information online. In this paper, we present a methodology and analyses to characterize the two competing COVID-19 misinformation communities online: (i) misinformed users or users who are actively posting misinformation, and (ii) informed users or users who are actively spreading true information, or calling out misinformation. The goals of this study are two-fold: (i) collecting a diverse set of annotated COVID-19 Twitter dataset that can be used by the research community to conduct meaningful analysis; and (ii) characterizing the two target communities in terms of their network structure, linguistic patterns, and their membership in other communities. Our analyses show that COVID-19 misinformed communities are denser, and more organized than informed communities, with a possibility of a high volume of the misinformation being part of disinformation campaigns. Our analyses also suggest that a large majority of misinformed users may be anti-vaxxers. Finally, our sociolinguistic analyses suggest that COVID-19 informed users tend to use more narratives than misinformed users.",cs.CL,NLP
A Study on Effects of Implicit and Explicit Language Model Information for DBLSTM-CTC Based Handwriting Recognition,"Deep Bidirectional Long Short-Term Memory (D-BLSTM) with a Connectionist Temporal Classification (CTC) output layer has been established as one of the state-of-the-art solutions for handwriting recognition. It is well known that the DBLSTM trained by using a CTC objective function will learn both local character image dependency for character modeling and long-range contextual dependency for implicit language modeling. In this paper, we study the effects of implicit and explicit language model information for DBLSTM-CTC based handwriting recognition by comparing the performance of using or without using an explicit language model in decoding. It is observed that even using one million lines of training sentences to train the DBLSTM, using an explicit language model is still helpful. To deal with such a large-scale training problem, a GPU-based training tool has been developed for CTC training of DBLSTM by using a mini-batch based epochwise Back Propagation Through Time (BPTT) algorithm.",cs.CL,NLP
An Empirical Study of Clarifying Question-Based Systems,"Search and recommender systems that take the initiative to ask clarifying questions to better understand users' information needs are receiving increasing attention from the research community. However, to the best of our knowledge, there is no empirical study to quantify whether and to what extent users are willing or able to answer these questions. In this work, we conduct an online experiment by deploying an experimental system, which interacts with users by asking clarifying questions against a product repository. We collect both implicit interaction behavior data and explicit feedback from users showing that: (a) users are willing to answer a good number of clarifying questions (11-21 on average), but not many more than that; (b) most users answer questions until they reach the target product, but also a fraction of them stops due to fatigue or due to receiving irrelevant questions; (c) part of the users' answers (12-17%) are actually opposite to the description of the target product; while (d) most of the users (66-84%) find the question-based system helpful towards completing their tasks. Some of the findings of the study contradict current assumptions on simulated evaluations in the field, while they point towards improvements in the evaluation framework and can inspire future interactive search/recommender system designs.",cs.CL,NLP
SemEval-2020 Task 7: Assessing Humor in Edited News Headlines,"This paper describes the SemEval-2020 shared task ""Assessing Humor in Edited News Headlines."" The task's dataset contains news headlines in which short edits were applied to make them funny, and the funniness of these edited headlines was rated using crowdsourcing. This task includes two subtasks, the first of which is to estimate the funniness of headlines on a humor scale in the interval 0-3. The second subtask is to predict, for a pair of edited versions of the same original headline, which is the funnier version. To date, this task is the most popular shared computational humor task, attracting 48 teams for the first subtask and 31 teams for the second.",cs.CL,NLP
Relation Extraction with Self-determined Graph Convolutional Network,"Relation Extraction is a way of obtaining the semantic relationship between entities in text. The state-of-the-art methods use linguistic tools to build a graph for the text in which the entities appear and then a Graph Convolutional Network (GCN) is employed to encode the pre-built graphs. Although their performance is promising, the reliance on linguistic tools results in a non end-to-end process. In this work, we propose a novel model, the Self-determined Graph Convolutional Network (SGCN), which determines a weighted graph using a self-attention mechanism, rather using any linguistic tool. Then, the self-determined graph is encoded using a GCN. We test our model on the TACRED dataset and achieve the state-of-the-art result. Our experiments show that SGCN outperforms the traditional GCN, which uses dependency parsing tools to build the graph.",cs.CL,NLP
SimulEval: An Evaluation Toolkit for Simultaneous Translation,"Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.",cs.CL,NLP
A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals,"The real-world capabilities of objective speech quality measures are limited since current measures (1) are developed from simulated data that does not adequately model real environments; or they (2) predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory (pBLSTM) network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments.",cs.CL,NLP
Evaluating Automatically Generated Phoneme Captions for Images,"Image2Speech is the relatively new task of generating a spoken description of an image. This paper presents an investigation into the evaluation of this task. For this, first an Image2Speech system was implemented which generates image captions consisting of phoneme sequences. This system outperformed the original Image2Speech system on the Flickr8k corpus. Subsequently, these phoneme captions were converted into sentences of words. The captions were rated by human evaluators for their goodness of describing the image. Finally, several objective metric scores of the results were correlated with these human ratings. Although BLEU4 does not perfectly correlate with human ratings, it obtained the highest correlation among the investigated metrics, and is the best currently existing metric for the Image2Speech task. Current metrics are limited by the fact that they assume their input to be words. A more appropriate metric for the Image2Speech task should assume its input to be parts of words, i.e. phonemes, instead.",cs.CL,NLP
Multi-task learning for natural language processing in the 2020s: where are we going?,"Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.",cs.CL,NLP
Toward Givenness Hierarchy Theoretic Natural Language Generation,"Language-capable interactive robots participating in dialogues with human interlocutors must be able to naturally and efficiently communicate about the entities in their environment. A key aspect of such communication is the use of anaphoric language. The linguistic theory of the Givenness Hierarchy(GH) suggests that humans use anaphora based on the cognitive statuses their referents have in the minds of their interlocutors. In previous work, researchers presented GH-theoretic approaches to robot anaphora understanding. In this paper we describe how the GH might need to be used quite differently to facilitate robot anaphora generation.",cs.CL,NLP
Neural Composition: Learning to Generate from Multiple Models,"Decomposing models into multiple components is critically important in many applications such as language modeling (LM) as it enables adapting individual components separately and biasing of some components to the user's personal preferences. Conventionally, contextual and personalized adaptation for language models, are achieved through class-based factorization, which requires class-annotated data, or through biasing to individual phrases which is limited in scale. In this paper, we propose a system that combines model-defined components, by learning when to activate the generation process from each individual component, and how to combine probability distributions from each component, directly from unlabeled text data.",cs.CL,NLP
Deep Contextual Embeddings for Address Classification in E-commerce,"E-commerce customers in developing nations like India tend to follow no fixed format while entering shipping addresses. Parsing such addresses is challenging because of a lack of inherent structure or hierarchy. It is imperative to understand the language of addresses, so that shipments can be routed without delays. In this paper, we propose a novel approach towards understanding customer addresses by deriving motivation from recent advances in Natural Language Processing (NLP). We also formulate different pre-processing steps for addresses using a combination of edit distance and phonetic algorithms. Then we approach the task of creating vector representations for addresses using Word2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these approaches with respect to sub-region classification task for North and South Indian cities. Through experiments, we demonstrate the effectiveness of generalized RoBERTa model, pre-trained over a large address corpus for language modelling task. Our proposed RoBERTa model achieves a classification accuracy of around 90% with minimal text preprocessing for sub-region classification task outperforming all other approaches. Once pre-trained, the RoBERTa model can be fine-tuned for various downstream tasks in supply chain like pincode suggestion and geo-coding. The model generalizes well for such tasks even with limited labelled data. To the best of our knowledge, this is the first of its kind research proposing a novel approach of understanding customer addresses in e-commerce domain by pre-training language models and fine-tuning them for different purposes.",cs.CL,NLP
EmotionGIF-Yankee: A Sentiment Classifier with Robust Model Based Ensemble Methods,"This paper provides a method to classify sentiment with robust model based ensemble methods. We preprocess tweet data to enhance coverage of tokenizer. To reduce domain bias, we first train tweet dataset for pre-trained language model. Besides, each classifier has its strengths and weakness, we leverage different types of models with ensemble methods: average and power weighted sum. From the experiments, we show that our approach has achieved positive effect for sentiment classification. Our system reached third place among 26 teams from the evaluation in SocialNLP 2020 EmotionGIF competition.",cs.CL,NLP
On the Evolution of Programming Languages,"This paper attempts to connects the evolution of computer languages with the evolution of life, where the later has been dictated by \emph{theory of evolution of species}, and tries to give supportive evidence that the new languages are more robust than the previous, carry-over the mixed features of older languages, such that strong features gets added into them and weak features of older languages gets removed. In addition, an analysis of most prominent programming languages is presented, emphasizing on how the features of existing languages have influenced the development of new programming languages. At the end, it suggests a set of experimental languages, which may rule the world of programming languages in the time of new multi-core architectures.
  Index terms- Programming languages' evolution, classifications of languages, future languages, scripting-languages.",cs.CL,NLP
"Identification, Tracking and Impact: Understanding the trade secret of catchphrases","Understanding the topical evolution in industrial innovation is a challenging problem. With the advancement in the digital repositories in the form of patent documents, it is becoming increasingly more feasible to understand the innovation secrets -- ""catchphrases"" of organizations. However, searching and understanding this enormous textual information is a natural bottleneck. In this paper, we propose an unsupervised method for the extraction of catchphrases from the abstracts of patents granted by the U.S. Patent and Trademark Office over the years. Our proposed system achieves substantial improvement, both in terms of precision and recall, against state-of-the-art techniques. As a second objective, we conduct an extensive empirical study to understand the temporal evolution of the catchphrases across various organizations. We also show how the overall innovation evolution in the form of introduction of newer catchphrases in an organization's patents correlates with the future citations received by the patents filed by that organization. Our code and data sets will be placed in the public domain soon.",cs.CL,NLP
Reed at SemEval-2020 Task 9: Fine-Tuning and Bag-of-Words Approaches to Code-Mixed Sentiment Analysis,"We explore the task of sentiment analysis on Hinglish (code-mixed Hindi-English) tweets as participants of Task 9 of the SemEval-2020 competition, known as the SentiMix task. We had two main approaches: 1) applying transfer learning by fine-tuning pre-trained BERT models and 2) training feedforward neural networks on bag-of-words representations. During the evaluation phase of the competition, we obtained an F-score of 71.3% with our best model, which placed $4^{th}$ out of 62 entries in the official system rankings.",cs.CL,NLP
COVID-19 Knowledge Graph: Accelerating Information Retrieval and Discovery for Scientific Literature,"The coronavirus disease (COVID-19) has claimed the lives of over 350,000 people and infected more than 6 million people worldwide. Several search engines have surfaced to provide researchers with additional tools to find and retrieve information from the rapidly growing corpora on COVID-19. These engines lack extraction and visualization tools necessary to retrieve and interpret complex relations inherent to scientific literature. Moreover, because these engines mainly rely upon semantic information, their ability to capture complex global relationships across documents is limited, which reduces the quality of similarity-based article recommendations for users. In this work, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for extracting and visualizing complex relationships between COVID-19 scientific articles. The CKG combines semantic information with document topological information for the application of similar document retrieval. The CKG is constructed using the latent schema of the data, and then enriched with biomedical entity information extracted from the unstructured text of articles using scalable AWS technologies to form relations in the graph. Finally, we propose a document similarity engine that leverages low-dimensional graph embeddings from the CKG with semantic embeddings for similar article retrieval. Analysis demonstrates the quality of relationships in the CKG and shows that it can be used to uncover meaningful information in COVID-19 scientific articles. The CKG helps power www.cord19.aws and is publicly available.",cs.CL,NLP
Unsupervised Subword Modeling Using Autoregressive Pretraining and Cross-Lingual Phone-Aware Modeling,"This study addresses unsupervised subword modeling, i.e., learning feature representations that can distinguish subword units of a language. The proposed approach adopts a two-stage bottleneck feature (BNF) learning framework, consisting of autoregressive predictive coding (APC) as a front-end and a DNN-BNF model as a back-end. APC pretrained features are set as input features to a DNN-BNF model. A language-mismatched ASR system is used to provide cross-lingual phone labels for DNN-BNF model training. Finally, BNFs are extracted as the subword-discriminative feature representation. A second aim of this work is to investigate the robustness of our approach's effectiveness to different amounts of training data. The results on Libri-light and the ZeroSpeech 2017 databases show that APC is effective in front-end feature pretraining. Our whole system outperforms the state of the art on both databases. Cross-lingual phone labels for English data by a Dutch ASR outperform those by a Mandarin ASR, possibly linked to the larger similarity of Dutch compared to Mandarin with English. Our system is less sensitive to training data amount when the training data is over 50 hours. APC pretraining leads to a reduction of needed training material from over 5,000 hours to around 200 hours with little performance degradation.",cs.CL,NLP
Exploring Deep Hybrid Tensor-to-Vector Network Architectures for Regression Based Speech Enhancement,"This paper investigates different trade-offs between the number of model parameters and enhanced speech qualities by employing several deep tensor-to-vector regression models for speech enhancement. We find that a hybrid architecture, namely CNN-TT, is capable of maintaining a good quality performance with a reduced model parameter size. CNN-TT is composed of several convolutional layers at the bottom for feature extraction to improve speech quality and a tensor-train (TT) output layer on the top to reduce model parameters. We first derive a new upper bound on the generalization power of the convolutional neural network (CNN) based vector-to-vector regression models. Then, we provide experimental evidence on the Edinburgh noisy speech corpus to demonstrate that, in single-channel speech enhancement, CNN outperforms DNN at the expense of a small increment of model sizes. Besides, CNN-TT slightly outperforms the CNN counterpart by utilizing only 32\% of the CNN model parameters. Besides, further performance improvement can be attained if the number of CNN-TT parameters is increased to 44\% of the CNN model size. Finally, our experiments of multi-channel speech enhancement on a simulated noisy WSJ0 corpus demonstrate that our proposed hybrid CNN-TT architecture achieves better results than both DNN and CNN models in terms of better-enhanced speech qualities and smaller parameter sizes.",cs.CL,NLP
Analysis of Predictive Coding Models for Phonemic Representation Learning in Small Datasets,"Neural network models using predictive coding are interesting from the viewpoint of computational modelling of human language acquisition, where the objective is to understand how linguistic units could be learned from speech without any labels. Even though several promising predictive coding -based learning algorithms have been proposed in the literature, it is currently unclear how well they generalise to different languages and training dataset sizes. In addition, despite that such models have shown to be effective phonemic feature learners, it is unclear whether minimisation of the predictive loss functions of these models also leads to optimal phoneme-like representations. The present study investigates the behaviour of two predictive coding models, Autoregressive Predictive Coding and Contrastive Predictive Coding, in a phoneme discrimination task (ABX task) for two languages with different dataset sizes. Our experiments show a strong correlation between the autoregressive loss and the phoneme discrimination scores with the two datasets. However, to our surprise, the CPC model shows rapid convergence already after one pass over the training data, and, on average, its representations outperform those of APC on both languages.",cs.CL,NLP
Tweets Sentiment Analysis via Word Embeddings and Machine Learning Techniques,"Sentiment analysis of social media data consists of attitudes, assessments, and emotions which can be considered a way human think. Understanding and classifying the large collection of documents into positive and negative aspects are a very difficult task. Social networks such as Twitter, Facebook, and Instagram provide a platform in order to gather information about peoples sentiments and opinions. Considering the fact that people spend hours daily on social media and share their opinion on various different topics helps us analyze sentiments better. More and more companies are using social media tools to provide various services and interact with customers. Sentiment Analysis (SA) classifies the polarity of given tweets to positive and negative tweets in order to understand the sentiments of the public. This paper aims to perform sentiment analysis of real-time 2019 election twitter data using the feature selection model word2vec and the machine learning algorithm random forest for sentiment classification. Word2vec with Random Forest improves the accuracy of sentiment analysis significantly compared to traditional methods such as BOW and TF-IDF. Word2vec improves the quality of features by considering contextual semantics of words in a text hence improving the accuracy of machine learning and sentiment analysis.",cs.CL,NLP
"Discourse Coherence, Reference Grounding and Goal Oriented Dialogue","Prior approaches to realizing mixed-initiative human--computer referential communication have adopted information-state or collaborative problem-solving approaches. In this paper, we argue for a new approach, inspired by coherence-based models of discourse such as SDRT \cite{asher-lascarides:2003a}, in which utterances attach to an evolving discourse structure and the associated knowledge graph of speaker commitments serves as an interface to real-world reasoning and conversational strategy. As first steps towards implementing the approach, we describe a simple dialogue system in a referential communication domain that accumulates constraints across discourse, interprets them using a learned probabilistic model, and plans clarification using reinforcement learning.",cs.CL,NLP
Targeting the Benchmark: On Methodology in Current Natural Language Processing Research,"It has become a common pattern in our field: One group introduces a language task, exemplified by a dataset, which they argue is challenging enough to serve as a benchmark. They also provide a baseline model for it, which then soon is improved upon by other groups. Often, research efforts then move on, and the pattern repeats itself. What is typically left implicit is the argumentation for why this constitutes progress, and progress towards what. In this paper, we try to step back for a moment from this pattern and work out possible argumentations and their parts.",cs.CL,NLP
The Go Transformer: Natural Language Modeling for Game Play,"This work applies natural language modeling to generate plausible strategic moves in the ancient game of Go. We train the Generative Pretrained Transformer (GPT-2) to mimic the style of Go champions as archived in Smart Game Format (SGF), which offers a text description of move sequences. The trained model further generates valid but previously unseen strategies for Go. Because GPT-2 preserves punctuation and spacing, the raw output of the text generator provides inputs to game visualization and creative patterns, such as the Sabaki project's game engine using auto-replays. Results demonstrate that language modeling can capture both the sequencing format of championship Go games and their strategic formations. Compared to random game boards, the GPT-2 fine-tuning shows efficient opening move sequences favoring corner play over less advantageous center and side play. Game generation as a language modeling task offers novel approaches to more than 40 other board games where historical text annotation provides training data (e.g., Amazons & Connect 4/6).",cs.CL,NLP
What Gives the Answer Away? Question Answering Bias Analysis on Video QA Datasets,"Question answering biases in video QA datasets can mislead multimodal model to overfit to QA artifacts and jeopardize the model's ability to generalize. Understanding how strong these QA biases are and where they come from helps the community measure progress more accurately and provide researchers insights to debug their models. In this paper, we analyze QA biases in popular video question answering datasets and discover pretrained language models can answer 37-48% questions correctly without using any multimodal context information, far exceeding the 20% random guess baseline for 5-choose-1 multiple-choice questions. Our ablation study shows biases can come from annotators and type of questions. Specifically, annotators that have been seen during training are better predicted by the model and reasoning, abstract questions incur more biases than factual, direct questions. We also show empirically that using annotator-non-overlapping train-test splits can reduce QA biases for video QA datasets.",cs.CL,NLP
CAT-Gen: Improving Robustness in NLP Models via Controlled Adversarial Text Generation,"NLP models are shown to suffer from robustness issues, i.e., a model's prediction can be easily changed under small perturbations to the input. In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels. For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews. Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches. We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.",cs.CL,NLP
Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction,"We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.",cs.CL,NLP
Improving QA Generalization by Concurrent Modeling of Multiple Biases,"Existing NLP datasets contain various biases that models can easily exploit to achieve high performances on the corresponding evaluation sets. However, focusing on dataset-specific biases limits their ability to learn more generalizable knowledge about the task from more general data patterns. In this paper, we investigate the impact of debiasing methods for improving generalization and propose a general framework for improving the performance on both in-domain and out-of-domain datasets by concurrent modeling of multiple biases in the training data. Our framework weights each example based on the biases it contains and the strength of those biases in the training data. It then uses these weights in the training objective so that the model relies less on examples with high bias weights. We extensively evaluate our framework on extractive question answering with training data from various domains with multiple biases of different strengths. We perform the evaluations in two different settings, in which the model is trained on a single domain or multiple domains simultaneously, and show its effectiveness in both settings compared to state-of-the-art debiasing methods.",cs.CL,NLP
Why do you think that? Exploring Faithful Sentence-Level Rationales Without Supervision,"Evaluating the trustworthiness of a model's prediction is essential for differentiating between `right for the right reasons' and `right for the wrong reasons'. Identifying textual spans that determine the target label, known as faithful rationales, usually relies on pipeline approaches or reinforcement learning. However, such methods either require supervision and thus costly annotation of the rationales or employ non-differentiable models. We propose a differentiable training-framework to create models which output faithful rationales on a sentence level, by solely applying supervision on the target task. To achieve this, our model solves the task based on each rationale individually and learns to assign high scores to those which solved the task best. Our evaluation on three different datasets shows competitive results compared to a standard BERT blackbox while exceeding a pipeline counterpart's performance in two cases. We further exploit the transparent decision-making process of these models to prefer selecting the correct rationales by applying direct supervision, thereby boosting the performance on the rationale-level.",cs.CL,NLP
gundapusunil at SemEval-2020 Task 9: Syntactic Semantic LSTM Architecture for SENTIment Analysis of Code-MIXed Data,"The phenomenon of mixing the vocabulary and syntax of multiple languages within the same utterance is called Code-Mixing. This is more evident in multilingual societies. In this paper, we have developed a system for SemEval 2020: Task 9 on Sentiment Analysis for Code-Mixed Social Media Text. Our system first generates two types of embeddings for the social media text. In those, the first one is character level embeddings to encode the character level information and to handle the out-of-vocabulary entries and the second one is FastText word embeddings for capturing morphology and semantics. These two embeddings were passed to the LSTM network and the system outperformed the baseline model.",cs.CL,NLP
Baseline System of Voice Conversion Challenge 2020 with Cyclic Variational Autoencoder and Parallel WaveGAN,"In this paper, we present a description of the baseline system of Voice Conversion Challenge (VCC) 2020 with a cyclic variational autoencoder (CycleVAE) and Parallel WaveGAN (PWG), i.e., CycleVAEPWG. CycleVAE is a nonparallel VAE-based voice conversion that utilizes converted acoustic features to consider cyclically reconstructed spectra during optimization. On the other hand, PWG is a non-autoregressive neural vocoder that is based on a generative adversarial network for a high-quality and fast waveform generator. In practice, the CycleVAEPWG system can be straightforwardly developed with the VCC 2020 dataset using a unified model for both Task 1 (intralingual) and Task 2 (cross-lingual), where our open-source implementation is available at https://github.com/bigpon/vcc20_baseline_cyclevae. The results of VCC 2020 have demonstrated that the CycleVAEPWG baseline achieves the following: 1) a mean opinion score (MOS) of 2.87 in naturalness and a speaker similarity percentage (Sim) of 75.37% for Task 1, and 2) a MOS of 2.56 and a Sim of 56.46% for Task 2, showing an approximately or nearly average score for naturalness and an above average score for speaker similarity.",cs.CL,NLP
Word Level Language Identification in English Telugu Code Mixed Data,"In a multilingual or sociolingual configuration Intra-sentential Code Switching (ICS) or Code Mixing (CM) is frequently observed nowadays. In the world, most of the people know more than one language. CM usage is especially apparent in social media platforms. Moreover, ICS is particularly significant in the context of technology, health, and law where conveying the upcoming developments are difficult in one's native language. In applications like dialog systems, machine translation, semantic parsing, shallow parsing, etc. CM and Code Switching pose serious challenges. To do any further advancement in code-mixed data, the necessary step is Language Identification. In this paper, we present a study of various models - Nave Bayes Classifier, Random Forest Classifier, Conditional Random Field (CRF), and Hidden Markov Model (HMM) for Language Identification in English - Telugu Code Mixed Data. Considering the paucity of resources in code mixed languages, we proposed the CRF model and HMM model for word level language identification. Our best performing system is CRF-based with an f1-score of 0.91.",cs.CL,NLP
Online Back-Parsing for AMR-to-Text Generation,"AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph. Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs. We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation. As the result, our outputs can better preserve the input meaning than standard decoders. Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.",cs.CL,NLP
Analysis of Disfluency in Children's Speech,"Disfluencies are prevalent in spontaneous speech, as shown in many studies of adult speech. Less is understood about children's speech, especially in pre-school children who are still developing their language skills. We present a novel dataset with annotated disfluencies of spontaneous explanations from 26 children (ages 5--8), interviewed twice over a year-long period. Our preliminary analysis reveals significant differences between children's speech in our corpus and adult spontaneous speech from two corpora (Switchboard and CallHome). Children have higher disfluency and filler rates, tend to use nasal filled pauses more frequently, and on average exhibit longer reparandums than repairs, in contrast to adult speakers. Despite the differences, an automatic disfluency detection system trained on adult (Switchboard) speech transcripts performs reasonably well on children's speech, achieving an F1 score that is 10\% higher than the score on an adult out-of-domain dataset (CallHome).",cs.CL,NLP
Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,"Document-level neural machine translation has yielded attractive improvements. However, majority of existing methods roughly use all context sentences in a fixed scope. They neglect the fact that different source sentences need different sizes of context. To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations. Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence. Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module. We train the two modules end-to-end via reinforcement learning. A novel reward is proposed to encourage the selection and utilization of dynamic context sentences. Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",cs.CL,NLP
Evaluating and Characterizing Human Rationales,"Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using ""fidelity curves"" to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.",cs.CL,NLP
Learning to Pronounce Chinese Without a Pronunciation Dictionary,"We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.",cs.CL,NLP
On Task-Level Dialogue Composition of Generative Transformer Model,"Task-oriented dialogue systems help users accomplish tasks such as booking a movie ticket and ordering food via conversation. Generative models parameterized by a deep neural network are widely used for next turn response generation in such systems. It is natural for users of the system to want to accomplish multiple tasks within the same conversation, but the ability of generative models to compose multiple tasks is not well studied. In this work, we begin by studying the effect of training human-human task-oriented dialogues towards improving the ability to compose multiple tasks on Transformer generative models. To that end, we propose and explore two solutions: (1) creating synthetic multiple task dialogue data for training from human-human single task dialogue and (2) forcing the encoder representation to be invariant to single and multiple task dialogues using an auxiliary loss. The results from our experiments highlight the difficulty of even the sophisticated variant of transformer model in learning to compose multiple tasks from single task dialogues.",cs.CL,NLP
MuSeM: Detecting Incongruent News Headlines using Mutual Attentive Semantic Matching,"Measuring the congruence between two texts has several useful applications, such as detecting the prevalent deceptive and misleading news headlines on the web. Many works have proposed machine learning based solutions such as text similarity between the headline and body text to detect the incongruence. Text similarity based methods fail to perform well due to different inherent challenges such as relative length mismatch between the news headline and its body content and non-overlapping vocabulary. On the other hand, more recent works that use headline guided attention to learn a headline derived contextual representation of the news body also result in convoluting overall representation due to the news body's lengthiness. This paper proposes a method that uses inter-mutual attention-based semantic matching between the original and synthetically generated headlines, which utilizes the difference between all pairs of word embeddings of words involved. The paper also investigates two more variations of our method, which use concatenation and dot-products of word embeddings of the words of original and synthetic headlines. We observe that the proposed method outperforms prior arts significantly for two publicly available datasets.",cs.CL,NLP
Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning to Rank,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",cs.CL,NLP
Latent linguistic embedding for cross-lingual text-to-speech and voice conversion,"As the recently proposed voice cloning system, NAUTILUS, is capable of cloning unseen voices using untranscribed speech, we investigate the feasibility of using it to develop a unified cross-lingual TTS/VC system. Cross-lingual speech generation is the scenario in which speech utterances are generated with the voices of target speakers in a language not spoken by them originally. This type of system is not simply cloning the voice of the target speaker, but essentially creating a new voice that can be considered better than the original under a specific framing. By using a well-trained English latent linguistic embedding to create a cross-lingual TTS and VC system for several German, Finnish, and Mandarin speakers included in the Voice Conversion Challenge 2020, we show that our method not only creates cross-lingual VC with high speaker similarity but also can be seamlessly used for cross-lingual TTS without having to perform any extra steps. However, the subjective evaluations of perceived naturalness seemed to vary between target speakers, which is one aspect for future improvement.",cs.CL,NLP
"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available.",cs.CL,NLP
Improving Attention Mechanism with Query-Value Interaction,"Attention mechanism has played critical roles in various state-of-the-art NLP models such as Transformer and BERT. It can be formulated as a ternary function that maps the input queries, keys and values into an output by using a summation of values weighted by the attention weights derived from the interactions between queries and keys. Similar with query-key interactions, there is also inherent relatedness between queries and values, and incorporating query-value interactions has the potential to enhance the output by learning customized values according to the characteristics of queries. However, the query-value interactions are ignored by existing attention methods, which may be not optimal. In this paper, we propose to improve the existing attention mechanism by incorporating query-value interactions. We propose a query-value interaction function which can learn query-aware attention values, and combine them with the original values and attention weights to form the final output. Extensive experiments on four datasets for different tasks show that our approach can consistently improve the performance of many attention-based models by incorporating query-value interactions.",cs.CL,NLP
Beyond [CLS] through Ranking by Generation,"Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document's language model, were very successful in various IR tasks in the past. However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead. Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet. In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task. Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.",cs.CL,NLP
VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling,"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, ""Cambridge"" and the first non-English corpus ""Robert"", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.",cs.CL,NLP
TeaForN: Teacher-Forcing with N-grams,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.",cs.CL,NLP
A Self-supervised Approach for Semantic Indexing in the Context of COVID-19 Pandemic,"The pandemic has accelerated the pace at which COVID-19 scientific papers are published. In addition, the process of manually assigning semantic indexes to these papers by experts is even more time-consuming and overwhelming in the current health crisis. Therefore, there is an urgent need for automatic semantic indexing models which can effectively scale-up to newly introduced concepts and rapidly evolving distributions of the hyperfocused related literature. In this research, we present a novel semantic indexing approach based on the state-of-the-art self-supervised representation learning and transformer encoding exclusively suitable for pandemic crises. We present a case study on a novel dataset that is based on COVID-19 papers published and manually indexed in PubMed. Our study shows that our self-supervised model outperforms the best performing models of BioASQ Task 8a by micro-F1 score of 0.1 and LCA-F score of 0.08 on average. Our model also shows superior performance on detecting the supplementary concepts which is quite important when the focus of the literature has drastically shifted towards specific concepts related to the pandemic. Our study sheds light on the main challenges confronting semantic indexing models during a pandemic, namely new domains and drastic changes of their distributions, and as a superior alternative for such situations, propose a model founded on approaches which have shown auspicious performance in improving generalization and data efficiency in various NLP tasks. We also show the joint indexing of major Medical Subject Headings (MeSH) and supplementary concepts improves the overall performance.",cs.CL,NLP
Decoupling entrainment from consistency using deep neural networks,"Human interlocutors tend to engage in adaptive behavior known as entrainment to become more similar to each other. Isolating the effect of consistency, i.e., speakers adhering to their individual styles, is a critical part of the analysis of entrainment. We propose to treat speakers' initial vocal features as confounds for the prediction of subsequent outputs. Using two existing neural approaches to deconfounding, we define new measures of entrainment that control for consistency. These successfully discriminate real interactions from fake ones. Interestingly, our stricter methods correlate with social variables in opposite direction from previous measures that do not account for consistency. These results demonstrate the advantages of using neural networks to model entrainment, and raise questions regarding how to interpret prior associations of conversation quality with entrainment measures that do not account for consistency.",cs.CL,NLP
DeL-haTE: A Deep Learning Tunable Ensemble for Hate Speech Detection,"Online hate speech on social media has become a fast-growing problem in recent times. Nefarious groups have developed large content delivery networks across several main-stream (Twitter and Facebook) and fringe (Gab, 4chan, 8chan, etc.) outlets to deliver cascades of hate messages directed both at individuals and communities. Thus addressing these issues has become a top priority for large-scale social media outlets. Three key challenges in automated detection and classification of hateful content are the lack of clearly labeled data, evolving vocabulary and lexicon - hashtags, emojis, etc. - and the lack of baseline models for fringe outlets such as Gab. In this work, we propose a novel framework with three major contributions. (a) We engineer an ensemble of deep learning models that combines the strengths of state-of-the-art approaches, (b) we incorporate a tuning factor into this framework that leverages transfer learning to conduct automated hate speech classification on unlabeled datasets, like Gab, and (c) we develop a weak supervised learning methodology that allows our framework to train on unlabeled data. Our ensemble models achieve an 83% hate recall on the HON dataset, surpassing the performance of the state-of-the-art deep models. We demonstrate that weak supervised training in combination with classifier tuning significantly increases model performance on unlabeled data from Gab, achieving a hate recall of 67%.",cs.CL,NLP
Reasoning Over History: Context Aware Visual Dialog,"While neural models have been shown to exhibit strong performance on single-turn visual question answering (VQA) tasks, extending VQA to a multi-turn, conversational setting remains a challenge. One way to address this challenge is to augment existing strong neural VQA models with the mechanisms that allow them to retain information from previous dialog turns. One strong VQA model is the MAC network, which decomposes a task into a series of attention-based reasoning steps. However, since the MAC network is designed for single-turn question answering, it is not capable of referring to past dialog turns. More specifically, it struggles with tasks that require reasoning over the dialog history, particularly coreference resolution. We extend the MAC network architecture with Context-aware Attention and Memory (CAM), which attends over control states in past dialog turns to determine the necessary reasoning operations for the current question. MAC nets with CAM achieve up to 98.25% accuracy on the CLEVR-Dialog dataset, beating the existing state-of-the-art by 30% (absolute). Our error analysis indicates that with CAM, the model's performance particularly improved on questions that required coreference resolution.",cs.CL,NLP
How Domain Terminology Affects Meeting Summarization Performance,"Meetings are essential to modern organizations. Numerous meetings are held and recorded daily, more than can ever be comprehended. A meeting summarization system that identifies salient utterances from the transcripts to automatically generate meeting minutes can help. It empowers users to rapidly search and sift through large meeting collections. To date, the impact of domain terminology on the performance of meeting summarization remains understudied, despite that meetings are rich with domain knowledge. In this paper, we create gold-standard annotations for domain terminology on a sizable meeting corpus; they are known as jargon terms. We then analyze the performance of a meeting summarization system with and without jargon terms. Our findings reveal that domain terminology can have a substantial impact on summarization performance. We publicly release all domain terminology to advance research in meeting summarization.",cs.CL,NLP
Adapting Pretrained Transformer to Lattices for Spoken Language Understanding,"Lattices are compact representations that encode multiple hypotheses, such as speech recognition results or different word segmentations. It is shown that encoding lattices as opposed to 1-best results generated by automatic speech recognizer (ASR) boosts the performance of spoken language understanding (SLU). Recently, pretrained language models with the transformer architecture have achieved the state-of-the-art results on natural language understanding, but their ability of encoding lattices has not been explored. Therefore, this paper aims at adapting pretrained transformers to lattice inputs in order to perform understanding tasks specifically for spoken language. Our experiments on the benchmark ATIS dataset show that fine-tuning pretrained transformers with lattice inputs yields clear improvement over fine-tuning with 1-best results. Further evaluation demonstrates the effectiveness of our methods under different acoustic conditions. Our code is available at https://github.com/MiuLab/Lattice-SLU",cs.CL,NLP
Hierarchical Bi-Directional Self-Attention Networks for Paper Review Rating Recommendation,"Review rating prediction of text reviews is a rapidly growing technology with a wide range of applications in natural language processing. However, most existing methods either use hand-crafted features or learn features using deep learning with simple text corpus as input for review rating prediction, ignoring the hierarchies among data. In this paper, we propose a Hierarchical bi-directional self-attention Network framework (HabNet) for paper review rating prediction and recommendation, which can serve as an effective decision-making tool for the academic paper review process. Specifically, we leverage the hierarchical structure of the paper reviews with three levels of encoders: sentence encoder (level one), intra-review encoder (level two) and inter-review encoder (level three). Each encoder first derives contextual representation of each level, then generates a higher-level representation, and after the learning process, we are able to identify useful predictors to make the final acceptance decision, as well as to help discover the inconsistency between numerical review ratings and text sentiment conveyed by reviewers. Furthermore, we introduce two new metrics to evaluate models in data imbalance situations. Extensive experiments on a publicly available dataset (PeerRead) and our own collected dataset (OpenReview) demonstrate the superiority of the proposed approach compared with state-of-the-art methods.",cs.CL,NLP
Be More with Less: Hypergraph Attention Networks for Inductive Text Classification,"Text classification is a critical research topic with broad applications in natural language processing. Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task. Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents. To address those issues, in this paper, we propose a principled model -- hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning. Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",cs.CL,NLP
Investigation of BERT Model on Biomedical Relation Extraction Based on Revised Fine-tuning Mechanism,"With the explosive growth of biomedical literature, designing automatic tools to extract information from the literature has great significance in biomedical research. Recently, transformer-based BERT models adapted to the biomedical domain have produced leading results. However, all the existing BERT models for relation classification only utilize partial knowledge from the last layer. In this paper, we will investigate the method of utilizing the entire layer in the fine-tuning process of BERT model. To the best of our knowledge, we are the first to explore this method. The experimental results illustrate that our method improves the BERT model performance and outperforms the state-of-the-art methods on three benchmark datasets for different relation extraction tasks. In addition, further analysis indicates that the key knowledge about the relations can be learned from the last layer of BERT model.",cs.CL,NLP
Analyzing the Effect of Multi-task Learning for Biomedical Named Entity Recognition,"Developing high-performing systems for detecting biomedical named entities has major implications. State-of-the-art deep-learning based solutions for entity recognition often require large annotated datasets, which is not available in the biomedical domain. Transfer learning and multi-task learning have been shown to improve performance for low-resource domains. However, the applications of these methods are relatively scarce in the biomedical domain, and a theoretical understanding of why these methods improve the performance is lacking. In this study, we performed an extensive analysis to understand the transferability between different biomedical entity datasets. We found useful measures to predict transferability between these datasets. Besides, we propose combining transfer learning and multi-task learning to improve the performance of biomedical named entity recognition systems, which is not applied before to the best of our knowledge.",cs.CL,NLP
CHIME: Cross-passage Hierarchical Memory Network for Generative Review Question Answering,"We introduce CHIME, a cross-passage hierarchical memory network for question answering (QA) via text generation. It extends XLNet introducing an auxiliary memory module consisting of two components: the context memory collecting cross-passage evidences, and the answer memory working as a buffer continually refining the generated answers. Empirically, we show the efficacy of the proposed architecture in the multi-passage generative QA, outperforming the state-of-the-art baselines with better syntactically well-formed answers and increased precision in addressing the questions of the AmazonQA review dataset. An additional qualitative analysis revealed the interpretability introduced by the memory module.",cs.CL,NLP
Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems: A Survey,"In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed that address the capacity to elicit and understand user's needs in task-oriented dialogue systems. We focus on two core tasks, slot filling (SF) and intent classification (IC), and survey how neural-based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent model, which model SF and IC separately, joint models, which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models, that scale the model to new domains. We discuss the current state of the research in SF and IC and highlight challenges that still require attention.",cs.CL,NLP
Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings,"Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention networks. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.",cs.CL,NLP
DNN-Based Semantic Model for Rescoring N-best Speech Recognition List,"The word error rate (WER) of an automatic speech recognition (ASR) system increases when a mismatch occurs between the training and the testing conditions due to the noise, etc. In this case, the acoustic information can be less reliable. This work aims to improve ASR by modeling long-term semantic relations to compensate for distorted acoustic features. We propose to perform this through rescoring of the ASR N-best hypotheses list. To achieve this, we train a deep neural network (DNN). Our DNN rescoring model is aimed at selecting hypotheses that have better semantic consistency and therefore lower WER. We investigate two types of representations as part of input features to our DNN model: static word embeddings (from word2vec) and dynamic contextual embeddings (from BERT). Acoustic and linguistic features are also included. We perform experiments on the publicly available dataset TED-LIUM mixed with real noise. The proposed rescoring approaches give significant improvement of the WER over the ASR system without rescoring models in two noisy conditions and with n-gram and RNNLM.",cs.CL,NLP
Evaluation of Siamese Networks for Semantic Code Search,"With the increase in the number of open repositories and discussion forums, the use of natural language for semantic code search has become increasingly common. The accuracy of the results returned by such systems, however, can be low due to 1) limited shared vocabulary between code and user query and 2) inadequate semantic understanding of user query and its relation to code syntax. Siamese networks are well suited to learning such joint relations between data, but have not been explored in the context of code search. In this work, we evaluate Siamese networks for this task by exploring multiple extraction network architectures. These networks independently process code and text descriptions before passing them to a Siamese network to learn embeddings in a common space. We experiment on two different datasets and discover that Siamese networks can act as strong regularizers on networks that extract rich information from code and text, which in turn helps achieve impressive performance on code search beating previous baselines on $2$ programming languages. We also analyze the embedding space of these networks and provide directions to fully leverage the power of Siamese networks for semantic code search.",cs.CL,NLP
Exploring Question-Specific Rewards for Generating Deep Questions,"Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log-likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to a thorough analysis to explore the effect of each QG-specific reward. We find that optimizing question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality. Our code is publicly available at https://github.com/YuxiXie/RL-for-Question-Generation.",cs.CL,NLP
"Leaf Segmentation and Counting with Deep Learning: on Model Certainty, Test-Time Augmentation, Trade-Offs","Plant phenotyping tasks such as leaf segmentation and counting are fundamental to the study of phenotypic traits. Since it is well-suited for these tasks, deep supervised learning has been prevalent in recent works proposing better performing models at segmenting and counting leaves. Despite good efforts from research groups, one of the main challenges for proposing better methods is still the limitation of labelled data availability. The main efforts of the field seem to be augmenting existing limited data sets, and some aspects of the modelling process have been under-discussed. This paper explores such topics and present experiments that led to the development of the best-performing method in the Leaf Segmentation Challenge and in another external data set of Komatsuna plants. The model has competitive performance while been arguably simpler than other recently proposed ones. The experiments also brought insights such as the fact that model cardinality and test-time augmentation may have strong applications in object segmentation of single class and high occlusion, and regarding the data distribution of recently proposed data sets for benchmarking.",cs.CV,Computer Vision
PointINet: Point Cloud Frame Interpolation Network,"LiDAR point cloud streams are usually sparse in time dimension, which is limited by hardware performance. Generally, the frame rates of mechanical LiDAR sensors are 10 to 20 Hz, which is much lower than other commonly used sensors like cameras. To overcome the temporal limitations of LiDAR sensors, a novel task named Point Cloud Frame Interpolation is studied in this paper. Given two consecutive point cloud frames, Point Cloud Frame Interpolation aims to generate intermediate frame(s) between them. To achieve that, we propose a novel framework, namely Point Cloud Frame Interpolation Network (PointINet). Based on the proposed method, the low frame rate point cloud streams can be upsampled to higher frame rates. We start by estimating bi-directional 3D scene flow between the two point clouds and then warp them to the given time step based on the 3D scene flow. To fuse the two warped frames and generate intermediate point cloud(s), we propose a novel learning-based points fusion module, which simultaneously takes two warped point clouds into consideration. We design both quantitative and qualitative experiments to evaluate the performance of the point cloud frame interpolation method and extensive experiments on two large scale outdoor LiDAR datasets demonstrate the effectiveness of the proposed PointINet. Our code is available at https://github.com/ispc-lab/PointINet.git.",cs.CV,Computer Vision
Prediction of Chronic Kidney Disease Using Deep Neural Network,"Deep neural Network (DNN) is becoming a focal point in Machine Learning research. Its application is penetrating into different fields and solving intricate and complex problems. DNN is now been applied in health image processing to detect various ailment such as cancer and diabetes. Another disease that is causing threat to our health is the kidney disease. This disease is becoming prevalent due to substances and elements we intake. Death is imminent and inevitable within few days without at least one functioning kidney. Ignoring the kidney malfunction can cause chronic kidney disease leading to death. Frequently, Chronic Kidney Disease (CKD) and its symptoms are mild and gradual, often go unnoticed for years only to be realized lately. Bade, a Local Government of Yobe state in Nigeria has been a center of attention by medical practitioners due to the prevalence of CKD. Unfortunately, a technical approach in culminating the disease is yet to be attained. We obtained a record of 400 patients with 10 attributes as our dataset from Bade General Hospital. We used DNN model to predict the absence or presence of CKD in the patients. The model produced an accuracy of 98%. Furthermore, we identified and highlighted the Features importance to provide the ranking of the features used in the prediction of the CKD. The outcome revealed that two attributes; Creatinine and Bicarbonate have the highest influence on the CKD prediction.",cs.CV,Computer Vision
A Deep Reinforcement Learning Approach for Ramp Metering Based on Traffic Video Data,"Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",cs.CV,Computer Vision
Image to Bengali Caption Generation Using Deep CNN and Bidirectional Gated Recurrent Unit,"There is very little notable research on generating descriptions of the Bengali language. About 243 million people speak in Bengali, and it is the 7th most spoken language on the planet. The purpose of this research is to propose a CNN and Bidirectional GRU based architecture model that generates natural language captions in the Bengali language from an image. Bengali people can use this research to break the language barrier and better understand each other's perspectives. It will also help many blind people with their everyday lives. This paper used an encoder-decoder approach to generate captions. We used a pre-trained Deep convolutional neural network (DCNN) called InceptonV3image embedding model as the encoder for analysis, classification, and annotation of the dataset's images Bidirectional Gated Recurrent unit (BGRU) layer as the decoder to generate captions. Argmax and Beam search is used to produce the highest possible quality of the captions. A new dataset called BNATURE is used, which comprises 8000 images with five captions per image. It is used for training and testing the proposed model. We obtained BLEU-1, BLEU-2, BLEU-3, BLEU-4 and Meteor is 42.6, 27.95, 23, 66, 16.41, 28.7 respectively.",cs.CV,Computer Vision
General Domain Adaptation Through Proportional Progressive Pseudo Labeling,"Domain adaptation helps transfer the knowledge gained from a labeled source domain to an unlabeled target domain. During the past few years, different domain adaptation techniques have been published. One common flaw of these approaches is that while they might work well on one input type, such as images, their performance drops when applied to others, such as text or time-series. In this paper, we introduce Proportional Progressive Pseudo Labeling (PPPL), a simple, yet effective technique that can be implemented in a few lines of code to build a more general domain adaptation technique that can be applied on several different input types. At the beginning of the training phase, PPPL progressively reduces target domain classification error, by training the model directly with pseudo-labeled target domain samples, while excluding samples with more likely wrong pseudo-labels from the training set and also postponing training on such samples. Experiments on 6 different datasets that include tasks such as anomaly detection, text sentiment analysis and image classification demonstrate that PPPL can beat other baselines and generalize better.",cs.CV,Computer Vision
Appearance-Invariant 6-DoF Visual Localization using Generative Adversarial Networks,"We propose a novel visual localization network when outside environment has changed such as different illumination, weather and season. The visual localization network is composed of a feature extraction network and pose regression network. The feature extraction network is made up of an encoder network based on the Generative Adversarial Network CycleGAN, which can capture intrinsic appearance-invariant feature maps from unpaired samples of different weathers and seasons. With such an invariant feature, we use a 6-DoF pose regression network to tackle long-term visual localization in the presence of outdoor illumination, weather and season changes. A variety of challenging datasets for place recognition and localization are used to prove our visual localization network, and the results show that our method outperforms state-of-the-art methods in the scenarios with various environment changes.",cs.CV,Computer Vision
"Underwater image filtering: methods, datasets and evaluation","Underwater images are degraded by the selective attenuation of light that distorts colours and reduces contrast. The degradation extent depends on the water type, the distance between an object and the camera, and the depth under the water surface the object is at. Underwater image filtering aims to restore or to enhance the appearance of objects captured in an underwater image. Restoration methods compensate for the actual degradation, whereas enhancement methods improve either the perceived image quality or the performance of computer vision algorithms. The growing interest in underwater image filtering methods--including learning-based approaches used for both restoration and enhancement--and the associated challenges call for a comprehensive review of the state of the art. In this paper, we review the design principles of filtering methods and revisit the oceanology background that is fundamental to identify the degradation causes. We discuss image formation models and the results of restoration methods in various water types. Furthermore, we present task-dependent enhancement methods and categorise datasets for training neural networks and for method evaluation. Finally, we discuss evaluation strategies, including subjective tests and quality assessment measures. We complement this survey with a platform ( https://puiqe.eecs.qmul.ac.uk/ ), which hosts state-of-the-art underwater filtering methods and facilitates comparisons.",cs.CV,Computer Vision
Flexible deep transfer learning by separate feature embeddings and manifold alignment,"Object recognition is a key enabler across industry and defense. As technology changes, algorithms must keep pace with new requirements and data. New modalities and higher resolution sensors should allow for increased algorithm robustness. Unfortunately, algorithms trained on existing labeled datasets do not directly generalize to new data because the data distributions do not match. Transfer learning (TL) or domain adaptation (DA) methods have established the groundwork for transferring knowledge from existing labeled source data to new unlabeled target datasets. However, current DA approaches assume similar source and target feature spaces and suffer in the case of massive domain shifts or changes in the feature space. Existing methods assume the data are either the same modality, or can be aligned to a common feature space. Therefore, most methods are not designed to support a fundamental domain change such as visual to auditory data. We propose a novel deep learning framework that overcomes this limitation by learning separate feature extractions for each domain while minimizing the distance between the domains in a latent lower-dimensional space. The alignment is achieved by considering the data manifold along with an adversarial training procedure. We demonstrate the effectiveness of the approach versus traditional methods with several ablation experiments on synthetic, measured, and satellite image datasets. We also provide practical guidelines for training the network while overcoming vanishing gradients which inhibit learning in some adversarial training settings.",cs.CV,Computer Vision
Randomized RX for target detection,"This work tackles the target detection problem through the well-known global RX method. The RX method models the clutter as a multivariate Gaussian distribution, and has been extended to nonlinear distributions using kernel methods. While the kernel RX can cope with complex clutters, it requires a considerable amount of computational resources as the number of clutter pixels gets larger. Here we propose random Fourier features to approximate the Gaussian kernel in kernel RX and consequently our development keep the accuracy of the nonlinearity while reducing the computational cost which is now controlled by an hyperparameter. Results over both synthetic and real-world image target detection problems show space and time efficiency of the proposed method while providing high detection performance.",cs.CV,Computer Vision
Multi-Task Multi-Sensor Fusion for 3D Object Detection,"In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and BEV object detection, while being real time.",cs.CV,Computer Vision
Multi-Contrast Computed Tomography Healthy Kidney Atlas,"The construction of three-dimensional multi-modal tissue maps provides an opportunity to spur interdisciplinary innovations across temporal and spatial scales through information integration. While the preponderance of effort is allocated to the cellular level and explore the changes in cell interactions and organizations, contextualizing findings within organs and systems is essential to visualize and interpret higher resolution linkage across scales. There is a substantial normal variation of kidney morphometry and appearance across body size, sex, and imaging protocols in abdominal computed tomography (CT). A volumetric atlas framework is needed to integrate and visualize the variability across scales. However, there is no abdominal and retroperitoneal organs atlas framework for multi-contrast CT. Hence, we proposed a high-resolution CT retroperitoneal atlas specifically optimized for the kidney across non-contrast CT and early arterial, late arterial, venous and delayed contrast enhanced CT. Briefly, we introduce a deep learning-based volume of interest extraction method and an automated two-stage hierarchal registration pipeline to register abdominal volumes to a high-resolution CT atlas template. To generate and evaluate the atlas, multi-contrast modality CT scans of 500 subjects (without reported history of renal disease, age: 15-50 years, 250 males & 250 females) were processed. We demonstrate a stable generalizability of the atlas template for integrating the normal kidney variation from small to large, across contrast modalities and populations with great variability of demographics. The linkage of atlas and demographics provided a better understanding of the variation of kidney anatomy across populations.",cs.CV,Computer Vision
"Joint super-resolution and synthesis of 1 mm isotropic MP-RAGE volumes from clinical MRI exams with scans of different orientation, resolution and contrast","Most existing algorithms for automatic 3D morphometry of human brain MRI scans are designed for data with near-isotropic voxels at approximately 1 mm resolution, and frequently have contrast constraints as well - typically requiring T1 scans (e.g., MP-RAGE). This limitation prevents the analysis of millions of MRI scans acquired with large inter-slice spacing (""thick slice"") in clinical settings every year. The inability to quantitatively analyze these scans hinders the adoption of quantitative neuroimaging in healthcare, and precludes research studies that could attain huge sample sizes and hence greatly improve our understanding of the human brain. Recent advances in CNNs are producing outstanding results in super-resolution and contrast synthesis of MRI. However, these approaches are very sensitive to the contrast, resolution and orientation of the input images, and thus do not generalize to diverse clinical acquisition protocols - even within sites. Here we present SynthSR, a method to train a CNN that receives one or more thick-slice scans with different contrast, resolution and orientation, and produces an isotropic scan of canonical contrast (typically a 1 mm MP-RAGE). The presented method does not require any preprocessing, e.g., skull stripping or bias field correction. Crucially, SynthSR trains on synthetic input images generated from 3D segmentations, and can thus be used to train CNNs for any combination of contrasts, resolutions and orientations without high-resolution training data. We test the images generated with SynthSR in an array of common downstream analyses, and show that they can be reliably used for subcortical segmentation and volumetry, image registration (e.g., for tensor-based morphometry), and, if some image quality requirements are met, even cortical thickness morphometry. The source code is publicly available at github.com/BBillot/SynthSR.",cs.CV,Computer Vision
Spatio-temporal Multi-task Learning for Cardiac MRI Left Ventricle Quantification,"Quantitative assessment of cardiac left ventricle (LV) morphology is essential to assess cardiac function and improve the diagnosis of different cardiovascular diseases. In current clinical practice, LV quantification depends on the measurement of myocardial shape indices, which is usually achieved by manual contouring of the endo- and epicardial. However, this process subjected to inter and intra-observer variability, and it is a time-consuming and tedious task. In this paper, we propose a spatio-temporal multi-task learning approach to obtain a complete set of measurements quantifying cardiac LV morphology, regional-wall thickness (RWT), and additionally detecting the cardiac phase cycle (systole and diastole) for a given 3D Cine-magnetic resonance (MR) image sequence. We first segment cardiac LVs using an encoder-decoder network and then introduce a multitask framework to regress 11 LV indices and classify the cardiac phase, as parallel tasks during model optimization. The proposed deep learning model is based on the 3D spatio-temporal convolutions, which extract spatial and temporal features from MR images. We demonstrate the efficacy of the proposed method using cine-MR sequences of 145 subjects and comparing the performance with other state-of-the-art quantification methods. The proposed method obtained high prediction accuracy, with an average mean absolute error (MAE) of 129 $mm^2$, 1.23 $mm$, 1.76 $mm$, Pearson correlation coefficient (PCC) of 96.4%, 87.2%, and 97.5% for LV and myocardium (Myo) cavity regions, 6 RWTs, 3 LV dimensions, and an error rate of 9.0\% for phase classification. The experimental results highlight the robustness of the proposed method, despite varying degrees of cardiac morphology, image appearance, and low contrast in the cardiac MR sequences.",cs.CV,Computer Vision
Skeleton-based Approaches based on Machine Vision: A Survey,"Recently, skeleton-based approaches have achieved rapid progress on the basis of great success in skeleton representation. Plenty of researches focus on solving specific problems according to skeleton features. Some skeleton-based approaches have been mentioned in several overviews on object detection as a non-essential part. Nevertheless, there has not been any thorough analysis of skeleton-based approaches attentively. Instead of describing these techniques in terms of theoretical constructs, we devote to summarizing skeleton-based approaches with regard to application fields and given tasks as comprehensively as possible. This paper is conducive to further understanding of skeleton-based application and dealing with particular issues.",cs.CV,Computer Vision
CholecSeg8k: A Semantic Segmentation Dataset for Laparoscopic Cholecystectomy Based on Cholec80,"Computer-assisted surgery has been developed to enhance surgery correctness and safety. However, researchers and engineers suffer from limited annotated data to develop and train better algorithms. Consequently, the development of fundamental algorithms such as Simultaneous Localization and Mapping (SLAM) is limited. This article elaborates on the efforts of preparing the dataset for semantic segmentation, which is the foundation of many computer-assisted surgery mechanisms. Based on the Cholec80 dataset [3], we extracted 8,080 laparoscopic cholecystectomy image frames from 17 video clips in Cholec80 and annotated the images. The dataset is named CholecSeg8K and its total size is 3GB. Each of these images is annotated at pixel-level for thirteen classes, which are commonly founded in laparoscopic cholecystectomy surgery. CholecSeg8k is released under the license CC BY- NC-SA 4.0.",cs.CV,Computer Vision
Unsupervised Domain Adaptation for Semantic Segmentation by Content Transfer,"In this paper, we tackle the unsupervised domain adaptation (UDA) for semantic segmentation, which aims to segment the unlabeled real data using labeled synthetic data. The main problem of UDA for semantic segmentation relies on reducing the domain gap between the real image and synthetic image. To solve this problem, we focused on separating information in an image into content and style. Here, only the content has cues for semantic segmentation, and the style makes the domain gap. Thus, precise separation of content and style in an image leads to effect as supervision of real data even when learning with synthetic data. To make the best of this effect, we propose a zero-style loss. Even though we perfectly extract content for semantic segmentation in the real domain, another main challenge, the class imbalance problem, still exists in UDA for semantic segmentation. We address this problem by transferring the contents of tail classes from synthetic to real domain. Experimental results show that the proposed method achieves the state-of-the-art performance in semantic segmentation on the major two UDA settings.",cs.CV,Computer Vision
Training DNNs in O(1) memory with MEM-DFA using Random Matrices,"This work presents a method for reducing memory consumption to a constant complexity when training deep neural networks. The algorithm is based on the more biologically plausible alternatives of the backpropagation (BP): direct feedback alignment (DFA) and feedback alignment (FA), which use random matrices to propagate error. The proposed method, memory-efficient direct feedback alignment (MEM-DFA), uses higher independence of layers in DFA and allows avoiding storing at once all activation vectors, unlike standard BP, FA, and DFA. Thus, our algorithm's memory usage is constant regardless of the number of layers in a neural network. The method increases the computational cost only by a constant factor of one extra forward pass.
  The MEM-DFA, BP, FA, and DFA were evaluated along with their memory profiles on MNIST and CIFAR-10 datasets on various neural network models. Our experiments agree with our theoretical results and show a significant decrease in the memory cost of MEM-DFA compared to the other algorithms.",cs.CV,Computer Vision
Deep Continuous Fusion for Multi-Sensor 3D Object Detection,"In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.",cs.CV,Computer Vision
Deep Bingham Networks: Dealing with Uncertainty and Ambiguity in Pose Estimation,"In this work, we introduce Deep Bingham Networks (DBN), a generic framework that can naturally handle pose-related uncertainties and ambiguities arising in almost all real life applications concerning 3D data. While existing works strive to find a single solution to the pose estimation problem, we make peace with the ambiguities causing high uncertainty around which solutions to identify as the best. Instead, we report a family of poses which capture the nature of the solution space. DBN extends the state of the art direct pose regression networks by (i) a multi-hypotheses prediction head which can yield different distribution modes; and (ii) novel loss functions that benefit from Bingham distributions on rotations. This way, DBN can work both in unambiguous cases providing uncertainty information, and in ambiguous scenes where an uncertainty per mode is desired. On a technical front, our network regresses continuous Bingham mixture models and is applicable to both 2D data such as images and to 3D data such as point clouds. We proposed new training strategies so as to avoid mode or posterior collapse during training and to improve numerical stability. Our methods are thoroughly tested on two different applications exploiting two different modalities: (i) 6D camera relocalization from images; and (ii) object pose estimation from 3D point clouds, demonstrating decent advantages over the state of the art. For the former we contributed our own dataset composed of five indoor scenes where it is unavoidable to capture images corresponding to views that are hard to uniquely identify. For the latter we achieve the top results especially for symmetric objects of ModelNet dataset.",cs.CV,Computer Vision
ResizeMix: Mixing Data with Preserved Object Information and True Labels,"Data augmentation is a powerful technique to increase the diversity of data, which can effectively improve the generalization ability of neural networks in image recognition tasks. Recent data mixing based augmentation strategies have achieved great success. Especially, CutMix uses a simple but effective method to improve the classifiers by randomly cropping a patch from one image and pasting it on another image. To further promote the performance of CutMix, a series of works explore to use the saliency information of the image to guide the mixing. We systematically study the importance of the saliency information for mixing data, and find that the saliency information is not so necessary for promoting the augmentation performance. Furthermore, we find that the cutting based data mixing methods carry two problems of label misallocation and object information missing, which cannot be resolved simultaneously. We propose a more effective but very easily implemented method, namely ResizeMix. We mix the data by directly resizing the source image to a small patch and paste it on another image. The obtained patch preserves more substantial object information compared with conventional cut-based methods. ResizeMix shows evident advantages over CutMix and the saliency-guided methods on both image classification and object detection tasks without additional computation cost, which even outperforms most costly search-based automatic augmentation methods.",cs.CV,Computer Vision
ANR: Articulated Neural Rendering for Virtual Avatars,"The combination of traditional rendering with neural networks in Deferred Neural Rendering (DNR) provides a compelling balance between computational complexity and realism of the resulting images. Using skinned meshes for rendering articulating objects is a natural extension for the DNR framework and would open it up to a plethora of applications. However, in this case the neural shading step must account for deformations that are possibly not captured in the mesh, as well as alignment inaccuracies and dynamics -- which can confound the DNR pipeline. We present Articulated Neural Rendering (ANR), a novel framework based on DNR which explicitly addresses its limitations for virtual human avatars. We show the superiority of ANR not only with respect to DNR but also with methods specialized for avatar creation and animation. In two user studies, we observe a clear preference for our avatar model and we demonstrate state-of-the-art performance on quantitative evaluation metrics. Perceptually, we observe better temporal stability, level of detail and plausibility.",cs.CV,Computer Vision
Model Optimization for Deep Space Exploration via Simulators and Deep Learning,"Machine learning, and eventually true artificial intelligence techniques, are extremely important advancements in astrophysics and astronomy. We explore the application of deep learning using neural networks in order to automate the detection of astronomical bodies for future exploration missions, such as missions to search for signatures or suitability of life. The ability to acquire images, analyze them, and send back those that are important, as determined by the deep learning algorithm, is critical in bandwidth-limited applications. Our previous foundational work solidified the concept of using simulator images and deep learning in order to detect planets. Optimization of this process is of vital importance, as even a small loss in accuracy might be the difference between capturing and completely missing a possibly-habitable nearby planet. Through computer vision, deep learning, and simulators, we introduce methods that optimize the detection of exoplanets. We show that maximum achieved accuracy can hit above 98% for multiple model architectures, even with a relatively small training set.",cs.CV,Computer Vision
"Diagnosis/Prognosis of COVID-19 Images: Challenges, Opportunities, and Applications","The novel Coronavirus disease, COVID-19, has rapidly and abruptly changed the world as we knew in 2020. It becomes the most unprecedent challenge to analytic epidemiology in general and signal processing theories in specific. Given its high contingency nature and adverse effects across the world, it is important to develop efficient processing/learning models to overcome this pandemic and be prepared for potential future ones. In this regard, medical imaging plays an important role for the management of COVID-19. Human-centered interpretation of medical images is, however, tedious and can be subjective. This has resulted in a surge of interest to develop Radiomics models for analysis and interpretation of medical images. Signal Processing (SP) and Deep Learning (DL) models can assist in development of robust Radiomics solutions for diagnosis/prognosis, severity assessment, treatment response, and monitoring of COVID-19 patients. In this article, we aim to present an overview of the current state, challenges, and opportunities of developing SP/DL-empowered models for diagnosis (screening/monitoring) and prognosis (outcome prediction and severity assessment) of COVID-19 infection. More specifically, the article starts by elaborating the latest development on the theoretical framework of analytic epidemiology and hypersignal processing for COVID-19. Afterwards, imaging modalities and Radiological characteristics of COVID-19 are discussed. SL/DL-based Radiomic models specific to the analysis of COVID-19 infection are then described covering the following four domains: Segmentation of COVID-19 lesions; Predictive models for outcome prediction; Severity assessment, and; Diagnosis/classification models. Finally, open problems and opportunities are presented in detail.",cs.CV,Computer Vision
Joint Intensity-Gradient Guided Generative Modeling for Colorization,"This paper proposes an iterative generative model for solving the automatic colorization problem. Although previous researches have shown the capability to generate plausible color, the edge color overflow and the requirement of the reference images still exist. The starting point of the unsupervised learning in this study is the observation that the gradient map possesses latent information of the image. Therefore, the inference process of the generative modeling is conducted in joint intensity-gradient domain. Specifically, a set of intensity-gradient formed high-dimensional tensors, as the network input, are used to train a powerful noise conditional score network at the training phase. Furthermore, the joint intensity-gradient constraint in data-fidelity term is proposed to limit the degree of freedom within generative model at the iterative colorization stage, and it is conducive to edge-preserving. Extensive experiments demonstrated that the system outperformed state-of-the-art methods whether in quantitative comparisons or user study.",cs.CV,Computer Vision
Perception Consistency Ultrasound Image Super-resolution via Self-supervised CycleGAN,"Due to the limitations of sensors, the transmission medium and the intrinsic properties of ultrasound, the quality of ultrasound imaging is always not ideal, especially its low spatial resolution. To remedy this situation, deep learning networks have been recently developed for ultrasound image super-resolution (SR) because of the powerful approximation capability. However, most current supervised SR methods are not suitable for ultrasound medical images because the medical image samples are always rare, and usually, there are no low-resolution (LR) and high-resolution (HR) training pairs in reality. In this work, based on self-supervision and cycle generative adversarial network (CycleGAN), we propose a new perception consistency ultrasound image super-resolution (SR) method, which only requires the LR ultrasound data and can ensure the re-degenerated image of the generated SR one to be consistent with the original LR image, and vice versa. We first generate the HR fathers and the LR sons of the test ultrasound LR image through image enhancement, and then make full use of the cycle loss of LR-SR-LR and HR-LR-SR and the adversarial characteristics of the discriminator to promote the generator to produce better perceptually consistent SR results. The evaluation of PSNR/IFC/SSIM, inference efficiency and visual effects under the benchmark CCA-US and CCA-US datasets illustrate our proposed approach is effective and superior to other state-of-the-art methods.",cs.CV,Computer Vision
Action Recognition with Kernel-based Graph Convolutional Networks,"Learning graph convolutional networks (GCNs) is an emerging field which aims at generalizing deep learning to arbitrary non-regular domains. Most of the existing GCNs follow a neighborhood aggregation scheme, where the representation of a node is recursively obtained by aggregating its neighboring node representations using averaging or sorting operations. However, these operations are either ill-posed or weak to be discriminant or increase the number of training parameters and thereby the computational complexity and the risk of overfitting. In this paper, we introduce a novel GCN framework that achieves spatial graph convolution in a reproducing kernel Hilbert space (RKHS). The latter makes it possible to design, via implicit kernel representations, convolutional graph filters in a high dimensional and more discriminating space without increasing the number of training parameters. The particularity of our GCN model also resides in its ability to achieve convolutions without explicitly realigning nodes in the receptive fields of the learned graph filters with those of the input graphs, thereby making convolutions permutation agnostic and well defined. Experiments conducted on the challenging task of skeleton-based action recognition show the superiority of the proposed method against different baselines as well as the related work.",cs.CV,Computer Vision
Combining CNN and Hybrid Active Contours for Head and Neck Tumor Segmentation in CT and PET images,"Automatic segmentation of head and neck tumors plays an important role in radiomics analysis. In this short paper, we propose an automatic segmentation method for head and neck tumors from PET and CT images based on the combination of convolutional neural networks (CNNs) and hybrid active contours. Specifically, we first introduce a multi-channel 3D U-Net to segment the tumor with the concatenated PET and CT images. Then, we estimate the segmentation uncertainty by model ensembles and define a segmentation quality score to select the cases with high uncertainties. Finally, we develop a hybrid active contour model to refine the high uncertainty cases. Our method ranked second place in the MICCAI 2020 HECKTOR challenge with average Dice Similarity Coefficient, precision, and recall of 0.752, 0.838, and 0.717, respectively.",cs.CV,Computer Vision
PaXNet: Dental Caries Detection in Panoramic X-ray using Ensemble Transfer Learning and Capsule Classifier,"Dental caries is one of the most chronic diseases involving the majority of the population during their lifetime. Caries lesions are typically diagnosed by radiologists relying only on their visual inspection to detect via dental x-rays. In many cases, dental caries is hard to identify using x-rays and can be misinterpreted as shadows due to different reasons such as low image quality. Hence, developing a decision support system for caries detection has been a topic of interest in recent years. Here, we propose an automatic diagnosis system to detect dental caries in Panoramic images for the first time, to the best of authors' knowledge. The proposed model benefits from various pretrained deep learning models through transfer learning to extract relevant features from x-rays and uses a capsule network to draw prediction results. On a dataset of 470 Panoramic images used for features extraction, including 240 labeled images for classification, our model achieved an accuracy score of 86.05\% on the test set. The obtained score demonstrates acceptable detection performance and an increase in caries detection speed, as long as the challenges of using Panoramic x-rays of real patients are taken into account. Among images with caries lesions in the test set, our model acquired recall scores of 69.44\% and 90.52\% for mild and severe ones, confirming the fact that severe caries spots are more straightforward to detect and efficient mild caries detection needs a more robust and larger dataset. Considering the novelty of current research study as using Panoramic images, this work is a step towards developing a fully automated efficient decision support system to assist domain experts.",cs.CV,Computer Vision
One-Shot Object Localization Using Learnt Visual Cues via Siamese Networks,"A robot that can operate in novel and unstructured environments must be capable of recognizing new, previously unseen, objects. In this work, a visual cue is used to specify a novel object of interest which must be localized in new environments. An end-to-end neural network equipped with a Siamese network is used to learn the cue, infer the object of interest, and then to localize it in new environments. We show that a simulated robot can pick-and-place novel objects pointed to by a laser pointer. We also evaluate the performance of the proposed approach on a dataset derived from the Omniglot handwritten character dataset and on a small dataset of toys.",cs.CV,Computer Vision
Hybrid and Non-Uniform quantization methods using retro synthesis data for efficient inference,"Existing quantization aware training methods attempt to compensate for the quantization loss by leveraging on training data, like most of the post-training quantization methods, and are also time consuming. Both these methods are not effective for privacy constraint applications as they are tightly coupled with training data. In contrast, this paper proposes a data-independent post-training quantization scheme that eliminates the need for training data. This is achieved by generating a faux dataset, hereafter referred to as Retro-Synthesis Data, from the FP32 model layer statistics and further using it for quantization. This approach outperformed state-of-the-art methods including, but not limited to, ZeroQ and DFQ on models with and without Batch-Normalization layers for 8, 6, and 4 bit precisions on ImageNet and CIFAR-10 datasets. We also introduced two futuristic variants of post-training quantization methods namely Hybrid Quantization and Non-Uniform Quantization",cs.CV,Computer Vision
Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies,"Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study.",cs.CV,Computer Vision
IAN: Combining Generative Adversarial Networks for Imaginative Face Generation,"Generative Adversarial Networks (GANs) have gained momentum for their ability to model image distributions. They learn to emulate the training set and that enables sampling from that domain and using the knowledge learned for useful applications. Several methods proposed enhancing GANs, including regularizing the loss with some feature matching. We seek to push GANs beyond the data in the training and try to explore unseen territory in the image manifold. We first propose a new regularizer for GAN based on K-nearest neighbor (K-NN) selective feature matching to a target set Y in high-level feature space, during the adversarial training of GAN on the base set X, and we call this novel model K-GAN. We show that minimizing the added term follows from cross-entropy minimization between the distributions of GAN and the set Y. Then, We introduce a cascaded framework for GANs that try to address the task of imagining a new distribution that combines the base set X and target set Y by cascading sampling GANs with translation GANs, and we dub the cascade of such GANs as the Imaginative Adversarial Network (IAN). We conduct an objective and subjective evaluation for different IAN setups in the addressed task and show some useful applications for these IANs, like manifold traversing and creative face generation for characters' design in movies or video games.",cs.CV,Computer Vision
Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations,"We tackle the problem of semantic boundary prediction, which aims to identify pixels that belong to object(class) boundaries. We notice that relevant datasets consist of a significant level of label noise, reflecting the fact that precise annotations are laborious to get and thus annotators trade-off quality with efficiency. We aim to learn sharp and precise semantic boundaries by explicitly reasoning about annotation noise during training. We propose a simple new layer and loss that can be used with existing learning-based boundary detectors. Our layer/loss enforces the detector to predict a maximum response along the normal direction at an edge, while also regularizing its direction. We further reason about true object boundaries during training using a level set formulation, which allows the network to learn from misaligned labels in an end-to-end fashion. Experiments show that we improve over the CASENet backbone network by more than 4% in terms of MF(ODS) and 18.61% in terms of AP, outperforming all current state-of-the-art methods including those that deal with alignment. Furthermore, we show that our learned network can be used to significantly improve coarse segmentation labels, lending itself as an efficient way to label new data.",cs.CV,Computer Vision
DNN Architecture for High Performance Prediction on Natural Videos Loses Submodule's Ability to Learn Discrete-World Dataset,"Is cognition a collection of loosely connected functions tuned to different tasks, or can there be a general learning algorithm? If such an hypothetical general algorithm did exist, tuned to our world, could it adapt seamlessly to a world with different laws of nature? We consider the theory that predictive coding is such a general rule, and falsify it for one specific neural architecture known for high-performance predictions on natural videos and replication of human visual illusions: PredNet. Our results show that PredNet's high performance generalizes without retraining on a completely different natural video dataset. Yet PredNet cannot be trained to reach even mediocre accuracy on an artificial video dataset created with the rules of the Game of Life (GoL). We also find that a submodule of PredNet, a Convolutional Neural Network trained alone, reaches perfect accuracy on the GoL while being mediocre for natural videos, showing that PredNet's architecture itself is responsible for both the high performance on natural videos and the loss of performance on the GoL. Just as humans cannot predict the dynamics of the GoL, our results suggest that there might be a trade-off between high performance on sensory inputs with different sets of rules.",cs.CV,Computer Vision
Clustered Object Detection in Aerial Images,"Detecting objects in aerial images is challenging for at least two reasons: (1) target objects like pedestrians are very small in pixels, making them hardly distinguished from surrounding background; and (2) targets are in general sparsely and non-uniformly distributed, making the detection very inefficient. In this paper, we address both issues inspired by observing that these targets are often clustered. In particular, we propose a Clustered Detection (ClusDet) network that unifies object clustering and detection in an end-to-end framework. The key components in ClusDet include a cluster proposal sub-network (CPNet), a scale estimation sub-network (ScaleNet), and a dedicated detection network (DetecNet). Given an input image, CPNet produces object cluster regions and ScaleNet estimates object scales for these regions. Then, each scale-normalized cluster region is fed into DetecNet for object detection. ClusDet has several advantages over previous solutions: (1) it greatly reduces the number of chips for final object detection and hence achieves high running time efficiency, (2) the cluster-based scale estimation is more accurate than previously used single-object based ones, hence effectively improves the detection for small objects, and (3) the final DetecNet is dedicated for clustered regions and implicitly models the prior context information so as to boost detection accuracy. The proposed method is tested on three popular aerial image datasets including VisDrone, UAVDT and DOTA. In all experiments, ClusDet achieves promising performance in comparison with state-of-the-art detectors. Code will be available in \url{https://github.com/fyangneil}.",cs.CV,Computer Vision
People infer recursive visual concepts from just a few examples,"Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal ""programs"" -- latent generating processes with nontrivial algorithmic properties -- from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pre-trained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.",cs.CV,Computer Vision
Collaboration Analysis Using Deep Learning,"The analysis of the collaborative learning process is one of the growing fields of education research, which has many different analytic solutions. In this paper, we provided a new solution to improve automated collaborative learning analyses using deep neural networks. Instead of using self-reported questionnaires, which are subject to bias and noise, we automatically extract group-working information by object recognition results using Mask R-CNN method. This process is based on detecting the people and other objects from pictures and video clips of the collaborative learning process, then evaluate the mobile learning performance using the collaborative indicators. We tested our approach to automatically evaluate the group-work collaboration in a controlled study of thirty-three dyads while performing an anatomy body painting intervention. The results indicate that our approach recognizes the differences of collaborations among teams of treatment and control groups in the case study. This work introduces new methods for automated quality prediction of collaborations among human-human interactions using computer vision techniques.",cs.CV,Computer Vision
Automatic Dataset Augmentation Using Virtual Human Simulation,"Virtual Human Simulation has been widely used for different purposes, such as comfort or accessibility analysis. In this paper, we investigate the possibility of using this type of technique to extend the training datasets of pedestrians to be used with machine learning techniques. Our main goal is to verify if Computer Graphics (CG) images of virtual humans with a simplistic rendering can be efficient in order to augment datasets used for training machine learning methods. In fact, from a machine learning point of view, there is a need to collect and label large datasets for ground truth, which sometimes demands manual annotation. In addition, find out images and videos with real people and also provide ground truth of people detection and counting is not trivial. If CG images, which can have a ground truth automatically generated, can also be used as training in machine learning techniques for pedestrian detection and counting, it can certainly facilitate and optimize the whole process of event detection. In particular, we propose to parametrize virtual humans using a data-driven approach. Results demonstrated that using the extended datasets with CG images outperforms the results when compared to only real images sequences.",cs.CV,Computer Vision
DeepSWIR: A Deep Learning Based Approach for the Synthesis of Short-Wave InfraRed Band using Multi-Sensor Concurrent Datasets,"Convolutional Neural Network (CNN) is achieving remarkable progress in various computer vision tasks. In the past few years, the remote sensing community has observed Deep Neural Network (DNN) finally taking off in several challenging fields. In this study, we propose a DNN to generate a predefined High Resolution (HR) synthetic spectral band using an ensemble of concurrent Low Resolution (LR) bands and existing HR bands. Of particular interest, the proposed network, namely DeepSWIR, synthesizes Short-Wave InfraRed (SWIR) band at 5m Ground Sampling Distance (GSD) using Green (G), Red (R) and Near InfraRed (NIR) bands at both 24m and 5m GSD, and SWIR band at 24m GSD. To our knowledge, the highest spatial resolution of commercially deliverable SWIR band is at 7.5m GSD. Also, we propose a Gaussian feathering based image stitching approach in light of processing large satellite imagery. To experimentally validate the synthesized HR SWIR band, we critically analyse the qualitative and quantitative results produced by DeepSWIR using state-of-the-art evaluation metrics. Further, we convert the synthesized DN values to Top Of Atmosphere (TOA) reflectance and compare with the corresponding band of Sentinel-2B. Finally, we show one real world application of the synthesized band by using it to map wetland resources over our region of interest.",cs.CV,Computer Vision
Robust Dense Mapping for Large-Scale Dynamic Environments,"We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project website (http://andreibarsan.github.io/dynslam).",cs.CV,Computer Vision
Feature Selection and Feature Extraction in Pattern Analysis: A Literature Review,"Pattern analysis often requires a pre-processing stage for extracting or selecting features in order to help the classification, prediction, or clustering stage discriminate or represent the data in a better way. The reason for this requirement is that the raw data are complex and difficult to process without extracting or selecting appropriate features beforehand. This paper reviews theory and motivation of different common methods of feature selection and extraction and introduces some of their applications. Some numerical implementations are also shown for these methods. Finally, the methods in feature selection and extraction are compared.",cs.CV,Computer Vision
ShapeGlot: Learning Language for Shape Differentiation,"In this work we explore how fine-grained differences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We first build a large scale, carefully controlled dataset of human utterances that each refers to a 2D rendering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not), and the neural architecture (e.g. with or without attention). We find models that perform well with both synthetic and human partners, and with held out utterances and objects. We also find that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from training on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indicate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and linguistic structure when it comes to object differentiation.",cs.CV,Computer Vision
Visibility Constrained Generative Model for Depth-based 3D Facial Pose Tracking,"In this paper, we propose a generative framework that unifies depth-based 3D facial pose tracking and face model adaptation on-the-fly, in the unconstrained scenarios with heavy occlusions and arbitrary facial expression variations. Specifically, we introduce a statistical 3D morphable model that flexibly describes the distribution of points on the surface of the face model, with an efficient switchable online adaptation that gradually captures the identity of the tracked subject and rapidly constructs a suitable face model when the subject changes. Moreover, unlike prior art that employed ICP-based facial pose estimation, to improve robustness to occlusions, we propose a ray visibility constraint that regularizes the pose based on the face model's visibility with respect to the input point cloud. Ablation studies and experimental results on Biwi and ICT-3DHP datasets demonstrate that the proposed framework is effective and outperforms completing state-of-the-art depth-based methods.",cs.CV,Computer Vision
Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems,"Thanks to recent advances in deep neural networks (DNNs), face recognition systems have become highly accurate in classifying a large number of face images. However, recent studies have found that DNNs could be vulnerable to adversarial examples, raising concerns about the robustness of such systems. Adversarial examples that are not restricted to small perturbations could be more serious since conventional certified defenses might be ineffective against them. To shed light on the vulnerability to such adversarial examples, we propose a flexible and efficient method for generating unrestricted adversarial examples using image translation techniques. Our method enables us to translate a source image into any desired facial appearance with large perturbations to deceive target face recognition systems. Our experimental results indicate that our method achieved about $90$ and $80\%$ attack success rates under white- and black-box settings, respectively, and that the translated images are perceptually realistic and maintain the identifiability of the individual while the perturbations are large enough to bypass certified defenses.",cs.CV,Computer Vision
Two-Stage Convolutional Neural Network Architecture for Lung Nodule Detection,"Early detection of lung cancer is an effective way to improve the survival rate of patients. It is a critical step to have accurate detection of lung nodules in computed tomography (CT) images for the diagnosis of lung cancer. However, due to the heterogeneity of the lung nodules and the complexity of the surrounding environment, robust nodule detection has been a challenging task. In this study, we propose a two-stage convolutional neural network (TSCNN) architecture for lung nodule detection. The CNN architecture in the first stage is based on the improved UNet segmentation network to establish an initial detection of lung nodules. Simultaneously, in order to obtain a high recall rate without introducing excessive false positive nodules, we propose a novel sampling strategy, and use the offline hard mining idea for training and prediction according to the proposed cascaded prediction method. The CNN architecture in the second stage is based on the proposed dual pooling structure, which is built into three 3D CNN classification networks for false positive reduction. Since the network training requires a significant amount of training data, we adopt a data augmentation method based on random mask. Furthermore, we have improved the generalization ability of the false positive reduction model by means of ensemble learning. The proposed method has been experimentally verified on the LUNA dataset. Experimental results show that the proposed TSCNN architecture can obtain competitive detection performance.",cs.CV,Computer Vision
PiNet: A Permutation Invariant Graph Neural Network for Graph Classification,"We propose an end-to-end deep learning learning model for graph classification and representation learning that is invariant to permutation of the nodes of the input graphs. We address the challenge of learning a fixed size graph representation for graphs of varying dimensions through a differentiable node attention pooling mechanism. In addition to a theoretical proof of its invariance to permutation, we provide empirical evidence demonstrating the statistically significant gain in accuracy when faced with an isomorphic graph classification task given only a small number of training examples. We analyse the effect of four different matrices to facilitate the local message passing mechanism by which graph convolutions are performed vs. a matrix parametrised by a learned parameter pair able to transition smoothly between the former. Finally, we show that our model achieves competitive classification performance with existing techniques on a set of molecule datasets.",cs.CV,Computer Vision
Training a Fast Object Detector for LiDAR Range Images Using Labeled Data from Sensors with Higher Resolution,"In this paper, we describe a strategy for training neural networks for object detection in range images obtained from one type of LiDAR sensor using labeled data from a different type of LiDAR sensor. Additionally, an efficient model for object detection in range images for use in self-driving cars is presented. Currently, the highest performing algorithms for object detection from LiDAR measurements are based on neural networks. Training these networks using supervised learning requires large annotated datasets. Therefore, most research using neural networks for object detection from LiDAR point clouds is conducted on a very small number of publicly available datasets. Consequently, only a small number of sensor types are used. We use an existing annotated dataset to train a neural network that can be used with a LiDAR sensor that has a lower resolution than the one used for recording the annotated dataset. This is done by simulating data from the lower resolution LiDAR sensor based on the higher resolution dataset. Furthermore, improvements to models that use LiDAR range images for object detection are presented. The results are validated using both simulated sensor data and data from an actual lower resolution sensor mounted to a research vehicle. It is shown that the model can detect objects from 360 range images in real time.",cs.CV,Computer Vision
Image-based reconstruction for the impact problems by using DPNNs,"With the improvement of the pattern recognition and feature extraction of Deep Neural Networks (DPNNs), image-based design and optimization have been widely used in multidisciplinary researches. Recently, a Reconstructive Neural Network (ReConNN) has been proposed to obtain an image-based model from an analysis-based model [1, 2], and a steady-state heat transfer of a heat sink has been successfully reconstructed. Commonly, this method is suitable to handle stable-state problems. However, it has difficulties handling nonlinear transient impact problems, due to the bottlenecks of the Deep Neural Network (DPNN). For example, nonlinear transient problems make it difficult for the Generative Adversarial Network (GAN) to generate various reasonable images. Therefore, in this study, an improved ReConNN method is proposed to address the mentioned weaknesses. Time-dependent ordered images can be generated. Furthermore, the improved method is successfully applied in impact simulation case and engineering experiment. Through the experiments, comparisons and analyses, the improved method is demonstrated to outperform the former one in terms of its accuracy, efficiency and costs.",cs.CV,Computer Vision
Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency,"We present a learning based approach for multi-view stereopsis (MVS). While current deep MVS methods achieve impressive results, they crucially rely on ground-truth 3D training data, and acquisition of such precise 3D geometry for supervision is a major hurdle. Our framework instead leverages photometric consistency between multiple views as supervisory signal for learning depth prediction in a wide baseline MVS setup. However, naively applying photo consistency constraints is undesirable due to occlusion and lighting changes across views. To overcome this, we propose a robust loss formulation that: a) enforces first order consistency and b) for each point, selectively enforces consistency with some views, thus implicitly handling occlusions. We demonstrate our ability to learn MVS without 3D supervision using a real dataset, and show that each component of our proposed robust loss results in a significant improvement. We qualitatively observe that our reconstructions are often more complete than the acquired ground truth, further showing the merits of this approach. Lastly, our learned model generalizes to novel settings, and our approach allows adaptation of existing CNNs to datasets without ground-truth 3D by unsupervised finetuning. Project webpage: https://tejaskhot.github.io/unsup_mvs",cs.CV,Computer Vision
Image Matters: Scalable Detection of Offensive and Non-Compliant Content / Logo in Product Images,"In e-commerce, product content, especially product images have a significant influence on a customer's journey from product discovery to evaluation and finally, purchase decision. Since many e-commerce retailers sell items from other third-party marketplace sellers besides their own, the content published by both internal and external content creators needs to be monitored and enriched, wherever possible. Despite guidelines and warnings, product listings that contain offensive and non-compliant images continue to enter catalogs. Offensive and non-compliant content can include a wide range of objects, logos, and banners conveying violent, sexually explicit, racist, or promotional messages. Such images can severely damage the customer experience, lead to legal issues, and erode the company brand. In this paper, we present a computer vision driven offensive and non-compliant image detection system for extremely large image datasets. This paper delves into the unique challenges of applying deep learning to real-world product image data from retail world. We demonstrate how we resolve a number of technical challenges such as lack of training data, severe class imbalance, fine-grained class definitions etc. using a number of practical yet unique technical strategies. Our system combines state-of-the-art image classification and object detection techniques with budgeted crowdsourcing to develop a solution customized for a massive, diverse, and constantly evolving product catalog.",cs.CV,Computer Vision
Searching for MobileNetV3,"We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2\% more accurate on ImageNet classification while reducing latency by 15\% compared to MobileNetV2. MobileNetV3-Small is 4.6\% more accurate while reducing latency by 5\% compared to MobileNetV2. MobileNetV3-Large detection is 25\% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 30\% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.",cs.CV,Computer Vision
Source Generator Attribution via Inversion,"With advances in Generative Adversarial Networks (GANs) leading to dramatically-improved synthetic images and video, there is an increased need for algorithms which extend traditional forensics to this new category of imagery. While GANs have been shown to be helpful in a number of computer vision applications, there are other problematic uses such as `deep fakes' which necessitate such forensics. Source camera attribution algorithms using various cues have addressed this need for imagery captured by a camera, but there are fewer options for synthetic imagery. We address the problem of attributing a synthetic image to a specific generator in a white box setting, by inverting the process of generation. This enables us to simultaneously determine whether the generator produced the image and recover an input which produces a close match to the synthetic image.",cs.CV,Computer Vision
Frame-wise Motion and Appearance for Real-time Multiple Object Tracking,"The main challenge of Multiple Object Tracking (MOT) is the efficiency in associating indefinite number of objects between video frames. Standard motion estimators used in tracking, e.g., Long Short Term Memory (LSTM), only deal with single object, while Re-IDentification (Re-ID) based approaches exhaustively compare object appearances. Both approaches are computationally costly when they are scaled to a large number of objects, making it very difficult for real-time MOT. To address these problems, we propose a highly efficient Deep Neural Network (DNN) that simultaneously models association among indefinite number of objects. The inference computation of the DNN does not increase with the number of objects. Our approach, Frame-wise Motion and Appearance (FMA), computes the Frame-wise Motion Fields (FMF) between two frames, which leads to very fast and reliable matching among a large number of object bounding boxes. As auxiliary information is used to fix uncertain matches, Frame-wise Appearance Features (FAF) are learned in parallel with FMFs. Extensive experiments on the MOT17 benchmark show that our method achieved real-time MOT with competitive results as the state-of-the-art approaches.",cs.CV,Computer Vision
Semantic Adversarial Network for Zero-Shot Sketch-Based Image Retrieval,"Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal retrieval task for retrieving natural images with free-hand sketches under zero-shot scenario. Previous works mostly focus on modeling the correspondence between images and sketches or synthesizing image features with sketch features. However, both of them ignore the large intra-class variance of sketches, thus resulting in unsatisfactory retrieval performance. In this paper, we propose a novel end-to-end semantic adversarial approach for ZS-SBIR. Specifically, we devise a semantic adversarial module to maximize the consistency between learned semantic features and category-level word vectors. Moreover, to preserve the discriminability of synthesized features within each training category, a triplet loss is employed for the generative module. Additionally, the proposed model is trained in an end-to-end strategy to exploit better semantic features suitable for ZS-SBIR. Extensive experiments conducted on two large-scale popular datasets demonstrate that our proposed approach remarkably outperforms state-of-the-art approaches by more than 12\% on Sketchy dataset and about 3\% on TU-Berlin dataset in the retrieval.",cs.CV,Computer Vision
Simulating CRF with CNN for CNN,"Combining CNN with CRF for modeling dependencies between pixel labels is a popular research direction. This task is far from trivial, especially if end-to-end training is desired. In this paper, we propose a novel simple approach to CNN+CRF combination. In particular, we propose to simulate a CRF regularizer with a trainable module that has standard CNN architecture. We call this module a CRF Simulator. We can automatically generate an unlimited amount of ground truth for training such CRF Simulator without any user interaction, provided we have an efficient algorithm for optimization of the actual CRF regularizer. After our CRF Simulator is trained, it can be directly incorporated as part of any larger CNN architecture, enabling a seamless end-to-end training. In particular, the other modules can learn parameters that are more attuned to the performance of the CRF Simulator module. We demonstrate the effectiveness of our approach on the task of salient object segmentation regularized with the standard binary CRF energy. In contrast to previous work we do not need to develop and implement the complex mechanics of optimizing a specific CRF as part of CNN. In fact, our approach can be easily extended to other CRF energies, including multi-label. To the best of our knowledge we are the first to study the question of whether the output of CNNs can have regularization properties of CRFs.",cs.CV,Computer Vision
CARAFE: Content-Aware ReAssembly of FEatures,"Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit sub-pixel neighborhood, CARAFE can aggregate contextual information within a large receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively) with negligible computational overhead. It has great potential to serve as a strong building block for future research. It has great potential to serve as a strong building block for future research. Code and models are available at https://github.com/open-mmlab/mmdetection.",cs.CV,Computer Vision
Back to the Future: Predicting Traffic Shockwave Formation and Propagation Using a Convolutional Encoder-Decoder Network,"This study proposes a deep learning methodology to predict the propagation of traffic shockwaves. The input to the deep neural network is time-space diagram of the study segment, and the output of the network is the predicted (future) propagation of the shockwave on the study segment in the form of time-space diagram. The main feature of the proposed methodology is the ability to extract the features embedded in the time-space diagram to predict the propagation of traffic shockwaves.",cs.CV,Computer Vision
ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes on the Blockchain,"We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives. First, we describe a novel deep network architecture for computing compact temporal content hashes (TCHs) from audio-visual streams with durations of minutes or hours. Our TCHs are sensitive to accidental or malicious content modification (tampering) but invariant to the codec used to encode the video. This is necessary due to the curatorial requirement for archives to format shift video over time to ensure future accessibility. Second, we describe how the TCHs (and the models used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, Estonia and Norway participated.",cs.CV,Computer Vision
Human-Centered Emotion Recognition in Animated GIFs,"As an intuitive way of expression emotion, the animated Graphical Interchange Format (GIF) images have been widely used on social media. Most previous studies on automated GIF emotion recognition fail to effectively utilize GIF's unique properties, and this potentially limits the recognition performance. In this study, we demonstrate the importance of human related information in GIFs and conduct human-centered GIF emotion recognition with a proposed Keypoint Attended Visual Attention Network (KAVAN). The framework consists of a facial attention module and a hierarchical segment temporal module. The facial attention module exploits the strong relationship between GIF contents and human characters, and extracts frame-level visual feature with a focus on human faces. The Hierarchical Segment LSTM (HS-LSTM) module is then proposed to better learn global GIF representations. Our proposed framework outperforms the state-of-the-art on the MIT GIFGIF dataset. Furthermore, the facial attention module provides reliable facial region mask predictions, which improves the model's interpretability.",cs.CV,Computer Vision
Semantic Referee: A Neural-Symbolic Framework for Enhancing Geospatial Semantic Segmentation,"Understanding why machine learning algorithms may fail is usually the task of the human expert that uses domain knowledge and contextual information to discover systematic shortcomings in either the data or the algorithm. In this paper, we propose a semantic referee, which is able to extract qualitative features of the errors emerging from deep machine learning frameworks and suggest corrections. The semantic referee relies on ontological reasoning about spatial knowledge in order to characterize errors in terms of their spatial relations with the environment. Using semantics, the reasoner interacts with the learning algorithm as a supervisor. In this paper, the proposed method of the interaction between a neural network classifier and a semantic referee shows how to improve the performance of semantic segmentation for satellite imagery data.",cs.CV,Computer Vision
Detecting Reflections by Combining Semantic and Instance Segmentation,"Reflections in natural images commonly cause false positives in automated detection systems. These false positives can lead to significant impairment of accuracy in the tasks of detection, counting and segmentation. Here, inspired by the recent panoptic approach to segmentation, we show how fusing instance and semantic segmentation can automatically identify reflection false positives, without explicitly needing to have the reflective regions labelled. We explore in detail how state of the art two-stage detectors suffer a loss of broader contextual features, and hence are unable to learn to ignore these reflections. We then present an approach to fuse instance and semantic segmentations for this application, and subsequently show how this reduces false positive detections in a real world surveillance data with a large number of reflective surfaces. This demonstrates how panoptic segmentation and related work, despite being in its infancy, can already be useful in real world computer vision problems.",cs.CV,Computer Vision
Patch alignment manifold matting,"Image matting is generally modeled as a space transform from the color space to the alpha space. By estimating the alpha factor of the model, the foreground of an image can be extracted. However, there is some dimensional information redundancy in the alpha space. It usually leads to the misjudgments of some pixels near the boundary between the foreground and the background. In this paper, a manifold matting framework named Patch Alignment Manifold Matting is proposed for image matting. In particular, we first propose a part modeling of color space in the local image patch. We then perform whole alignment optimization for approximating the alpha results using subspace reconstructing error. Furthermore, we utilize Nesterov's algorithm to solve the optimization problem. Finally, we apply some manifold learning methods in the framework, and obtain several image matting methods, such as named ISOMAP matting and its derived Cascade ISOMAP matting. The experimental results reveal that the manifold matting framework and its two examples are effective when compared with several representative matting methods.",cs.CV,Computer Vision
Relation-Shape Convolutional Neural Network for Point Cloud Analysis,"Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.",cs.CV,Computer Vision
Semantically Aligned Bias Reducing Zero Shot Learning,"Zero shot learning (ZSL) aims to recognize unseen classes by exploiting semantic relationships between seen and unseen classes. Two major problems faced by ZSL algorithms are the hubness problem and the bias towards the seen classes. Existing ZSL methods focus on only one of these problems in the conventional and generalized ZSL setting. In this work, we propose a novel approach, Semantically Aligned Bias Reducing (SABR) ZSL, which focuses on solving both the problems. It overcomes the hubness problem by learning a latent space that preserves the semantic relationship between the labels while encoding the discriminating information about the classes. Further, we also propose ways to reduce the bias of the seen classes through a simple cross-validation process in the inductive setting and a novel weak transfer constraint in the transductive setting. Extensive experiments on three benchmark datasets suggest that the proposed model significantly outperforms existing state-of-the-art algorithms by ~1.5-9% in the conventional ZSL setting and by ~2-14% in the generalized ZSL for both the inductive and transductive settings.",cs.CV,Computer Vision
Multi-Scale Geometric Consistency Guided Multi-View Stereo,"In this paper, we propose an efficient multi-scale geometric consistency guided multi-view stereo method for accurate and complete depth map estimation. We first present our basic multi-view stereo method with Adaptive Checkerboard sampling and Multi-Hypothesis joint view selection (ACMH). It leverages structured region information to sample better candidate hypotheses for propagation and infer the aggregation view subset at each pixel. For the depth estimation of low-textured areas, we further propose to combine ACMH with multi-scale geometric consistency guidance (ACMM) to obtain the reliable depth estimates for low-textured areas at coarser scales and guarantee that they can be propagated to finer scales. To correct the erroneous estimates propagated from the coarser scales, we present a novel detail restorer. Experiments on extensive datasets show our method achieves state-of-the-art performance, recovering the depth estimation not only in low-textured areas but also in details.",cs.CV,Computer Vision
Automated Design of Deep Learning Methods for Biomedical Image Segmentation,"Biomedical imaging is a driver of scientific discovery and core component of medical care, currently stimulated by the field of deep learning. While semantic segmentation algorithms enable 3D image analysis and quantification in many applications, the design of respective specialised solutions is non-trivial and highly dependent on dataset properties and hardware conditions. We propose nnU-Net, a deep learning framework that condenses the current domain knowledge and autonomously takes the key decisions required to transfer a basic architecture to different datasets and segmentation tasks. Without manual tuning, nnU-Net surpasses most specialised deep learning pipelines in 19 public international competitions and sets a new state of the art in the majority of the 49 tasks. The results demonstrate a vast hidden potential in the systematic adaptation of deep learning methods to different datasets. We make nnU-Net publicly available as an open-source tool that can effectively be used out-of-the-box, rendering state of the art segmentation accessible to non-experts and catalyzing scientific progress as a framework for automated method design.",cs.CV,Computer Vision
Downhole Track Detection via Multiscale Conditional Generative Adversarial Nets,"Frequent mine disasters cause a large number of casualties and property losses. Autonomous driving is a fundamental measure for solving this problem, and track detection is one of the key technologies for computer vision to achieve downhole automatic driving. The track detection result based on the traditional convolutional neural network (CNN) algorithm lacks the detailed and unique description of the object and relies too much on visual postprocessing technology. Therefore, this paper proposes a track detection algorithm based on a multiscale conditional generative adversarial network (CGAN). The generator is decomposed into global and local parts using a multigranularity structure in the generator network. A multiscale shared convolution structure is adopted in the discriminator network to further supervise training the generator. Finally, the Monte Carlo search technique is introduced to search the intermediate state of the generator, and the result is sent to the discriminator for comparison. Compared with the existing work, our model achieved 82.43\% pixel accuracy and an average intersection-over-union (IOU) of 0.6218, and the detection of the track reached 95.01\% accuracy in the downhole roadway scene test set.",cs.CV,Computer Vision
CenterNet: Keypoint Triplets for Object Detection,"In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of 47.0%, which outperforms all existing one-stage detectors by at least 4.9%. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at https://github.com/Duankaiwen/CenterNet.",cs.CV,Computer Vision
LO-Net: Deep Real-time Lidar Odometry,"We present a novel deep convolutional network pipeline, LO-Net, for real-time lidar odometry estimation. Unlike most existing lidar odometry (LO) estimations that go through individually designed feature selection, feature matching, and pose estimation pipeline, LO-Net can be trained in an end-to-end manner. With a new mask-weighted geometric constraint loss, LO-Net can effectively learn feature representation for LO estimation, and can implicitly exploit the sequential dependencies and dynamics in the data. We also design a scan-to-map module, which uses the geometric and semantic information learned in LO-Net, to improve the estimation accuracy. Experiments on benchmark datasets demonstrate that LO-Net outperforms existing learning based approaches and has similar accuracy with the state-of-the-art geometry-based approach, LOAM.",cs.CV,Computer Vision
BS-Nets: An End-to-End Framework For Band Selection of Hyperspectral Image,"Hyperspectral image (HSI) consists of hundreds of continuous narrow bands with high spectral correlation, which would lead to the so-called Hughes phenomenon and the high computational cost in processing. Band selection has been proven effective in avoiding such problems by removing the redundant bands. However, many of existing band selection methods separately estimate the significance for every single band and cannot fully consider the nonlinear and global interaction between spectral bands. In this paper, by assuming that a complete HSI can be reconstructed from its few informative bands, we propose a general band selection framework, Band Selection Network (termed as BS-Net). The framework consists of a band attention module (BAM), which aims to explicitly model the nonlinear inter-dependencies between spectral bands, and a reconstruction network (RecNet), which is used to restore the original HSI cube from the learned informative bands, resulting in a flexible architecture. The resulting framework is end-to-end trainable, making it easier to train from scratch and to combine with existing networks. We implement two BS-Nets respectively using fully connected networks (BS-Net-FC) and convolutional neural networks (BS-Net-Conv), and compare the results with many existing band selection approaches for three real hyperspectral images, demonstrating that the proposed BS-Nets can accurately select informative band subset with less redundancy and achieve significantly better classification performance with an acceptable time cost.",cs.CV,Computer Vision
Sex-Prediction from Periocular Images across Multiple Sensors and Spectra,"In this paper, we provide a comprehensive analysis of periocular-based sex-prediction (commonly referred to as gender classification) using state-of-the-art machine learning techniques. In order to reflect a more challenging scenario where periocular images are likely to be obtained from an unknown source, i.e. sensor, convolutional neural networks are trained on fused sets composed of several near-infrared (NIR) and visible wavelength (VW) image databases. In a cross-sensor scenario within each spectrum an average classification accuracy of approximately 85\% is achieved. When sex-prediction is performed across spectra an average classification accuracy of about 82\% is obtained. Finally, a multi-spectral sex-prediction yields a classification accuracy of 83\% on average. Compared to proposed works, obtained results provide a more realistic estimation of the feasibility to predict a subject's sex from the periocular region.",cs.CV,Computer Vision
Globally optimal vertical direction estimation in Atlanta World,"In man-made environments, such as indoor and urban scenes, most of the objects and structures are organized in the form of orthogonal and parallel planes. These planes can be approximated by the Atlanta world assumption, in which the normals of planes can be represented by the Atlanta frames. Atlanta world assumption, which can be considered as a generalized Manhattan world assumption, has one vertical frame and multiple horizontal frames. Conventionally, given a set of inputs such as surface normals, the Atlanta frame estimation problem can be solved in one-time by branch-and-bound (BnB). However, the runtime of the BnB algorithm will increase greatly when the dimensionality (i.e., the number of horizontal frames) increases. In this paper, we estimate only the vertical direction instead of all Atlanta frames at once. Accordingly, we propose a vertical direction estimation method by considering the relationship between the vertical frame and horizontal frames. Concretely, our approach employs a BnB algorithm to search the vertical direction guaranteeing global optimality without requiring prior knowledge of the number of Atlanta frames. Four novel bounds by mapping 3D-hemisphere to a 2D region are investigated to guarantee convergence. We verify the validity of the proposed method in various challenging synthetic and real-world data.",cs.CV,Computer Vision
Solo or Ensemble? Choosing a CNN Architecture for Melanoma Classification,"Convolutional neural networks (CNNs) deliver exceptional results for computer vision, including medical image analysis. With the growing number of available architectures, picking one over another is far from obvious. Existing art suggests that, when performing transfer learning, the performance of CNN architectures on ImageNet correlates strongly with their performance on target tasks. We evaluate that claim for melanoma classification, over 9 CNNs architectures, in 5 sets of splits created on the ISIC Challenge 2017 dataset, and 3 repeated measures, resulting in 135 models. The correlations we found were, to begin with, much smaller than those reported by existing art, and disappeared altogether when we considered only the top-performing networks: uncontrolled nuisances (i.e., splits and randomness) overcome any of the analyzed factors. Whenever possible, the best approach for melanoma classification is still to create ensembles of multiple models. We compared two choices for selecting which models to ensemble: picking them at random (among a pool of high-quality ones) vs. using the validation set to determine which ones to pick first. For small ensembles, we found a slight advantage on the second approach but found that random choice was also competitive. Although our aim in this paper was not to maximize performance, we easily reached AUCs comparable to the first place on the ISIC Challenge 2017.",cs.CV,Computer Vision
Multi-scale Microaneurysms Segmentation Using Embedding Triplet Loss,"Deep learning techniques are recently being used in fundus image analysis and diabetic retinopathy detection. Microaneurysms are an important indicator of diabetic retinopathy progression. We introduce a two-stage deep learning approach for microaneurysms segmentation using multiple scales of the input with selective sampling and embedding triplet loss. The model first segments on two scales and then the segmentations are refined with a classification model. To enhance the discriminative power of the classification model, we incorporate triplet embedding loss with a selective sampling routine. The model is evaluated quantitatively to assess the segmentation performance and qualitatively to analyze the model predictions. This approach introduces a 30.29% relative improvement over the fully convolutional neural network.",cs.CV,Computer Vision
DeepHMap++: Combined Projection Grouping and Correspondence Learning for Full DoF Pose Estimation,"In recent years, estimating the 6D pose of object instances with convolutional neural network (CNN) has received considerable attention. Depending on whether intermediate cues are used, the relevant literature can be roughly divided into two broad categories: direct methods and two stage pipelines. For the latter, intermediate cues, such as 3D object coordinates, semantic keypoints, or virtual control points instead of pose parameters are regressed by CNN in the first stage. Object pose can then be solved by correspondence constraints constructed with these intermediate cues. In this paper, we focus on the postprocessing of a two-stage pipeline and propose to combine two learning concepts for estimating object pose under challenging scenes: projection grouping on one side, and correspondence learning on the other. We firstly employ a local patch based method to predict projection heatmaps which denote the confidence distribution of projection of 3D bounding box's corners. A projection grouping module is then proposed to remove redundant local maxima from each layer of heatmaps. Instead of directly feeding 2D-3D correspondences to the perspective-n-point (PnP) algorithm, multiple correspondence hypotheses are sampled from local maxima and its corresponding neighborhood and ranked by a correspondence-evaluation network. Finally, correspondences with higher confidence are selected to determine object pose. Extensive experiments on three public datasets demonstrate that the proposed framework outperforms several state of the art methods.",cs.CV,Computer Vision
Style Transfer by Relaxed Optimal Transport and Self-Similarity,"Style transfer algorithms strive to render the content of one image using the style of another. We propose Style Transfer by Relaxed Optimal Transport and Self-Similarity (STROTSS), a new optimization-based style transfer algorithm. We extend our method to allow user-specified point-to-point or region-to-region control over visual similarity between the style image and the output. Such guidance can be used to either achieve a particular visual effect or correct errors made by unconstrained style transfer. In order to quantitatively compare our method to prior work, we conduct a large-scale user study designed to assess the style-content tradeoff across settings in style transfer algorithms. Our results indicate that for any desired level of content preservation, our method provides higher quality stylization than prior work. Code is available at https://github.com/nkolkin13/STROTSS",cs.CV,Computer Vision
Unsupervised Data Augmentation for Consistency Training,"Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",cs.CV,Computer Vision
Learning to Find Common Objects Across Few Image Collections,"Given a collection of bags where each bag is a set of images, our goal is to select one image from each bag such that the selected images are from the same object class. We model the selection as an energy minimization problem with unary and pairwise potential functions. Inspired by recent few-shot learning algorithms, we propose an approach to learn the potential functions directly from the data. Furthermore, we propose a fast greedy inference algorithm for energy minimization. We evaluate our approach on few-shot common object recognition as well as object co-localization tasks. Our experiments show that learning the pairwise and unary terms greatly improves the performance of the model over several well-known methods for these tasks. The proposed greedy optimization algorithm achieves performance comparable to state-of-the-art structured inference algorithms while being ~10 times faster. The code is publicly available on https://github.com/haamoon/finding_common_object.",cs.CV,Computer Vision
SurfelWarp: Efficient Non-Volumetric Single View Dynamic Reconstruction,"We contribute a dense SLAM system that takes a live stream of depth images as input and reconstructs non-rigid deforming scenes in real time, without templates or prior models. In contrast to existing approaches, we do not maintain any volumetric data structures, such as truncated signed distance function (TSDF) fields or deformation fields, which are performance and memory intensive. Our system works with a flat point (surfel) based representation of geometry, which can be directly acquired from commodity depth sensors. Standard graphics pipelines and general purpose GPU (GPGPU) computing are leveraged for all central operations: i.e., nearest neighbor maintenance, non-rigid deformation field estimation and fusion of depth measurements. Our pipeline inherently avoids expensive volumetric operations such as marching cubes, volumetric fusion and dense deformation field update, leading to significantly improved performance. Furthermore, the explicit and flexible surfel based geometry representation enables efficient tackling of topology changes and tracking failures, which makes our reconstructions consistent with updated depth observations. Our system allows robots to maintain a scene description with non-rigidly deformed objects that potentially enables interactions with dynamic working environments.",cs.CV,Computer Vision
Anomaly Detection in Traffic Scenes via Spatial-aware Motion Reconstruction,"Anomaly detection from a driver's perspective when driving is important to autonomous vehicles. As a part of Advanced Driver Assistance Systems (ADAS), it can remind the driver about dangers timely. Compared with traditional studied scenes such as the university campus and market surveillance videos, it is difficult to detect abnormal event from a driver's perspective due to camera waggle, abidingly moving background, drastic change of vehicle velocity, etc. To tackle these specific problems, this paper proposes a spatial localization constrained sparse coding approach for anomaly detection in traffic scenes, which firstly measures the abnormality of motion orientation and magnitude respectively and then fuses these two aspects to obtain a robust detection result. The main contributions are threefold: 1) This work describes the motion orientation and magnitude of the object respectively in a new way, which is demonstrated to be better than the traditional motion descriptors. 2) The spatial localization of object is taken into account of the sparse reconstruction framework, which utilizes the scene's structural information and outperforms the conventional sparse coding methods. 3) Results of motion orientation and magnitude are adaptively weighted and fused by a Bayesian model, which makes the proposed method more robust and handle more kinds of abnormal events. The efficiency and effectiveness of the proposed method are validated by testing on nine difficult video sequences captured by ourselves. Observed from the experimental results, the proposed method is more effective and efficient than the popular competitors, and yields a higher performance.",cs.CV,Computer Vision
State Classification of Cooking Objects Using a VGG CNN,"In machine learning, it is very important for a robot to know the state of an object and recognize particular desired states. This is an image classification problem that can be solved using a convolutional neural network. In this paper, we will discuss the use of a VGG convolutional neural network to recognize those states of cooking objects. We will discuss the uses of activation functions, optimizers, data augmentation, layer additions, and other different versions of architectures. The results of this paper will be used to identify alternatives to the VGG convolutional neural network to improve accuracy.",cs.CV,Computer Vision
PYRO-NN: Python Reconstruction Operators in Neural Networks,"Purpose: Recently, several attempts were conducted to transfer deep learning to medical image reconstruction. An increasingly number of publications follow the concept of embedding the CT reconstruction as a known operator into a neural network. However, most of the approaches presented lack an efficient CT reconstruction framework fully integrated into deep learning environments. As a result, many approaches are forced to use workarounds for mathematically unambiguously solvable problems. Methods: PYRO-NN is a generalized framework to embed known operators into the prevalent deep learning framework Tensorflow. The current status includes state-of-the-art parallel-, fan- and cone-beam projectors and back-projectors accelerated with CUDA provided as Tensorflow layers. On top, the framework provides a high level Python API to conduct FBP and iterative reconstruction experiments with data from real CT systems. Results: The framework provides all necessary algorithms and tools to design end-to-end neural network pipelines with integrated CT reconstruction algorithms. The high level Python API allows a simple use of the layers as known from Tensorflow. To demonstrate the capabilities of the layers, the framework comes with three baseline experiments showing a cone-beam short scan FDK reconstruction, a CT reconstruction filter learning setup, and a TV regularized iterative reconstruction. All algorithms and tools are referenced to a scientific publication and are compared to existing non deep learning reconstruction frameworks. The framework is available as open-source software at \url{https://github.com/csyben/PYRO-NN}. Conclusions: PYRO-NN comes with the prevalent deep learning framework Tensorflow and allows to setup end-to-end trainable neural networks in the medical image reconstruction context. We believe that the framework will be a step towards reproducible research",cs.CV,Computer Vision
GazeCorrection:Self-Guided Eye Manipulation in the wild using Self-Supervised Generative Adversarial Networks,"Gaze correction aims to redirect the person's gaze into the camera by manipulating the eye region, and it can be considered as a specific image resynthesis problem. Gaze correction has a wide range of applications in real life, such as taking a picture with staring at the camera. In this paper, we propose a novel method that is based on the inpainting model to learn from the face image to fill in the missing eye regions with new contents representing corrected eye gaze. Moreover, our model does not require the training dataset labeled with the specific head pose and eye angle information, thus, the training data is easy to collect. To retain the identity information of the eye region in the original input, we propose a self-guided pretrained model to learn the angle-invariance feature. Experiments show our model achieves very compelling gaze-corrected results in the wild dataset which is collected from the website and will be introduced in details. Code is available at https://github.com/zhangqianhui/GazeCorrection.",cs.CV,Computer Vision
Self-supervised Body Image Acquisition Using a Deep Neural Network for Sensorimotor Prediction,"This work investigates how a naive agent can acquire its own body image in a self-supervised way, based on the predictability of its sensorimotor experience. Our working hypothesis is that, due to its temporal stability, an agent's body produces more consistent sensory experiences than the environment, which exhibits a greater variability. Given its motor experience, an agent can thus reliably predict what appearance its body should have. This intrinsic predictability can be used to automatically isolate the body image from the rest of the environment. We propose a two-branches deconvolutional neural network to predict the visual sensory state associated with an input motor state, as well as the prediction error associated with this input. We train the network on a dataset of first-person images collected with a simulated Pepper robot, and show how the network outputs can be used to automatically isolate its visible arm from the rest of the environment. Finally, the quality of the body image produced by the network is evaluated.",cs.CV,Computer Vision
From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots,"The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behavior, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image-pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach significantly outperforms other state-of-the-art methods.",cs.CV,Computer Vision
A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation,"Action recognition has become a rapidly developing research field within the last decade. But with the increasing demand for large scale data, the need of hand annotated data for the training becomes more and more impractical. One way to avoid frame-based human annotation is the use of action order information to learn the respective action classes. In this context, we propose a hierarchical approach to address the problem of weakly supervised learning of human actions from ordered action labels by structuring recognition in a coarse-to-fine manner. Given a set of videos and an ordered list of the occurring actions, the task is to infer start and end frames of the related action classes within the video and to train the respective action classifiers without any need for hand labeled frame boundaries. We address this problem by combining a framewise RNN model with a coarse probabilistic inference. This combination allows for the temporal alignment of long sequences and thus, for an iterative training of both elements. While this system alone already generates good results, we show that the performance can be further improved by approximating the number of subactions to the characteristics of the different action classes as well as by the introduction of a regularizing length prior. The proposed system is evaluated on two benchmark datasets, the Breakfast and the Hollywood extended dataset, showing a competitive performance on various weak learning tasks such as temporal action segmentation and action alignment.",cs.CV,Computer Vision
Transfer Learning with intelligent training data selection for prediction of Alzheimer's Disease,"Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI through machine learning has been a subject of intense research in recent years. Recent success of deep learning in computer vision has progressed such research further. However, common limitations with such algorithms are reliance on a large number of training images, and requirement of careful optimization of the architecture of deep networks. In this paper, we attempt solving these issues with transfer learning, where the state-of-the-art VGG architecture is initialized with pre-trained weights from large benchmark datasets consisting of natural images. The network is then fine-tuned with layer-wise tuning, where only a pre-defined group of layers are trained on MRI images. To shrink the training data size, we employ image entropy to select the most informative slices. Through experimentation on the ADNI dataset, we show that with training size of 10 to 20 times smaller than the other contemporary methods, we reach state-of-the-art performance in AD vs. NC, AD vs. MCI, and MCI vs. NC classification problems, with a 4% and a 7% increase in accuracy over the state-of-the-art for AD vs. MCI and MCI vs. NC, respectively. We also provide detailed analysis of the effect of the intelligent training data selection method, changing the training size, and changing the number of layers to be fine-tuned. Finally, we provide Class Activation Maps (CAM) that demonstrate how the proposed model focuses on discriminative image regions that are neuropathologically relevant, and can help the healthcare practitioner in interpreting the model's decision making process.",cs.CV,Computer Vision
Evaluation of an AI system for the automated detection of glaucoma from stereoscopic optic disc photographs: the European Optic Disc Assessment Study,"Objectives: To evaluate the performance of a deep learning based Artificial Intelligence (AI) software for detection of glaucoma from stereoscopic optic disc photographs, and to compare this performance to the performance of a large cohort of ophthalmologists and optometrists.
  Methods: A retrospective study evaluating the diagnostic performance of an AI software (Pegasus v1.0, Visulytix Ltd., London UK) and comparing it to that of 243 European ophthalmologists and 208 British optometrists, as determined in previous studies, for the detection of glaucomatous optic neuropathy from 94 scanned stereoscopic photographic slides scanned into digital format.
  Results: Pegasus was able to detect glaucomatous optic neuropathy with an accuracy of 83.4% (95% CI: 77.5-89.2). This is comparable to an average ophthalmologist accuracy of 80.5% (95% CI: 67.2-93.8) and average optometrist accuracy of 80% (95% CI: 67-88) on the same images. In addition, the AI system had an intra-observer agreement (Cohen's Kappa, $$) of 0.74 (95% CI: 0.63-0.85), compared to 0.70 (range: -0.13-1.00; 95% CI: 0.67-0.73) and 0.71 (range: 0.08-1.00) for ophthalmologists and optometrists, respectively. There was no statistically significant difference between the performance of the deep learning system and ophthalmologists or optometrists. There was no statistically significant difference between the performance of the deep learning system and ophthalmologists or optometrists.
  Conclusion: The AI system obtained a diagnostic performance and repeatability comparable to that of the ophthalmologists and optometrists. We conclude that deep learning based AI systems, such as Pegasus, demonstrate significant promise in the assisted detection of glaucomatous optic neuropathy.",cs.CV,Computer Vision
Information Competing Process for Learning Diversified Representations,"Learning representations with diversified information remains as an open problem. Towards learning diversified representations, a new approach, termed Information Competing Process (ICP), is proposed in this paper. Aiming to enrich the information carried by feature representations, ICP separates a representation into two parts with different mutual information constraints. The separated parts are forced to accomplish the downstream task independently in a competitive environment which prevents the two parts from learning what each other learned for the downstream task. Such competing parts are then combined synergistically to complete the task. By fusing representation parts learned competitively under different conditions, ICP facilitates obtaining diversified representations which contain rich information. Experiments on image classification and image reconstruction tasks demonstrate the great potential of ICP to learn discriminative and disentangled representations in both supervised and self-supervised learning settings.",cs.CV,Computer Vision
Relational Reasoning using Prior Knowledge for Visual Captioning,"Exploiting relationships among objects has achieved remarkable progress in interpreting images or videos by natural language. Most existing methods resort to first detecting objects and their relationships, and then generating textual descriptions, which heavily depends on pre-trained detectors and leads to performance drop when facing problems of heavy occlusion, tiny-size objects and long-tail in object detection. In addition, the separate procedure of detecting and captioning results in semantic inconsistency between the pre-defined object/relation categories and the target lexical words. We exploit prior human commonsense knowledge for reasoning relationships between objects without any pre-trained detectors and reaching semantic coherency within one image or video in captioning. The prior knowledge (e.g., in the form of knowledge graph) provides commonsense semantic correlation and constraint between objects that are not explicit in the image and video, serving as useful guidance to build semantic graph for sentence generation. Particularly, we present a joint reasoning method that incorporates 1) commonsense reasoning for embedding image or video regions into semantic space to build semantic graph and 2) relational reasoning for encoding semantic graph to generate sentences. Extensive experiments on the MS-COCO image captioning benchmark and the MSVD video captioning benchmark validate the superiority of our method on leveraging prior commonsense knowledge to enhance relational reasoning for visual captioning.",cs.CV,Computer Vision
Optimal Unsupervised Domain Translation,"Domain Translation is the problem of finding a meaningful correspondence between two domains. Since in a majority of settings paired supervision is not available, much work focuses on Unsupervised Domain Translation (UDT) where data samples from each domain are unpaired. Following the seminal work of CycleGAN for UDT, many variants and extensions of this model have been proposed. However, there is still little theoretical understanding behind their success. We observe that these methods yield solutions which are approximately minimal w.r.t. a given transportation cost, leading us to reformulate the problem in the Optimal Transport (OT) framework. This viewpoint gives us a new perspective on Unsupervised Domain Translation and allows us to prove the existence and uniqueness of the retrieved mapping, given a large family of transport costs. We then propose a novel framework to efficiently compute optimal mappings in a dynamical setting. We show that it generalizes previous methods and enables a more explicit control over the computed optimal mapping. It also provides smooth interpolations between the two domains. Experiments on toy and real world datasets illustrate the behavior of our method.",cs.CV,Computer Vision
State-aware Re-identification Feature for Multi-target Multi-camera Tracking,"Multi-target Multi-camera Tracking (MTMCT) aims to extract the trajectories from videos captured by a set of cameras. Recently, the tracking performance of MTMCT is significantly enhanced with the employment of re-identification (Re-ID) model. However, the appearance feature usually becomes unreliable due to the occlusion and orientation variance of the targets. Directly applying Re-ID model in MTMCT will encounter the problem of identity switches (IDS) and tracklet fragment caused by occlusion. To solve these problems, we propose a novel tracking framework in this paper. In this framework, the occlusion status and orientation information are utilized in Re-ID model with human pose information considered. In addition, the tracklet association using the proposed fused tracking feature is adopted to handle the fragment problem. The proposed tracker achieves 81.3\% IDF1 on the multiple-camera hard sequence, which outperforms all other reference methods by a large margin.",cs.CV,Computer Vision
Geo-Aware Networks for Fine-Grained Recognition,"Fine-grained recognition distinguishes among categories with subtle visual differences. In order to differentiate between these challenging visual categories, it is helpful to leverage additional information. Geolocation is a rich source of additional information that can be used to improve fine-grained classification accuracy, but has been understudied. Our contributions to this field are twofold. First, to the best of our knowledge, this is the first paper which systematically examined various ways of incorporating geolocation information into fine-grained image classification through the use of geolocation priors, post-processing or feature modulation. Secondly, to overcome the situation where no fine-grained dataset has complete geolocation information, we release two fine-grained datasets with geolocation by providing complementary information to existing popular datasets - iNaturalist and YFCC100M. By leveraging geolocation information we improve top-1 accuracy in iNaturalist from 70.1% to 79.0% for a strong baseline image-only model. Comparing several models, we found that best performance was achieved by a post-processing model that consumed the output of the image-only baseline alongside geolocation. However, for a resource-constrained model (MobileNetV2), performance was better with a feature modulation model that trains jointly over pixels and geolocation: accuracy increased from 59.6% to 72.2%. Our work makes a strong case for incorporating geolocation information in fine-grained recognition models for both server and on-device.",cs.CV,Computer Vision
Truncated Cauchy Non-negative Matrix Factorization,"Non-negative matrix factorization (NMF) minimizes the Euclidean distance between the data matrix and its low rank approximation, and it fails when applied to corrupted data because the loss function is sensitive to outliers. In this paper, we propose a Truncated CauchyNMF loss that handle outliers by truncating large errors, and develop a Truncated CauchyNMF to robustly learn the subspace on noisy datasets contaminated by outliers. We theoretically analyze the robustness of Truncated CauchyNMF comparing with the competing models and theoretically prove that Truncated CauchyNMF has a generalization bound which converges at a rate of order $O(\sqrt{{\ln n}/{n}})$, where $n$ is the sample size. We evaluate Truncated CauchyNMF by image clustering on both simulated and real datasets. The experimental results on the datasets containing gross corruptions validate the effectiveness and robustness of Truncated CauchyNMF for learning robust subspaces.",cs.CV,Computer Vision
Computing Valid p-values for Image Segmentation by Selective Inference,"Image segmentation is one of the most fundamental tasks of computer vision. In many practical applications, it is essential to properly evaluate the reliability of individual segmentation results. In this study, we propose a novel framework to provide the statistical significance of segmentation results in the form of p-values. Specifically, we consider a statistical hypothesis test for determining the difference between the object and the background regions. This problem is challenging because the difference can be deceptively large (called segmentation bias) due to the adaptation of the segmentation algorithm to the data. To overcome this difficulty, we introduce a statistical approach called selective inference, and develop a framework to compute valid p-values in which the segmentation bias is properly accounted for. Although the proposed framework is potentially applicable to various segmentation algorithms, we focus in this paper on graph cut-based and threshold-based segmentation algorithms, and develop two specific methods to compute valid p-values for the segmentation results obtained by these algorithms. We prove the theoretical validity of these two methods and demonstrate their practicality by applying them to segmentation problems for medical images.",cs.CV,Computer Vision
Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning,"In this paper, the problem of describing visual contents of a video sequence with natural language is addressed. Unlike previous video captioning work mainly exploiting the cues of video contents to make a language description, we propose a reconstruction network (RecNet) in a novel encoder-decoder-reconstructor architecture, which leverages both forward (video to sentence) and backward (sentence to video) flows for video captioning. Specifically, the encoder-decoder component makes use of the forward flow to produce a sentence description based on the encoded video semantic features. Two types of reconstructors are subsequently proposed to employ the backward flow and reproduce the video features from local and global perspectives, respectively, capitalizing on the hidden state sequence generated by the decoder. Moreover, in order to make a comprehensive reconstruction of the video features, we propose to fuse the two types of reconstructors together. The generation loss yielded by the encoder-decoder component and the reconstruction loss introduced by the reconstructor are jointly cast into training the proposed RecNet in an end-to-end fashion. Furthermore, the RecNet is fine-tuned by CIDEr optimization via reinforcement learning, which significantly boosts the captioning performance. Experimental results on benchmark datasets demonstrate that the proposed reconstructor can boost the performance of video captioning consistently.",cs.CV,Computer Vision
Text-based Editing of Talking-head Video,"Editing talking-head video to change the speech content or to remove filler words is challenging. We propose a novel method to edit talking-head video based on its transcript to produce a realistic output video in which the dialogue of the speaker has been modified, while maintaining a seamless audio-visual flow (i.e. no jump cuts). Our method automatically annotates an input talking-head video with phonemes, visemes, 3D face pose and geometry, reflectance, expression and scene illumination per frame. To edit a video, the user has to only edit the transcript, and an optimization strategy then chooses segments of the input corpus as base material. The annotated parameters corresponding to the selected segments are seamlessly stitched together and used to produce an intermediate video representation in which the lower half of the face is rendered with a parametric face model. Finally, a recurrent video generation network transforms this representation to a photorealistic video that matches the edited transcript. We demonstrate a large variety of edits, such as the addition, removal, and alteration of words, as well as convincing language translation and full sentence synthesis.",cs.CV,Computer Vision
Weakly Supervised Vessel Segmentation in X-ray Angiograms by Self-Paced Learning from Noisy Labels with Suggestive Annotation,"The segmentation of coronary arteries in X-ray angiograms by convolutional neural networks (CNNs) is promising yet limited by the requirement of precisely annotating all pixels in a large number of training images, which is extremely labor-intensive especially for complex coronary trees. To alleviate the burden on the annotator, we propose a novel weakly supervised training framework that learns from noisy pseudo labels generated from automatic vessel enhancement, rather than accurate labels obtained by fully manual annotation. A typical self-paced learning scheme is used to make the training process robust against label noise while challenged by the systematic biases in pseudo labels, thus leading to the decreased performance of CNNs at test time. To solve this problem, we propose an annotation-refining self-paced learning framework (AR-SPL) to correct the potential errors using suggestive annotation. An elaborate model-vesselness uncertainty estimation is also proposed to enable the minimal annotation cost for suggestive annotation, based on not only the CNNs in training but also the geometric features of coronary arteries derived directly from raw data. Experiments show that our proposed framework achieves 1) comparable accuracy to fully supervised learning, which also significantly outperforms other weakly supervised learning frameworks; 2) largely reduced annotation cost, i.e., 75.18% of annotation time is saved, and only 3.46% of image regions are required to be annotated; and 3) an efficient intervention process, leading to superior performance with even fewer manual interactions.",cs.CV,Computer Vision
Self-supervised Modal and View Invariant Feature Learning,"Most of the existing self-supervised feature learning methods for 3D data either learn 3D features from point cloud data or from multi-view images. By exploring the inherent multi-modality attributes of 3D objects, in this paper, we propose to jointly learn modal-invariant and view-invariant features from different modalities including image, point cloud, and mesh with heterogeneous networks for 3D data. In order to learn modal- and view-invariant features, we propose two types of constraints: cross-modal invariance constraint and cross-view invariant constraint. Cross-modal invariance constraint forces the network to maximum the agreement of features from different modalities for same objects, while the cross-view invariance constraint forces the network to maximum agreement of features from different views of images for same objects. The quality of learned features has been tested on different downstream tasks with three modalities of data including point cloud, multi-view images, and mesh. Furthermore, the invariance cross different modalities and views are evaluated with the cross-modal retrieval task. Extensive evaluation results demonstrate that the learned features are robust and have strong generalizability across different tasks.",cs.CV,Computer Vision
Modified Segmentation Algorithm for Recognition of Older Geez Scripts Written on Vellum,"Recognition of handwritten document aims at transforming document images into a machine understandable format. Handwritten document recognition is the most challenging area in the field of pattern recognition. It becomes more complex when a document was written on vellum before hundreds of years, like older Geez scripts. In this study, we introduced a modified segmentation approach to recognize older Geez scripts. We used adaptive filtering for noise reduction, Isodata iterative global thresholding for document image binarization, modified bounding box projection to segment distinct strokes between Geez characters, numbers, and punctuation marks. SVM multiclass classifier scored 79.32% recognition accuracy with the modified segmentation algorithm.",cs.CV,Computer Vision
DeepMark++: Real-time Clothing Detection at the Edge,"Clothing recognition is the most fundamental AI application challenge within the fashion domain. While existing solutions offer decent recognition accuracy, they are generally slow and require significant computational resources. In this paper we propose a single-stage approach to overcome this obstacle and deliver rapid clothing detection and keypoint estimation. Our solution is based on a multi-target network CenterNet, and we introduce several powerful post-processing techniques to enhance performance. Our most accurate model achieves results comparable to state-of-the-art solutions on the DeepFashion2 dataset, and our light and fast model runs at 17 FPS on the Huawei P40 Pro smartphone. In addition, we achieved second place in the DeepFashion2 Landmark Estimation Challenge 2020 with 0.582 mAP on the test dataset.",cs.CV,Computer Vision
Using Generative Models for Pediatric wbMRI,"Early detection of cancer is key to a good prognosis and requires frequent testing, especially in pediatrics. Whole-body magnetic resonance imaging (wbMRI) is an essential part of several well-established screening protocols, with screening starting in early childhood. To date, machine learning (ML) has been used on wbMRI images to stage adult cancer patients. It is not possible to use such tools in pediatrics due to the changing bone signal throughout growth, the difficulty of obtaining these images in young children due to movement and limited compliance, and the rarity of positive cases. We evaluate the quality of wbMRI images generated using generative adversarial networks (GANs) trained on wbMRI data from The Hospital for Sick Children in Toronto. We use the Frchet Inception Distance (FID) metric, Domain Frchet Distance (DFD), and blind tests with a radiology fellow for evaluation. We demonstrate that StyleGAN2 provides the best performance in generating wbMRI images with respect to all three metrics.",cs.CV,Computer Vision
Glaucoma Detection From Raw Circumapillary OCT Images Using Fully Convolutional Neural Networks,"Nowadays, glaucoma is the leading cause of blindness worldwide. We propose in this paper two different deep-learning-based approaches to address glaucoma detection just from raw circumpapillary OCT images. The first one is based on the development of convolutional neural networks (CNNs) trained from scratch. The second one lies in fine-tuning some of the most common state-of-the-art CNNs architectures. The experiments were performed on a private database composed of 93 glaucomatous and 156 normal B-scans around the optic nerve head of the retina, which were diagnosed by expert ophthalmologists. The validation results evidence that fine-tuned CNNs outperform the networks trained from scratch when small databases are addressed. Additionally, the VGG family of networks reports the most promising results, with an area under the ROC curve of 0.96 and an accuracy of 0.92, during the prediction of the independent test set.",cs.CV,Computer Vision
Automated Measurements of Key Morphological Features of Human Embryos for IVF,"A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the highest quality embryo to transfer to the patient in the hopes of achieving a pregnancy. Time-lapse microscopy provides clinicians with a wealth of information for selecting embryos. However, the resulting movies of embryos are currently analyzed manually, which is time consuming and subjective. Here, we automate feature extraction of time-lapse microscopy of human embryos with a machine-learning pipeline of five convolutional neural networks (CNNs). Our pipeline consists of (1) semantic segmentation of the regions of the embryo, (2) regression predictions of fragment severity, (3) classification of the developmental stage, and object instance segmentation of (4) cells and (5) pronuclei. Our approach greatly speeds up the measurement of quantitative, biologically relevant features that may aid in embryo selection.",cs.CV,Computer Vision
Approximating the Ideal Observer for joint signal detection and localization tasks by use of supervised learning methods,"Medical imaging systems are commonly assessed and optimized by use of objective measures of image quality (IQ). The Ideal Observer (IO) performance has been advocated to provide a figure-of-merit for use in assessing and optimizing imaging systems because the IO sets an upper performance limit among all observers. When joint signal detection and localization tasks are considered, the IO that employs a modified generalized likelihood ratio test maximizes observer performance as characterized by the localization receiver operating characteristic (LROC) curve. Computations of likelihood ratios are analytically intractable in the majority of cases. Therefore, sampling-based methods that employ Markov-Chain Monte Carlo (MCMC) techniques have been developed to approximate the likelihood ratios. However, the applications of MCMC methods have been limited to relatively simple object models. Supervised learning-based methods that employ convolutional neural networks have been recently developed to approximate the IO for binary signal detection tasks. In this paper, the ability of supervised learning-based methods to approximate the IO for joint signal detection and localization tasks is explored. Both background-known-exactly and background-known-statistically signal detection and localization tasks are considered. The considered object models include a lumpy object model and a clustered lumpy model, and the considered measurement noise models include Laplacian noise, Gaussian noise, and mixed Poisson-Gaussian noise. The LROC curves produced by the supervised learning-based method are compared to those produced by the MCMC approach or analytical computation when feasible. The potential utility of the proposed method for computing objective measures of IQ for optimizing imaging system performance is explored.",cs.CV,Computer Vision
A Light-Weighted Convolutional Neural Network for Bitemporal SAR Image Change Detection,"Recently, many Convolution Neural Networks (CNN) have been successfully employed in bitemporal SAR image change detection. However, most of the existing networks are too heavy and occupy a large volume of memory for storage and calculation. Motivated by this, in this paper, we propose a lightweight neural network to reduce the computational and spatial complexity and facilitate the change detection on an edge device. In the proposed network, we replace normal convolutional layers with bottleneck layers that keep the same number of channels between input and output. Next, we employ dilated convolutional kernels with a few non-zero entries that reduce the running time in convolutional operators. Comparing with the conventional convolutional neural network, our light-weighted neural network will be more efficient with fewer parameters. We verify our light-weighted neural network on four sets of bitemporal SAR images. The experimental results show that the proposed network can obtain better performance than the conventional CNN and has better model generalization, especially on the challenging datasets with complex scenes.",cs.CV,Computer Vision
Hyperspectral Image Super-resolution via Deep Spatio-spectral Convolutional Neural Networks,"Hyperspectral images are of crucial importance in order to better understand features of different materials. To reach this goal, they leverage on a high number of spectral bands. However, this interesting characteristic is often paid by a reduced spatial resolution compared with traditional multispectral image systems. In order to alleviate this issue, in this work, we propose a simple and efficient architecture for deep convolutional neural networks to fuse a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution hyperspectral image (HR-HSI). The network is designed to preserve both spatial and spectral information thanks to an architecture from two folds: one is to utilize the HR-HSI at a different scale to get an output with a satisfied spectral preservation; another one is to apply concepts of multi-resolution analysis to extract high-frequency information, aiming to output high quality spatial details. Finally, a plain mean squared error loss function is used to measure the performance during the training. Extensive experiments demonstrate that the proposed network architecture achieves best performance (both qualitatively and quantitatively) compared with recent state-of-the-art hyperspectral image super-resolution approaches. Moreover, other significant advantages can be pointed out by the use of the proposed approach, such as, a better network generalization ability, a limited computational burden, and a robustness with respect to the number of training samples.",cs.CV,Computer Vision
NuClick: A Deep Learning Framework for Interactive Segmentation of Microscopy Images,"Object segmentation is an important step in the workflow of computational pathology. Deep learning based models generally require large amount of labeled data for precise and reliable prediction. However, collecting labeled data is expensive because it often requires expert knowledge, particularly in medical imaging domain where labels are the result of a time-consuming analysis made by one or more human experts. As nuclei, cells and glands are fundamental objects for downstream analysis in computational pathology/cytology, in this paper we propose a simple CNN-based approach to speed up collecting annotations for these objects which requires minimum interaction from the annotator. We show that for nuclei and cells in histology and cytology images, one click inside each object is enough for NuClick to yield a precise annotation. For multicellular structures such as glands, we propose a novel approach to provide the NuClick with a squiggle as a guiding signal, enabling it to segment the glandular boundaries. These supervisory signals are fed to the network as auxiliary inputs along with RGB channels. With detailed experiments, we show that NuClick is adaptable to the object scale, robust against variations in the user input, adaptable to new domains, and delivers reliable annotations. An instance segmentation model trained on masks generated by NuClick achieved the first rank in LYON19 challenge. As exemplar outputs of our framework, we are releasing two datasets: 1) a dataset of lymphocyte annotations within IHC images, and 2) a dataset of segmented WBCs in blood smear images.",cs.CV,Computer Vision
Positron Emission Tomography (PET) image enhancement using a gradient vector orientation based nonlinear diffusion filter (GVOF) for accurate quantitation of radioactivity concentration,"To accurately quantify in vivo radiotracer uptake using Positron Emission Tomography (PET) is a challenging task due to low signal-to-noise ratio (SNR) and poor spatial resolution of PET camera along with the finite image sampling constraint. Furthermore, inter lesion variations of the SNR and contrast along with the variations in size of the lesion make the quantitation even more difficult. One of the ways to improve the quantitation is via post reconstruction filtering with Gaussian Filter (GF). Edge preserving Bilateral Filter (BF) and Nonlinear Diffusion Filter (NDF) are the alternatives to GF that can improve the SNR without degrading the image resolution. However, the performance of these edge preserving methods are only optimum for high count and low noise cases. A novel parameter free gradient vector orientation based nonlinear diffusion filter (GVOF) is proposed in this paper that is insensitive to statistical fluctuations (e. g., SNR, contrast, size etc.). GVOF method applied on the PET images collected with the NEMA phantom with varying levels of contrast and noise reveals that the GVOF method provides the highest SNR, CNR (contrast-to-noise ratio) and resolution compared to the original and other filtered images. The percentage bias in estimating the maximum activity representing SUVmax (Maximum Standardized Uptake Value) for the spheres with diameter > 2cm where the partial volume effects (PVE) is negligible is the lowest for the GVOF method. The GVOF method also improves the maximum intensity reproducibility. Robustness of the GVOF against variation in sizes, contrast levels and SNR makes it a suitable post filtering method for both accurate diagnosis and response assessment. Furthermore, its capability to provide accurate quantitative measurements irrespective of the SNR, it can also be effective in reduction of radioactivity dose.",cs.CV,Computer Vision
Residual Squeeze-and-Excitation Network for Fast Image Deraining,"Image deraining is an important image processing task as rain streaks not only severely degrade the visual quality of images but also significantly affect the performance of high-level vision tasks. Traditional methods progressively remove rain streaks via different recurrent neural networks. However, these methods fail to yield plausible rain-free images in an efficient manner. In this paper, we propose a residual squeeze-and-excitation network called RSEN for fast image deraining as well as superior deraining performance compared with state-of-the-art approaches. Specifically, RSEN adopts a lightweight encoder-decoder architecture to conduct rain removal in one stage. Besides, both encoder and decoder adopt a novel residual squeeze-and-excitation block as the core of feature extraction, which contains a residual block for producing hierarchical features, followed by a squeeze-and-excitation block for channel-wisely enhancing the resulted hierarchical features. Experimental results demonstrate that our method can not only considerably reduce the computational complexity but also significantly improve the deraining performance compared with state-of-the-art methods.",cs.CV,Computer Vision
Dual-stream Maximum Self-attention Multi-instance Learning,"Multi-instance learning (MIL) is a form of weakly supervised learning where a single class label is assigned to a bag of instances while the instance-level labels are not available. Training classifiers to accurately determine the bag label and instance labels is a challenging but critical task in many practical scenarios, such as computational histopathology. Recently, MIL models fully parameterized by neural networks have become popular due to the high flexibility and superior performance. Most of these models rely on attention mechanisms that assign attention scores across the instance embeddings in a bag and produce the bag embedding using an aggregation operator. In this paper, we proposed a dual-stream maximum self-attention MIL model (DSMIL) parameterized by neural networks. The first stream deploys a simple MIL max-pooling while the top-activated instance embedding is determined and used to obtain self-attention scores across instance embeddings in the second stream. Different from most of the previous methods, the proposed model jointly learns an instance classifier and a bag classifier based on the same instance embeddings. The experiments results show that our method achieves superior performance compared to the best MIL methods and demonstrates state-of-the-art performance on benchmark MIL datasets.",cs.CV,Computer Vision
A gaze driven fast-forward method for first-person videos,"The growing data sharing and life-logging cultures are driving an unprecedented increase in the amount of unedited First-Person Videos. In this paper, we address the problem of accessing relevant information in First-Person Videos by creating an accelerated version of the input video and emphasizing the important moments to the recorder. Our method is based on an attention model driven by gaze and visual scene analysis that provides a semantic score of each frame of the input video. We performed several experimental evaluations on publicly available First-Person Videos datasets. The results show that our methodology can fast-forward videos emphasizing moments when the recorder visually interact with scene components while not including monotonous clips.",cs.CV,Computer Vision
DoubleU-Net: A Deep Convolutional Neural Network for Medical Image Segmentation,"Semantic image segmentation is the process of labeling each pixel of an image with its corresponding class. An encoder-decoder based approach, like U-Net and its variants, is a popular strategy for solving medical image segmentation tasks. To improve the performance of U-Net on various segmentation tasks, we propose a novel architecture called DoubleU-Net, which is a combination of two U-Net architectures stacked on top of each other. The first U-Net uses a pre-trained VGG-19 as the encoder, which has already learned features from ImageNet and can be transferred to another task easily. To capture more semantic information efficiently, we added another U-Net at the bottom. We also adopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information within the network. We have evaluated DoubleU-Net using four medical segmentation datasets, covering various imaging modalities such as colonoscopy, dermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation challenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the Lesion boundary segmentation datasets demonstrate that the DoubleU-Net outperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more accurate segmentation masks, especially in the case of the CVC-ClinicDB and MICCAI 2015 segmentation challenge datasets, which have challenging images such as smaller and flat polyps. These results show the improvement over the existing U-Net model. The encouraging results, produced on various medical image segmentation datasets, show that DoubleU-Net can be used as a strong baseline for both medical image segmentation and cross-dataset evaluation testing to measure the generalizability of Deep Learning (DL) models.",cs.CV,Computer Vision
KiU-Net: Towards Accurate Segmentation of Biomedical Images using Over-complete Representations,"Due to its excellent performance, U-Net is the most widely used backbone architecture for biomedical image segmentation in the recent years. However, in our studies, we observe that there is a considerable performance drop in the case of detecting smaller anatomical landmarks with blurred noisy boundaries. We analyze this issue in detail, and address it by proposing an over-complete architecture (Ki-Net) which involves projecting the data onto higher dimensions (in the spatial sense). This network, when augmented with U-Net, results in significant improvements in the case of segmenting small anatomical landmarks and blurred noisy boundaries while obtaining better overall performance. Furthermore, the proposed network has additional benefits like faster convergence and fewer number of parameters. We evaluate the proposed method on the task of brain anatomy segmentation from 2D Ultrasound (US) of preterm neonates, and achieve an improvement of around 4% in terms of the DICE accuracy and Jaccard index as compared to the standard-U-Net, while outperforming the recent best methods by 2%. Code: https://github.com/jeya-maria-jose/KiU-Net-pytorch .",cs.CV,Computer Vision
Ensemble Model with Batch Spectral Regularization and Data Blending for Cross-Domain Few-Shot Learning with Unlabeled Data,"In this paper, we present our proposed ensemble model with batch spectral regularization and data blending mechanisms for the Track 2 problem of the cross-domain few-shot learning (CD-FSL) challenge. We build a multi-branch ensemble framework by using diverse feature transformation matrices, while deploying batch spectral feature regularization on each branch to improve the model's transferability. Moreover, we propose a data blending method to exploit the unlabeled data and augment the sparse support set in the target domain. Our proposed model demonstrates effective performance on the CD-FSL benchmark tasks.",cs.CV,Computer Vision
Deep Neural Network Based Real-time Kiwi Fruit Flower Detection in an Orchard Environment,"In this paper, we present a novel approach to kiwi fruit flower detection using Deep Neural Networks (DNNs) to build an accurate, fast, and robust autonomous pollination robot system. Recent work in deep neural networks has shown outstanding performance on object detection tasks in many areas. Inspired this, we aim for exploiting DNNs for kiwi fruit flower detection and present intensive experiments and their analysis on two state-of-the-art object detectors; Faster R-CNN and Single Shot Detector (SSD) Net, and feature extractors; Inception Net V2 and NAS Net with real-world orchard datasets. We also compare those approaches to find an optimal model which is suitable for a real-time agricultural pollination robot system in terms of accuracy and processing speed. We perform experiments with dataset collected from different seasons and locations (spatio-temporal consistency) in order to demonstrate the performance of the generalized model. The proposed system demonstrates promising results of 0.919, 0.874, and 0.889 for precision, recall, and F1-score respectively on our real-world dataset, and the performance satisfies the requirement for deploying the system onto an autonomous pollination robotics system.",cs.CV,Computer Vision
Neural Sparse Representation for Image Restoration,"Inspired by the robustness and efficiency of sparse representation in sparse coding based image restoration models, we investigate the sparsity of neurons in deep networks. Our method structurally enforces sparsity constraints upon hidden neurons. The sparsity constraints are favorable for gradient-based learning algorithms and attachable to convolution layers in various networks. Sparsity in neurons enables computation saving by only operating on non-zero components without hurting accuracy. Meanwhile, our method can magnify representation dimensionality and model capacity with negligible additional computation cost. Experiments show that sparse representation is crucial in deep neural networks for multiple image restoration tasks, including image super-resolution, image denoising, and image compression artifacts removal. Code is available at https://github.com/ychfan/nsr",cs.CV,Computer Vision
Learning the Compositional Visual Coherence for Complementary Recommendations,"Complementary recommendations, which aim at providing users product suggestions that are supplementary and compatible with their obtained items, have become a hot topic in both academia and industry in recent years. %However, it is challenging due to its complexity and subjectivity. Existing work mainly focused on modeling the co-purchased relations between two items, but the compositional associations of item collections are largely unexplored. Actually, when a user chooses the complementary items for the purchased products, it is intuitive that she will consider the visual semantic coherence (such as color collocations, texture compatibilities) in addition to global impressions. Towards this end, in this paper, we propose a novel Content Attentive Neural Network (CANN) to model the comprehensive compositional coherence on both global contents and semantic contents. Specifically, we first propose a \textit{Global Coherence Learning} (GCL) module based on multi-heads attention to model the global compositional coherence. Then, we generate the semantic-focal representations from different semantic regions and design a \textit{Focal Coherence Learning} (FCL) module to learn the focal compositional coherence from different semantic-focal representations. Finally, we optimize the CANN in a novel compositional optimization strategy. Extensive experiments on the large-scale real-world data clearly demonstrate the effectiveness of CANN compared with several state-of-the-art methods.",cs.CV,Computer Vision
Incorporating Image Gradients as Secondary Input Associated with Input Image to Improve the Performance of the CNN Model,"CNN is very popular neural network architecture in modern days. It is primarily most used tool for vision related task to extract the important features from the given image. Moreover, CNN works as a filter to extract the important features using convolutional operation in distinct layers. In existing CNN architectures, to train the network on given input, only single form of given input is fed to the network. In this paper, new architecture has been proposed where given input is passed in more than one form to the network simultaneously by sharing the layers with both forms of input. We incorporate image gradient as second form of the input associated with the original input image and allowing both inputs to flow in the network using same number of parameters to improve the performance of the model for better generalization. The results of the proposed CNN architecture, applying on diverse set of datasets such as MNIST, CIFAR10 and CIFAR100 show superior result compared to the benchmark CNN architecture considering inputs in single form.",cs.CV,Computer Vision
Single Image Deraining via Scale-space Invariant Attention Neural Network,"Image enhancement from degradation of rainy artifacts plays a critical role in outdoor visual computing systems. In this paper, we tackle the notion of scale that deals with visual changes in appearance of rain steaks with respect to the camera. Specifically, we revisit multi-scale representation by scale-space theory, and propose to represent the multi-scale correlation in convolutional feature domain, which is more compact and robust than that in pixel domain. Moreover, to improve the modeling ability of the network, we do not treat the extracted multi-scale features equally, but design a novel scale-space invariant attention mechanism to help the network focus on parts of the features. In this way, we summarize the most activated presence of feature maps as the salient features. Extensive experiments results on synthetic and real rainy scenes demonstrate the superior performance of our scheme over the state-of-the-arts.",cs.CV,Computer Vision
Breaking the Limits of Remote Sensing by Simulation and Deep Learning for Flood and Debris Flow Mapping,"We propose a framework that estimates inundation depth (maximum water level) and debris-flow-induced topographic deformation from remote sensing imagery by integrating deep learning and numerical simulation. A water and debris flow simulator generates training data for various artificial disaster scenarios. We show that regression models based on Attention U-Net and LinkNet architectures trained on such synthetic data can predict the maximum water level and topographic deformation from a remote sensing-derived change detection map and a digital elevation model. The proposed framework has an inpainting capability, thus mitigating the false negatives that are inevitable in remote sensing image analysis. Our framework breaks the limits of remote sensing and enables rapid estimation of inundation depth and topographic deformation, essential information for emergency response, including rescue and relief activities. We conduct experiments with both synthetic and real data for two disaster events that caused simultaneous flooding and debris flows and demonstrate the effectiveness of our approach quantitatively and qualitatively.",cs.CV,Computer Vision
A Note on Deepfake Detection with Low-Resources,"Deepfakes are videos that include changes, quite often substituting face of a portrayed individual with a different face using neural networks. Even though the technology gained its popularity as a carrier of jokes and parodies it raises a serious threat to ones security - via biometric impersonation or besmearing. In this paper we present two methods that allow detecting Deepfakes for a user without significant computational power. In particular, we enhance MesoNet by replacing the original activation functions allowing a nearly 1% improvement as well as increasing the consistency of the results. Moreover, we introduced and verified a new activation function - Pish that at the cost of slight time overhead allows even higher consistency.
  Additionally, we present a preliminary results of Deepfake detection method based on Local Feature Descriptors (LFD), that allows setting up the system even faster and without resorting to GPU computation. Our method achieved Equal Error Rate of 0.28, with both accuracy and recall exceeding 0.7.",cs.CV,Computer Vision
"A Comparative Analysis of E-Scooter and E-Bike Usage Patterns: Findings from the City of Austin, TX","E-scooter-sharing and e-bike-sharing systems are accommodating and easing the increased traffic in dense cities and are expanding considerably. However, these new micro-mobility transportation modes raise numerous operational and safety concerns. This study analyzes e-scooter and dockless e-bike sharing system user behavior. We investigate how average trip speed change depending on the day of the week and the time of the day. We used a dataset from the city of Austin, TX from December 2018 to May 2019. Our results generally show that the trip average speed for e-bikes ranges between 3.01 and 3.44 m/s, which is higher than that for e-scooters (2.19 to 2.78 m/s). Results also show a similar usage pattern for the average speed of e-bikes and e-scooters throughout the days of the week and a different usage pattern for the average speed of e-bikes and e-scooters over the hours of the day. We found that users tend to ride e-bikes and e-scooters with a slower average speed for recreational purposes compared to when they are ridden for commuting purposes. This study is a building block in this field, which serves as a first of its kind, and sheds the light of significant new understanding of this emerging class of shared-road users.",cs.CV,Computer Vision
End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera,"Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.",cs.CV,Computer Vision
Realistic text replacement with non-uniform style conditioning,"In this work, we study the possibility of realistic text replacement, the goal of which is to replace text present in the image with user-supplied text. The replacement should be performed in a way that will not allow distinguishing the resulting image from the original one. We achieve this goal by developing a novel non-uniform style conditioning layer and apply it to an encoder-decoder ResNet based architecture. The resulting model is a single-stage model, with no post-processing. The proposed model achieves realistic text replacement and outperforms existing approaches on ICDAR MLT.",cs.CV,Computer Vision
Finger Texture Biometric Characteristic: a Survey,"\begin{abstract}
  In recent years, the Finger Texture (FT) has attracted considerable attention as a biometric characteristic. It can provide efficient human recognition performance, because it has different human-specific features of apparent lines, wrinkles and ridges distributed along the inner surface of all fingers. Also, such pattern structures are reliable, unique and remain stable throughout a human's life. Efficient biometric systems can be established based only on FTs. In this paper, a comprehensive survey of the relevant FT studies is presented. We also summarise the main drawbacks and obstacles of employing the FT as a biometric characteristic, and provide useful suggestions to further improve the work on FT. \end{abstract}",cs.CV,Computer Vision
Learning Local Features with Context Aggregation for Visual Localization,"Keypoint detection and description is fundamental yet important in many vision applications. Most existing methods use detect-then-describe or detect-and-describe strategy to learn local features without considering their context information. Consequently, it is challenging for these methods to learn robust local features. In this paper, we focus on the fusion of low-level textual information and high-level semantic context information to improve the discrimitiveness of local features. Specifically, we first estimate a score map to represent the distribution of potential keypoints according to the quality of descriptors of all pixels. Then, we extract and aggregate multi-scale high-level semantic features based by the guidance of the score map. Finally, the low-level local features and high-level semantic features are fused and refined using a residual module. Experiments on the challenging local feature benchmark dataset demonstrate that our method achieves the state-of-the-art performance in the local feature challenge of the visual localization benchmark.",cs.CV,Computer Vision
Deep Convolutional Neural Network-based Bernoulli Heatmap for Head Pose Estimation,"Head pose estimation is a crucial problem for many tasks, such as driver attention, fatigue detection, and human behaviour analysis. It is well known that neural networks are better at handling classification problems than regression problems. It is an extremely nonlinear process to let the network output the angle value directly for optimization learning, and the weight constraint of the loss function will be relatively weak. This paper proposes a novel Bernoulli heatmap for head pose estimation from a single RGB image. Our method can achieve the positioning of the head area while estimating the angles of the head. The Bernoulli heatmap makes it possible to construct fully convolutional neural networks without fully connected layers and provides a new idea for the output form of head pose estimation. A deep convolutional neural network (CNN) structure with multiscale representations is adopted to maintain high-resolution information and low-resolution information in parallel. This kind of structure can maintain rich, high-resolution representations. In addition, channelwise fusion is adopted to make the fusion weights learnable instead of simple addition with equal weights. As a result, the estimation is spatially more precise and potentially more accurate. The effectiveness of the proposed method is empirically demonstrated by comparing it with other state-of-the-art methods on public datasets.",cs.CV,Computer Vision
Rethinking of Pedestrian Attribute Recognition: Realistic Datasets with Efficient Method,"Despite various methods are proposed to make progress in pedestrian attribute recognition, a crucial problem on existing datasets is often neglected, namely, a large number of identical pedestrian identities in train and test set, which is not consistent with practical application. Thus, images of the same pedestrian identity in train set and test set are extremely similar, leading to overestimated performance of state-of-the-art methods on existing datasets. To address this problem, we propose two realistic datasets PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$} following zero-shot setting of pedestrian identities based on PETA and RAPv2 datasets. Furthermore, compared to our strong baseline method, we have observed that recent state-of-the-art methods can not make performance improvement on PETA, RAPv2, PETA\textsubscript{$zs$} and RAPv2\textsubscript{$zs$}. Thus, through solving the inherent attribute imbalance in pedestrian attribute recognition, an efficient method is proposed to further improve the performance. Experiments on existing and proposed datasets verify the superiority of our method by achieving state-of-the-art performance.",cs.CV,Computer Vision
Road Segmentation on low resolution Lidar point clouds for autonomous vehicles,"Point cloud datasets for perception tasks in the context of autonomous driving often rely on high resolution 64-layer Light Detection and Ranging (LIDAR) scanners. They are expensive to deploy on real-world autonomous driving sensor architectures which usually employ 16/32 layer LIDARs. We evaluate the effect of subsampling image based representations of dense point clouds on the accuracy of the road segmentation task. In our experiments the low resolution 16/32 layer LIDAR point clouds are simulated by subsampling the original 64 layer data, for subsequent transformation in to a feature map in the Bird-Eye-View (BEV) and SphericalView (SV) representations of the point cloud. We introduce the usage of the local normal vector with the LIDAR's spherical coordinates as an input channel to existing LoDNN architectures. We demonstrate that this local normal feature in conjunction with classical features not only improves performance for binary road segmentation on full resolution point clouds, but it also reduces the negative impact on the accuracy when subsampling dense point clouds as compared to the usage of classical features alone. We assess our method with several experiments on two datasets: KITTI Road-segmentation benchmark and the recently released Semantic KITTI dataset.",cs.CV,Computer Vision
Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering,"Pig counting is a crucial task for large-scale pig farming, which is usually completed by human visually. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. Existing methods only focused on pig counting using single image, and its accuracy is challenged by several factors, including pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the requirements of pig counting for large pig grouping houses. To that end, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it produces accurate results surpassing human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformation of pigs. A deep convolution neural network (CNN) is designed to detect keypoints of pig body part and associate the keypoints to identify individual pigs. After that, an efficient on-line tracking method is used to associate pigs across video frames. Finally, a novel spatial-aware temporal response filtering (STRF) method is proposed to predict the counts of pigs, which is effective to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.",cs.CV,Computer Vision
A Feature-map Discriminant Perspective for Pruning Deep Neural Networks,"Network pruning has become the de facto tool to accelerate deep neural networks for mobile and edge applications. Recently, feature-map discriminant based channel pruning has shown promising results, as it aligns well with the CNN objective of differentiating multiple classes and offers better interpretability of the pruning decision. However, existing discriminant-based methods are challenged by computation inefficiency, as there is a lack of theoretical guidance on quantifying the feature-map discriminant power. In this paper, we present a new mathematical formulation to accurately and efficiently quantify the feature-map discriminativeness, which gives rise to a novel criterion,Discriminant Information(DI). We analyze the theoretical property of DI, specifically the non-decreasing property, that makes DI a valid selection criterion. DI-based pruning removes channels with minimum influence to DI value, as they contain little information regarding to the discriminant power. The versatility of DI criterion also enables an intra-layer mixed precision quantization to further compress the network. Moreover, we propose a DI-based greedy pruning algorithm and structure distillation technique to automatically decide the pruned structure that satisfies certain resource budget, which is a common requirement in reality. Extensive experiments demonstratethe effectiveness of our method: our pruned ResNet50 on ImageNet achieves 44% FLOPs reduction without any Top-1 accuracy loss compared to unpruned model",cs.CV,Computer Vision
Traditional Method Inspired Deep Neural Network for Edge Detection,"Recently, Deep-Neural-Network (DNN) based edge prediction is progressing fast. Although the DNN based schemes outperform the traditional edge detectors, they have much higher computational complexity. It could be that the DNN based edge detectors often adopt the neural net structures designed for high-level computer vision tasks, such as image segmentation and object recognition. Edge detection is a rather local and simple job, the over-complicated architecture and massive parameters may be unnecessary. Therefore, we propose a traditional method inspired framework to produce good edges with minimal complexity. We simplify the network architecture to include Feature Extractor, Enrichment, and Summarizer, which roughly correspond to gradient, low pass filter, and pixel connection in the traditional edge detection schemes. The proposed structure can effectively reduce the complexity and retain the edge prediction quality. Our TIN2 (Traditional Inspired Network) model has an accuracy higher than the recent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.",cs.CV,Computer Vision
CGGAN: A Context Guided Generative Adversarial Network For Single Image Dehazing,"Image haze removal is highly desired for the application of computer vision. This paper proposes a novel Context Guided Generative Adversarial Network (CGGAN) for single image dehazing. Of which, an novel new encoder-decoder is employed as the generator. And it consists of a feature-extraction-net, a context-extractionnet, and a fusion-net in sequence. The feature extraction-net acts as a encoder, and is used for extracting haze features. The context-extraction net is a multi-scale parallel pyramid decoder, and is used for extracting the deep features of the encoder and generating coarse dehazing image. The fusion-net is a decoder, and is used for obtaining the final haze-free image. To obtain more better results, multi-scale information obtained during the decoding process of the context extraction decoder is used for guiding the fusion decoder. By introducing an extra coarse decoder to the original encoder-decoder, the CGGAN can make better use of the deep feature information extracted by the encoder. To ensure our CGGAN work effectively for different haze scenarios, different loss functions are employed for the two decoders. Experiments results show the advantage and the effectiveness of our proposed CGGAN, evidential improvements over existing state-of-the-art methods are obtained.",cs.CV,Computer Vision
P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds,"Towards 3D object tracking in point clouds, a novel point-to-box network termed P2B is proposed in an end-to-end learning manner. Our main idea is to first localize potential target centers in 3D search area embedded with target information. Then point-driven 3D target proposal and verification are executed jointly. In this way, the time-consuming 3D exhaustive search can be avoided. Specifically, we first sample seeds from the point clouds in template and search area respectively. Then, we execute permutation-invariant feature augmentation to embed target clues from template into search area seeds and represent them with target-specific features. Consequently, the augmented search area seeds regress the potential target centers via Hough voting. The centers are further strengthened with seed-wise targetness scores. Finally, each center clusters its neighbors to leverage the ensemble power for joint 3D target proposal and verification. We apply PointNet++ as our backbone and experiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s improvement over state-of-the-art). Note that P2B can run with 40FPS on a single NVIDIA 1080Ti GPU. Our code and model are available at https://github.com/HaozheQi/P2B.",cs.CV,Computer Vision
Anomaly Detection Based on Deep Learning Using Video for Prevention of Industrial Accidents,This paper proposes an anomaly detection method for the prevention of industrial accidents using machine learning technology.,cs.CV,Computer Vision
Universal Lesion Detection by Learning from Multiple Heterogeneously Labeled Datasets,"Lesion detection is an important problem within medical imaging analysis. Most previous work focuses on detecting and segmenting a specialized category of lesions (e.g., lung nodules). However, in clinical practice, radiologists are responsible for finding all possible types of anomalies. The task of universal lesion detection (ULD) was proposed to address this challenge by detecting a large variety of lesions from the whole body. There are multiple heterogeneously labeled datasets with varying label completeness: DeepLesion, the largest dataset of 32,735 annotated lesions of various types, but with even more missing annotation instances; and several fully-labeled single-type lesion datasets, such as LUNA for lung nodules and LiTS for liver tumors. In this work, we propose a novel framework to leverage all these datasets together to improve the performance of ULD. First, we learn a multi-head multi-task lesion detector using all datasets and generate lesion proposals on DeepLesion. Second, missing annotations in DeepLesion are retrieved by a new method of embedding matching that exploits clinical prior knowledge. Last, we discover suspicious but unannotated lesions using knowledge transfer from single-type lesion detectors. In this way, reliable positive and negative regions are obtained from partially-labeled and unlabeled images, which are effectively utilized to train ULD. To assess the clinically realistic protocol of 3D volumetric ULD, we fully annotated 1071 CT sub-volumes in DeepLesion. Our method outperforms the current state-of-the-art approach by 29% in the metric of average sensitivity.",cs.CV,Computer Vision
DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution,"Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level, we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance segmentation, and 50.0% PQ for panoptic segmentation. The code is made publicly available.",cs.CV,Computer Vision
High-quality Panorama Stitching based on Asymmetric Bidirectional Optical Flow,"In this paper, we propose a panorama stitching algorithm based on asymmetric bidirectional optical flow. This algorithm expects multiple photos captured by fisheye lens cameras as input, and then, through the proposed algorithm, these photos can be merged into a high-quality 360-degree spherical panoramic image. For photos taken from a distant perspective, the parallax among them is relatively small, and the obtained panoramic image can be nearly seamless and undistorted. For photos taken from a close perspective or with a relatively large parallax, a seamless though partially distorted panoramic image can also be obtained. Besides, with the help of Graphics Processing Unit (GPU), this algorithm can complete the whole stitching process at a very fast speed: typically, it only takes less than 30s to obtain a panoramic image of 9000-by-4000 pixels, which means our panorama stitching algorithm is of high value in many real-time applications. Our code is available at https://github.com/MungoMeng/Panorama-OpticalFlow.",cs.CV,Computer Vision
LFTag: A Scalable Visual Fiducial System with Low Spatial Frequency,"Visual fiducial systems are a key component of many robotics and AR/VR applications for 6-DOF monocular relative pose estimation and target identification. This paper presents LFTag, a visual fiducial system based on topological detection and relative position data encoding which optimizes data density within spatial frequency constraints. The marker is constructed to resolve rotational ambiguity, which combined with the robust geometric and topological false positive rejection, allows all marker bits to be used for data.
  When compared to existing state-of-the-art square binary markers (AprilTag) and topological markers (TopoTag) in simulation, the proposed fiducial system (LFTag) offers significant advances in dictionary size and range. LFTag 3x3 achieves 546 times the dictionary size of AprilTag 25h9 and LFTag 4x4 achieves 126 thousand times the dictionary size of AprilTag 41h12 while simultaneously achieving longer detection range. LFTag 3x3 also achieves more than twice the detection range of TopoTag 4x4 at the same dictionary size.",cs.CV,Computer Vision
Implementing AI-powered semantic character recognition in motor racing sports,"Oftentimes TV producers of motor-racing programs overlay visual and textual media to provide on-screen context about drivers, such as a driver's name, position or photo. Typically this is accomplished by a human producer who visually identifies the drivers on screen, manually toggling the contextual media associated to each one and coordinating with cameramen and other TV producers to keep the racer in the shot while the contextual media is on screen. This labor-intensive and highly dedicated process is mostly suited to static overlays and makes it difficult to overlay contextual information about many drivers at the same time in short shots. This paper presents a system that largely automates these tasks and enables dynamic overlays using deep learning to track the drivers as they move on screen, without human intervention. This system is not merely theoretical, but an implementation has already been deployed during live races by a TV production company at Formula E races. We present the challenges faced during the implementation and discuss the implications. Additionally, we cover future applications and roadmap of this new technological development.",cs.CV,Computer Vision
Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019,"Person identification based on eye movements is getting more and more attention, as it is anti-spoofing resistant and can be useful for continuous authentication. Therefore, it is noteworthy for researchers to know who and what is relevant in the field, including authors, journals, conferences, and institutions. This paper presents a comprehensive quantitative overview of the field of eye movement biometrics using a bibliometric approach. All data and analyses are based on documents written in English published between 2004 and 2019. Scopus was used to perform information retrieval. This research focused on temporal evolution, leading authors, most cited papers, leading journals, competitions and collaboration networks.",cs.CV,Computer Vision
CT-based COVID-19 Triage: Deep Multitask Learning Improves Joint Identification and Severity Quantification,"The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight studies of severe patients and direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods provide reasonable quality only for one of these setups. We employ a multitask approach to consolidate both triage approaches and propose a convolutional neural network to combine all available labels within a single model. In contrast with the most popular multitask approaches, we add classification layers to the most spatially detailed upper part of U-Net instead of the bottom, less detailed latent representation. We train our model on approximately 2000 publicly available CT studies and test it with a carefully designed set consisting of 32 COVID-19 studies, 30 cases with bacterial pneumonia, 31 healthy patients, and 30 patients with other lung pathologies to emulate a typical patient flow in an out-patient hospital. The proposed multitask model outperforms the latent-based one and achieves ROC AUC scores ranging from 0.87+-01 (bacterial pneumonia) to 0.97+-01 (healthy controls) for Identification of COVID-19 and 0.97+-01 Spearman Correlation for Severity quantification. We release all the code and create a public leaderboard, where other community members can test their models on our test dataset.",cs.CV,Computer Vision
Studying The Effect of MIL Pooling Filters on MIL Tasks,"There are different multiple instance learning (MIL) pooling filters used in MIL models. In this paper, we study the effect of different MIL pooling filters on the performance of MIL models in real world MIL tasks. We designed a neural network based MIL framework with 5 different MIL pooling filters: `max', `mean', `attention', `distribution' and `distribution with attention'. We also formulated 5 different MIL tasks on a real world lymph node metastases dataset. We found that the performance of our framework in a task is different for different filters. We also observed that the performances of the five pooling filters are also different from task to task. Hence, the selection of a correct MIL pooling filter for each MIL task is crucial for better performance. Furthermore, we noticed that models with `distribution' and `distribution with attention' pooling filters consistently perform well in almost all of the tasks. We attribute this phenomena to the amount of information captured by `distribution' based pooling filters. While point estimate based pooling filters, like `max' and `mean', produce point estimates of distributions, `distribution' based pooling filters capture the full information in distributions. Lastly, we compared the performance of our neural network model with `distribution' pooling filter with the performance of the best MIL methods in the literature on classical MIL datasets and our model outperformed the others.",cs.CV,Computer Vision
Making Sense of CNNs: Interpreting Deep Representations & Their Invariances with INNs,"To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance. Our implementation is available at https://compvis.github.io/invariances/ .",cs.CV,Computer Vision
Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition,"Self-supervised learning has shown great potentials in improving the deep learning model in an unsupervised manner by constructing surrogate supervision signals directly from the unlabeled data. Different from existing works, we present a novel way to obtain the surrogate supervision signal based on high-level feature maps under consistency regularization. In this paper, we propose a Spatio-Temporal Consistency Regularization between different output features generated from a siamese network including a clean path fed with original video and a noise path fed with the corresponding augmented video. Based on the Spatio-Temporal characteristics of video, we develop two video-based data augmentation methods, i.e., Spatio-Temporal Transformation and Intra-Video Mixup. Consistency of the former one is proposed to model transformation consistency of features, while the latter one aims at retaining spatial invariance to extract action-related features. Extensive experiments demonstrate that our method achieves substantial improvements compared with state-of-the-art self-supervised learning methods for action recognition. When using our method as an additional regularization term and combine with current surrogate supervision signals, we achieve 22% relative improvement over the previous state-of-the-art on HMDB51 and 7% on UCF101.",cs.CV,Computer Vision
Cascade Graph Neural Networks for RGB-D Salient Object Detection,"In this paper, we study the problem of salient object detection (SOD) for RGB-D images using both color and depth information.A major technical challenge in performing salient object detection fromRGB-D images is how to fully leverage the two complementary data sources. Current works either simply distill prior knowledge from the corresponding depth map for handling the RGB-image or blindly fuse color and geometric information to generate the coarse depth-aware representations, hindering the performance of RGB-D saliency detectors.In this work, we introduceCascade Graph Neural Networks(Cas-Gnn),a unified framework which is capable of comprehensively distilling and reasoning the mutual benefits between these two data sources through a set of cascade graphs, to learn powerful representations for RGB-D salient object detection. Cas-Gnn processes the two data sources individually and employs a novelCascade Graph Reasoning(CGR) module to learn powerful dense feature embeddings, from which the saliency map can be easily inferred. Contrast to the previous approaches, the explicitly modeling and reasoning of high-level relations between complementary data sources allows us to better overcome challenges such as occlusions and ambiguities. Extensive experiments demonstrate that Cas-Gnn achieves significantly better performance than all existing RGB-DSOD approaches on several widely-used benchmarks.",cs.CV,Computer Vision
Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework,"We propose a self-supervised method to learn feature representations from videos. A standard approach in traditional self-supervised methods uses positive-negative data pairs to train with contrastive learning strategy. In such a case, different modalities of the same video are treated as positives and video clips from a different video are treated as negatives. Because the spatio-temporal information is important for video representation, we extend the negative samples by introducing intra-negative samples, which are transformed from the same anchor video by breaking temporal relations in video clips. With the proposed Inter-Intra Contrastive (IIC) framework, we can train spatio-temporal convolutional networks to learn video representations. There are many flexible options in our IIC framework and we conduct experiments by using several different configurations. Evaluations are conducted on video retrieval and video recognition tasks using the learned video representation. Our proposed IIC outperforms current state-of-the-art results by a large margin, such as 16.7% and 9.5% points improvements in top-1 accuracy on UCF101 and HMDB51 datasets for video retrieval, respectively. For video recognition, improvements can also be obtained on these two benchmark datasets. Code is available at https://github.com/BestJuly/Inter-intra-video-contrastive-learning.",cs.CV,Computer Vision
Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into Cognizance,"In recent years, researchers have focused on reducing the model size and number of computations (measured as ""multiply-accumulate"" or MAC operations) of DNNs. The energy consumption of a DNN depends on both the number of MAC operations and the energy efficiency of each MAC operation. The former can be estimated at design time; however, the latter depends on the intricate data reuse patterns and underlying hardware architecture. Hence, estimating it at design time is challenging. This work shows that the conventional approach to estimate the data reuse, viz. arithmetic intensity, does not always correctly estimate the degree of data reuse in DNNs since it gives equal importance to all the data types. We propose a novel model, termed ""data type aware weighted arithmetic intensity"" ($DI$), which accounts for the unequal importance of different data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs on two GPUs. We show that our model accurately models data-reuse for all possible data reuse patterns for different types of convolution and different types of layers. We show that our model is a better indicator of the energy efficiency of DNNs. We also show its generality using the central limit theorem.",cs.CV,Computer Vision
MultiCheXNet: A Multi-Task Learning Deep Network For Pneumonia-like Diseases Diagnosis From X-ray Scans,"We present MultiCheXNet, an end-to-end Multi-task learning model, that is able to take advantage of different X-rays data sets of Pneumonia-like diseases in one neural architecture, performing three tasks at the same time; diagnosis, segmentation and localization. The common encoder in our architecture can capture useful common features present in the different tasks. The common encoder has another advantage of efficient computations, which speeds up the inference time compared to separate models. The specialized decoders heads can then capture the task-specific features. We employ teacher forcing to address the issue of negative samples that hurt the segmentation and localization performance. Finally,we employ transfer learning to fine tune the classifier on unseen pneumonia-like diseases. The MTL architecture can be trained on joint or dis-joint labeled data sets. The training of the architecture follows a carefully designed protocol, that pre trains different sub-models on specialized datasets, before being integrated in the joint MTL model. Our experimental setup involves variety of data sets, where the baseline performance of the 3 tasks is compared to the MTL architecture performance. Moreover, we evaluate the transfer learning mode to COVID-19 data set,both from individual classifier model, and from MTL architecture classification head.",cs.CV,Computer Vision
F2GAN: Fusing-and-Filling GAN for Few-shot Image Generation,"In order to generate images for a given category, existing deep generative models generally rely on abundant training images. However, extensive data acquisition is expensive and fast learning ability from limited data is necessarily required in real-world applications. Also, these existing methods are not well-suited for fast adaptation to a new category.
  Few-shot image generation, aiming to generate images from only a few images for a new category, has attracted some research interest. In this paper, we propose a Fusing-and-Filling Generative Adversarial Network (F2GAN) to generate realistic and diverse images for a new category with only a few images. In our F2GAN, a fusion generator is designed to fuse the high-level features of conditional images with random interpolation coefficients, and then fills in attended low-level details with non-local attention module to produce a new image. Moreover, our discriminator can ensure the diversity of generated images by a mode seeking loss and an interpolation regression loss. Extensive experiments on five datasets demonstrate the effectiveness of our proposed method for few-shot image generation.",cs.CV,Computer Vision
Fast top-K Cosine Similarity Search through XOR-Friendly Binary Quantization on GPUs,"We explore the use of GPU for accelerating large scale nearest neighbor search and we propose a fast vector-quantization-based exhaustive nearest neighbor search algorithm that can achieve high accuracy without any indexing construction specifically designed for cosine similarity. This algorithm uses a novel XOR-friendly binary quantization method to encode floating-point numbers such that high-complexity multiplications can be optimized as low-complexity bitwise operations. Experiments show that, our quantization method takes short preprocessing time, and helps make the search speed of our exhaustive search method much more faster than that of popular approximate nearest neighbor algorithms when high accuracy is needed.",cs.CV,Computer Vision
Learning Boost by Exploiting the Auxiliary Task in Multi-task Domain,"Learning two tasks in a single shared function has some benefits. Firstly by acquiring information from the second task, the shared function leverages useful information that could have been neglected or underestimated in the first task. Secondly, it helps to generalize the function that can be learned using generally applicable information for both tasks. To fully enjoy these benefits, Multi-task Learning (MTL) has long been researched in various domains such as computer vision, language understanding, and speech synthesis. While MTL benefits from the positive transfer of information from multiple tasks, in a real environment, tasks inevitably have a conflict between them during the learning phase, called negative transfer. The negative transfer hampers function from achieving the optimality and degrades the performance. To solve the problem of the task conflict, previous works only suggested partial solutions that are not fundamental, but ad-hoc. A common approach is using a weighted sum of losses. The weights are adjusted to induce positive transfer. Paradoxically, this kind of solution acknowledges the problem of negative transfer and cannot remove it unless the weight of the task is set to zero. Therefore, these previous methods had limited success. In this paper, we introduce a novel approach that can drive positive transfer and suppress negative transfer by leveraging class-wise weights in the learning process. The weights act as an arbitrator of the fundamental unit of information to determine its positive or negative status to the main task.",cs.CV,Computer Vision
Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs,"To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. In spite of the reasonable visualization, lack of clear and sufficient theoretical support is the main limitation of these methods. In this paper, we introduce two axioms -- Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of conservation and sensitivity. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is available at https://github.com/Fu0511/XGrad-CAM.",cs.CV,Computer Vision
Global Voxel Transformer Networks for Augmented Microscopy,"Advances in deep learning have led to remarkable success in augmented microscopy, enabling us to obtain high-quality microscope images without using expensive microscopy hardware and sample preparation techniques. However, current deep learning models for augmented microscopy are mostly U-Net based neural networks, thus sharing certain drawbacks that limit the performance. In this work, we introduce global voxel transformer networks (GVTNets), an advanced deep learning tool for augmented microscopy that overcomes intrinsic limitations of the current U-Net based models and achieves improved performance. GVTNets are built on global voxel transformer operators (GVTOs), which are able to aggregate global information, as opposed to local operators like convolutions. We apply the proposed methods on existing datasets for three different augmented microscopy tasks under various settings. The performance is significantly and consistently better than previous U-Net based approaches.",cs.CV,Computer Vision
Cross-Model Image Annotation Platform with Active Learning,"We have seen significant leapfrog advancement in machine learning in recent decades. The central idea of machine learnability lies on constructing learning algorithms that learn from good data. The availability of more data being made publicly available also accelerates the growth of AI in recent years. In the domain of computer vision, the quality of image data arises from the accuracy of image annotation. Labeling large volume of image data is a daunting and tedious task. This work presents an End-to-End pipeline tool for object annotation and recognition aims at enabling quick image labeling. We have developed a modular image annotation platform which seamlessly incorporates assisted image annotation (annotation assistance), active learning and model training and evaluation. Our approach provides a number of advantages over current image annotation tools. Firstly, the annotation assistance utilizes reference hierarchy and reference images to locate the objects in the images, thus reducing the need for annotating the whole object. Secondly, images can be annotated using polygon points allowing for objects of any shape to be annotated. Thirdly, it is also interoperable across several image models, and the tool provides an interface for object model training and evaluation across a series of pre-trained models. We have tested the model and embeds several benchmarking deep learning models. The highest accuracy achieved is 74%.",cs.CV,Computer Vision
Salvage Reusable Samples from Noisy Data for Robust Learning,"Due to the existence of label noise in web images and the high memorization capacity of deep neural networks, training deep fine-grained (FG) models directly through web images tends to have an inferior recognition ability. In the literature, to alleviate this issue, loss correction methods try to estimate the noise transition matrix, but the inevitable false correction would cause severe accumulated errors. Sample selection methods identify clean (""easy"") samples based on the fact that small losses can alleviate the accumulated errors. However, ""hard"" and mislabeled examples that can both boost the robustness of FG models are also dropped. To this end, we propose a certainty-based reusable sample selection and correction approach, termed as CRSSC, for coping with label noise in training deep FG models with web images. Our key idea is to additionally identify and correct reusable samples, and then leverage them together with clean examples to update the networks. We demonstrate the superiority of the proposed approach from both theoretical and experimental perspectives.",cs.CV,Computer Vision
Group Activity Prediction with Sequential Relational Anticipation Model,"In this paper, we propose a novel approach to predict group activities given the beginning frames with incomplete activity executions. Existing action prediction approaches learn to enhance the representation power of the partial observation. However, for group activity prediction, the relation evolution of people's activity and their positions over time is an important cue for predicting group activity. To this end, we propose a sequential relational anticipation model (SRAM) that summarizes the relational dynamics in the partial observation and progressively anticipates the group representations with rich discriminative information. Our model explicitly anticipates both activity features and positions by two graph auto-encoders, aiming to learn a discriminative group representation for group activity prediction. Experimental results on two popularly used datasets demonstrate that our approach significantly outperforms the state-of-the-art activity prediction methods.",cs.CV,Computer Vision
X-Ray bone abnormalities detection using MURA dataset,"We introduce the deep network trained on the MURA dataset from the Stanford University released in 2017. Our system is able to detect bone abnormalities on the radiographs and visualise such zones. We found that our solution has the accuracy comparable to the best results that have been achieved by other development teams that used MURA dataset, in particular the overall Kappa score that was achieved by our team is about 0.942 on the wrist, 0.862 on the hand and o.735 on the shoulder (compared to the best available results to this moment on the official web-site 0.931, 0.851 and 0.729 accordingly). However, despite the good results there are a lot of directions for the future enhancement of the proposed technology. We see a big potential in the further development computer aided systems (CAD) for the radiographs as the one that will help practical specialists diagnose bone fractures as well as bone oncology cases faster and with the higher accuracy.",cs.CV,Computer Vision
Single-stage intake gesture detection using CTC loss and extended prefix beam search,"Accurate detection of individual intake gestures is a key step towards automatic dietary monitoring. Both inertial sensor data of wrist movements and video data depicting the upper body have been used for this purpose. The most advanced approaches to date use a two-stage approach, in which (i) frame-level intake probabilities are learned from the sensor data using a deep neural network, and then (ii) sparse intake events are detected by finding the maxima of the frame-level probabilities. In this study, we propose a single-stage approach which directly decodes the probabilities learned from sensor data into sparse intake detections. This is achieved by weakly supervised training using Connectionist Temporal Classification (CTC) loss, and decoding using a novel extended prefix beam search decoding algorithm. Benefits of this approach include (i) end-to-end training for detections, (ii) simplified timing requirements for intake gesture labels, and (iii) improved detection performance compared to existing approaches. Across two separate datasets, we achieve relative $F_1$ score improvements between 1.9% and 6.2% over the two-stage approach for intake detection and eating/drinking detection tasks, for both video and inertial sensors.",cs.CV,Computer Vision
Confidence-guided Lesion Mask-based Simultaneous Synthesis of Anatomic and Molecular MR Images in Patients with Post-treatment Malignant Gliomas,"Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas in neuro-oncology, especially with the help of standard anatomic and advanced molecular MR images. However, data quantity and quality remain a key determinant of, and a significant limit on, the potential of such applications. In our previous work, we explored synthesis of anatomic and molecular MR image network (SAMR) in patients with post-treatment malignant glioms. Now, we extend it and propose Confidence Guided SAMR (CG-SAMR) that synthesizes data from lesion information to multi-modal anatomic sequences, including T1-weighted (T1w), gadolinium enhanced T1w (Gd-T1w), T2-weighted (T2w), and fluid-attenuated inversion recovery (FLAIR), and the molecular amide proton transfer-weighted (APTw) sequence. We introduce a module which guides the synthesis based on confidence measure about the intermediate results. Furthermore, we extend the proposed architecture for unsupervised synthesis so that unpaired data can be used for training the network. Extensive experiments on real clinical data demonstrate that the proposed model can perform better than the state-of-theart synthesis methods.",cs.CV,Computer Vision
RocNet: Recursive Octree Network for Efficient 3D Deep Representation,"We introduce a deep recursive octree network for the compression of 3D voxel data. Our network compresses a voxel grid of any size down to a very small latent space in an autoencoder-like network. We show results for compressing 32, 64 and 128 grids down to just 80 floats in the latent space. We demonstrate the effectiveness and efficiency of our proposed method on several publicly available datasets with three experiments: 3D shape classification, 3D shape reconstruction, and shape generation. Experimental results show that our algorithm maintains accuracy while consuming less memory with shorter training times compared to existing methods, especially in 3D reconstruction tasks.",cs.CV,Computer Vision
DQI: A Guide to Benchmark Evaluation,"A `state of the art' model A surpasses humans in a benchmark B, but fails on similar benchmarks C, D, and E. What does B have that the other benchmarks do not? Recent research provides the answer: spurious bias. However, developing A to solve benchmarks B through E does not guarantee that it will solve future benchmarks. To progress towards a model that `truly learns' an underlying task, we need to quantify the differences between successive benchmarks, as opposed to existing binary and black-box approaches. We propose a novel approach to solve this underexplored task of quantifying benchmark quality by debuting a data quality metric: DQI.",cs.CV,Computer Vision
2nd Place Scheme on Action Recognition Track of ECCV 2020 VIPriors Challenges: An Efficient Optical Flow Stream Guided Framework,"To address the problem of training on small datasets for action recognition tasks, most prior works are either based on a large number of training samples or require pre-trained models transferred from other large datasets to tackle overfitting problems. However, it limits the research within organizations that have strong computational abilities. In this work, we try to propose a data-efficient framework that can train the model from scratch on small datasets while achieving promising results. Specifically, by introducing a 3D central difference convolution operation, we proposed a novel C3D neural network-based two-stream (Rank Pooling RGB and Optical Flow) framework for the task. The method is validated on the action recognition track of the ECCV 2020 VIPriors challenges and got the 2nd place (88.31%). It is proved that our method can achieve a promising result even without a pre-trained model on large scale datasets. The code will be released soon.",cs.CV,Computer Vision
PROFIT: A Novel Training Method for sub-4-bit MobileNet Models,"4-bit and lower precision mobile models are required due to the ever-increasing demand for better energy efficiency in mobile devices. In this work, we report that the activation instability induced by weight quantization (AIWQ) is the key obstacle to sub-4-bit quantization of mobile networks. To alleviate the AIWQ problem, we propose a novel training method called PROgressive-Freezing Iterative Training (PROFIT), which attempts to freeze layers whose weights are affected by the instability problem stronger than the other layers. We also propose a differentiable and unified quantization method (DuQ) and a negative padding idea to support asymmetric activation functions such as h-swish. We evaluate the proposed methods by quantizing MobileNet-v1, v2, and v3 on ImageNet and report that 4-bit quantization offers comparable (within 1.48 % top-1 accuracy) accuracy to full precision baseline. In the ablation study of the 3-bit quantization of MobileNet-v3, our proposed method outperforms the state-of-the-art method by a large margin, 12.86 % of top-1 accuracy.",cs.CV,Computer Vision
Transfer Learning for Protein Structure Classification at Low Resolution,"Structure determination is key to understanding protein function at a molecular level. Whilst significant advances have been made in predicting structure and function from amino acid sequence, researchers must still rely on expensive, time-consuming analytical methods to visualise detailed protein conformation. In this study, we demonstrate that it is possible to make accurate ($\geq$80%) predictions of protein class and architecture from structures determined at low ($>$3A) resolution, using a deep convolutional neural network trained on high-resolution ($\leq$3A) structures represented as 2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein structure classification at low resolution, and a basis for extension to prediction of function. We investigate the impact of the input representation on classification performance, showing that side-chain information may not be necessary for fine-grained structure predictions. Finally, we confirm that high-resolution, low-resolution and NMR-determined structures inhabit a common feature space, and thus provide a theoretical foundation for boosting with single-image super-resolution.",cs.CV,Computer Vision
Unified Representation Learning for Cross Model Compatibility,"We propose a unified representation learning framework to address the Cross Model Compatibility (CMC) problem in the context of visual search applications. Cross compatibility between different embedding models enables the visual search systems to correctly recognize and retrieve identities without re-encoding user images, which are usually not available due to privacy concerns. While there are existing approaches to address CMC in face identification, they fail to work in a more challenging setting where the distributions of embedding models shift drastically. The proposed solution improves CMC performance by introducing a light-weight Residual Bottleneck Transformation (RBT) module and a new training scheme to optimize the embedding spaces. Extensive experiments demonstrate that our proposed solution outperforms previous approaches by a large margin for various challenging visual search scenarios of face recognition and person re-identification.",cs.CV,Computer Vision
HASeparator: Hyperplane-Assisted Softmax,"Efficient feature learning with Convolutional Neural Networks (CNNs) constitutes an increasingly imperative property since several challenging tasks of computer vision tend to require cascade schemes and modalities fusion. Feature learning aims at CNN models capable of extracting embeddings, exhibiting high discrimination among the different classes, as well as intra-class compactness. In this paper, a novel approach is introduced that has separator, which focuses on an effective hyperplane-based segregation of the classes instead of the common class centers separation scheme. Accordingly, an innovatory separator, namely the Hyperplane-Assisted Softmax separator (HASeparator), is proposed that demonstrates superior discrimination capabilities, as evaluated on popular image classification benchmarks.",cs.CV,Computer Vision
Block Shuffle: A Method for High-resolution Fast Style Transfer with Limited Memory,"Fast Style Transfer is a series of Neural Style Transfer algorithms that use feed-forward neural networks to render input images. Because of the high dimension of the output layer, these networks require much memory for computation. Therefore, for high-resolution images, most mobile devices and personal computers cannot stylize them, which greatly limits the application scenarios of Fast Style Transfer. At present, the two existing solutions are purchasing more memory and using the feathering-based method, but the former requires additional cost, and the latter has poor image quality. To solve this problem, we propose a novel image synthesis method named \emph{block shuffle}, which converts a single task with high memory consumption to multiple subtasks with low memory consumption. This method can act as a plug-in for Fast Style Transfer without any modification to the network architecture. We use the most popular Fast Style Transfer repository on GitHub as the baseline. Experiments show that the quality of high-resolution images generated by our method is better than that of the feathering-based method. Although our method is an order of magnitude slower than the baseline, it can stylize high-resolution images with limited memory, which is impossible with the baseline. The code and models will be made available on \url{https://github.com/czczup/block-shuffle}.",cs.CV,Computer Vision
3D Human Motion Estimation via Motion Compression and Refinement,"We develop a technique for generating smooth and accurate 3D human pose and motion estimates from RGB video sequences. Our method, which we call Motion Estimation via Variational Autoencoder (MEVA), decomposes a temporal sequence of human motion into a smooth motion representation using auto-encoder-based motion compression and a residual representation learned through motion refinement. This two-step encoding of human motion captures human motion in two stages: a general human motion estimation step that captures the coarse overall motion, and a residual estimation that adds back person-specific motion details. Experiments show that our method produces both smooth and accurate 3D human pose and motion estimates.",cs.CV,Computer Vision
Extension of JPEG XS for Two-Layer Lossless Coding,"A two-layer lossless image coding method compatible with JPEG XS is proposed. JPEG XS is a new international standard for still image coding that has the characteristics of very low latency and very low complexity. However, it does not support lossless coding, although it can achieve visual lossless coding. The proposed method has a two-layer structure similar to JPEG XT, which consists of JPEG XS coding and a lossless coding method. As a result, it enables us to losslessly restore original images, while maintaining compatibility with JPEG XS.",cs.CV,Computer Vision
R-MNet: A Perceptual Adversarial Network for Image Inpainting,"Facial image inpainting is a problem that is widely studied, and in recent years the introduction of Generative Adversarial Networks, has led to improvements in the field. Unfortunately some issues persists, in particular when blending the missing pixels with the visible ones. We address the problem by proposing a Wasserstein GAN combined with a new reverse mask operator, namely Reverse Masking Network (R-MNet), a perceptual adversarial network for image inpainting. The reverse mask operator transfers the reverse masked image to the end of the encoder-decoder network leaving only valid pixels to be inpainted. Additionally, we propose a new loss function computed in feature space to target only valid pixels combined with adversarial training. These then capture data distributions and generate images similar to those in the training data with achieved realism (realistic and coherent) on the output images. We evaluate our method on publicly available dataset, and compare with state-of-the-art methods. We show that our method is able to generalize to high-resolution inpainting task, and further show more realistic outputs that are plausible to the human visual system when compared with the state-of-the-art methods.",cs.CV,Computer Vision
Adversarial Generative Grammars for Human Activity Prediction,"In this paper we propose an adversarial generative grammar model for future prediction. The objective is to learn a model that explicitly captures temporal dependencies, providing a capability to forecast multiple, distinct future activities. Our adversarial grammar is designed so that it can learn stochastic production rules from the data distribution, jointly with its latent non-terminal representations. Being able to select multiple production rules during inference leads to different predicted outcomes, thus efficiently modeling many plausible futures. The adversarial generative grammar is evaluated on the Charades, MultiTHUMOS, Human3.6M, and 50 Salads datasets and on two activity prediction tasks: future 3D human pose prediction and future activity prediction. The proposed adversarial grammar outperforms the state-of-the-art approaches, being able to predict much more accurately and further in the future, than prior work.",cs.CV,Computer Vision
Boosting Weakly Supervised Object Detection with Progressive Knowledge Transfer,"In this paper, we propose an effective knowledge transfer framework to boost the weakly supervised object detection accuracy with the help of an external fully-annotated source dataset, whose categories may not overlap with the target domain. This setting is of great practical value due to the existence of many off-the-shelf detection datasets. To more effectively utilize the source dataset, we propose to iteratively transfer the knowledge from the source domain by a one-class universal detector and learn the target-domain detector. The box-level pseudo ground truths mined by the target-domain detector in each iteration effectively improve the one-class universal detector. Therefore, the knowledge in the source dataset is more thoroughly exploited and leveraged. Extensive experiments are conducted with Pascal VOC 2007 as the target weakly-annotated dataset and COCO/ImageNet as the source fully-annotated dataset. With the proposed solution, we achieved an mAP of $59.7\%$ detection performance on the VOC test set and an mAP of $60.2\%$ after retraining a fully supervised Faster RCNN with the mined pseudo ground truths. This is significantly better than any previously known results in related literature and sets a new state-of-the-art of weakly supervised object detection under the knowledge transfer setting. Code: \url{https://github.com/mikuhatsune/wsod_transfer}.",cs.CV,Computer Vision
MovieNet: A Holistic Dataset for Movie Understanding,"Recent years have seen remarkable advances in visual understanding. However, how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet -- a holistic dataset for movie understanding. MovieNet contains 1,100 movies with a large amount of multi-modal data, e.g. trailers, photos, plot descriptions, etc. Besides, different aspects of manual annotations are provided in MovieNet, including 1.1M characters with bounding boxes and identities, 42K scene boundaries, 2.5K aligned description sentences, 65K tags of place and action, and 92K tags of cinematic style. To the best of our knowledge, MovieNet is the largest dataset with richest annotations for comprehensive movie understanding. Based on MovieNet, we set up several benchmarks for movie understanding from different angles. Extensive experiments are executed on these benchmarks to show the immeasurable value of MovieNet and the gap of current approaches towards comprehensive movie understanding. We believe that such a holistic dataset would promote the researches on story-based long video understanding and beyond. MovieNet will be published in compliance with regulations at https://movienet.github.io.",cs.CV,Computer Vision
Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling,"Monocular visual odometry (VO) suffers severely from error accumulation during frame-to-frame pose estimation. In this paper, we present a self-supervised learning method for VO with special consideration for consistency over longer sequences. To this end, we model the long-term dependency in pose prediction using a pose network that features a two-layer convolutional LSTM module. We train the networks with purely self-supervised losses, including a cycle consistency loss that mimics the loop closure module in geometric VO. Inspired by prior geometric systems, we allow the networks to see beyond a small temporal window during training, through a novel a loss that incorporates temporally distant (e.g., O(100)) frames. Given GPU memory constraints, we propose a stage-wise training mechanism, where the first stage operates in a local time window and the second stage refines the poses with a ""global"" loss given the first stage features. We demonstrate competitive results on several standard VO datasets, including KITTI and TUM RGB-D.",cs.CV,Computer Vision
Least squares surface reconstruction on arbitrary domains,"Almost universally in computer vision, when surface derivatives are required, they are computed using only first order accurate finite difference approximations. We propose a new method for computing numerical derivatives based on 2D Savitzky-Golay filters and K-nearest neighbour kernels. The resulting derivative matrices can be used for least squares surface reconstruction over arbitrary (even disconnected) domains in the presence of large noise and allowing for higher order polynomial local surface approximations. They are useful for a range of tasks including normal-from-depth (i.e. surface differentiation), height-from-normals (i.e. surface integration) and shape-from-x. We show how to write both orthographic or perspective height-from-normals as a linear least squares problem using the same formulation and avoiding a nonlinear change of variables in the perspective case. We demonstrate improved performance relative to state-of-the-art across these tasks on both synthetic and real data and make available an open source implementation of our method.",cs.CV,Computer Vision
AE-Net: Autonomous Evolution Image Fusion Method Inspired by Human Cognitive Mechanism,"In order to solve the robustness and generality problems of the image fusion task,inspired by the human brain cognitive mechanism, we propose a robust and general image fusion method with autonomous evolution ability, and is therefore denoted with AE-Net. Through the collaborative optimization of multiple image fusion methods to simulate the cognitive process of human brain, unsupervised learning image fusion task can be transformed into semi-supervised image fusion task or supervised image fusion task, thus promoting the evolutionary ability of network model weight. Firstly, the relationship between human brain cognitive mechanism and image fusion task is analyzed and a physical model is established to simulate human brain cognitive mechanism. Secondly, we analyze existing image fusion methods and image fusion loss functions, select the image fusion method with complementary features to construct the algorithm module, establish the multi-loss joint evaluation function to obtain the optimal solution of algorithm module. The optimal solution of each image is used to guide the weight training of network model. Our image fusion method can effectively unify the cross-modal image fusion task and the same modal image fusion task, and effectively overcome the difference of data distribution between different datasets. Finally, extensive numerical results verify the effectiveness and superiority of our method on a variety of image fusion datasets, including multi-focus dataset, infrared and visi-ble dataset, medical image dataset and multi-exposure dataset. Comprehensive experiments demonstrate the superiority of our image fusion method in robustness and generality. In addition, experimental results also demonstate the effectiveness of human brain cognitive mechanism to improve the robustness and generality of image fusion.",cs.CV,Computer Vision
Learn to Propagate Reliably on Noisy Affinity Graphs,"Recent works have shown that exploiting unlabeled data through label propagation can substantially reduce the labeling cost, which has been a critical issue in developing visual recognition models. Yet, how to propagate labels reliably, especially on a dataset with unknown outliers, remains an open question. Conventional methods such as linear diffusion lack the capability of handling complex graph structures and may perform poorly when the seeds are sparse. Latest methods based on graph neural networks would face difficulties on performance drop as they scale out to noisy graphs. To overcome these difficulties, we propose a new framework that allows labels to be propagated reliably on large-scale real-world data. This framework incorporates (1) a local graph neural network to predict accurately on varying local structures while maintaining high scalability, and (2) a confidence-based path scheduler that identifies outliers and moves forward the propagation frontier in a prudent way. Experiments on both ImageNet and Ms-Celeb-1M show that our confidence guided framework can significantly improve the overall accuracies of the propagated labels, especially when the graph is very noisy.",cs.CV,Computer Vision
Cross-View Image Synthesis with Deformable Convolution and Attention Mechanism,"Learning to generate natural scenes has always been a daunting task in computer vision. This is even more laborious when generating images with very different views. When the views are very different, the view fields have little overlap or objects are occluded, leading the task very challenging. In this paper, we propose to use Generative Adversarial Networks(GANs) based on a deformable convolution and attention mechanism to solve the problem of cross-view image synthesis (see Fig.1). It is difficult to understand and transform scenes appearance and semantic information from another view, thus we use deformed convolution in the U-net network to improve the network's ability to extract features of objects at different scales. Moreover, to better learn the correspondence between images from different views, we apply an attention mechanism to refine the intermediate feature map thus generating more realistic images. A large number of experiments on different size images on the Dayton dataset[1] show that our model can produce better results than state-of-the-art methods.",cs.CV,Computer Vision
Context-Aware RCNN: A Baseline for Action Detection in Videos,"Video action detection approaches usually conduct actor-centric action recognition over RoI-pooled features following the standard pipeline of Faster-RCNN. In this work, we first empirically find the recognition accuracy is highly correlated with the bounding box size of an actor, and thus higher resolution of actors contributes to better performance. However, video models require dense sampling in time to achieve accurate recognition. To fit in GPU memory, the frames to backbone network must be kept low-resolution, resulting in a coarse feature map in RoI-Pooling layer. Thus, we revisit RCNN for actor-centric action recognition via cropping and resizing image patches around actors before feature extraction with I3D deep network. Moreover, we found that expanding actor bounding boxes slightly and fusing the context features can further boost the performance. Consequently, we develop a surpringly effective baseline (Context-Aware RCNN) and it achieves new state-of-the-art results on two challenging action detection benchmarks of AVA and JHMDB. Our observations challenge the conventional wisdom of RoI-Pooling based pipeline and encourage researchers rethink the importance of resolution in actor-centric action recognition. Our approach can serve as a strong baseline for video action detection and is expected to inspire new ideas for this filed. The code is available at \url{https://github.com/MCG-NJU/CRCNN-Action}.",cs.CV,Computer Vision
Interpretable Foreground Object Search As Knowledge Distillation,"This paper proposes a knowledge distillation method for foreground object search (FoS). Given a background and a rectangle specifying the foreground location and scale, FoS retrieves compatible foregrounds in a certain category for later image composition. Foregrounds within the same category can be grouped into a small number of patterns. Instances within each pattern are compatible with any query input interchangeably. These instances are referred to as interchangeable foregrounds. We first present a pipeline to build pattern-level FoS dataset containing labels of interchangeable foregrounds. We then establish a benchmark dataset for further training and testing following the pipeline. As for the proposed method, we first train a foreground encoder to learn representations of interchangeable foregrounds. We then train a query encoder to learn query-foreground compatibility following a knowledge distillation framework. It aims to transfer knowledge from interchangeable foregrounds to supervise representation learning of compatibility. The query feature representation is projected to the same latent space as interchangeable foregrounds, enabling very efficient and interpretable instance-level search. Furthermore, pattern-level search is feasible to retrieve more controllable, reasonable and diverse foregrounds. The proposed method outperforms the previous state-of-the-art by 10.42% in absolute difference and 24.06% in relative improvement evaluated by mean average precision (mAP). Extensive experimental results also demonstrate its efficacy from various aspects. The benchmark dataset and code will be release shortly.",cs.CV,Computer Vision
Complementary Boundary Generator with Scale-Invariant Relation Modeling for Temporal Action Localization: Submission to ActivityNet Challenge 2020,"This technical report presents an overview of our solution used in the submission to ActivityNet Challenge 2020 Task 1 (\textbf{temporal action localization/detection}). Temporal action localization requires to not only precisely locate the temporal boundaries of action instances, but also accurately classify the untrimmed videos into specific categories. In this paper, we decouple the temporal action localization task into two stages (i.e. proposal generation and classification) and enrich the proposal diversity through exhaustively exploring the influences of multiple components from different but complementary perspectives. Specifically, in order to generate high-quality proposals, we consider several factors including the video feature encoder, the proposal generator, the proposal-proposal relations, the scale imbalance, and ensemble strategy. Finally, in order to obtain accurate detections, we need to further train an optimal video classifier to recognize the generated proposals. Our proposed scheme achieves the state-of-the-art performance on the temporal action localization task with \textbf{42.26} average mAP on the challenge testing set.",cs.CV,Computer Vision
Solving Long-tailed Recognition with Deep Realistic Taxonomic Classifier,"Long-tail recognition tackles the natural non-uniformly distributed data in real-world scenarios. While modern classifiers perform well on populated classes, its performance degrades significantly on tail classes. Humans, however, are less affected by this since, when confronted with uncertain examples, they simply opt to provide coarser predictions. Motivated by this, a deep realistic taxonomic classifier (Deep-RTC) is proposed as a new solution to the long-tail problem, combining realism with hierarchical predictions. The model has the option to reject classifying samples at different levels of the taxonomy, once it cannot guarantee the desired performance. Deep-RTC is implemented with a stochastic tree sampling during training to simulate all possible classification conditions at finer or coarser levels and a rejection mechanism at inference time. Experiments on the long-tailed version of four datasets, CIFAR100, AWA2, Imagenet, and iNaturalist, demonstrate that the proposed approach preserves more information on all classes with different popularity levels. Deep-RTC also outperforms the state-of-the-art methods in longtailed recognition, hierarchical classification, and learning with rejection literature using the proposed correctly predicted bits (CPB) metric.",cs.CV,Computer Vision
Evaluating a Simple Retraining Strategy as a Defense Against Adversarial Attacks,"Though deep neural networks (DNNs) have shown superiority over other techniques in major fields like computer vision, natural language processing, robotics, recently, it has been proven that they are vulnerable to adversarial attacks. The addition of a simple, small and almost invisible perturbation to the original input image can be used to fool DNNs into making wrong decisions. With more attack algorithms being designed, a need for defending the neural networks from such attacks arises. Retraining the network with adversarial images is one of the simplest techniques. In this paper, we evaluate the effectiveness of such a retraining strategy in defending against adversarial attacks. We also show how simple algorithms like KNN can be used to determine the labels of the adversarial images needed for retraining. We present the results on two standard datasets namely, CIFAR-10 and TinyImageNet.",cs.CV,Computer Vision
A Macro-Micro Weakly-supervised Framework for AS-OCT Tissue Segmentation,"Primary angle closure glaucoma (PACG) is the leading cause of irreversible blindness among Asian people. Early detection of PACG is essential, so as to provide timely treatment and minimize the vision loss. In the clinical practice, PACG is diagnosed by analyzing the angle between the cornea and iris with anterior segment optical coherence tomography (AS-OCT). The rapid development of deep learning technologies provides the feasibility of building a computer-aided system for the fast and accurate segmentation of cornea and iris tissues. However, the application of deep learning methods in the medical imaging field is still restricted by the lack of enough fully-annotated samples. In this paper, we propose a novel framework to segment the target tissues accurately for the AS-OCT images, by using the combination of weakly-annotated images (majority) and fully-annotated images (minority). The proposed framework consists of two models which provide reliable guidance for each other. In addition, uncertainty guided strategies are adopted to increase the accuracy and stability of the guidance. Detailed experiments on the publicly available AGE dataset demonstrate that the proposed framework outperforms the state-of-the-art semi-/weakly-supervised methods and has a comparable performance as the fully-supervised method. Therefore, the proposed method is demonstrated to be effective in exploiting information contained in the weakly-annotated images and has the capability to substantively relieve the annotation workload.",cs.CV,Computer Vision
Making Affine Correspondences Work in Camera Geometry Computation,"Local features e.g. SIFT and its affine and learned variants provide region-to-region rather than point-to-point correspondences. This has recently been exploited to create new minimal solvers for classical problems such as homography, essential and fundamental matrix estimation. The main advantage of such solvers is that their sample size is smaller, e.g., only two instead of four matches are required to estimate a homography. Works proposing such solvers often claim a significant improvement in run-time thanks to fewer RANSAC iterations. We show that this argument is not valid in practice if the solvers are used naively. To overcome this, we propose guidelines for effective use of region-to-region matches in the course of a full model estimation pipeline. We propose a method for refining the local feature geometries by symmetric intensity-based matching, combine uncertainty propagation inside RANSAC with preemptive model verification, show a general scheme for computing uncertainty of minimal solvers results, and adapt the sample cheirality check for homography estimation. Our experiments show that affine solvers can achieve accuracy comparable to point-based solvers at faster run-times when following our guidelines. We make code available at https://github.com/danini/affine-correspondences-for-camera-geometry.",cs.CV,Computer Vision
Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting,"Recent end-to-end trainable methods for scene text spotting, integrating detection and recognition, showed much progress. However, most of the current arbitrary-shape scene text spotters use region proposal networks (RPN) to produce proposals. RPN relies heavily on manually designed anchors and its proposals are represented with axis-aligned rectangles. The former presents difficulties in handling text instances of extreme aspect ratios or irregular shapes, and the latter often includes multiple neighboring instances into a single proposal, in cases of densely oriented text. To tackle these problems, we propose Mask TextSpotter v3, an end-to-end trainable scene text spotter that adopts a Segmentation Proposal Network (SPN) instead of an RPN. Our SPN is anchor-free and gives accurate representations of arbitrary-shape proposals. It is therefore superior to RPN in detecting text instances of extreme aspect ratios or irregular shapes. Furthermore, the accurate proposals produced by SPN allow masked RoI features to be used for decoupling neighboring text instances. As a result, our Mask TextSpotter v3 can handle text instances of extreme aspect ratios or irregular shapes, and its recognition accuracy won't be affected by nearby text or background noise. Specifically, we outperform state-of-the-art methods by 21.9 percent on the Rotated ICDAR 2013 dataset (rotation robustness), 5.9 percent on the Total-Text dataset (shape robustness), and achieve state-of-the-art performance on the MSRA-TD500 dataset (aspect ratio robustness). Code is available at: https://github.com/MhLiao/MaskTextSpotterV3",cs.CV,Computer Vision
MIX'EM: Unsupervised Image Classification using a Mixture of Embeddings,"We present MIX'EM, a novel solution for unsupervised image classification. MIX'EM generates representations that by themselves are sufficient to drive a general-purpose clustering algorithm to deliver high-quality classification. This is accomplished by building a mixture of embeddings module into a contrastive visual representation learning framework in order to disentangle representations at the category level. It first generates a set of embedding and mixing coefficients from a given visual representation, and then combines them into a single embedding. We introduce three techniques to successfully train MIX'EM and avoid degenerate solutions; (i) diversify mixture components by maximizing entropy, (ii) minimize instance conditioned component entropy to enforce a clustered embedding space, and (iii) use an associative embedding loss to enforce semantic separability. By applying (i) and (ii), semantic categories emerge through the mixture coefficients, making it possible to apply (iii). Subsequently, we run K-means on the representations to acquire semantic classification. We conduct extensive experiments and analyses on STL10, CIFAR10, and CIFAR100-20 datasets, achieving state-of-the-art classification accuracy of 78\%, 82\%, and 44\%, respectively. To achieve robust and high accuracy, it is essential to use the mixture components to initialize K-means. Finally, we report competitive baselines (70\% on STL10) obtained by applying K-means to the ""normalized"" representations learned using the contrastive loss.",cs.CV,Computer Vision
ContactPose: A Dataset of Grasps with Object Contact and Hand Pose,"Grasping is natural for humans. However, it involves complex hand configurations and soft tissue deformation that can result in complicated regions of contact between the hand and the object. Understanding and modeling this contact can potentially improve hand models, AR/VR experiences, and robotic grasping. Yet, we currently lack datasets of hand-object contact paired with other data modalities, which is crucial for developing and evaluating contact modeling techniques. We introduce ContactPose, the first dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. Analysis of ContactPose data reveals interesting relationships between hand pose and contact. We use this data to rigorously evaluate various data representations, heuristics from the literature, and learning methods for contact modeling. Data, code, and trained models are available at https://contactpose.cc.gatech.edu.",cs.CV,Computer Vision
Sat2Graph: Road Graph Extraction through Graph-Tensor Encoding,"Inferring road graphs from satellite imagery is a challenging computer vision task. Prior solutions fall into two categories: (1) pixel-wise segmentation-based approaches, which predict whether each pixel is on a road, and (2) graph-based approaches, which predict the road graph iteratively. We find that these two approaches have complementary strengths while suffering from their own inherent limitations.
  In this paper, we propose a new method, Sat2Graph, which combines the advantages of the two prior categories into a unified framework. The key idea in Sat2Graph is a novel encoding scheme, graph-tensor encoding (GTE), which encodes the road graph into a tensor representation. GTE makes it possible to train a simple, non-recurrent, supervised model to predict a rich set of features that capture the graph structure directly from an image. We evaluate Sat2Graph using two large datasets. We find that Sat2Graph surpasses prior methods on two widely used metrics, TOPO and APLS. Furthermore, whereas prior work only infers planar road graphs, our approach is capable of inferring stacked roads (e.g., overpasses), and does so robustly.",cs.CV,Computer Vision
Predicting risk of late age-related macular degeneration using deep learning,"By 2040, age-related macular degeneration (AMD) will affect approximately 288 million people worldwide. Identifying individuals at high risk of progression to late AMD, the sight-threatening stage, is critical for clinical actions, including medical interventions and timely monitoring. Although deep learning has shown promise in diagnosing/screening AMD using color fundus photographs, it remains difficult to predict individuals' risks of late AMD accurately. For both tasks, these initial deep learning attempts have remained largely unvalidated in independent cohorts. Here, we demonstrate how deep learning and survival analysis can predict the probability of progression to late AMD using 3,298 participants (over 80,000 images) from the Age-Related Eye Disease Studies AREDS and AREDS2, the largest longitudinal clinical trials in AMD. When validated against an independent test dataset of 601 participants, our model achieved high prognostic accuracy (five-year C-statistic 86.4 (95% confidence interval 86.2-86.6)) that substantially exceeded that of retinal specialists using two existing clinical standards (81.3 (81.1-81.5) and 82.0 (81.8-82.3), respectively). Interestingly, our approach offers additional strengths over the existing clinical standards in AMD prognosis (e.g., risk ascertainment above 50%) and is likely to be highly generalizable, given the breadth of training data from 82 US retinal specialty clinics. Indeed, during external validation through training on AREDS and testing on AREDS2 as an independent cohort, our model retained substantially higher prognostic accuracy than existing clinical standards. These results highlight the potential of deep learning systems to enhance clinical decision-making in AMD patients.",cs.CV,Computer Vision
PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments,"Object detection using an oriented bounding box (OBB) can better target rotated objects by reducing the overlap with background areas. Existing OBB approaches are mostly built on horizontal bounding box detectors by introducing an additional angle dimension optimized by a distance loss. However, as the distance loss only minimizes the angle error of the OBB and that it loosely correlates to the IoU, it is insensitive to objects with high aspect ratios. Therefore, a novel loss, Pixels-IoU (PIoU) Loss, is formulated to exploit both the angle and IoU for accurate OBB regression. The PIoU loss is derived from IoU metric with a pixel-wise form, which is simple and suitable for both horizontal and oriented bounding box. To demonstrate its effectiveness, we evaluate the PIoU loss on both anchor-based and anchor-free frameworks. The experimental results show that PIoU loss can dramatically improve the performance of OBB detectors, particularly on objects with high aspect ratios and complex backgrounds. Besides, previous evaluation datasets did not include scenarios where the objects have high aspect ratios, hence a new dataset, Retail50K, is introduced to encourage the community to adapt OBB detectors for more complex environments.",cs.CV,Computer Vision
Object-Centric Multi-View Aggregation,"We present an approach for aggregating a sparse set of views of an object in order to compute a semi-implicit 3D representation in the form of a volumetric feature grid. Key to our approach is an object-centric canonical 3D coordinate system into which views can be lifted, without explicit camera pose estimation, and then combined -- in a manner that can accommodate a variable number of views and is view order independent. We show that computing a symmetry-aware mapping from pixels to the canonical coordinate system allows us to better propagate information to unseen regions, as well as to robustly overcome pose ambiguities during inference. Our aggregate representation enables us to perform 3D inference tasks like volumetric reconstruction and novel view synthesis, and we use these tasks to demonstrate the benefits of our aggregation approach as compared to implicit or camera-centric alternatives.",cs.CV,Computer Vision
Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation,"Our contribution is a unified cross-modality feature disentagling approach for multi-domain image translation and multiple organ segmentation. Using CT as the labeled source domain, our approach learns to segment multi-modal (T1-weighted and T2-weighted) MRI having no labeled data. Our approach uses a variational auto-encoder (VAE) to disentangle the image content from style. The VAE constrains the style feature encoding to match a universal prior (Gaussian) that is assumed to span the styles of all the source and target modalities. The extracted image style is converted into a latent style scaling code, which modulates the generator to produce multi-modality images according to the target domain code from the image content features. Finally, we introduce a joint distribution matching discriminator that combines the translated images with task-relevant segmentation probability maps to further constrain and regularize image-to-image (I2I) translations. We performed extensive comparisons to multiple state-of-the-art I2I translation and segmentation methods. Our approach resulted in the lowest average multi-domain image reconstruction error of 1.34$\pm$0.04. Our approach produced an average Dice similarity coefficient (DSC) of 0.85 for T1w and 0.90 for T2w MRI for multi-organ segmentation, which was highly comparable to a fully supervised MRI multi-organ segmentation network (DSC of 0.86 for T1w and 0.90 for T2w MRI).",cs.CV,Computer Vision
E$^2$Net: An Edge Enhanced Network for Accurate Liver and Tumor Segmentation on CT Scans,"Developing an effective liver and liver tumor segmentation model from CT scans is very important for the success of liver cancer diagnosis, surgical planning and cancer treatment. In this work, we propose a two-stage framework for 2D liver and tumor segmentation. The first stage is a coarse liver segmentation network, while the second stage is an edge enhanced network (E$^2$Net) for more accurate liver and tumor segmentation. E$^2$Net explicitly models complementary objects (liver and tumor) and their edge information within the network to preserve the organ and lesion boundaries. We introduce an edge prediction module in E$^2$Net and design an edge distance map between liver and tumor boundaries, which is used as an extra supervision signal to train the edge enhanced network. We also propose a deep cross feature fusion module to refine multi-scale features from both objects and their edges. E$^2$Net is more easily and efficiently trained with a small labeled dataset, and it can be trained/tested on the original 2D CT slices (resolve resampling error issue in 3D models). The proposed framework has shown superior performance on both liver and liver tumor segmentation compared to several state-of-the-art 2D, 3D and 2D/3D hybrid frameworks.",cs.CV,Computer Vision
A Gated and Bifurcated Stacked U-Net Module for Document Image Dewarping,"Capturing images of documents is one of the easiest and most used methods of recording them. These images however, being captured with the help of handheld devices, often lead to undesirable distortions that are hard to remove. We propose a supervised Gated and Bifurcated Stacked U-Net module to predict a dewarping grid and create a distortion free image from the input. While the network is trained on synthetically warped document images, results are calculated on the basis of real world images. The novelty in our methods exists not only in a bifurcation of the U-Net to help eliminate the intermingling of the grid coordinates, but also in the use of a gated network which adds boundary and other minute line level details to the model. The end-to-end pipeline proposed by us achieves state-of-the-art performance on the DocUNet dataset after being trained on just 8 percent of the data used in previous methods.",cs.CV,Computer Vision
Feature-metric Loss for Self-supervised Learning of Depth and Egomotion,"Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by $_1$ for depth estimation, and significantly outperforms previous method for visual odometry.",cs.CV,Computer Vision
Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images,"Semi-supervised learning has attracted much attention in medical image segmentation due to challenges in acquiring pixel-wise image annotations, which is a crucial step for building high-performance deep learning methods. Most existing semi-supervised segmentation approaches either tend to neglect geometric constraint in object segments, leading to incomplete object coverage, or impose strong shape prior that requires extra alignment. In this work, we propose a novel shapeaware semi-supervised segmentation strategy to leverage abundant unlabeled data and to enforce a geometric shape constraint on the segmentation output. To achieve this, we develop a multi-task deep network that jointly predicts semantic segmentation and signed distance map(SDM) of object surfaces. During training, we introduce an adversarial loss between the predicted SDMs of labeled and unlabeled data so that our network is able to capture shape-aware features more effectively. Experiments on the Atrial Segmentation Challenge dataset show that our method outperforms current state-of-the-art approaches with improved shape estimation, which validates its efficacy. Code is available at https://github.com/kleinzcy/SASSnet.",cs.CV,Computer Vision
Analysis of Safe Ultrawideband Human-Robot Communication in Automated Collaborative Warehouse,"The paper presents the propagation analysis of ultrawideband Gaussian signal in an automated collaborative warehouse environment where human and robots communicate to ensure that mutual collisions do not occur. The warehouse racks are principally modeled as clusters of metallic (PEC) parallelepipeds, with dimensions chosen to approximate the realistic warehouse. The signal propagation is analyzed using a ray tracing software, with the goal to calculate the path loss profile for different representative scenarios and antenna polarizations. The influence of the rack surface roughness onto propagation is also analyzed. The guidelines for optimum antenna positions on humans and robots for safe communication are proposed according to the simulations results.",cs.RO,Robotics
High-Speed Robot Navigation using Predicted Occupancy Maps,"Safe and high-speed navigation is a key enabling capability for real world deployment of robotic systems. A significant limitation of existing approaches is the computational bottleneck associated with explicit mapping and the limited field of view (FOV) of existing sensor technologies. In this paper, we study algorithmic approaches that allow the robot to predict spaces extending beyond the sensor horizon for robust planning at high speeds. We accomplish this using a generative neural network trained from real-world data without requiring human annotated labels. Further, we extend our existing control algorithms to support leveraging the predicted spaces to improve collision-free planning and navigation at high speeds. Our experiments are conducted on a physical robot based on the MIT race car using an RGBD sensor where were able to demonstrate improved performance at 4 m/s compared to a controller not operating on predicted regions of the map.",cs.RO,Robotics
Robotic Following of Flexible Extended Objects: Relevant Technical Facts on the Kinematics of a Moving Continuum,The paper offers general technical facts on the kinematics of a moving continuum involved in research on robotic following of flexible extended objects.,cs.RO,Robotics
GIS-Based Estimation of Seasonal Solar Energy Potential for Parking Lots and Roads,"The amount of sun cast on roads and parking lots determines the charging opportunities for solar vehicles and impacts the efficiency of conventional vehicles. Estimates of solar energy potential on urban surfaces to assess parking and driving conditions need to account for the shadows cast by surrounding trees and buildings. However, though existing GIS tools can calculate solar potential on surfaces that have buildings and trees, these tools do not estimate the conditions beneath trees and do not consider the seasonal changes in deciduous trees. We introduce a new approach to address these factors using pixel substitution and a light penetration factor. In this paper, we describe how to integrate these techniques into a workflow for computing solar potential estimates for parking and driving conditions. We demonstrate the methodology in an urban setting in North Carolina that includes a mixture of urban structures and trees. We provide code samples so that this workflow is easily repeatable. The solar maps produced with our method are a useful resource for planning solar vehicle parking and routing, and identifying shaded conditions for conventional vehicles.",cs.RO,Robotics
Modeling Dispositional and Initial learned Trust in Automated Vehicles with Predictability and Explainability,"Technological advances in the automotive industry are bringing automated driving closer to road use. However, one of the most important factors affecting public acceptance of automated vehicles (AVs) is the public's trust in AVs. Many factors can influence people's trust, including perception of risks and benefits, feelings, and knowledge of AVs. This study aims to use these factors to predict people's dispositional and initial learned trust in AVs using a survey study conducted with 1175 participants. For each participant, 23 features were extracted from the survey questions to capture his or her knowledge, perception, experience, behavioral assessment, and feelings about AVs. These features were then used as input to train an eXtreme Gradient Boosting (XGBoost) model to predict trust in AVs. With the help of SHapley Additive exPlanations (SHAP), we were able to interpret the trust predictions of XGBoost to further improve the explainability of the XGBoost model. Compared to traditional regression models and black-box machine learning models, our findings show that this approach was powerful in providing a high level of explainability and predictability of trust in AVs, simultaneously.",cs.RO,Robotics
One-Shot Object Localization Using Learnt Visual Cues via Siamese Networks,"A robot that can operate in novel and unstructured environments must be capable of recognizing new, previously unseen, objects. In this work, a visual cue is used to specify a novel object of interest which must be localized in new environments. An end-to-end neural network equipped with a Siamese network is used to learn the cue, infer the object of interest, and then to localize it in new environments. We show that a simulated robot can pick-and-place novel objects pointed to by a laser pointer. We also evaluate the performance of the proposed approach on a dataset derived from the Omniglot handwritten character dataset and on a small dataset of toys.",cs.RO,Robotics
"Modeling, Vibration Control, and Trajectory Tracking of a Kinematically Constrained Planar Hybrid Cable-Driven Parallel Robot","This paper presents a kinematically constrained planar hybrid cable-driven parallel robot (HCDPR) for warehousing applications as well as other potential applications such as rehabilitation. The proposed HCDPR can harness the strengths and benefits of serial and cable-driven parallel robots. Based on this robotic platform, the goal in this paper is to develop an integrated control system to reduce vibrations and improve the trajectory accuracy and performance of the HCDPR, including deriving kinematic and dynamic equations, proposing solutions for redundancy resolution and optimization of stiffness, and developing two motion and vibration control strategies (controllers I and II). Finally, different case studies are conducted to evaluate the control performance, and the results show that the controller II can achieve the goal better.",cs.RO,Robotics
Accurate Energetic Constraints for Passive Grasp Stability Analysis,"Passive reaction effects in grasp stability analysis occur when the contact forces and joint torques applied by a grasp change in response to external disturbances applied to the grasped object. For example, nonbackdrivable actuators (e.g. highly geared servos) will passively resist external disturbances without an actively applied command; for numerous robot hands using such motors, these effects can be highly beneficial as they increase grasp resistance without requiring active control. We introduce a grasp stability analysis method that can model these effects, and, for a given grasp, distinguish between disturbances that will be passively resisted and those that will not. We find that, in order to achieve this, the grasp model must include accurate energetic constraints. One way to achieve this is to consider the Maximum Dissipation Principle (MDP), a part of the Coulomb friction model that is rarely used in grasp stability analysis. However, the MDP constraints are non-convex, and difficult to solve efficiently. We thus introduce a convex relaxation method, along with an algorithm that successively refines this relaxation locally in order to obtain solutions to arbitrary accuracy efficiently. Our resulting algorithm can determine if a grasp is passively stable, solve for equilibrium contact forces and compute optimal actuator commands for stability. Its implementation is publicly available as part of the open-source GraspIt! simulator.",cs.RO,Robotics
MAVSec: Securing the MAVLink Protocol for Ardupilot/PX4 Unmanned Aerial Systems,"The MAVLink is a lightweight communication protocol between Unmanned Aerial Vehicles (UAVs) and ground control stations (GCSs). It defines a set of bi-directional messages exchanged between a UAV (aka drone) and a ground station. The messages carry out information about the UAV's states and control commands sent from the ground station. However, the MAVLink protocol is not secure and has several vulnerabilities to different attacks that result in critical threats and safety concerns. Very few studies provided solutions to this problem. In this paper, we discuss the security vulnerabilities of the MAVLink protocol and propose MAVSec, a security-integrated mechanism for MAVLink that leverages the use of encryption algorithms to ensure the protection of exchanged MAVLink messages between UAVs and GCSs. To validate MAVSec, we implemented it in Ardupilot and evaluated the performance of different encryption algorithms (i.e. AES-CBC, AES-CTR, RC4, and ChaCha20) in terms of memory usage and CPU consumption. The experimental results show that ChaCha20 has a better performance and is more efficient than other encryption algorithms. Integrating ChaCha20 into MAVLink can guarantee its messages confidentiality, without affecting its performance, while occupying less memory and CPU consumption, thus, preserving memory and saving the battery for the resource-constrained drone.",cs.RO,Robotics
Robust Dense Mapping for Large-Scale Dynamic Environments,"We present a stereo-based dense mapping algorithm for large-scale dynamic urban environments. In contrast to other existing methods, we simultaneously reconstruct the static background, the moving objects, and the potentially moving but currently stationary objects separately, which is desirable for high-level mobile robotic tasks such as path planning in crowded environments. We use both instance-aware semantic segmentation and sparse scene flow to classify objects as either background, moving, or potentially moving, thereby ensuring that the system is able to model objects with the potential to transition from static to dynamic, such as parked cars. Given camera poses estimated from visual odometry, both the background and the (potentially) moving objects are reconstructed separately by fusing the depth maps computed from the stereo input. In addition to visual odometry, sparse scene flow is also used to estimate the 3D motions of the detected moving objects, in order to reconstruct them accurately. A map pruning technique is further developed to improve reconstruction accuracy and reduce memory consumption, leading to increased scalability. We evaluate our system thoroughly on the well-known KITTI dataset. Our system is capable of running on a PC at approximately 2.5Hz, with the primary bottleneck being the instance-aware semantic segmentation, which is a limitation we hope to address in future work. The source code is available from the project website (http://andreibarsan.github.io/dynslam).",cs.RO,Robotics
Training a Fast Object Detector for LiDAR Range Images Using Labeled Data from Sensors with Higher Resolution,"In this paper, we describe a strategy for training neural networks for object detection in range images obtained from one type of LiDAR sensor using labeled data from a different type of LiDAR sensor. Additionally, an efficient model for object detection in range images for use in self-driving cars is presented. Currently, the highest performing algorithms for object detection from LiDAR measurements are based on neural networks. Training these networks using supervised learning requires large annotated datasets. Therefore, most research using neural networks for object detection from LiDAR point clouds is conducted on a very small number of publicly available datasets. Consequently, only a small number of sensor types are used. We use an existing annotated dataset to train a neural network that can be used with a LiDAR sensor that has a lower resolution than the one used for recording the annotated dataset. This is done by simulating data from the lower resolution LiDAR sensor based on the higher resolution dataset. Furthermore, improvements to models that use LiDAR range images for object detection are presented. The results are validated using both simulated sensor data and data from an actual lower resolution sensor mounted to a research vehicle. It is shown that the model can detect objects from 360 range images in real time.",cs.RO,Robotics
Cooperative Distributed Robust Control of Modular Mobile Robots with Bounded Curvature and Velocity,"A novel motion control system for Compliant Framed wheeled Modular Mobile Robots (CFMMR) is studied in this paper. This type of wheeled mobile robot uses rigid axles coupled by compliant frame modules to provide both full suspension and enhanced steering capability without additional hardware. The proposed control system is developed by combining a bounded curvature-based kinematic controller and a nonlinear damping dynamic controller. In particular, multiple forms of controller interaction are examined. A twoaxle scout CFMMR configuration is used to evaluate the different control structures. Experimental results verify efficient motion control of posture regulation.",cs.RO,Robotics
Relation-Shape Convolutional Neural Network for Point Cloud Analysis,"Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.",cs.RO,Robotics
Exploration of Self-Propelling Droplets Using a Curiosity Driven Robotic Assistant,"We describe a chemical robotic assistant equipped with a curiosity algorithm (CA) that can efficiently explore the state a complex chemical system can exhibit. The CA-robot is designed to explore formulations in an open-ended way with no explicit optimization target. By applying the CA-robot to the study of self-propelling multicomponent oil-in-water droplets, we are able to observe an order of magnitude more variety of droplet behaviours than possible with a random parameter search and given the same budget. We demonstrate that the CA-robot enabled the discovery of a sudden and highly specific response of droplets to slight temperature changes. Six modes of self-propelled droplets motion were identified and classified using a time-temperature phase diagram and probed using a variety of techniques including NMR. This work illustrates how target free search can significantly increase the rate of unpredictable observations leading to new discoveries with potential applications in formulation chemistry.",cs.RO,Robotics
Globally optimal vertical direction estimation in Atlanta World,"In man-made environments, such as indoor and urban scenes, most of the objects and structures are organized in the form of orthogonal and parallel planes. These planes can be approximated by the Atlanta world assumption, in which the normals of planes can be represented by the Atlanta frames. Atlanta world assumption, which can be considered as a generalized Manhattan world assumption, has one vertical frame and multiple horizontal frames. Conventionally, given a set of inputs such as surface normals, the Atlanta frame estimation problem can be solved in one-time by branch-and-bound (BnB). However, the runtime of the BnB algorithm will increase greatly when the dimensionality (i.e., the number of horizontal frames) increases. In this paper, we estimate only the vertical direction instead of all Atlanta frames at once. Accordingly, we propose a vertical direction estimation method by considering the relationship between the vertical frame and horizontal frames. Concretely, our approach employs a BnB algorithm to search the vertical direction guaranteeing global optimality without requiring prior knowledge of the number of Atlanta frames. Four novel bounds by mapping 3D-hemisphere to a 2D region are investigated to guarantee convergence. We verify the validity of the proposed method in various challenging synthetic and real-world data.",cs.RO,Robotics
Challenges of Real-World Reinforcement Learning,"Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.",cs.RO,Robotics
Frontal Plane Bipedal Zero Dynamics Control,"In bipedal gait design literature, one of the common ways of generating stable 3D walking gait is by designing the frontal and sagittal controllers as decoupled dynamics. The study of the decoupled frontal dynamics is, however, still understudied if compared with the sagittal dynamics. In this paper it is presented a formal approach to the problem of frontal dynamics stabilization by extending the hybrid zero dynamics framework to deal with the frontal gait design problem.",cs.RO,Robotics
Self-supervised Body Image Acquisition Using a Deep Neural Network for Sensorimotor Prediction,"This work investigates how a naive agent can acquire its own body image in a self-supervised way, based on the predictability of its sensorimotor experience. Our working hypothesis is that, due to its temporal stability, an agent's body produces more consistent sensory experiences than the environment, which exhibits a greater variability. Given its motor experience, an agent can thus reliably predict what appearance its body should have. This intrinsic predictability can be used to automatically isolate the body image from the rest of the environment. We propose a two-branches deconvolutional neural network to predict the visual sensory state associated with an input motor state, as well as the prediction error associated with this input. We train the network on a dataset of first-person images collected with a simulated Pepper robot, and show how the network outputs can be used to automatically isolate its visible arm from the rest of the environment. Finally, the quality of the body image produced by the network is evaluated.",cs.RO,Robotics
Grid-based Localization Stack for Inspection Drones towards Automation of Large Scale Warehouse Systems,"SLAM based techniques are often adopted for solving the navigation problem for the drones in GPS denied environment. Despite the widespread success of these approaches, they have not yet been fully exploited for automation in a warehouse system due to expensive sensors and setup requirements. This paper focuses on the use of low-cost monocular camera-equipped drones for performing warehouse management tasks like inventory scanning and position update. The methods introduced are at par with the existing state of warehouse environment present today, that is, the existence of a grid network for the ground vehicles, hence eliminating any additional infrastructure requirement for drone deployment. As we lack scale information, that in itself forbids us to use any 3D techniques, we focus more towards optimizing standard image processing algorithms like the thick line detection and further developing it into a fast and robust grid localization framework. In this paper, we show different line detection algorithms, their significance in grid localization and their limitations. We further extend our proposed implementation towards a real-time navigation stack for an actual warehouse inspection case scenario. Our line detection method using skeletonization and centroid strategy works considerably even with varying light conditions, line thicknesses, colors, orientations, and partial occlusions. A simple yet effective Kalman Filter has been used for smoothening the  and  outputs of the two different line detection methods for better drone control while grid following. A generic strategy that handles the navigation of the drone on a grid for completion of the allotted task is also developed. Based on the simulation and real-life experiments, the final developments on the drone localization and navigation in a structured environment are discussed.",cs.RO,Robotics
Rapidly-Exploring Quotient-Space Trees: Motion Planning using Sequential Simplifications,"Motion planning problems can be simplified by admissible projections of the configuration space to sequences of lower-dimensional quotient-spaces, called sequential simplifications. To exploit sequential simplifications, we present the Quotient-space Rapidly-exploring Random Trees (QRRT) algorithm. QRRT takes as input a start and a goal configuration, and a sequence of quotient-spaces. The algorithm grows trees on the quotient-spaces both sequentially and simultaneously to guarantee a dense coverage. QRRT is shown to be (1) probabilistically complete, and (2) can reduce the runtime by at least one order of magnitude. However, we show in experiments that the runtime varies substantially between different quotient-space sequences. To find out why, we perform an additional experiment, showing that the more narrow an environment, the more a quotient-space sequence can reduce runtime.",cs.RO,Robotics
Unlucky Explorer: A Complete non-Overlapping Map Exploration,"Nowadays, the field of Artificial Intelligence in Computer Games (AI in Games) is going to be more alluring since computer games challenge many aspects of AI with a wide range of problems, particularly general problems. One of these kinds of problems is Exploration, which states that an unknown environment must be explored by one or several agents. In this work, we have first introduced the Maze Dash puzzle as an exploration problem where the agent must find a Hamiltonian Path visiting all the cells. Then, we have investigated to find suitable methods by a focus on Monte-Carlo Tree Search (MCTS) and SAT to solve this puzzle quickly and accurately. An optimization has been applied to the proposed MCTS algorithm to obtain a promising result. Also, since the prefabricated test cases of this puzzle are not large enough to assay the proposed method, we have proposed and employed a technique to generate solvable test cases to evaluate the approaches. Eventually, the MCTS-based method has been assessed by the auto-generated test cases and compared with our implemented SAT approach that is considered a good rival. Our comparison indicates that the MCTS-based approach is an up-and-coming method that could cope with the test cases with small and medium sizes with faster run-time compared to SAT. However, for certain discussed reasons, including the features of the problem, tree search organization, and also the approach of MCTS in the Simulation step, MCTS takes more time to execute in Large size scenarios. Consequently, we have found the bottleneck for the MCTS-based method in significant test cases that could be improved in two real-world problems.",cs.RO,Robotics
"VIR-SLAM: Visual, Inertial, and Ranging SLAM for single and multi-robot systems","Monocular cameras coupled with inertial measurements generally give high performance visual inertial odometry. However, drift can be significant with long trajectories, especially when the environment is visually challenging. In this paper, we propose a system that leverages ultra-wideband ranging with one static anchor placed in the environment to correct the accumulated error whenever the anchor is visible. We also use this setup for collaborative SLAM: different robots use mutual ranging (when available) and the common anchor to estimate the transformation between each other, facilitating map fusion Our system consists of two modules: a double layer ranging, visual, and inertial odometry for single robots, and a transformation estimation module for collaborative SLAM. We test our system on public datasets by simulating an ultra-wideband sensor as well as on real robots. Experiments show our method can outperform state-of-the-art visual-inertial odometry by more than 20%. For visually challenging environments, our method works even the visual-inertial odometry has significant drift Furthermore, we can compute the collaborative SLAM transformation matrix at almost no extra computation cost.",cs.RO,Robotics
HMPO: Human Motion Prediction in Occluded Environments for Safe Motion Planning,"We present a novel approach to generate collision-free trajectories for a robot operating in close proximity with a human obstacle in an occluded environment. The self-occlusions of the robot can significantly reduce the accuracy of human motion prediction, and we present a novel deep learning-based prediction algorithm. Our formulation uses CNNs and LSTMs and we augment human-action datasets with synthetically generated occlusion information for training. We also present an occlusion-aware planner that uses our motion prediction algorithm to compute collision-free trajectories. We highlight performance of the overall approach (HMPO) in complex scenarios and observe upto 68% performance improvement in motion prediction accuracy, and 38% improvement in terms of error distance between the ground-truth and the predicted human joint positions.",cs.RO,Robotics
MAV Development Towards Navigation in Unknown and Dark Mining Tunnels,"The Mining industry considers the deployment of MAV for autonomous inspection of tunnels and shafts to increase safety and productivity. However, mines are challenging and harsh environments that have a direct effect on the degradation of high-end and expensive utilized components over time. Inspired by this effect, this article presents a low cost and modular platform for designing a fully autonomous navigating MAV without requiring any prior information from the surrounding environment. The design of the proposed aerial vehicle can be considered as a consumable platform that can be instantly replaced in case of damage or defect, thus comes into agreement with the vision of mining companies for utilizing stable aerial robots with reasonable cost. In the proposed design, the operator has access to all on-board data, thus increasing the overall customization of the design and the execution of the mine inspection mission. The MAV platform has a software core based on ROS operating on an Aaeon UP-Board, while it is equipped with a sensor suite to accomplish the autonomous navigation equally reliable when compared to high-end and expensive platforms.",cs.RO,Robotics
Deep Neural Network Based Real-time Kiwi Fruit Flower Detection in an Orchard Environment,"In this paper, we present a novel approach to kiwi fruit flower detection using Deep Neural Networks (DNNs) to build an accurate, fast, and robust autonomous pollination robot system. Recent work in deep neural networks has shown outstanding performance on object detection tasks in many areas. Inspired this, we aim for exploiting DNNs for kiwi fruit flower detection and present intensive experiments and their analysis on two state-of-the-art object detectors; Faster R-CNN and Single Shot Detector (SSD) Net, and feature extractors; Inception Net V2 and NAS Net with real-world orchard datasets. We also compare those approaches to find an optimal model which is suitable for a real-time agricultural pollination robot system in terms of accuracy and processing speed. We perform experiments with dataset collected from different seasons and locations (spatio-temporal consistency) in order to demonstrate the performance of the generalized model. The proposed system demonstrates promising results of 0.919, 0.874, and 0.889 for precision, recall, and F1-score respectively on our real-world dataset, and the performance satisfies the requirement for deploying the system onto an autonomous pollination robotics system.",cs.RO,Robotics
Thermal deflection decoupled 6-DOF pose measurement of hexapods,"Calibration is crucial for hexapods with high-accuracy positioning capability. Many of these calibration procedures require measurement of hexapod's platform pose (position and orientation) at constant temperature. Consequently, thermal deflection of the hexapod's platform during pose measurements impacts the accuracy of calibrated parameters. This paper presents a method to eliminate the impact of thermal deflection of hexapods on their pose measurement. In this method, a reference pose is measured before each measurement of any particular pose. The measurements of reference pose are used to estimate thermal deflection of hexapod's legs. This is in turn used to estimate and correct the resulting pose error at the particular pose to be measured. The advantage of using the proposed method is demonstrated experimentally by means of pose measurements of a high-precision hexapod using a CMM.",cs.RO,Robotics
End-to-end Learning for Inter-Vehicle Distance and Relative Velocity Estimation in ADAS with a Monocular Camera,"Inter-vehicle distance and relative velocity estimations are two basic functions for any ADAS (Advanced driver-assistance systems). In this paper, we propose a monocular camera-based inter-vehicle distance and relative velocity estimation method based on end-to-end training of a deep neural network. The key novelty of our method is the integration of multiple visual clues provided by any two time-consecutive monocular frames, which include deep feature clue, scene geometry clue, as well as temporal optical flow clue. We also propose a vehicle-centric sampling mechanism to alleviate the effect of perspective distortion in the motion field (i.e. optical flow). We implement the method by a light-weight deep neural network. Extensive experiments are conducted which confirm the superior performance of our method over other state-of-the-art methods, in terms of estimation accuracy, computational speed, and memory footprint.",cs.RO,Robotics
An Autonomous Path Planning Method for Unmanned Aerial Vehicle based on A Tangent Intersection and Target Guidance Strategy,"Unmanned aerial vehicle (UAV) path planning enables UAVs to avoid obstacles and reach the target efficiently. To generate high-quality paths without obstacle collision for UAVs, this paper proposes a novel autonomous path planning algorithm based on a tangent intersection and target guidance strategy (APPATT). Guided by a target, the elliptic tangent graph method is used to generate two sub-paths, one of which is selected based on heuristic rules when confronting an obstacle. The UAV flies along the selected sub-path and repeatedly adjusts its flight path to avoid obstacles through this way until the collision-free path extends to the target. Considering the UAV kinematic constraints, the cubic B-spline curve is employed to smooth the waypoints for obtaining a feasible path. Compared with A*, PRM, RRT and VFH, the experimental results show that APPATT can generate the shortest collision-free path within 0.05 seconds for each instance under static environments. Moreover, compared with VFH and RRTRW, APPATT can generate satisfactory collision-free paths under uncertain environments in a nearly real-time manner. It is worth noting that APPATT has the capability of escaping from simple traps within a reasonable time.",cs.RO,Robotics
Robotic Motion Planning using Learned Critical Sources and Local Sampling,"Sampling based methods are widely used for robotic motion planning. Traditionally, these samples are drawn from probabilistic ( or deterministic ) distributions to cover the state space uniformly. Despite being probabilistically complete, they fail to find a feasible path in a reasonable amount of time in constrained environments where it is essential to go through narrow passages (bottleneck regions). Current state of the art techniques train a learning model (learner) to predict samples selectively on these bottleneck regions. However, these algorithms depend completely on samples generated by this learner to navigate through the bottleneck regions. As the complexity of the planning problem increases, the amount of data and time required to make this learner robust to fine variations in the structure of the workspace becomes computationally intractable. In this work, we present (1) an efficient and robust method to use a learner to locate the bottleneck regions and (2) two algorithms that use local sampling methods to leverage the location of these bottleneck regions for efficient motion planning while maintaining probabilistic completeness.
  We test our algorithms on 2 dimensional planning problems and 7 dimensional robotic arm planning, and report significant gains over heuristics as well as learned baselines.",cs.RO,Robotics
A Smooth Robustness Measure of Signal Temporal Logic for Symbolic Control,"Recent years have seen an increasing use of Signal Temporal Logic (STL) as a formal specification language for symbolic control, due to its expressiveness and closeness to natural language. Furthermore, STL specifications can be encoded as cost functions using STL's robust semantics, transforming the synthesis problem into an optimization problem. Unfortunately, these cost functions are non-smooth and non-convex, and exact solutions using mixed-integer programming do not scale well. Recent work has focused on using smooth approximations of robustness, which enable faster gradient-based methods to find local maxima, at the expense of soundness and/or completeness. We propose a novel robustness approximation that is smooth everywhere, sound, and asymptotically complete. Our approach combines the benefits of existing approximations, while enabling an explicit tradeoff between conservativeness and completeness.",cs.RO,Robotics
Efficient Pig Counting in Crowds with Keypoints Tracking and Spatial-aware Temporal Response Filtering,"Pig counting is a crucial task for large-scale pig farming, which is usually completed by human visually. But this process is very time-consuming and error-prone. Few studies in literature developed automated pig counting method. Existing methods only focused on pig counting using single image, and its accuracy is challenged by several factors, including pig movements, occlusion and overlapping. Especially, the field of view of a single image is very limited, and could not meet the requirements of pig counting for large pig grouping houses. To that end, we presented a real-time automated pig counting system in crowds using only one monocular fisheye camera with an inspection robot. Our system showed that it produces accurate results surpassing human. Our pipeline began with a novel bottom-up pig detection algorithm to avoid false negatives due to overlapping, occlusion and deformation of pigs. A deep convolution neural network (CNN) is designed to detect keypoints of pig body part and associate the keypoints to identify individual pigs. After that, an efficient on-line tracking method is used to associate pigs across video frames. Finally, a novel spatial-aware temporal response filtering (STRF) method is proposed to predict the counts of pigs, which is effective to suppress false positives caused by pig or camera movements or tracking failures. The whole pipeline has been deployed in an edge computing device, and demonstrated the effectiveness.",cs.RO,Robotics
Learning Active Task-Oriented Exploration Policies for Bridging the Sim-to-Real Gap,"Training robotic policies in simulation suffers from the sim-to-real gap, as simulated dynamics can be different from real-world dynamics. Past works tackled this problem through domain randomization and online system-identification. The former is sensitive to the manually-specified training distribution of dynamics parameters and can result in behaviors that are overly conservative. The latter requires learning policies that concurrently perform the task and generate useful trajectories for system identification. In this work, we propose and analyze a framework for learning exploration policies that explicitly perform task-oriented exploration actions to identify task-relevant system parameters. These parameters are then used by model-based trajectory optimization algorithms to perform the task in the real world. We instantiate the framework in simulation with the Linear Quadratic Regulator as well as in the real world with pouring and object dragging tasks. Experiments show that task-oriented exploration helps model-based policies adapt to systems with initially unknown parameters, and it leads to better task performance than task-agnostic exploration.",cs.RO,Robotics
A novel approach for multi-agent cooperative pursuit to capture grouped evaders,"An approach of mobile multi-agent pursuit based on application of self-organizing feature map (SOFM) and along with that reinforcement learning based on agent group role membership function (AGRMF) model is proposed. This method promotes dynamic organization of the pursuers' groups and also makes pursuers' group evader according to their desire based on SOFM and AGRMF techniques. This helps to overcome the shortcomings of the pursuers that they cannot fully reorganize when the goal is too independent in process of AGRMF models operation. Besides, we also discuss a new reward function. After the formation of the group, reinforcement learning is applied to get the optimal solution for each agent. The results of each step in capturing process will finally affect the AGR membership function to speed up the convergence of the competitive neural network. The experiments result shows that this approach is more effective for the mobile agents to capture evaders.",cs.RO,Robotics
Invariant Policy Optimization: Towards Stronger Generalization in Reinforcement Learning,"A fundamental challenge in reinforcement learning is to learn policies that generalize beyond the operating domains experienced during training. In this paper, we approach this challenge through the following invariance principle: an agent must find a representation such that there exists an action-predictor built on top of this representation that is simultaneously optimal across all training domains. Intuitively, the resulting invariant policy enhances generalization by finding causes of successful actions. We propose a novel learning algorithm, Invariant Policy Optimization (IPO), that implements this principle and learns an invariant policy during training. We compare our approach with standard policy gradient methods and demonstrate significant improvements in generalization performance on unseen domains for linear quadratic regulator and grid-world problems, and an example where a robot must learn to open doors with varying physical properties.",cs.RO,Robotics
Deep Visual Odometry with Adaptive Memory,"We propose a novel deep visual odometry (VO) method that considers global information by selecting memory and refining poses. Existing learning-based methods take the VO task as a pure tracking problem via recovering camera poses from image snippets, leading to severe error accumulation. Global information is crucial for alleviating accumulated errors. However, it is challenging to effectively preserve such information for end-to-end systems. To deal with this challenge, we design an adaptive memory module, which progressively and adaptively saves the information from local to global in a neural analogue of memory, enabling our system to process long-term dependency. Benefiting from global information in the memory, previous results are further refined by an additional refining module. With the guidance of previous outputs, we adopt a spatial-temporal attention to select features for each view based on the co-visibility in feature domain. Specifically, our architecture consisting of Tracking, Remembering and Refining modules works beyond tracking. Experiments on the KITTI and TUM-RGBD datasets demonstrate that our approach outperforms state-of-the-art methods by large margins and produces competitive results against classic approaches in regular scenes. Moreover, our model achieves outstanding performance in challenging scenarios such as texture-less regions and abrupt motions, where classic algorithms tend to fail.",cs.RO,Robotics
Robust Reinforcement Learning using Adversarial Populations,"Reinforcement Learning (RL) is an effective tool for controller design but can struggle with issues of robustness, failing catastrophically when the underlying system dynamics are perturbed. The Robust RL formulation tackles this by adding worst-case adversarial noise to the dynamics and constructing the noise distribution as the solution to a zero-sum minimax game. However, existing work on learning solutions to the Robust RL formulation has primarily focused on training a single RL agent against a single adversary. In this work, we demonstrate that using a single adversary does not consistently yield robustness to dynamics variations under standard parametrizations of the adversary; the resulting policy is highly exploitable by new adversaries. We propose a population-based augmentation to the Robust RL formulation in which we randomly initialize a population of adversaries and sample from the population uniformly during training. We empirically validate across robotics benchmarks that the use of an adversarial population results in a more robust policy that also improves out-of-distribution generalization. Finally, we demonstrate that this approach provides comparable robustness and generalization as domain randomization on these benchmarks while avoiding a ubiquitous domain randomization failure mode.",cs.RO,Robotics
The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning,"Many real-world problems require the coordination of multiple autonomous agents. Recent work has shown the promise of Graph Neural Networks (GNNs) to learn explicit communication strategies that enable complex multi-agent coordination. These works use models of cooperative multi-agent systems whereby agents strive to achieve a shared global goal. When considering agents with self-interested local objectives, the standard design choice is to model these as separate learning systems (albeit sharing the same environment). Such a design choice, however, precludes the existence of a single, differentiable communication channel, and consequently prohibits the learning of inter-agent communication strategies. In this work, we address this gap by presenting a learning model that accommodates individual non-shared rewards and a differentiable communication channel that is common among all agents. We focus on the case where agents have self-interested objectives, and develop a learning algorithm that elicits the emergence of adversarial communications. We perform experiments on multi-agent coverage and path planning problems, and employ a post-hoc interpretability technique to visualize the messages that agents communicate to each other. We show how a single self-interested agent is capable of learning highly manipulative communication strategies that allows it to significantly outperform a cooperative team of agents.",cs.RO,Robotics
Deep Reinforcement Learning for Tactile Robotics: Learning to Type on a Braille Keyboard,"Artificial touch would seem well-suited for Reinforcement Learning (RL), since both paradigms rely on interaction with an environment. Here we propose a new environment and set of tasks to encourage development of tactile reinforcement learning: learning to type on a braille keyboard. Four tasks are proposed, progressing in difficulty from arrow to alphabet keys and from discrete to continuous actions. A simulated counterpart is also constructed by sampling tactile data from the physical environment. Using state-of-the-art deep RL algorithms, we show that all of these tasks can be successfully learnt in simulation, and 3 out of 4 tasks can be learned on the real robot. A lack of sample efficiency currently makes the continuous alphabet task impractical on the robot. To the best of our knowledge, this work presents the first demonstration of successfully training deep RL agents in the real world using observations that exclusively consist of tactile images. To aid future research utilising this environment, the code for this project has been released along with designs of the braille keycaps for 3D printing and a guide for recreating the experiments. A brief video summary is also available at https://youtu.be/eNylCA2uE_E.",cs.RO,Robotics
Robust and Scalable Techniques for TWR and TDoA based localization using Ultra Wide Band Radios,Current trends in autonomous vehicles and their applications indicates an increasing need in positioning at low battery and compute cost. Lidars provide accurate localization at the cost of high compute and power consumption which could be detrimental for drones. Modern requirements for autonomous drones such as No-Permit-No-Takeoff (NPNT) and applications restricting drones to a corridor require the infrastructure to constantly determine the location of the drone. Ultra Wide Band Radios (UWB) fulfill such requirements and offer high precision localization and fast position update rates at a fraction of the cost and battery consumption as compared to lidars and also have greater network availability than GPS in a dense forested campus or an indoor setting. We present in this paper a novel protocol and technique to localize a drone for such applications using a Time Difference of Arrival (TDoA) approach. This further increases the position update rates without sacrificing on accuracy and compare it to traditional methods,cs.RO,Robotics
Human Robot Collaborative Assembly Planning: An Answer Set Programming Approach,"For planning an assembly of a product from a given set of parts, robots necessitate certain cognitive skills: high-level planning is needed to decide the order of actuation actions, while geometric reasoning is needed to check the feasibility of these actions. For collaborative assembly tasks with humans, robots require further cognitive capabilities, such as commonsense reasoning, sensing, and communication skills, not only to cope with the uncertainty caused by incomplete knowledge about the humans' behaviors but also to ensure safer collaborations. We propose a novel method for collaborative assembly planning under uncertainty, that utilizes hybrid conditional planning extended with commonsense reasoning and a rich set of communication actions for collaborative tasks. Our method is based on answer set programming. We show the applicability of our approach in a real-world assembly domain, where a bi-manual Baxter robot collaborates with a human teammate to assemble furniture. This manuscript is under consideration for acceptance in TPLP.",cs.RO,Robotics
Neural Manipulation Planning on Constraint Manifolds,"The presence of task constraints imposes a significant challenge to motion planning. Despite all recent advancements, existing algorithms are still computationally expensive for most planning problems. In this paper, we present Constrained Motion Planning Networks (CoMPNet), the first neural planner for multimodal kinematic constraints. Our approach comprises the following components: i) constraint and environment perception encoders; ii) neural robot configuration generator that outputs configurations on/near the constraint manifold(s), and iii) a bidirectional planning algorithm that takes the generated configurations to create a feasible robot motion trajectory. We show that CoMPNet solves practical motion planning tasks involving both unconstrained and constrained problems. Furthermore, it generalizes to new unseen locations of the objects, i.e., not seen during training, in the given environments with high success rates. When compared to the state-of-the-art constrained motion planning algorithms, CoMPNet outperforms by order of magnitude improvement in computational speed with a significantly lower variance.",cs.RO,Robotics
UAV Target Tracking in Urban Environments Using Deep Reinforcement Learning,"Persistent target tracking in urban environments using UAV is a difficult task due to the limited field of view, visibility obstruction from obstacles and uncertain target motion. The vehicle needs to plan intelligently in 3D such that the target visibility is maximized. In this paper, we introduce Target Following DQN (TF-DQN), a deep reinforcement learning technique based on Deep Q-Networks with a curriculum training framework for the UAV to persistently track the target in the presence of obstacles and target motion uncertainty. The algorithm is evaluated through several simulation experiments qualitatively as well as quantitatively. The results show that the UAV tracks the target persistently in diverse environments while avoiding obstacles on the trained environments as well as on unseen environments.",cs.RO,Robotics
Anticipating Human Intention for Full-Body Motion Prediction in Object Grasping and Placing Tasks,"Motion prediction in unstructured environments is a difficult problem and is essential for safe and efficient human-robot space sharing and collaboration. In this work, we focus on manipulation movements in environments such as homes, workplaces or restaurants, where the overall task and environment can be leveraged to produce accurate motion prediction. For these cases we propose an algorithmic framework that accounts explicitly for the environment geometry based on a model of affordances and a model of short-term human dynamics both trained on motion capture data. We propose dedicated function networks for graspability and placebility affordances and we make use of a dedicated RNN for short-term motion prediction. The prediction of grasp and placement probability densities are used by a constraint-based trajectory optimizer to produce a full-body motion prediction over the entire horizon. We show by comparing to ground truth data that we achieve similar performance for full-body motion predictions as using oracle grasp and place locations.",cs.RO,Robotics
Non-Markov Policies to Reduce Sequential Failures in Robot Bin Picking,"A new generation of automated bin picking systems using deep learning is evolving to support increasing demand for e-commerce. To accommodate a wide variety of products, many automated systems include multiple gripper types and/or tool changers. However, for some objects, sequential grasp failures are common: when a computed grasp fails to lift and remove the object, the bin is often left unchanged; as the sensor input is consistent, the system retries the same grasp over and over, resulting in a significant reduction in mean successful picks per hour (MPPH). Based on an empirical study of sequential failures, we characterize a class of ""sequential failure objects"" (SFOs) -- objects prone to sequential failures based on a novel taxonomy. We then propose three non-Markov picking policies that incorporate memory of past failures to modify subsequent actions. Simulation experiments on SFO models and the EGAD dataset suggest that the non-Markov policies significantly outperform the Markov policy in terms of the sequential failure rate and MPPH. In physical experiments on 50 heaps of 12 SFOs the most effective Non-Markov policy increased MPPH over the Dex-Net Markov policy by 107%.",cs.RO,Robotics
"Push, Stop, and Replan: An Application of Pebble Motion on Graphs to Planning in Automated Warehouses","The pebble-motion on graphs is a subcategory of multi-agent pathfinding problems dealing with moving multiple pebble-like objects from a node to a node in a graph with a constraint that only one pebble can occupy one node at a given time. Additionally, algorithms solving this problem assume that individual pebbles (robots) cannot move at the same time and their movement is discrete. These assumptions disqualify them from being directly used in practical applications, although they have otherwise nice theoretical properties. We present modifications of the Push and Rotate algorithm [1], which relax the presumptions mentioned above and demonstrate, through a set of experiments, that the modified algorithm is applicable for planning in automated warehouses.",cs.RO,Robotics
Roboat II: A Novel Autonomous Surface Vessel for Urban Environments,"This paper presents a novel autonomous surface vessel (ASV), called Roboat II for urban transportation. Roboat II is capable of accurate simultaneous localization and mapping (SLAM), receding horizon tracking control and estimation, and path planning. Roboat II is designed to maximize the internal space for transport and can carry payloads several times of its own weight. Moreover, it is capable of holonomic motions to facilitate transporting, docking, and inter-connectivity between boats. The proposed SLAM system receives sensor data from a 3D LiDAR, an IMU, and a GPS, and utilizes a factor graph to tackle the multi-sensor fusion problem. To cope with the complex dynamics in the water, Roboat II employs an online nonlinear model predictive controller (NMPC), where we experimentally estimated the dynamical model of the vessel in order to achieve superior performance for tracking control. The states of Roboat II are simultaneously estimated using a nonlinear moving horizon estimation (NMHE) algorithm. Experiments demonstrate that Roboat II is able to successfully perform online mapping and localization, plan its path and robustly track the planned trajectory in the confined river, implying that this autonomous vessel holds the promise on potential applications in transporting humans and goods in many of the waterways nowadays.",cs.RO,Robotics
Bridging the Gap between Optimal Trajectory Planning and Safety-Critical Control with Applications to Autonomous Vehicles,"We address the problem of optimizing the performance of a dynamic system while satisfying hard safety constraints at all times. Implementing an optimal control solution is limited by the computational cost required to derive it in real time, especially when constraints become active, as well as the need to rely on simple linear dynamics, simple objective functions, and ignoring noise. The recently proposed Control Barrier Function (CBF) method may be used for safety-critical control at the expense of sub-optimal performance. In this paper, we develop a real-time control framework that combines optimal trajectories generated through optimal control with the computationally efficient CBF method providing safety guarantees. We use Hamiltonian analysis to obtain a tractable optimal solution for a linear or linearized system, then employ High Order CBFs (HOCBFs) and Control Lyapunov Functions (CLFs) to account for constraints with arbitrary relative degrees and to track the optimal state, respectively. We further show how to deal with noise in arbitrary relative degree systems. The proposed framework is then applied to the optimal traffic merging problem for Connected and Automated Vehicles (CAVs) where the objective is to jointly minimize the travel time and energy consumption of each CAV subject to speed, acceleration, and speed-dependent safety constraints. In addition, when considering more complex objective functions, nonlinear dynamics and passenger comfort requirements for which analytical optimal control solutions are unavailable, we adapt the HOCBF method to such problems. Simulation examples are included to compare the performance of the proposed framework to optimal solutions (when available) and to a baseline provided by human-driven vehicles with results showing significant improvements in all metrics.",cs.RO,Robotics
Safe Reinforcement Learning in Constrained Markov Decision Processes,"Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a stepwise approach for optimizing safety and cumulative reward. In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-SAFETY-GYM, and the other simulates Mars surface exploration by using real observation data.",cs.RO,Robotics
Autonomous Braking and Throttle System: A Deep Reinforcement Learning Approach for Naturalistic Driving,"Autonomous Braking and Throttle control is key in developing safe driving systems for the future. There exists a need for autonomous vehicles to negotiate a multi-agent environment while ensuring safety and comfort. A Deep Reinforcement Learning based autonomous throttle and braking system is presented. For each time step, the proposed system makes a decision to apply the brake or throttle. The throttle and brake are modelled as continuous action space values. We demonstrate 2 scenarios where there is a need for a sophisticated braking and throttle system, i.e when there is a static obstacle in front of our agent like a car, stop sign. The second scenario consists of 2 vehicles approaching an intersection. The policies for brake and throttle control are learned through computer simulation using Deep deterministic policy gradients. The experiment shows that the system not only avoids a collision, but also it ensures that there is smooth change in the values of throttle/brake as it gets out of the emergency situation and abides by the speed regulations, i.e the system resembles human driving.",cs.RO,Robotics
Real-Time Spatio-Temporal LiDAR Point Cloud Compression,"Compressing massive LiDAR point clouds in real-time is critical to autonomous machines such as drones and self-driving cars. While most of the recent prior work has focused on compressing individual point cloud frames, this paper proposes a novel system that effectively compresses a sequence of point clouds. The idea to exploit both the spatial and temporal redundancies in a sequence of point cloud frames. We first identify a key frame in a point cloud sequence and spatially encode the key frame by iterative plane fitting. We then exploit the fact that consecutive point clouds have large overlaps in the physical space, and thus spatially encoded data can be (re-)used to encode the temporal stream. Temporal encoding by reusing spatial encoding data not only improves the compression rate, but also avoids redundant computations, which significantly improves the compression speed. Experiments show that our compression system achieves 40x to 90x compression rate, significantly higher than the MPEG's LiDAR point cloud compression standard, while retaining high end-to-end application accuracies. Meanwhile, our compression system has a compression speed that matches the point cloud generation rate by today LiDARs and out-performs existing compression systems, enabling real-time point cloud transmission.",cs.RO,Robotics
Learning the Latent Space of Robot Dynamics for Cutting Interaction Inference,"Utilization of latent space to capture a lower-dimensional representation of a complex dynamics model is explored in this work. The targeted application is of a robotic manipulator executing a complex environment interaction task, in particular, cutting a wooden object. We train two flavours of Variational Autoencoders---standard and Vector-Quantised---to learn the latent space which is then used to infer certain properties of the cutting operation, such as whether the robot is cutting or not, as well as, material and geometry of the object being cut. The two VAE models are evaluated with reconstruction, prediction and a combined reconstruction/prediction decoders. The results demonstrate the expressiveness of the latent space for robotic interaction inference and the competitive prediction performance against recurrent neural networks.",cs.RO,Robotics
Collaborative Localization of Aerial and Ground Mobile Robots through Orthomosaic Map,"With the deepening of research on the SLAM system, the possibility of cooperative SLAM with multi-robots has been proposed. This paper presents a map matching and localization approach considering the cooperative SLAM of an aerial-ground system. The proposed approach aims to help precisely matching the map constructed by two independent systems that have large scale variance of viewpoints of the same route and eventually enables the ground mobile robot to localize itself in the global map given by the drone. It contains dense mapping with Elevation Map and software ""Metashape"", map matching with a proposed template matching algorithm, weighted normalized cross-correlation (WNCC) and localization with particle filter. The approach enables map matching for cooperative SLAM with the feasibility of multiple scene sensors, varies from stereo cameras to lidars, and is insensitive to the synchronization of the two systems. We demonstrate the accuracy, robustness, and the speed of the approach under experiments of the Aero-Ground Dataset.",cs.RO,Robotics
Multi-robot Cooperative Object Transportation using Decentralized Deep Reinforcement Learning,"Object transportation could be a challenging problem for a single robot due to the oversize and/or overweight issues. A multi-robot system can take the advantage of increased driving power and more flexible configuration to solve such a problem. However, increased number of individuals also changed the dynamics of the system which makes control of a multi-robot system more complicated. Even worse, if the whole system is sitting on a centralized decision making unit, the data flow could be easily overloaded due to the upscaling of the system. In this research, we propose a decentralized control scheme on a multi-robot system with each individual equipped with a deep Q-network (DQN) controller to perform an oversized object transportation task. DQN is a deep reinforcement learning algorithm thus does not require the knowledge of system dynamics, instead, it enables the robots to learn appropriate control strategies through trial-and-error style interactions within the task environment. Since analogous controllers are distributed on the individuals, the computational bottleneck is avoided systematically. We demonstrate such a system in a scenario of carrying an oversized rod through a doorway by a two-robot team. The presented multi-robot system learns abstract features of the task and cooperative behaviors are observed. The decentralized DQN-style controller is showing strong robustness against uncertainties. In addition, We propose a universal metric to assess the cooperation quantitatively.",cs.RO,Robotics
Semi-supervised Learning From Demonstration Through Program Synthesis: An Inspection Robot Case Study,"Semi-supervised learning improves the performance of supervised machine learning by leveraging methods from unsupervised learning to extract information not explicitly available in the labels. Through the design of a system that enables a robot to learn inspection strategies from a human operator, we present a hybrid semi-supervised system capable of learning interpretable and verifiable models from demonstrations. The system induces a controller program by learning from immersive demonstrations using sequential importance sampling. These visual servo controllers are parametrised by proportional gains and are visually verifiable through observation of the position of the robot in the environment. Clustering and effective particle size filtering allows the system to discover goals in the state space. These goals are used to label the original demonstration for end-to-end learning of behavioural models. The behavioural models are used for autonomous model predictive control and scrutinised for explanations. We implement causal sensitivity analysis to identify salient objects and generate counterfactual conditional explanations. These features enable decision making interpretation and post hoc discovery of the causes of a failure. The proposed system expands on previous approaches to program synthesis by incorporating repellers in the attribution prior of the sampling process. We successfully learn the hybrid system from an inspection scenario where an unmanned ground vehicle has to inspect, in a specific order, different areas of the environment. The system induces an interpretable computer program of the demonstration that can be synthesised to produce novel inspection behaviours. Importantly, the robot successfully runs the synthesised program on an unseen configuration of the environment while presenting explanations of its autonomous behaviour.",cs.RO,Robotics
Autonomous Exploration Under Uncertainty via Deep Reinforcement Learning on Graphs,"We consider an autonomous exploration problem in which a range-sensing mobile robot is tasked with accurately mapping the landmarks in an a priori unknown environment efficiently in real-time; it must choose sensing actions that both curb localization uncertainty and achieve information gain. For this problem, belief space planning methods that forward-simulate robot sensing and estimation may often fail in real-time implementation, scaling poorly with increasing size of the state, belief and action spaces. We propose a novel approach that uses graph neural networks (GNNs) in conjunction with deep reinforcement learning (DRL), enabling decision-making over graphs containing exploration information to predict a robot's optimal sensing action in belief space. The policy, which is trained in different random environments without human intervention, offers a real-time, scalable decision-making process whose high-performance exploratory sensing actions yield accurate maps and high rates of information gain.",cs.RO,Robotics
Decentralized Safe Reactive Planning under TWTL Specifications,"We investigate a multi-agent planning problem, where each agent aims to achieve an individual task while avoiding collisions with others. We assume that each agent's task is expressed as a Time-Window Temporal Logic (TWTL) specification defined over a 3D environment. We propose a decentralized receding horizon algorithm for online planning of trajectories. We show that when the environment is sufficiently connected, the resulting agent trajectories are always safe (collision-free) and lead to the satisfaction of the TWTL specifications or their finite temporal relaxations. Accordingly, deadlocks are always avoided and each agent is guaranteed to safely achieve its task with a finite time-delay in the worst case. Performance of the proposed algorithm is demonstrated via numerical simulations and experiments with quadrotors.",cs.RO,Robotics
Balanced Depth Completion between Dense Depth Inference and Sparse Range Measurements via KISS-GP,"Estimating a dense and accurate depth map is the key requirement for autonomous driving and robotics. Recent advances in deep learning have allowed depth estimation in full resolution from a single image. Despite this impressive result, many deep-learning-based monocular depth estimation (MDE) algorithms have failed to keep their accuracy yielding a meter-level estimation error. In many robotics applications, accurate but sparse measurements are readily available from Light Detection and Ranging (LiDAR). Although they are highly accurate, the sparsity limits full resolution depth map reconstruction. Targeting the problem of dense and accurate depth map recovery, this paper introduces the fusion of these two modalities as a depth completion (DC) problem by dividing the role of depth inference and depth regression. Utilizing the state-of-the-art MDE and our Gaussian process (GP) based depth-regression method, we propose a general solution that can flexibly work with various MDE modules by enhancing its depth with sparse range measurements. To overcome the major limitation of GP, we adopt Kernel Interpolation for Scalable Structured (KISS)-GP and mitigate the computational complexity from O(N^3) to O(N). Our experiments demonstrate that the accuracy and robustness of our method outperform state-of-the-art unsupervised methods for sparse and biased measurements.",cs.RO,Robotics
Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction,"We present a novel method for testing the safety of self-driving vehicles in simulation. We propose an alternative to sensor simulation, as sensor simulation is expensive and has large domain gaps. Instead, we directly simulate the outputs of the self-driving vehicle's perception and prediction system, enabling realistic motion planning testing. Specifically, we use paired data in the form of ground truth labels and real perception and prediction outputs to train a model that predicts what the online system will produce. Importantly, the inputs to our system consists of high definition maps, bounding boxes, and trajectories, which can be easily sketched by a test engineer in a matter of minutes. This makes our approach a much more scalable solution. Quantitative results on two large-scale datasets demonstrate that we can realistically test motion planning using our simulations.",cs.RO,Robotics
DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features,"A robust and efficient Simultaneous Localization and Mapping (SLAM) system is essential for robot autonomy. For visual SLAM algorithms, though the theoretical framework has been well established for most aspects, feature extraction and association is still empirically designed in most cases, and can be vulnerable in complex environments. This paper shows that feature extraction with deep convolutional neural networks (CNNs) can be seamlessly incorporated into a modern SLAM framework. The proposed SLAM system utilizes a state-of-the-art CNN to detect keypoints in each image frame, and to give not only keypoint descriptors, but also a global descriptor of the whole image. These local and global features are then used by different SLAM modules, resulting in much more robustness against environmental changes and viewpoint changes compared with using hand-crafted features. We also train a visual vocabulary of local features with a Bag of Words (BoW) method. Based on the local features, global features, and the vocabulary, a highly reliable loop closure detection method is built. Experimental results show that all the proposed modules significantly outperforms the baseline, and the full system achieves much lower trajectory errors and much higher correct rates on all evaluated data. Furthermore, by optimizing the CNN with Intel OpenVINO toolkit and utilizing the Fast BoW library, the system benefits greatly from the SIMD (single-instruction-multiple-data) techniques in modern CPUs. The full system can run in real-time without any GPU or other accelerators. The code is public at https://github.com/ivipsourcecode/dxslam.",cs.RO,Robotics
Visuomotor Mechanical Search: Learning to Retrieve Target Objects in Clutter,"When searching for objects in cluttered environments, it is often necessary to perform complex interactions in order to move occluding objects out of the way and fully reveal the object of interest and make it graspable. Due to the complexity of the physics involved and the lack of accurate models of the clutter, planning and controlling precise predefined interactions with accurate outcome is extremely hard, when not impossible. In problems where accurate (forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be a viable solution to map observations (e.g. images) to good interactions in the form of close-loop visuomotor policies. However, Deep RL is sample inefficient and fails when applied directly to the problem of unoccluding objects based on images. In this work we present a novel Deep RL procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our experiments show that our approach trains faster and converges to more efficient uncovering solutions than baselines and ablations, and that our uncovering policies lead to an average improvement in the graspability of the target object, facilitating downstream retrieval applications.",cs.RO,Robotics
Group Decision Support for agriculture planning by a combination of Mathematical Model and Collaborative Tool,"Decision making in the Agriculture domain can be a complex task. The land area allocated to each crop should be fixed every season according to several parameters: prices, demand, harvesting periods, seeds, ground, season etc... The decision to make becomes more difficult when a group of farmers must fix the price and all parameters all together. Generally, optimization models are useful for farmers to find no dominated solutions, but it remains difficult if the farmers have to agree on one solution. We combine two approaches in order to support a group of farmers engaged in this kind of decision making process. We firstly generate a set of no dominated solutions thanks to a centralized optimization model. Based on this set of solution we then used a Group Decision Support System called GRUS for choosing the best solution for the group of farmers. The combined approach allows us to determine the best solution for the group in a consensual way. This combination of approaches is very innovative for the Agriculture. This approach has been tested in laboratory in a previous work. In the current work the same experiment has been conducted with real business (farmers) in order to benefit from their expertise. The two experiments are compared.",cs.RO,Robotics
Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection,"Detecting vehicles and representing their position and orientation in the three dimensional space is a key technology for autonomous driving. Recently, methods for 3D vehicle detection solely based on monocular RGB images gained popularity. In order to facilitate this task as well as to compare and drive state-of-the-art methods, several new datasets and benchmarks have been published. Ground truth annotations of vehicles are usually obtained using lidar point clouds, which often induces errors due to imperfect calibration or synchronization between both sensors. To this end, we propose Cityscapes 3D, extending the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles. In contrast to existing datasets, our 3D annotations were labeled using stereo RGB images only and capture all nine degrees of freedom. This leads to a pixel-accurate reprojection in the RGB image and a higher range of annotations compared to lidar-based approaches. In order to ease multitask learning, we provide a pairing of 2D instance segments with 3D bounding boxes. In addition, we complement the Cityscapes benchmark suite with 3D vehicle detection based on the new annotations as well as metrics presented in this work. Dataset and benchmark are available online.",cs.RO,Robotics
A Quick Review on Recent Trends in 3D Point Cloud Data Compression Techniques and the Challenges of Direct Processing in 3D Compressed Domain,"Automatic processing of 3D Point Cloud data for object detection, tracking and segmentation is the latest trending research in the field of AI and Data Science, which is specifically aimed at solving different challenges of autonomous driving cars and getting real time performance. However, the amount of data that is being produced in the form of 3D point cloud (with LiDAR) is very huge, due to which the researchers are now on the way inventing new data compression algorithms to handle huge volumes of data thus generated. However, compression on one hand has an advantage in overcoming space requirements, but on the other hand, its processing gets expensive due to the decompression, which indents additional computing resources. Therefore, it would be novel to think of developing algorithms that can operate/analyse directly with the compressed data without involving the stages of decompression and recompression (required as many times, the compressed data needs to be operated or analyzed). This research field is termed as Compressed Domain Processing. In this paper, we will quickly review few of the recent state-of-the-art developments in the area of LiDAR generated 3D point cloud data compression, and highlight the future challenges of compressed domain processing of 3D point cloud data.",cs.RO,Robotics
Augmenting Differentiable Simulators with Neural Networks to Close the Sim2Real Gap,"We present a differentiable simulation architecture for articulated rigid-body dynamics that enables the augmentation of analytical models with neural networks at any point of the computation. Through gradient-based optimization, identification of the simulation parameters and network weights is performed efficiently in preliminary experiments on a real-world dataset and in sim2sim transfer applications, while poor local optima are overcome through a random search approach.",cs.RO,Robotics
Camera-Lidar Integration: Probabilistic sensor fusion for semantic mapping,"An automated vehicle operating in an urban environment must be able to perceive and recognise object/obstacles in a three-dimensional world while navigating in a constantly changing environment. In order to plan and execute accurate sophisticated driving maneuvers, a high-level contextual understanding of the surroundings is essential. Due to the recent progress in image processing, it is now possible to obtain high definition semantic information in 2D from monocular cameras, though cameras cannot reliably provide the highly accurate 3D information provided by lasers. The fusion of these two sensor modalities can overcome the shortcomings of each individual sensor, though there are a number of important challenges that need to be addressed in a probabilistic manner. In this paper, we address the common, yet challenging, lidar/camera/semantic fusion problems which are seldom approached in a wholly probabilistic manner. Our approach is capable of using a multi-sensor platform to build a three-dimensional semantic voxelized map that considers the uncertainty of all of the processes involved. We present a probabilistic pipeline that incorporates uncertainties from the sensor readings (cameras, lidar, IMU and wheel encoders), compensation for the motion of the vehicle, and heuristic label probabilities for the semantic images. We also present a novel and efficient viewpoint validation algorithm to check for occlusions from the camera frames. A probabilistic projection is performed from the camera images to the lidar point cloud. Each labelled lidar scan then feeds into an octree map building algorithm that updates the class probabilities of the map voxels every time a new observation is available. We validate our approach using a set of qualitative and quantitative experimental tests on the USyd Dataset.",cs.RO,Robotics
NaviGAN: A Generative Approach for Socially Compliant Navigation,"Robots navigating in human crowds need to optimize their paths not only for their task performance but also for their compliance to social norms. One of the key challenges in this context is the lack of standard metrics for evaluating and optimizing a socially compliant behavior. Existing works in social navigation can be grouped according to the differences in their optimization objectives. For instance, the reinforcement learning approaches tend to optimize on the \textit{comfort} aspect of the socially compliant navigation, whereas the inverse reinforcement learning approaches are designed to achieve \textit{natural} behavior. In this paper, we propose NaviGAN, a generative navigation algorithm that jointly optimizes both of the \textit{comfort} and \textit{naturalness} aspects. Our approach is designed as an adversarial training framework that can learn to generate a navigation path that is both optimized for achieving a goal and for complying with latent social rules. A set of experiments has been carried out on multiple datasets to demonstrate the strengths of the proposed approach quantitatively. We also perform extensive experiments using a physical robot in a real-world environment to qualitatively evaluate the trained social navigation behavior. The video recordings of the robot experiments can be found in the link: https://youtu.be/61blDymjCpw.",cs.RO,Robotics
Robotic Non-Destructive Testing of Manmade Structures: A Review of the Literature,"This literature review investigates how robots can be used for the maintenance of manmade structures, such as pipes, reinforced concrete decks, and space stations as a sampling of the broad spectrum of robotic non-destructive testing (NDT) applications. Robotic NDT can be used to find plaque in pipes, corrosion in steel buildings, and impact damage in space stations, which would normally be invisible to the eye. After inspection, the inspected material is preserved in its original condition. The paper's structure is as follows: first, the definition of NDT is elaborated upon with the discussion of specific methods that will be used in the inspection of the structures mentioned above. Second, an explanation follows on why robots are suited to inspection, specifically focusing on robots' advantages over humans. Third, three real-world examples notify the reader of current progress in robot NDT. Lastly, a summary of robot problems serves as a reminder that testing and development must continue for robot NDT to become mainstream.",cs.RO,Robotics
Feedback Enhanced Motion Planning for Autonomous Vehicles,"In this work, we address the motion planning problem for autonomous vehicles through a new lattice planning approach, called Feedback Enhanced Lattice Planner (FELP). Existing lattice planners have two major limitations, namely the high dimensionality of the lattice and the lack of modeling of agent vehicle behaviors. We propose to apply the Intelligent Driver Model (IDM) as a speed feedback policy to address both of these limitations. IDM both enables the responsive behavior of the agents, and uniquely determines the acceleration and speed profile of the ego vehicle on a given path. Therefore, only a spatial lattice is needed, while discretization of higher order dimensions is no longer required. Additionally, we propose a directed-graph map representation to support the implementation and execution of lattice planners. The map can reflect local geometric structure, embed the traffic rules adhering to the road, and is efficient to construct and update. We show that FELP is more efficient compared to other existing lattice planners through runtime complexity analysis, and we propose two variants of FELP to further reduce the complexity to polynomial time. We demonstrate the improvement by comparing FELP with an existing spatiotemporal lattice planner using simulations of a merging scenario and continuous highway traffic. We also study the performance of FELP under different traffic densities.",cs.RO,Robotics
Goal-Aware Prediction: Learning to Model What Matters,"Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning.",cs.RO,Robotics
Learning Accurate and Human-Like Driving using Semantic Maps and Attention,"This paper investigates how end-to-end driving models can be improved to drive more accurately and human-like. To tackle the first issue we exploit semantic and visual maps from HERE Technologies and augment the existing Drive360 dataset with such. The maps are used in an attention mechanism that promotes segmentation confidence masks, thus focusing the network on semantic classes in the image that are important for the current driving situation. Human-like driving is achieved using adversarial learning, by not only minimizing the imitation loss with respect to the human driver but by further defining a discriminator, that forces the driving model to produce action sequences that are human-like. Our models are trained and evaluated on the Drive360 + HERE dataset, which features 60 hours and 3000 km of real-world driving data. Extensive experiments show that our driving models are more accurate and behave more human-like than previous methods.",cs.RO,Robotics
Strengthening Deterministic Policies for POMDPs,"The synthesis problem for partially observable Markov decision processes (POMDPs) is to compute a policy that satisfies a given specification. Such policies have to take the full execution history of a POMDP into account, rendering the problem undecidable in general. A common approach is to use a limited amount of memory and randomize over potential choices. Yet, this problem is still NP-hard and often computationally intractable in practice. A restricted problem is to use neither history nor randomization, yielding policies that are called stationary and deterministic. Previous approaches to compute such policies employ mixed-integer linear programming (MILP). We provide a novel MILP encoding that supports sophisticated specifications in the form of temporal logic constraints. It is able to handle an arbitrary number of such specifications. Yet, randomization and memory are often mandatory to achieve satisfactory policies. First, we extend our encoding to deliver a restricted class of randomized policies. Second, based on the results of the original MILP, we employ a preprocessing of the POMDP to encompass memory-based decisions. The advantages of our approach over state-of-the-art POMDP solvers lie (1) in the flexibility to strengthen simple deterministic policies without losing computational tractability and (2) in the ability to enforce the provable satisfaction of arbitrarily many specifications. The latter point allows taking trade-offs between performance and safety aspects of typical POMDP examples into account. We show the effectiveness of our method on a broad range of benchmarks.",cs.RO,Robotics
Specification mining and automated task planning for autonomous robots based on a graph-based spatial temporal logic,"We aim to enable an autonomous robot to learn new skills from demo videos and use these newly learned skills to accomplish non-trivial high-level tasks. The goal of developing such autonomous robot involves knowledge representation, specification mining, and automated task planning. For knowledge representation, we use a graph-based spatial temporal logic (GSTL) to capture spatial and temporal information of related skills demonstrated by demo videos. We design a specification mining algorithm to generate a set of parametric GSTL formulas from demo videos by inductively constructing spatial terms and temporal formulas. The resulting parametric GSTL formulas from specification mining serve as a domain theory, which is used in automated task planning for autonomous robots. We propose an automatic task planning based on GSTL where a proposer is used to generate ordered actions, and a verifier is used to generate executable task plans. A table setting example is used throughout the paper to illustrate the main ideas.",cs.RO,Robotics
Stochastic Grounded Action Transformation for Robot Learning in Simulation,"Robot control policies learned in simulation do not often transfer well to the real world. Many existing solutions to this sim-to-real problem, such as the Grounded Action Transformation (GAT) algorithm, seek to correct for or ground these differences by matching the simulator to the real world. However, the efficacy of these approaches is limited if they do not explicitly account for stochasticity in the target environment. In this work, we analyze the problems associated with grounding a deterministic simulator in a stochastic real world environment, and we present examples where GAT fails to transfer a good policy due to stochastic transitions in the target domain. In response, we introduce the Stochastic Grounded Action Transformation(SGAT) algorithm,which models this stochasticity when grounding the simulator. We find experimentally for both simulated and physical target domains that SGAT can find policies that are robust to stochasticity in the target domain",cs.RO,Robotics
PillarFlow: End-to-end Birds-eye-view Flow Estimation for Autonomous Driving,"In autonomous driving, accurately estimating the state of surrounding obstacles is critical for safe and robust path planning. However, this perception task is difficult, particularly for generic obstacles/objects, due to appearance and occlusion changes. To tackle this problem, we propose an end-to-end deep learning framework for LIDAR-based flow estimation in bird's eye view (BeV). Our method takes consecutive point cloud pairs as input and produces a 2-D BeV flow grid describing the dynamic state of each cell. The experimental results show that the proposed method not only estimates 2-D BeV flow accurately but also improves tracking performance of both dynamic and static objects.",cs.RO,Robotics
Concurrent Training Improves the Performance of Behavioral Cloning from Observation,"Learning from demonstration is widely used as an efficient way for robots to acquire new skills. However, it typically requires that demonstrations provide full access to the state and action sequences. In contrast, learning from observation offers a way to utilize unlabeled demonstrations (e.g., video) to perform imitation learning. One approach to this is behavioral cloning from observation (BCO). The original implementation of BCO proceeds by first learning an inverse dynamics model and then using that model to estimate action labels, thereby reducing the problem to behavioral cloning. However, existing approaches to BCO require a large number of initial interactions in the first step. Here, we provide a novel theoretical analysis of BCO, introduce a modification BCO*, and show that in the semi-supervised setting, BCO* can concurrently improve both its estimate for the inverse dynamics model and the expert policy. This result allows us to eliminate the dependence on initial interactions and dramatically improve the sample complexity of BCO. We evaluate the effectiveness of our algorithm through experiments on various benchmark domains. The results demonstrate that concurrent training not only improves over the performance of BCO but also results in performance that is competitive with state-of-the-art imitation learning methods such as GAIL and Value-Dice.",cs.RO,Robotics
"Getting to Know One Another: Calibrating Intent, Capabilities and Trust for Human-Robot Collaboration","Common experience suggests that agents who know each other well are better able to work together. In this work, we address the problem of calibrating intention and capabilities in human-robot collaboration. In particular, we focus on scenarios where the robot is attempting to assist a human who is unable to directly communicate her intent. Moreover, both agents may have differing capabilities that are unknown to one another. We adopt a decision-theoretic approach and propose the TICC-POMDP for modeling this setting, with an associated online solver. Experiments show our approach leads to better team performance both in simulation and in a real-world study with human subjects.",cs.RO,Robotics
Learning to Drive (L2D) as a Low-Cost Benchmark for Real-World Reinforcement Learning,"We present Learning to Drive (L2D), a low-cost benchmark for real-world reinforcement learning (RL). L2D involves a simple and reproducible experimental setup where an RL agent has to learn to drive a Donkey car around three miniature tracks, given only monocular image observations and speed of the car. The agent has to learn to drive from disengagements, which occurs when it drives off the track. We present and open-source our training pipeline, which makes it straightforward to apply any existing RL algorithm to the task of autonomous driving with a Donkey car. We test imitation learning, state-of-the-art model-free, and model-based algorithms on the proposed L2D benchmark. Our results show that existing RL algorithms can learn to drive the car from scratch in less than five minutes of interaction. We demonstrate that RL algorithms can learn from sparse and noisy disengagement to drive even faster than imitation learning and a human operator.",cs.RO,Robotics
Real-Time Point Cloud Fusion of Multi-LiDAR Infrastructure Sensor Setups with Unknown Spatial Location and Orientation,"The use of infrastructure sensor technology for traffic detection has already been proven several times. However, extrinsic sensor calibration is still a challenge for the operator. While previous approaches are unable to calibrate the sensors without the use of reference objects in the sensor field of view (FOV), we present an algorithm that is completely detached from external assistance and runs fully automatically. Our method focuses on the high-precision fusion of LiDAR point clouds and is evaluated in simulation as well as on real measurements. We set the LiDARs in a continuous pendulum motion in order to simulate real-world operation as closely as possible and to increase the demands on the algorithm. However, it does not receive any information about the initial spatial location and orientation of the LiDARs throughout the entire measurement period. Experiments in simulation as well as with real measurements have shown that our algorithm performs a continuous point cloud registration of up to four 64-layer LiDARs in real-time. The averaged resulting translational error is within a few centimeters and the averaged error in rotation is below 0.15 degrees.",cs.RO,Robotics
An Electrocommunication System Using FSK Modulation and Deep Learning Based Demodulation for Underwater Robots,"Underwater communication is extremely challenging for small underwater robots which typically have stringent power and size constraints. In our previous work, we developed an artificial electrocommunication system which could be an alternative for the communication of small underwater robots. This paper further presents a new electrocommunication system that utilizes Binary Frequency Shift Keying (2FSK) modulation and deep-learning-based demodulation for underwater robots. We first derive an underwater electrocommunication model that covers both the near-field area and a large transition area outside of the near-field area. 2FSK modulation is adopted to improve the anti-interference ability of the electric signal. A deep learning algorithm is used to demodulate the electric signal by the receiver. Simulations and experiments show that with the same testing condition, the new communication system outperforms the previous system in both the communication distance and the data transmitting rate. In specific, the newly developed communication system achieves stable communication within the distance of 10 m at a data transfer rate of 5 Kbps with a power consumption of less than 0.1 W. The substantial increase in communication distance further improves the possibility of electrocommunication in underwater robotics.",cs.RO,Robotics
Planning to Score a Goal in Robotic Football with Heuristic Search,"This paper considers a problem of planning an attack in robotic football (RoboCup). The problem is reduced to finding a trajectory of the ball from its current position to the opponents goals. Heuristic search algorithm, i.e. A*, is used to find such a trajectory. For this algorithm to be applicable we introduce a discretized model of the environment, i.e. a graph, as well as the core search components: cost function and heuristic function. Both are designed to take into account all the available information of the game state. We extensively evaluate the suggested approach in simulation comparing it to a range of baselines. The result of the conducted evaluation clearly shows the benefit of utilizing heuristic search within the RoboCup context.",cs.RO,Robotics
Better Together: Online Probabilistic Clique Change Detection in 3D Landmark-Based Maps,"Many modern simultaneous localization and mapping (SLAM) techniques rely on sparse landmark-based maps due to their real-time performance. However, these techniques frequently assert that these landmarks are fixed in position over time, known as the static-world assumption. This is rarely, if ever, the case in most real-world environments. Even worse, over long deployments, robots are bound to observe traditionally static landmarks change, for example when an autonomous vehicle encounters a construction zone. This work addresses this challenge, accounting for changes in complex three-dimensional environments with the creation of a probabilistic filter that operates on the features that give rise to landmarks. To accomplish this, landmarks are clustered into cliques and a filter is developed to estimate their persistence jointly among observations of the landmarks in a clique. This filter uses estimated spatial-temporal priors of geometric objects, allowing for dynamic and semi-static objects to be removed from a formally static map. The proposed algorithm is validated in a 3D simulated environment.",cs.RO,Robotics
Toward Campus Mail Delivery Using BDI,"Autonomous systems developed with the Belief-Desire-Intention (BDI) architecture are usually mostly implemented in simulated environments. In this project we sought to build a BDI agent for use in the real world for campus mail delivery in the tunnel system at Carleton University. Ideally, the robot should receive a delivery order via a mobile application, pick up the mail at a station, navigate the tunnels to the destination station, and notify the recipient.
  We linked the Robot Operating System (ROS) with a BDI reasoning system to achieve a subset of the required use cases. ROS handles the low-level sensing and actuation, while the BDI reasoning system handles the high-level reasoning and decision making. Sensory data is orchestrated and sent from ROS to the reasoning system as perceptions. These perceptions are then deliberated upon, and an action string is sent back to ROS for interpretation and driving of the necessary actuator for the action to be performed.
  In this paper we present our current implementation, which closes the loop on the hardware-software integration, and implements a subset of the use cases required for the full system.",cs.RO,Robotics
Mixed-Reality Robotic Games: Design Guidelines for Effective Entertainment with Consumer Robots,"In recent years, there has been an increasing interest in the use of robotic technology at home. A number of service robots appeared on the market, supporting customers in the execution of everyday tasks. Roughly at the same time, consumer level robots started to be used also as toys or gaming companions. However, gaming possibilities provided by current off-the-shelf robotic products are generally quite limited, and this fact makes them quickly loose their attractiveness. A way that has been proven capable to boost robotic gaming and related devices consists in creating playful experiences in which physical and digital elements are combined together using Mixed Reality technologies. However, these games differ significantly from digital- or physical only experiences, and new design principles are required to support developers in their creative work. This papers addresses such need, by drafting a set of guidelines which summarize developments carried out by the research community and their findings.",cs.RO,Robotics
Curating Long-term Vector Maps,"Autonomous service mobile robots need to consistently, accurately, and robustly localize in human environments despite changes to such environments over time. Episodic non-Markov Localization addresses the challenge of localization in such changing environments by classifying observations as arising from Long-Term, Short-Term, or Dynamic Features. However, in order to do so, EnML relies on an estimate of the Long-Term Vector Map (LTVM) that does not change over time. In this paper, we introduce a recursive algorithm to build and update the LTVM over time by reasoning about visibility constraints of objects observed over multiple robot deployments. We use a signed distance function (SDF) to filter out observations of short-term and dynamic features from multiple deployments of the robot. The remaining long-term observations are used to build a vector map by robust local linear regression. The uncertainty in the resulting LTVM is computed via Monte Carlo resampling the observations arising from long-term features. By combining occupancy-grid based SDF filtering of observations with continuous space regression of the filtered observations, our proposed approach builds, updates, and amends LTVMs over time, reasoning about all observations from all robot deployments in an environment. We present experimental results demonstrating the accuracy, robustness, and compact nature of the extracted LTVMs from several long-term robot datasets.",cs.RO,Robotics
Laser2Vec: Similarity-based Retrieval for Robotic Perception Data,"As mobile robot capabilities improve and deployment times increase, tools to analyze the growing volume of data are becoming necessary. Current state-of-the-art logging, playback, and exploration systems are insufficient for practitioners seeking to discover systemic points of failure in robotic systems. This paper presents a suite of algorithms for similarity-based queries of robotic perception data and implements a system for storing 2D LiDAR data from many deployments cheaply and evaluating top-k queries for complete or partial scans efficiently. We generate compressed representations of laser scans via a convolutional variational autoencoder and store them in a database, where a light-weight dense network for distance function approximation is run at query time. Our query evaluator leverages the local continuity of the embedding space to generate evaluation orders that, in expectation, dominate full linear scans of the database. The accuracy, robustness, scalability, and efficiency of our system is tested on real-world data gathered from dozens of deployments and synthetic data generated by corrupting real data. We find our system accurately and efficiently identifies similar scans across a number of episodes where the robot encountered the same location, or similar indoor structures or objects.",cs.RO,Robotics
Mechatronics-Driven Musical Expressivity for Robotic Percussionists,"Musical expressivity is an important aspect of musical performance for humans as well as robotic musicians. We present a novel mechatronics-driven implementation of Brushless Direct Current (BLDC) motors in a robotic marimba player, named Shimon, designed to improve speed, dynamic range (loudness), and ultimately perceived musical expressivity in comparison to state-of-the-art robotic percussionist actuators. In an objective test of dynamic range, we find that our implementation provides wider and more consistent dynamic range response in comparison with solenoid-based robotic percussionists. Our implementation also outperforms both solenoid and human marimba players in striking speed. In a subjective listening test measuring musical expressivity, our system performs significantly better than a solenoid-based system and is statistically indistinguishable from human performers.",cs.RO,Robotics
Learning to predict metal deformations in hot-rolling processes,"Hot-rolling is a metal forming process that produces a workpiece with a desired target cross-section from an input workpiece through a sequence of plastic deformations; each deformation is generated by a stand composed of opposing rolls with a specific geometry. In current practice, the rolling sequence (i.e., the sequence of stands and the geometry of their rolls) needed to achieve a given final cross-section is designed by experts based on previous experience, and iteratively refined in a costly trial-and-error process. Finite Element Method simulations are increasingly adopted to make this process more efficient and to test potential rolling sequences, achieving good accuracy at the cost of long simulation times, limiting the practical use of the approach. We propose a supervised learning approach to predict the deformation of a given workpiece by a set of rolls with a given geometry; the model is trained on a large dataset of procedurally-generated FEM simulations, which we publish as supplementary material. The resulting predictor is four orders of magnitude faster than simulations, and yields an average Jaccard Similarity Index of 0.972 (against ground truth from simulations) and 0.925 (against real-world measured deformations); we additionally report preliminary results on using the predictor for automatic planning of rolling sequences.",cs.RO,Robotics
Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints,"Estimating relative camera poses from consecutive frames is a fundamental problem in visual odometry (VO) and simultaneous localization and mapping (SLAM), where classic methods consisting of hand-crafted features and sampling-based outlier rejection have been a dominant choice for over a decade. Although multiple works propose to replace these modules with learning-based counterparts, most have not yet been as accurate, robust and generalizable as conventional methods. In this paper, we design an end-to-end trainable framework consisting of learnable modules for detection, feature extraction, matching and outlier rejection, while directly optimizing for the geometric pose objective. We show both quantitatively and qualitatively that pose estimation performance may be achieved on par with the classic pipeline. Moreover, we are able to show by end-to-end training, the key components of the pipeline could be significantly improved, which leads to better generalizability to unseen datasets compared to existing learning-based methods.",cs.RO,Robotics
SplitFlyer: a Modular Quadcoptor that Disassembles into Two Flying Robots,"We introduce SplitFlyer--a novel quadcopter with an ability to disassemble into two self-contained bicopters through human assistance. As a subunit, the bicopter is a severely underactuated aerial vehicle equipped with only two propellers. Still, each bicopter is capable of independent flight. To achieve this, we provide an analysis of the system dynamics by relaxing the control over the yaw rotation, allowing the bicopter to maintain its large spinning rate in flight. Taking into account the gyroscopic motion, the dynamics are described and a cascaded control strategy is developed. We constructed a transformable prototype to demonstrate consecutive flights in both configurations. The results verify the proposed control strategy and show the potential of the platform for future research in modular aerial swarm robotics.",cs.RO,Robotics
Autonomous Navigation in Complex Environments with Deep Multimodal Fusion Network,"Autonomous navigation in complex environments is a crucial task in time-sensitive scenarios such as disaster response or search and rescue. However, complex environments pose significant challenges for autonomous platforms to navigate due to their challenging properties: constrained narrow passages, unstable pathway with debris and obstacles, or irregular geological structures and poor lighting conditions. In this work, we propose a multimodal fusion approach to address the problem of autonomous navigation in complex environments such as collapsed cites, or natural caves. We first simulate the complex environments in a physics-based simulation engine and collect a large-scale dataset for training. We then propose a Navigation Multimodal Fusion Network (NMFNet) which has three branches to effectively handle three visual modalities: laser, RGB images, and point cloud data. The extensively experimental results show that our NMFNet outperforms recent state of the art by a fair margin while achieving real-time performance. We further show that the use of multiple modalities is essential for autonomous navigation in complex environments. Finally, we successfully deploy our network to both simulated and real mobile robots.",cs.RO,Robotics
Conscious Intelligence Requires Lifelong Autonomous Programming For General Purposes,"Universal Turing Machines [29, 10, 18] are well known in computer science but they are about manual programming for general purposes. Although human children perform conscious learning (i.e., learning while being conscious) from infancy [24, 23, 14, 4], it is unknown that Universal Turing Machiness can facilitate not only our understanding of Autonomous Programming For General Purposes (APFGP) by machines, but also enable early-age conscious learning. This work reports a new kind of AI---conscious learning AI from a machine's ""baby"" time. Instead of arguing what static tasks a conscious machine should be able to do during its ""adulthood"", this work suggests that APFGP is a computationally clearer and necessary criterion for us to judge whether a machine is capable of conscious learning so that it can autonomously acquire skills along its ""career path"". The results here report new concepts and experimental studies for early vision, audition, natural language understanding, and emotion, with conscious learning capabilities that are absent from traditional AI systems.",cs.RO,Robotics
Deep Learning for Vision-based Prediction: A Survey,"Vision-based prediction algorithms have a wide range of applications including autonomous driving, surveillance, human-robot interaction, weather prediction. The objective of this paper is to provide an overview of the field in the past five years with a particular focus on deep learning approaches. For this purpose, we categorize these algorithms into video prediction, action prediction, trajectory prediction, body motion prediction, and other prediction applications. For each category, we highlight the common architectures, training methods and types of data used. In addition, we discuss the common evaluation metrics and datasets used for vision-based prediction tasks. A database of all the information presented in this survey including, cross-referenced according to papers, datasets and metrics, can be found online at https://github.com/aras62/vision-based-prediction.",cs.RO,Robotics
A Real-Time Receding Horizon Sequence Planner for Disassembly in A Human-Robot Collaboration Setting,"Product disassembly is a labor-intensive process and is far from being automated. Typically, disassembly is not robust enough to handle product varieties from different shapes, models, and physical uncertainties due to component imperfections, damage throughout component usage, or insufficient product information. To overcome these difficulties and to automate the disassembly procedure through human-robot collaboration without excessive computational cost, this paper proposes a real-time receding horizon sequence planner that distributes tasks between robot and human operator while taking real-time human motion into consideration. The sequence planner aims to address several issues in the disassembly line, such as varying orientations, safety constraints of human operators, uncertainty of human operation, and the computational cost of large number of disassembly tasks. The proposed disassembly sequence planner identifies both the positions and orientations of the to-be-disassembled items, as well as the locations of human operator, and obtains an optimal disassembly sequence that follows disassembly rules and safety constraints for human operation. Experimental tests have been conducted to validate the proposed planner: the robot can locate and disassemble the components following the optimal sequence, and consider explicitly human operator's real-time motion, and collaborate with the human operator without violating safety constraints.",cs.RO,Robotics
Experimental Evaluation of 3D-LIDAR Camera Extrinsic Calibration,In this paper we perform an experimental comparison of three different target based 3D-LIDAR camera calibration algorithms. We briefly elucidate the mathematical background behind each method and provide insights into practical aspects like ease of data collection for all of them. We extensively evaluate these algorithms on a sensor suite which consists multiple cameras and LIDARs by assessing their robustness to random initialization and by using metrics like Mean Line Re-projection Error (MLRE) and Factory Stereo Calibration Error. We also show the effect of noisy sensor on the calibration result from all the algorithms and conclude with a note on which calibration algorithm should be used under what circumstances.,cs.RO,Robotics
A Survey on Sensor Technologies for Unmanned Ground Vehicles,"Unmanned ground vehicles have a huge development potential in both civilian and military fields, and have become the focus of research in various countries. In addition, high-precision, high-reliability sensors are significant for UGVs' efficient operation. This paper proposes a brief review on sensor technologies for UGVs. Firstly, characteristics of various sensors are introduced. Then the strengths and weaknesses of different sensors as well as their application scenarios are compared. Furthermore, sensor applications in some existing UGVs are summarized. Finally, the hotspots of sensor technologies are forecasted to point the development direction.",cs.RO,Robotics
Passive Quadrupedal Gait Synchronization for Extra Robotic Legs Using a Dynamically Coupled Double Rimless Wheel Model,"The Extra Robotic Legs (XRL) system is a robotic augmentation worn by a human operator consisting of two articulated robot legs that walk with the operator and help bear a heavy backpack payload. It is desirable for the Human-XRL quadruped system to walk with the rear legs lead the front by 25% of the gait period, minimizing the energy lost from foot impacts while maximizing balance stability. Unlike quadrupedal robots, the XRL cannot command the human's limbs to coordinate quadrupedal locomotion. Using a pair of Rimless Wheel models, it is shown that the systems coupled with a spring and damper converge to the desired 25% phase difference. A Poincar return map was generated using numerical simulation to examine the convergence properties to different coupler design parameters, and initial conditions. The Dynamically Coupled Double Rimless Wheel system was physically realized with a spring and dashpot chosen from the theoretical results, and initial experiments indicate that the desired synchronization properties may be achieved within several steps using this set of passive components alone.",cs.RO,Robotics
Dynamics and Aerial Attitude Control for Rapid Emergency Deployment of the Agile Ground Robot AGRO,"In this work we present a Four-Wheeled Independent Drive and Steering (4WIDS) robot named AGRO and a method of controlling its orientation while airborne using wheel reaction torques. This is the first documented use of independently steerable wheels to both drive on the ground and achieve aerial attitude control when thrown. Inspired by a cat's self-righting reflex, this capability was developed to allow emergency response personnel to rapidly deploy AGRO by throwing it over walls and fences or through windows without the risk of it landing upside down. It also allows AGRO to drive off of ledges and ensure it lands on all four wheels. We have demonstrated a successful thrown deployment of AGRO. A novel parametrization and singularity analysis of 4WIDS kinematics reveals independent yaw authority with simultaneous adjustment of the ratio between roll and pitch authority. Simple PD controllers allow for stabilization of roll, pitch, and yaw. These controllers were tested in a simulation using derived dynamic equations of motion, then implemented on the AGRO prototype. An experiment comparing a controlled and non-controlled fall was conducted in which AGRO was dropped from a height of 0.85 m with an initial roll and pitch angle of 16 degrees and -23 degrees respectively. With the controller enabled, AGRO can use the reaction torque from its wheels to stabilize its orientation within 402 milliseconds.",cs.RO,Robotics
Multi-Sensor Next-Best-View Planning as Matroid-Constrained Submodular Maximization,"3D scene models are useful in robotics for tasks such as path planning, object manipulation, and structural inspection. We consider the problem of creating a 3D model using depth images captured by a team of multiple robots. Each robot selects a viewpoint and captures a depth image from it, and the images are fused to update the scene model. The process is repeated until a scene model of desired quality is obtained. Next-best-view planning uses the current scene model to select the next viewpoints. The objective is to select viewpoints so that the images captured using them improve the quality of the scene model the most. In this paper, we address next-best-view planning for multiple depth cameras. We propose a utility function that scores sets of viewpoints and avoids overlap between multiple sensors. We show that multi-sensor next-best-view planning with this utility function is an instance of submodular maximization under a matroid constraint. This allows the planning problem to be solved by a polynomial-time greedy algorithm that yields a solution within a constant factor from the optimal. We evaluate the performance of our planning algorithm in simulated experiments with up to 8 sensors, and in real-world experiments using two robot arms equipped with depth cameras.",cs.RO,Robotics
RMPflow: A Geometric Framework for Generation of Multi-Task Motion Policies,"Generating robot motion for multiple tasks in dynamic environments is challenging, requiring an algorithm to respond reactively while accounting for complex nonlinear relationships between tasks. In this paper, we develop a novel policy synthesis algorithm, RMPflow, based on geometrically consistent transformations of Riemannian Motion Policies (RMPs). RMPs are a class of reactive motion policies that parameterize non-Euclidean behaviors as dynamical systems in intrinsically nonlinear task spaces. Given a set of RMPs designed for individual tasks, RMPflow can combine these policies to generate an expressive global policy, while simultaneously exploiting sparse structure for computational efficiency. We study the geometric properties of RMPflow and provide sufficient conditions for stability. Finally, we experimentally demonstrate that accounting for the natural Riemannian geometry of task policies can simplify classically difficult problems, such as planning through clutter on high-DOF manipulation systems.",cs.RO,Robotics
WGANVO: Monocular Visual Odometry based on Generative Adversarial Networks,"In this work we present WGANVO, a Deep Learning based monocular Visual Odometry method. In particular, a neural network is trained to regress a pose estimate from an image pair. The training is performed using a semi-supervised approach. Unlike geometry based monocular methods, the proposed method can recover the absolute scale of the scene without neither prior knowledge nor extra information. The evaluation of the system is carried out on the well-known KITTI dataset where it is shown to work in real time and the accuracy obtained is encouraging to continue the development of Deep Learning based methods.",cs.RO,Robotics
Noisy Agents: Self-supervised Exploration by Predicting Auditory Events,"Humans integrate multiple sensory modalities (e.g. visual and audio) to build a causal understanding of the physical world. In this work, we propose a novel type of intrinsic motivation for Reinforcement Learning (RL) that encourages the agent to understand the causal effect of its actions through auditory event prediction. First, we allow the agent to collect a small amount of acoustic data and use K-means to discover underlying auditory event clusters. We then train a neural network to predict the auditory events and use the prediction errors as intrinsic rewards to guide RL exploration. Experimental results on Atari games show that our new intrinsic motivation significantly outperforms several state-of-the-art baselines. We further visualize our noisy agents' behavior in a physics environment and demonstrate that our newly designed intrinsic reward leads to the emergence of physical interaction behaviors (e.g. contact with objects).",cs.RO,Robotics
"Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?","Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes \emph{prediction} challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess \emph{control}, we introduce an autonomous car novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.",cs.RO,Robotics
SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic Association and Salient Point Clustering Optimization,"We propose a novel 3D point cloud segmentation framework named SASO, which jointly performs semantic and instance segmentation tasks. For semantic segmentation task, inspired by the inherent correlation among objects in spatial context, we propose a Multi-scale Semantic Association (MSA) module to explore the constructive effects of the semantic context information. For instance segmentation task, different from previous works that utilize clustering only in inference procedure, we propose a Salient Point Clustering Optimization (SPCO) module to introduce a clustering procedure into the training process and impel the network focusing on points that are difficult to be distinguished. In addition, because of the inherent structures of indoor scenes, the imbalance problem of the category distribution is rarely considered but severely limits the performance of 3D scene perception. To address this issue, we introduce an adaptive Water Filling Sampling (WFS) algorithm to balance the category distribution of training data. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on benchmark datasets in both semantic segmentation and instance segmentation tasks.",cs.RO,Robotics
Imitation Learning Approach for AI Driving Olympics Trained on Real-world and Simulation Data Simultaneously,"In this paper, we describe our winning approach to solving the Lane Following Challenge at the AI Driving Olympics Competition through imitation learning on a mixed set of simulation and real-world data. AI Driving Olympics is a two-stage competition: at stage one, algorithms compete in a simulated environment with the best ones advancing to a real-world final. One of the main problems that participants encounter during the competition is that algorithms trained for the best performance in simulated environments do not hold up in a real-world environment and vice versa. Classic control algorithms also do not translate well between tasks since most of them have to be tuned to specific driving conditions such as lighting, road type, camera position, etc. To overcome this problem, we employed the imitation learning algorithm and trained it on a dataset collected from sources both from simulation and real-world, forcing our model to perform equally well in all environments.",cs.RO,Robotics
Single Storage Semi-Global Matching for Real Time Depth Processing,"Depth-map is the key computation in computer vision and robotics. One of the most popular approach is via computation of disparity-map of images obtained from Stereo Camera. Semi Global Matching (SGM) method is a popular choice for good accuracy with reasonable computation time. To use such compute-intensive algorithms for real-time applications such as for autonomous aerial vehicles, blind Aid, etc. acceleration using GPU, FPGA is necessary. In this paper, we show the design and implementation of a stereo-vision system, which is based on FPGA-implementation of More Global Matching(MGM). MGM is a variant of SGM. We use 4 paths but store a single cumulative cost value for a corresponding pixel. Our stereo-vision prototype uses Zedboard containing an ARM-based Zynq-SoC, ZED-stereo-camera / ELP stereo-camera / Intel RealSense D435i, and VGA for visualization. The power consumption attributed to the custom FPGA-based acceleration of disparity map computation required for depth-map is just 0.72 watt. The update rate of the disparity map is realistic 10.5 fps.",cs.RO,Robotics
Projection Mapping Implementation: Enabling Direct Externalization of Perception Results and Action Intent to Improve Robot Explainability,"Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not accurately present a robot's internal states such as perception results and action intent. Projecting the states directly onto a robot's operating environment has the advantages of being direct, accurate, and more salient, eliminating mental inference about the robot's intention. However, there is a lack of tools for projection mapping in robotics, compared to established motion planning libraries (e.g., MoveIt). In this paper, we detail the implementation of projection mapping to enable researchers and practitioners to push the boundaries for better interaction between robots and humans. We also provide practical documentation and code for a sample manipulation projection mapping on GitHub: https://github.com/uml-robotics/projection_mapping.",cs.RO,Robotics
Task-Space Control Interface for SoftBank Humanoid Robots and its Human-Robot Interaction Applications,"We present an open-source software interface, called mc_naoqi, that allows to perform whole-body task-space Quadratic Programming based control, implemented in mc_rtc framework, on the SoftBank Robotics Europe humanoid robots. We describe the control interface, associated robot description packages, robot modules and sample whole-body controllers. We demonstrate the use of these tools in simulation for a robot interacting with a human model. Finally, we showcase and discuss the use of the developed open-source tools for running the human-robot close contact interaction experiments with real human subjects inspired from assistance scenarios.",cs.RO,Robotics
Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction,"Explainable artificial intelligence (XAI) can help foster trust in and acceptance of intelligent and autonomous systems. Moreover, understanding the motivation for an agent's behavior results in better and more successful collaborations between robots and humans. However, not only can humans benefit from a robot's explanation but the robot itself can also benefit from explanations given to him. Currently, most attention is paid to explaining deep neural networks and black-box models. However, a lot of these approaches are not applicable to humanoid robots. Therefore, in this position paper, current problems with adapting XAI methods to explainable neurorobotics are described. Furthermore, NICO, an open-source humanoid robot platform, is introduced and how the interaction of intrinsic explanations by the robot itself and extrinsic explanations provided by the environment enable efficient robotic behavior.",cs.RO,Robotics
Emotional Musical Prosody: Validated Vocal Dataset for Human Robot Interaction,"Human collaboration with robotics is dependant on the development of a relationship between human and robot, without which performance and utilization can decrease. Emotion and personality conveyance has been shown to enhance robotic collaborations, with improved human-robot relationships and increased trust. One under-explored way for an artificial agent to convey emotions is through non-linguistic musical prosody. In this work we present a new 4.2 hour dataset of improvised emotional vocal phrases based on the Geneva Emotion Wheel. This dataset has been validated through extensive listening tests and shows promising preliminary results for use in generative systems.",cs.RO,Robotics
A novel control mode of bionic morphing tail based on deep reinforcement learning,"In the field of fixed wing aircraft, many morphing technologies have been applied to the wing, such as adaptive airfoil, variable span aircraft, variable swept angle aircraft, etc., but few are aimed at the tail. The traditional fixed wing tail includes horizontal and vertical tail. Inspired by the bird tail, this paper will introduce a new bionic tail. The tail has a novel control mode, which has multiple control variables. Compared with the traditional fixed wing tail, it adds the area control and rotation control around the longitudinal symmetry axis, so it can control the pitch and yaw of the aircraft at the same time. When the area of the tail changes, the maneuverability and stability of the aircraft can be changed, and the aerodynamic efficiency of the aircraft can also be improved. The aircraft with morphing ability is often difficult to establish accurate mathematical model, because the model has a strong nonlinear, model-based control method is difficult to deal with the strong nonlinear aircraft. In recent years, with the rapid development of artificial intelligence technology, learning based control methods are also brilliant, in which the deep reinforcement learning algorithm can be a good solution to the control object which is difficult to establish model. In this paper, the model-free control algorithm PPO is used to control the tail, and the traditional PID is used to control the aileron and throttle. After training in simulation, the tail shows excellent attitude control ability.",cs.RO,Robotics
"Safe, Passive Control for Mechanical Systems with Application to Physical Human-Robot Interactions","In this paper, we propose a novel safe, passive, and robust control law for mechanical systems. The proposed approach addresses safety from a physical human-robot interaction perspective, where a robot must not only stay inside a pre-defined region, but respect velocity constraints and ensure passivity with respect to external perturbations that may arise from a human or the environment. The proposed control is written in closed-form, behaves well even during singular configurations, and allows any nominal control law to be applied inside the operating region as long as the safety requirements (e.g., velocity) are adhered to. The proposed method is implemented on a 6-DOF robot to demonstrate its effectiveness during a physical human-robot interaction task.",cs.RO,Robotics
DRF: A Framework for High-Accuracy Autonomous Driving Vehicle Modeling,"An accurate vehicle dynamic model is the key to bridge the gap between simulation and real road test in autonomous driving. In this paper, we present a Dynamic model-Residual correction model Framework (DRF) for vehicle dynamic modeling. On top of any existing open-loop dynamic model, this framework builds a Residual Correction Model (RCM) by integrating deep Neural Networks (NN) with Sparse Variational Gaussian Process (SVGP) model. RCM takes a sequence of vehicle control commands and dynamic status for a certain time duration as modeling inputs, extracts underlying context from this sequence with deep encoder networks, and predicts open-loop dynamic model prediction errors. Five vehicle dynamic models are derived from DRF via encoder variation. Our contribution is consolidated by experiments on evaluation of absolute trajectory error and similarity between DRF outputs and the ground truth. Compared to classic rule-based and learning-based vehicle dynamic models, DRF accomplishes as high as 74.12% to 85.02% of absolute trajectory error drop among all DRF variations.",cs.RO,Robotics
APPLR: Adaptive Planner Parameter Learning from Reinforcement,"Classical navigation systems typically operate using a fixed set of hand-picked parameters (e.g. maximum speed, sampling rate, inflation radius, etc.) and require heavy expert re-tuning in order to work in new environments. To mitigate this requirement, it has been proposed to learn parameters for different contexts in a new environment using human demonstrations collected via teleoperation. However, learning from human demonstration limits deployment to the training environment, and limits overall performance to that of a potentially-suboptimal demonstrator. In this paper, we introduce APPLR, Adaptive Planner Parameter Learning from Reinforcement, which allows existing navigation systems to adapt to new scenarios by using a parameter selection scheme discovered via reinforcement learning (RL) in a wide variety of simulation environments. We evaluate APPLR on a robot in both simulated and physical experiments, and show that it can outperform both a fixed set of hand-tuned parameters and also a dynamic parameter tuning scheme learned from human demonstration.",cs.RO,Robotics
Dark Reciprocal-Rank: Boosting Graph-Convolutional Self-Localization Network via Teacher-to-student Knowledge Transfer,"In visual robot self-localization, graph-based scene representation and matching have recently attracted research interest as robust and discriminative methods for selflocalization. Although effective, their computational and storage costs do not scale well to large-size environments. To alleviate this problem, we formulate self-localization as a graph classification problem and attempt to use the graph convolutional neural network (GCN) as a graph classification engine. A straightforward approach is to use visual feature descriptors that are employed by state-of-the-art self-localization systems, directly as graph node features. However, their superior performance in the original self-localization system may not necessarily be replicated in GCN-based self-localization. To address this issue, we introduce a novel teacher-to-student knowledge-transfer scheme based on rank matching, in which the reciprocal-rank vector output by an off-the-shelf state-of-the-art teacher self-localization model is used as the dark knowledge to transfer. Experiments indicate that the proposed graph-convolutional self-localization network can significantly outperform state-of-the-art self-localization systems, as well as the teacher classifier.",cs.RO,Robotics
Semantic Task Planning for Service Robots in Open World,"In this paper, we present a planning system based on semantic reasoning for a general-purpose service robot, which is aimed at behaving more intelligently in domains that contain incomplete information, under-specified goals, and dynamic changes. First, Two kinds of data are generated by Natural Language Processing module from the speech: (i) action frames and their relationships; (ii) the modifier used to indicate some property or characteristic of a variable in the action frame. Next, the goals of the task are generated from these action frames and modifiers. These goals are represented as AI symbols, combining world state and domain knowledge, which are used to generate plans by an Answer Set Programming solver. Finally, the actions of the plan are executed one by one, and continuous sensing grounds useful information, which make the robot to use contingent knowledge to adapt to dynamic changes and faults. For each action in the plan, the planner gets its preconditions and effects from domain knowledge, so during the execution of the task, the environmental changes, especially those conflict with the actions, not only the action being performed, but also the subsequent actions, can be detected and handled as early as possible. A series of case studies are used to evaluate the system and verify its ability to acquire knowledge through dialogue with users, solve problems with the acquired causal knowledge, and plan for complex tasks autonomously in the open world.",cs.RO,Robotics
Vision-Based Control for Robots by a Fully Spiking Neural System Relying on Cerebellar Predictive Learning,"The cerebellum plays a distinctive role within our motor control system to achieve fine and coordinated motions. While cerebellar lesions do not lead to a complete loss of motor functions, both action and perception are severally impacted. Hence, it is assumed that the cerebellum uses an internal forward model to provide anticipatory signals by learning from the error in sensory states. In some studies, it was demonstrated that the learning process relies on the joint-space error. However, this may not exist. This work proposes a novel fully spiking neural system that relies on a forward predictive learning by means of a cellular cerebellar model. The forward model is learnt thanks to the sensory feedback in task-space and it acts as a Smith predictor. The latter predicts sensory corrections in input to a differential mapping spiking neural network during a visual servoing task of a robot arm manipulator. In this paper, we promote the developed control system to achieve more accurate target reaching actions and reduce the motion execution time for the robotic reaching tasks thanks to the cerebellar predictive capabilities.",cs.RO,Robotics
Differentiable Physics Models for Real-world Offline Model-based Reinforcement Learning,"A limitation of model-based reinforcement learning (MBRL) is the exploitation of errors in the learned models. Black-box models can fit complex dynamics with high fidelity, but their behavior is undefined outside of the data distribution.Physics-based models are better at extrapolating, due to the general validity of their informed structure, but underfit in the real world due to the presence of unmodeled phenomena. In this work, we demonstrate experimentally that for the offline model-based reinforcement learning setting, physics-based models can be beneficial compared to high-capacity function approximators if the mechanical structure is known. Physics-based models can learn to perform the ball in a cup (BiC) task on a physical manipulator using only 4 minutes of sampled data using offline MBRL. We find that black-box models consistently produce unviable policies for BiC as all predicted trajectories diverge to physically impossible state, despite having access to more data than the physics-based model. In addition, we generalize the approach of physics parameter identification from modeling holonomic multi-body systems to systems with nonholonomic dynamics using end-to-end automatic differentiation.
  Videos: https://sites.google.com/view/ball-in-a-cup-in-4-minutes/",cs.RO,Robotics
LyRN (Lyapunov Reaching Network): A Real-Time Closed Loop approach from Monocular Vision,"We propose a closed-loop, multi-instance control algorithm for visually guided reaching based on novel learning principles. A control Lyapunov function methodology is used to design a reaching action for a complex multi-instance task in the case where full state information (poses of all potential reaching points) is available. The proposed algorithm uses monocular vision and manipulator joint angles as the input to a deep convolution neural network to predict the value of the control Lyapunov function (cLf) and corresponding velocity control. The resulting network output is used in real-time as visual control for the grasping task with the multi-instance capability emerging naturally from the design of the control Lyapunov function.
  We demonstrate the proposed algorithm grasping mugs (textureless and symmetric objects) on a table-top from an over-the-shoulder monocular RGB camera. The manipulator dynamically converges to the best-suited target among multiple identical instances from any random initial pose within the workspace. The system trained with only simulated data is able to achieve 90.3% grasp success rate in the real-world experiments with up to 85Hz closed-loop control on one GTX 1080Ti GPU and significantly outperforms a Pose-Based-Visual-Servo (PBVS) grasping system adapted from a state-of-the-art single shot RGB 6D pose estimation algorithm. A key contribution of the paper is the inclusion of a first-order differential constraint associated with the cLf as a regularisation term during learning, and we provide evidence that this leads to more robust and reliable reaching/grasping performance than vanilla regression on general control inputs.",cs.RO,Robotics
Collision-free Trajectory Planning for Autonomous Surface Vehicle,"In this paper, we propose an efficient and accurate method for autonomous surface vehicles to generate a smooth and collision-free trajectory considering its dynamics constraints. We decouple the trajectory planning problem as a front-end feasible path searching and a back-end kinodynamic trajectory optimization. Firstly, we model the type of two-thrusts under-actuated surface vessel. Then we adopt a sampling-based path searching to find an asymptotic optimal path through the obstacle-surrounding environment and extract several waypoints from it. We apply a numerical optimization method in the back-end to generate the trajectory. From the perspective of security in the field voyage, we propose the sailing corridor method to guarantee the trajectory away from obstacles. Moreover, considering limited fuel ASV carrying, we design a numerical objective function which can optimize a fuel-saving trajectory. Finally, we validate and compare the proposed method in simulation environments and the results fit our expected trajectory.",cs.RO,Robotics
On the Potential of Smarter Multi-layer Maps,"The most common way for robots to handle environmental information is by using maps. At present, each kind of data is hosted on a separate map, which complicates planning because a robot attempting to perform a task needs to access and process information from many different maps. Also, most often correlation among the information contained in maps obtained from different sources is not evaluated or exploited. In this paper, we argue that in robotics a shift from single-source maps to a multi-layer mapping formalism has the potential to revolutionize the way robots interact with knowledge about their environment. This observation stems from the raise in metric-semantic mapping research, but expands to include in its formulation also layers containing other information sources, e.g., people flow, room semantic, or environment topology. Such multi-layer maps, here named hypermaps, not only can ease processing spatial data information but they can bring added benefits arising from the interaction between maps. We imagine that a new research direction grounded in such multi-layer mapping formalism for robots can use artificial intelligence to process the information it stores to present to the robot task-specific information simplifying planning and bringing us one step closer to high-level reasoning in robots.",cs.RO,Robotics
Egocentric Object Manipulation Graphs,"We introduce Egocentric Object Manipulation Graphs (Ego-OMG) - a novel representation for activity modeling and anticipation of near future actions integrating three components: 1) semantic temporal structure of activities, 2) short-term dynamics, and 3) representations for appearance. Semantic temporal structure is modeled through a graph, embedded through a Graph Convolutional Network, whose states model characteristics of and relations between hands and objects. These state representations derive from all three levels of abstraction, and span segments delimited by the making and breaking of hand-object contact. Short-term dynamics are modeled in two ways: A) through 3D convolutions, and B) through anticipating the spatiotemporal end points of hand trajectories, where hands come into contact with objects. Appearance is modeled through deep spatiotemporal features produced through existing methods. We note that in Ego-OMG it is simple to swap these appearance features, and thus Ego-OMG is complementary to most existing action anticipation methods. We evaluate Ego-OMG on the EPIC Kitchens Action Anticipation Challenge. The consistency of the egocentric perspective of EPIC Kitchens allows for the utilization of the hand-centric cues upon which Ego-OMG relies. We demonstrate state-of-the-art performance, outranking all other previous published methods by large margins and ranking first on the unseen test set and second on the seen test set of the EPIC Kitchens Action Anticipation Challenge. We attribute the success of Ego-OMG to the modeling of semantic structure captured over long timespans. We evaluate the design choices made through several ablation studies. Code will be released upon acceptance",cs.RO,Robotics
Segmentation of Surgical Instruments for Minimally-Invasive Robot-Assisted Procedures Using Generative Deep Neural Networks,"This work proves that semantic segmentation on minimally invasive surgical instruments can be improved by using training data that has been augmented through domain adaptation. The benefit of this method is twofold. Firstly, it suppresses the need of manually labeling thousands of images by transforming synthetic data into realistic-looking data. To achieve this, a CycleGAN model is used, which transforms a source dataset to approximate the domain distribution of a target dataset. Secondly, this newly generated data with perfect labels is utilized to train a semantic segmentation neural network, U-Net. This method shows generalization capabilities on data with variability regarding its rotation- position- and lighting conditions. Nevertheless, one of the caveats of this approach is that the model is unable to generalize well to other surgical instruments with a different shape from the one used for training. This is driven by the lack of a high variance in the geometric distribution of the training data. Future work will focus on making the model more scale-invariant and able to adapt to other types of surgical instruments previously unseen by the training.",cs.RO,Robotics
Autonomous Vehicle Benchmarking using Unbiased Metrics,"With the recent development of autonomous vehicle technology, there have been active efforts on the deployment of this technology at different scales that include urban and highway driving. While many of the prototypes showcased have been shown to operate under specific cases, little effort has been made to better understand their shortcomings and generalizability to new areas. Distance, uptime and number of manual disengagements performed during autonomous driving provide a high-level idea on the performance of an autonomous system but without proper data normalization, testing location information, and the number of vehicles involved in testing, the disengagement reports alone do not fully encompass system performance and robustness. Thus, in this study a complete set of metrics are applied for benchmarking autonomous vehicle systems in a variety of scenarios that can be extended for comparison with human drivers and other autonomous vehicle systems. These metrics have been used to benchmark UC San Diego's autonomous vehicle platforms during early deployments for micro-transit and autonomous mail delivery applications.",cs.RO,Robotics
"A Characterization of Semi-Synchrony for Asynchronous Robots with Limited Visibility, and its Application to Luminous Synchronizer Design","A mobile robot system consists of anonymous mobile robots, each of which autonomously performs sensing, computation, and movement according to a common algorithm, so that the robots collectively achieve a given task. There are two main models of time and activation of the robots. In the semi-synchronous model (SSYNC), the robots share a common notion of time; at each time unit, a subset of the robots is activated, and each performs all three actions (sensing, computation, and movement) in that time unit. In the asynchronous model (ASYNC), there is no common notion of time, the robots are activated at arbitrary times, and the duration of each action is arbitrary but finite.
  In this paper, we investigate the problem of synchronizing ASNYC robots with limited sensing range, i.e., limited visibility. We first present a sufficient condition for an ASYNC execution of a common algorithm ${\cal A}$ to have a corresponding SSYNC execution of ${\cal A}$; our condition imposes timing constraints on the activation schedule of the robots and visibility constraints during movement. Then, we prove that this condition is necessary (with probability $1$) under a randomized ASYNC adversary. Finally, we present a synchronization algorithm for luminous ASYNC robots with limited visibility, each equipped with a light that can take a constant number of colors. Our algorithm enables luminous ASYNC robots to simulate any algorithm ${\cal A}$, designed for the (non-luminous) SSYNC robots and satisfying visibility constraints.",cs.RO,Robotics
A Preliminary Study for a Quantum-like Robot Perception Model,"Formalisms based on quantum theory have been used in Cognitive Science for decades due to their descriptive features. A quantum-like (QL) approach provides descriptive features such as state superposition and probabilistic interference behavior. Moreover, quantum systems dynamics have been found isomorphic to cognitive or biological systems dynamics. The objective of this paper is to study the feasibility of a QL perception model for a robot with limited sensing capabilities. We introduce a case study, we highlight its limitations, and we investigate and analyze actual robot behaviors through simulations, while actual implementations based on quantum devices encounter errors for unbalanced situations. In order to investigate QL models for robot behavior, and to study the advantages leveraged by QL approaches for robot knowledge representation and processing, we argue that it is preferable to proceed with simulation-oriented techniques rather than actual realizations on quantum backends.",cs.RO,Robotics
Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control,"Learning a stable Linear Dynamical System (LDS) from data involves creating models that both minimize reconstruction error and enforce stability of the learned representation. We propose a novel algorithm for learning stable LDSs. Using a recent characterization of stable matrices, we present an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived in this paper. When applied to LDSs with inputs, our approach---in contrast to current methods for learning stable LDSs---updates both the state and control matrices, expanding the solution space and allowing for models with lower reconstruction error. We apply our algorithm in simulations and experiments to a variety of problems, including learning dynamic textures from image sequences and controlling a robotic manipulator. Compared to existing approaches, our proposed method achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance. In addition, it is provably more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of competing alternatives, thus scaling to higher-dimensional systems when the other methods fail.",cs.RO,Robotics
Modeling Human Temporal Uncertainty in Human-Agent Teams,"Automated scheduling is potentially a very useful tool for facilitating efficient, intuitive interactions between a robot and a human teammate. However, a current gapin automated scheduling is that it is not well understood how to best represent the timing uncertainty that human teammates introduce. This paper attempts to address this gap by designing an online human-robot collaborative packaging game that we use to build a model of human timing uncertainty from a population of crowd-workers. We conclude that heavy-tailed distributions are the best models of human temporal uncertainty, with a Log-Normal distribution achieving the best fit to our experimental data. We discuss how these results along with our collaborative online game will inform and facilitate future explorations into scheduling for improved human-robot fluency.",cs.RO,Robotics
Helpfulness as a Key Metric of Human-Robot Collaboration,"As robotic teammates become more common in society, people will assess the robots' roles in their interactions along many dimensions. One such dimension is effectiveness: people will ask whether their robotic partners are trustworthy and effective collaborators. This begs a crucial question: how can we quantitatively measure the helpfulness of a robotic partner for a given task at hand? This paper seeks to answer this question with regards to the interactive robot's decision making. We describe a clear, concise, and task-oriented metric applicable to many different planning and execution paradigms. The proposed helpfulness metric is fundamental to assessing the benefit that a partner has on a team for a given task. In this paper, we define helpfulness, illustrate it on concrete examples from a variety of domains, discuss its properties and ramifications for planning interactions with humans, and present preliminary results.",cs.RO,Robotics
A Termination Criterion for Probabilistic PointClouds Registration,"Probabilistic Point Clouds Registration (PPCR) is an algorithm that, in its multi-iteration version, outperformed state of the art algorithms for local point clouds registration. However, its performances have been tested using a fixed high number of iterations. To be of practical usefulness, we think that the algorithm should decide by itself when to stop, to avoid an excessive number of iterations and, therefore, wasting computational time. With this work, we compare different termination criterion on several datasets and prove that the chosen one produce very good results that are comparable to those obtained using a very high number of iterations while saving computational time.",cs.RO,Robotics
"Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning","Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion planning algorithms, such as rapidly-exploring random trees, are capable of efficiently exploring large state spaces and computing long-horizon, sequential plans. However, these algorithms are generally challenged with complex, stochastic, and high-dimensional state spaces as well as in the presence of narrow passages, which naturally emerge in tasks that interact with the environment. Machine learning offers a promising solution for its ability to learn general policies that can handle complex interactions and high-dimensional observations. However, these policies are generally limited in horizon length. Our approach, Broadly-Exploring, Local-policy Trees (BELT), merges these two approaches to leverage the strengths of both through a task-conditioned, model-based tree search. BELT uses an RRT-inspired tree search to efficiently explore the state space. Locally, the exploration is guided by a task-conditioned, learned policy capable of performing general short-horizon tasks. This task space can be quite general and abstract; its only requirements are to be sampleable and to well-cover the space of useful tasks. This search is aided by a task-conditioned model that temporally extends dynamics propagation to allow long-horizon search and sequential reasoning over tasks. BELT is demonstrated experimentally to be able to plan long-horizon, sequential trajectories with a goal conditioned policy and generate plans that are robust.",cs.RO,Robotics
Waymo's Safety Methodologies and Safety Readiness Determinations,"Waymo's safety methodologies, which draw on well established engineering processes and address new safety challenges specific to Automated Vehicle technology, provide a firm foundation for safe deployment of Waymo's Level 4 ADS, which Waymo also refers to as the Waymo Driver. Waymo's determination of its readiness to deploy its AVs safely in different settings rests on that firm foundation and on a thorough analysis of risks specific to a particular Operational Design Domain. Waymo's process for making these readiness determinations entails an ordered examination of the relevant outputs from all of its safety methodologies combined with careful safety and engineering judgment focused on the specific facts relevant for a particular determination. Waymo will approve when it determines the ADS is ready for the new conditions without creating any unreasonable risks to safety. This paper explains Waymo's methodologies as applied to the three layers of its technology: hardware, ADS behavior, and operations, and also explains Waymo's safety governance. Waymo will continue to apply and adapt those methodologies, and to learn from the important contributions of others in the AV industry, as Waymo continues to build an ever safer and more able ADS.",cs.RO,Robotics
Optimizing Mixed Autonomy Traffic Flow With Decentralized Autonomous Vehicles and Multi-Agent RL,"We study the ability of autonomous vehicles to improve the throughput of a bottleneck using a fully decentralized control scheme in a mixed autonomy setting. We consider the problem of improving the throughput of a scaled model of the San Francisco-Oakland Bay Bridge: a two-stage bottleneck where four lanes reduce to two and then reduce to one. Although there is extensive work examining variants of bottleneck control in a centralized setting, there is less study of the challenging multi-agent setting where the large number of interacting AVs leads to significant optimization difficulties for reinforcement learning methods. We apply multi-agent reinforcement algorithms to this problem and demonstrate that significant improvements in bottleneck throughput, from 20\% at a 5\% penetration rate to 33\% at a 40\% penetration rate, can be achieved. We compare our results to a hand-designed feedback controller and demonstrate that our results sharply outperform the feedback controller despite extensive tuning. Additionally, we demonstrate that the RL-based controllers adopt a robust strategy that works across penetration rates whereas the feedback controllers degrade immediately upon penetration rate variation. We investigate the feasibility of both action and observation decentralization and demonstrate that effective strategies are possible using purely local sensing. Finally, we open-source our code at  https://github.com/eugenevinitsky/decentralized_bottlenecks.",cs.RO,Robotics
Multi-Agent Motion Planning using Deep Learning for Space Applications,"State-of-the-art motion planners cannot scale to a large number of systems. Motion planning for multiple agents is an NP (non-deterministic polynomial-time) hard problem, so the computation time increases exponentially with each addition of agents. This computational demand is a major stumbling block to the motion planner's application to future NASA missions involving the swarm of space vehicles. We applied a deep neural network to transform computationally demanding mathematical motion planning problems into deep learning-based numerical problems. We showed optimal motion trajectories can be accurately replicated using deep learning-based numerical models in several 2D and 3D systems with multiple agents. The deep learning-based numerical model demonstrates superior computational efficiency with plans generated 1000 times faster than the mathematical model counterpart.",cs.RO,Robotics
Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments,"Deep reinforcement learning (RL) agents are able to learn contact-rich manipulation tasks by maximizing a reward signal, but require large amounts of experience, especially in environments with many obstacles that complicate exploration. In contrast, motion planners use explicit models of the agent and environment to plan collision-free paths to faraway goals, but suffer from inaccurate models in tasks that require contacts with the environment. To combine the benefits of both approaches, we propose motion planner augmented RL (MoPA-RL) which augments the action space of an RL agent with the long-horizon planning capabilities of motion planners. Based on the magnitude of the action, our approach smoothly transitions between directly executing the action and invoking a motion planner. We evaluate our approach on various simulated manipulation tasks and compare it to alternative action spaces in terms of learning efficiency and safety. The experiments demonstrate that MoPA-RL increases learning efficiency, leads to a faster exploration, and results in safer policies that avoid collisions with the environment. Videos and code are available at https://clvrai.com/mopa-rl .",cs.RO,Robotics
Force and state-feedback control for robots with non-collocated environmental and actuator forces,"In this paper, we present an impedance control design for multi-variable linear and nonlinear robotic systems. The control design considers force and state feedback to improve the performance of the closed loop. Simultaneous feedback of forces and states allows the controller for an extra degree of freedom to approximate the desired impedance port behaviour. A numerical analysis is used to demonstrate the desired impedance closed-loop behaviour.",cs.RO,Robotics
Kinodynamic Planning for an Energy-Efficient Autonomous Ornithopter,"This paper presents a novel algorithm to plan energy-efficient trajectories for autonomous ornithopters. In general, trajectory optimization is quite a relevant problem for practical applications with \emph{Unmanned Aerial Vehicles} (UAVs). Even though the problem has been well studied for fixed and rotatory-wing vehicles, there are far fewer works exploring it for flapping-wing UAVs like ornithopters. These are of interest for many applications where long flight endurance, but also hovering capabilities are required. We propose an efficient approach to plan ornithopter trajectories that minimize energy consumption by combining gliding and flapping maneuvers. Our algorithm builds a tree of dynamically feasible trajectories and applies heuristic search for efficient online planning, using reference curves to guide the search and prune states. We present computational experiments to analyze and tune key parameters, as well as a comparison against a recent alternative probabilistic planning, showing best performance. Finally, we demonstrate how our algorithm can be used for planning perching maneuvers online.",cs.RO,Robotics
VIRAL-Fusion: A Visual-Inertial-Ranging-Lidar Sensor Fusion Approach,"In recent years, Onboard Self Localization (OSL) methods based on cameras or Lidar have achieved many significant progresses. However, some issues such as estimation drift and feature-dependence still remain inherent limitations. On the other hand, infrastructure-based methods can generally overcome these issues, but at the expense of some installation cost. This poses an interesting problem of how to effectively combine these methods, so as to achieve localization with long-term consistency as well as flexibility compared to any single method. To this end, we propose a comprehensive optimization-based estimator for 15-dimensional state of an Unmanned Aerial Vehicle (UAV), fusing data from an extensive set of sensors: inertial measurement units (IMUs), Ultra-Wideband (UWB) ranging sensors, and multiple onboard Visual-Inertial and Lidar odometry subsystems. In essence, a sliding window is used to formulate a sequence of robot poses, where relative rotational and translational constraints between these poses are observed in the IMU preintegration and OSL observations, while orientation and position are coupled in body-offset UWB range observations. An optimization-based approach is developed to estimate the trajectory of the robot in this sliding window. We evaluate the performance of the proposed scheme in multiple scenarios, including experiments on public datasets, high-fidelity graphical-physical simulator, and field-collected data from UAV flight tests. The result demonstrates that our integrated localization method can effectively resolve the drift issue, while incurring minimal installation requirements.",cs.RO,Robotics
Model-Free Reinforcement Learning for Stochastic Games with Linear Temporal Logic Objectives,"We study the problem of synthesizing control strategies for Linear Temporal Logic (LTL) objectives in unknown environments. We model this problem as a turn-based zero-sum stochastic game between the controller and the environment, where the transition probabilities and the model topology are fully unknown. The winning condition for the controller in this game is the satisfaction of the given LTL specification, which can be captured by the acceptance condition of a deterministic Rabin automaton (DRA) directly derived from the LTL specification. We introduce a model-free reinforcement learning (RL) methodology to find a strategy that maximizes the probability of satisfying a given LTL specification when the Rabin condition of the derived DRA has a single accepting pair. We then generalize this approach to LTL formulas for which the Rabin condition has a larger number of accepting pairs, providing a lower bound on the satisfaction probability. Finally, we illustrate applicability of our RL method on two motion planning case studies.",cs.RO,Robotics
Target State Estimation and Prediction for High Speed Interception,"Accurate estimation and prediction of trajectory is essential for interception of any high speed target. In this paper, an extended Kalman filter is used to estimate the current location of target from its visual information and then predict its future position by using the observation sequence. Target motion model is developed considering the approximate known pattern of the target trajectory. In this work, we utilise visual information of the target to carry out the predictions. The proposed algorithm is developed in ROS-Gazebo environment and is verified using hardware implementation.",cs.RO,Robotics
Optimization of Robot Grasping Forces and Worst Case Loading,"We consider the optimization of the vector of grasping forces that support a known generalized force acting on the grasped object---a rigid body or a mechanism. Working in the framework of finite-dimensional normed vector spaces and their dual spaces, the cost function to be minimized is assumed to be a norm on the space of grasping forces. We present an expression for the optimum which depends on the external force and the kinematics of the grasping system. Next, assuming that optimal grasping forces are applied using force control, and assuming that there is a bound on the norm of the admissible grasping forces, we characterize the largest norm of an external force that the grasping system may support, that is, the norm of the worst-case loading that may be applied and still be supported. A few simple examples are given for the sake of illustration.",cs.RO,Robotics
Optimization-Based Framework for Excavation Trajectory Generation,"In this paper, we present a novel optimization-based framework for autonomous excavator trajectory generation under various objectives, including minimum joint displacement and minimum time. Traditional methods on excavation trajectory generation usually separate the excavation motion into a sequence of fixed phases, resulting in limited trajectory searching space. Our framework explores the space of all possible excavation trajectories represented with waypoints interpolated by a polynomial spline, thereby enabling optimization over a larger searching space. We formulate a generic task specification for excavation by constraining the instantaneous motion of the bucket and further add a target-oriented constraint, i.e. swept volume that indicates the estimated amount of excavated materials. To formulate time related objectives and constraints, we introduce time intervals between waypoints as variables into the optimization framework. We implement the proposed framework and evaluate its performance on a UR5 robotic arm. The experimental results demonstrate that the generated trajectories are able to excavate sufficient mass of soil for different terrain shapes and have 60% shorter minimal length than traditional excavation methods. We further compare our one-stage time optimal trajectory generation with the two-stage method. The result shows that trajectories generated by our one-stage method cost 18% less time on average.",cs.RO,Robotics
Behavioral decision-making for urban autonomous driving in the presence of pedestrians using Deep Recurrent Q-Network,"Decision making for autonomous driving in urban environments is challenging due to the complexity of the road structure and the uncertainty in the behavior of diverse road users. Traditional methods consist of manually designed rules as the driving policy, which require expert domain knowledge, are difficult to generalize and might give sub-optimal results as the environment gets complex. Whereas, using reinforcement learning, optimal driving policy could be learned and improved automatically through several interactions with the environment. However, current research in the field of reinforcement learning for autonomous driving is mainly focused on highway setup with little to no emphasis on urban environments. In this work, a deep reinforcement learning based decision-making approach for high-level driving behavior is proposed for urban environments in the presence of pedestrians. For this, the use of Deep Recurrent Q-Network (DRQN) is explored, a method combining state-of-the art Deep Q-Network (DQN) with a long term short term memory (LSTM) layer helping the agent gain a memory of the environment. A 3-D state representation is designed as the input combined with a well defined reward function to train the agent for learning an appropriate behavior policy in a real-world like urban simulator. The proposed method is evaluated for dense urban scenarios and compared with a rule-based approach and results show that the proposed DRQN based driving behavior decision maker outperforms the rule-based approach.",cs.RO,Robotics
A Path-Dependent Variational Framework for Incremental Information Gathering,"Information gathered along a path is inherently submodular; the incremental amount of information gained along a path decreases due to redundant observations. In addition to submodularity, the incremental amount of information gained is a function of not only the current state but also the entire history as well. This paper presents the construction of the first-order necessary optimality conditions for memory (history-dependent) Lagrangians. Path-dependent problems frequently appear in robotics and artificial intelligence, where the state such as a map is partially observable, and information can only be obtained along a trajectory by local sensing. Robotic exploration and environmental monitoring has numerous real-world applications and can be formulated using the proposed approach.",cs.RO,Robotics
Application of sequential processing of computer vision methods for solving the problem of detecting the edges of a honeycomb block,"The article describes the application of the Hough transform to a honeycomb block image. The problem of cutting a mold from a honeycomb block is described. A number of image transformations are considered to increase the efficiency of the Hough algorithm. A method for obtaining a binary image using a simple threshold, a method for obtaining a binary image using Otsu binarization, and the Canny Edge Detection algorithm are considered. The method of binary skeleton (skeletonization) is considered, in which the skeleton is obtained using 2 main morphological operations: Dilation and Erosion. As a result of a number of experiments, the optimal sequence of processing the original image was revealed, which allows obtaining the coordinates of the maximum number of faces. This result allows one to choose the optimal places for cutting a honeycomb block, which will improve the quality of the resulting shapes.",cs.RO,Robotics
Exploiting the Nonlinear Stiffness of TMP Origami Folding to Enhance Robotic Jumping Performance,"Via numerical simulation and experimental assessment, this study examines the use of origami folding to develop robotic jumping mechanisms with tailored nonlinear stiffness to improve dynamic performance. Specifically, we use Tachi-Miura Polyhedron (TMP) bellow origami -- which exhibits a nonlinear ""strain-softening"" force-displacement curve -- as a jumping robotic skeleton with embedded energy storage. TMP's nonlinear stiffness allows it to store more energy than a linear spring and offers improved jumping height and airtime. Moreover, the nonlinearity can be tailored by directly changing the underlying TMP crease geometry. A critical challenge is to minimize the TMP's hysteresis and energy loss during its compression stage right before jumping. So we used the plastically annealed lamina emergent origami (PALEO) concept to modify the TMP creases. PALEO increases the folding limit before plastic deformation occurs, thus improving the overall strain energy retention. Jumping experiments confirmed that a nonlinear TMP mechanism achieved roughly 9% improvement in air time and a 13% improvement in jumping height compared to a ""control"" TMP sample with a relatively linear stiffness. This study's results validate the advantages of using origami in robotic jumping mechanisms and demonstrate the benefits of utilizing nonlinear spring elements for improving jumping performance. Therefore, they could foster a new family of energetically efficient jumping mechanisms with optimized performance in the future.",cs.RO,Robotics
Sampling-based Reachability Analysis: A Random Set Theory Approach with Adversarial Sampling,"Reachability analysis is at the core of many applications, from neural network verification, to safe trajectory planning of uncertain systems. However, this problem is notoriously challenging, and current approaches tend to be either too restrictive, too slow, too conservative, or approximate and therefore lack guarantees. In this paper, we propose a simple yet effective sampling-based approach to perform reachability analysis for arbitrary dynamical systems. Our key novel idea consists of using random set theory to give a rigorous interpretation of our method, and prove that it returns sets which are guaranteed to converge to the convex hull of the true reachable sets. Additionally, we leverage recent work on robust deep learning and propose a new adversarial sampling approach to robustify our algorithm and accelerate its convergence. We demonstrate that our method is faster and less conservative than prior work, present results for approximate reachability analysis of neural networks and robust trajectory optimization of high-dimensional uncertain nonlinear systems, and discuss future applications.",cs.RO,Robotics
Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach,"Industrial robot manipulators are playing a more significant role in modern manufacturing industries. Though peg-in-hole assembly is a common industrial task which has been extensively researched, safely solving complex high precision assembly in an unstructured environment remains an open problem. Reinforcement Learning (RL) methods have been proven successful in solving manipulation tasks autonomously. However, RL is still not widely adopted on real robotic systems because working with real hardware entails additional challenges, especially when using position-controlled manipulators. The main contribution of this work is a learning-based method to solve peg-in-hole tasks with position uncertainty of the hole. We proposed the use of an off-policy model-free reinforcement learning method and bootstrap the training speed by using several transfer learning techniques (sim2real) and domain randomization. Our proposed learning framework for position-controlled robots was extensively evaluated on contact-rich insertion tasks on a variety of environments.",cs.RO,Robotics
An Information-Theoretic Approach to Persistent Environment Monitoring Through Low Rank Model Based Planning and Prediction,"Robots can be used to collect environmental data in regions that are difficult for humans to traverse. However, limitations remain in the size of region that a robot can directly observe per unit time. We introduce a method for selecting a limited number of observation points in a large region, from which we can predict the state of unobserved points in the region. We combine a low rank model of a target attribute with an information-maximizing path planner to predict the state of the attribute throughout a region. Our approach is agnostic to the choice of target attribute and robot monitoring platform. We evaluate our method in simulation on two real-world environment datasets, each containing observations from one to two million possible sampling locations. We compare against a random sampler and four variations of a baseline sampler from the ecology literature. Our method outperforms the baselines in terms of average Fisher information gain per samples taken and performs comparably for average reconstruction error in most trials.",cs.RO,Robotics
LoCUS: A multi-robot loss-tolerant algorithm for surveying volcanic plumes,"Measurement of volcanic CO2 flux by a drone swarm poses special challenges. Drones must be able to follow gas concentration gradients while tolerating frequent drone loss. We present the LoCUS algorithm as a solution to this problem and prove its robustness. LoCUS relies on swarm coordination and self-healing to solve the task. As a point of contrast we also implement the MoBS algorithm, derived from previously published work, which allows drones to solve the task independently. We compare the effectiveness of these algorithms using drone simulations, and find that LoCUS provides a reliable and efficient solution to the volcano survey problem. Further, the novel data-structures and algorithms underpinning LoCUS have application in other areas of fault-tolerant algorithm research.",cs.RO,Robotics
Control Framework for a Hybrid-steel Bridge Inspection Robot,"Autonomous navigation of steel bridge inspection robots is essential for proper maintenance. The majority of existing robotic solutions for bridge inspection require human intervention to assist in the control and navigation. In this paper, a control system framework has been proposed for a previously designed ARA robot [1], which facilitates autonomous real-time navigation and minimizes human involvement. The mechanical design and control framework of ARA robot enables two different configurations, namely the mobile and inch-worm transformation. In addition, a switching control was developed with 3D point clouds of steel surfaces as the input which allows the robot to switch between mobile and inch-worm transformation. The surface availability algorithm (considers plane, area, and height) of the switching control enables the robot to perform inch-worm jumps autonomously. Themobiletransformationallows the robot to move on continuous steel surfaces and perform visual inspection of steel bridge structures. Practical experiments on actual steel bridge structures highlight the effective performance of ARA robot with the proposed control framework for autonomous navigation during a visual inspection of steel bridges.",cs.RO,Robotics
Predicting Sim-to-Real Transfer with Probabilistic Dynamics Models,"We propose a method to predict the sim-to-real transfer performance of RL policies. Our transfer metric simplifies the selection of training setups (such as algorithm, hyperparameters, randomizations) and policies in simulation, without the need for extensive and time-consuming real-world rollouts. A probabilistic dynamics model is trained alongside the policy and evaluated on a fixed set of real-world trajectories to obtain the transfer metric. Experiments show that the transfer metric is highly correlated with policy performance in both simulated and real-world robotic environments for complex manipulation tasks. We further show that the transfer metric can predict the effect of training setups on policy transfer performance.",cs.RO,Robotics
"Enhancing a Neurocognitive Shared Visuomotor Model for Object Identification, Localization, and Grasping With Learning From Auxiliary Tasks","We present a follow-up study on our unified visuomotor neural model for the robotic tasks of identifying, localizing, and grasping a target object in a scene with multiple objects. Our Retinanet-based model enables end-to-end training of visuomotor abilities in a biologically inspired developmental approach. In our initial implementation, a neural model was able to grasp selected objects from a planar surface. We embodied the model on the NICO humanoid robot. In this follow-up study, we expand the task and the model to reaching for objects in a three-dimensional space with a novel dataset based on augmented reality and a simulation environment. We evaluate the influence of training with auxiliary tasks, i.e., if learning of the primary visuomotor task is supported by learning to classify and locate different objects. We show that the proposed visuomotor model can learn to reach for objects in a three-dimensional space. We analyze the results for biologically-plausible biases based on object locations or properties. We show that the primary visuomotor task can be successfully trained simultaneously with one of the two auxiliary tasks. This is enabled by a complex neurocognitive model with shared and task-specific components, similar to models found in biological systems.",cs.RO,Robotics
Heteroscedastic Bayesian Optimisation for Stochastic Model Predictive Control,"Model predictive control (MPC) has been successful in applications involving the control of complex physical systems. This class of controllers leverages the information provided by an approximate model of the system's dynamics to simulate the effect of control actions. MPC methods also present a few hyper-parameters which may require a relatively expensive tuning process by demanding interactions with the physical system. Therefore, we investigate fine-tuning MPC methods in the context of stochastic MPC, which presents extra challenges due to the randomness of the controller's actions. In these scenarios, performance outcomes present noise, which is not homogeneous across the domain of possible hyper-parameter settings, but which varies in an input-dependent way. To address these issues, we propose a Bayesian optimisation framework that accounts for heteroscedastic noise to tune hyper-parameters in control problems. Empirical results on benchmark continuous control tasks and a physical robot support the proposed framework's suitability relative to baselines, which do not take heteroscedasticity into account.",cs.RO,Robotics
A General Framework for Charger Scheduling Optimization Problems,"This paper presents a general framework to tackle a diverse range of NP-hard charger scheduling problems, optimizing the trajectory of mobile chargers to prolong the life of Wireless Rechargeable Sensor Network (WRSN), a system consisting of sensors with rechargeable batteries and mobile chargers. Existing solutions to charger scheduling problems require problem-specific design and a trade-off between the solution quality and computing time. Instead, we observe that instances of the same type of charger scheduling problem are solved repeatedly with similar combinatorial structure but different data. We consider searching an optimal charger scheduling as a trial and error process, and the objective function of a charging optimization problem as reward, a scalar feedback signal for each search. We propose a deep reinforcement learning-based charger scheduling optimization framework. The biggest advantage of the framework is that a diverse range of domain-specific charger scheduling strategy can be learned automatically from previous experiences. A framework also simplifies the complexity of algorithm design for individual charger scheduling optimization problem. We pick three representative charger scheduling optimization problems, design algorithms based on the proposed deep reinforcement learning framework, implement them, and compare them with existing ones. Extensive simulation results show that our algorithms based on the proposed framework outperform all existing ones.",cs.RO,Robotics
Computing Systems for Autonomous Driving: State-of-the-Art and Challenges,"The recent proliferation of computing technologies (e.g., sensors, computer vision, machine learning, and hardware acceleration), and the broad deployment of communication mechanisms (e.g., DSRC, C-V2X, 5G) have pushed the horizon of autonomous driving, which automates the decision and control of vehicles by leveraging the perception results based on multiple sensors. The key to the success of these autonomous systems is making a reliable decision in real-time fashion. However, accidents and fatalities caused by early deployed autonomous vehicles arise from time to time. The real traffic environment is too complicated for current autonomous driving computing systems to understand and handle. In this paper, we present state-of-the-art computing systems for autonomous driving, including seven performance metrics and nine key technologies, followed by twelve challenges to realize autonomous driving. We hope this paper will gain attention from both the computing and automotive communities and inspire more research in this direction.",cs.RO,Robotics
Continuous close-range 3D object pose estimation,"In the context of future manufacturing lines, removing fixtures will be a fundamental step to increase the flexibility of autonomous systems in assembly and logistic operations. Vision-based 3D pose estimation is a necessity to accurately handle objects that might not be placed at fixed positions during the robot task execution. Industrial tasks bring multiple challenges for the robust pose estimation of objects such as difficult object properties, tight cycle times and constraints on camera views. In particular, when interacting with objects, we have to work with close-range partial views of objects that pose a new challenge for typical view-based pose estimation methods. In this paper, we present a 3D pose estimation method based on a gradient-ascend particle filter that integrates new observations on-the-fly to improve the pose estimate. Thereby, we can apply this method online during task execution to save valuable cycle time. In contrast to other view-based pose estimation methods, we model potential views in full 6- dimensional space that allows us to cope with close-range partial objects views. We demonstrate the approach on a real assembly task, in which the algorithm usually converges to the correct pose within 10-15 iterations with an average accuracy of less than 8mm.",cs.RO,Robotics
Modeling and Testing Multi-Agent Traffic Rules within Interactive Behavior Planning,"Autonomous vehicles need to abide by the same rules that humans follow. Some of these traffic rules may depend on multiple agents or time. Especially in situations with traffic participants that interact densely, the interactions with other agents need to be accounted for during planning. To study how multi-agent and time-dependent traffic rules shall be modeled, a framework is needed that restricts the behavior to rule-conformant actions during planning, and that can eventually evaluate the satisfaction of these rules. This work presents a method to model the conformance to traffic rules for interactive behavior planning and to test the ramifications of the traffic rule formulations on metrics such as collision, progress, or rule violations. The interactive behavior planning problem is formulated as a dynamic game and solved using Monte Carlo Tree Search, for which we contribute a new method to integrate history-dependent traffic rules into a decision tree. To study the effect of the rules, we treat it as a multi-objective problem and apply a relaxed lexicographical ordering to the vectorized rewards. We demonstrate our approach in a merging scenario. We evaluate the effect of modeling and combining traffic rules to the eventual compliance in simulation. We show that with our approach, interactive behavior planning while satisfying even complex traffic rules can be achieved. Moving forward, this gives us a generic framework to formalize traffic rules for autonomous vehicles.",cs.RO,Robotics
Modeling and Validation of Soft Robotic Snake Locomotion,Snakes are a remarkable evolutionary success story. Many snake-inspired robots have been proposed over the years. Soft robotic snakes (SRS) with their continuous and smooth bending capability better mimic their biological counterparts' unique characteristics. Prior SRSs are limited to planar operation with a limited number of planar gaits. We propose a novel SRS with spatial bending and investigate snake locomotion gaits beyond the capabilities of the state-of-the-art systems. We derive a complete floating-base kinematic model of the robot and use the model to derive jointspace trajectories for serpentine and inward/outward rolling locomotion gaits. The locomotion gaits for the proposed SRS are experimentally validated under varying frequency and amplitude of gait cycles. The results qualitatively and quantitatively validate the SRS ability to leverage spatial bending to achieve locomotion gaits not possible with current SRS.,cs.RO,Robotics
TNT: Target-driveN Trajectory Prediction,"Predicting the future behavior of moving agents is essential for real world applications. It is challenging as the intent of the agent and the corresponding behavior is unknown and intrinsically multimodal. Our key insight is that for prediction within a moderate time horizon, the future modes can be effectively captured by a set of target states. This leads to our target-driven trajectory prediction (TNT) framework. TNT has three stages which are trained end-to-end. It first predicts an agent's potential target states $T$ steps into the future, by encoding its interactions with the environment and the other agents. TNT then generates trajectory state sequences conditioned on targets. A final stage estimates trajectory likelihoods and a final compact set of trajectory predictions is selected. This is in contrast to previous work which models agent intents as latent variables, and relies on test-time sampling to generate diverse trajectories. We benchmark TNT on trajectory prediction of vehicles and pedestrians, where we outperform state-of-the-art on Argoverse Forecasting, INTERACTION, Stanford Drone and an in-house Pedestrian-at-Intersection dataset.",cs.RO,Robotics
Analysis of Social Robotic Navigation approaches: CNN Encoder and Incremental Learning as an alternative to Deep Reinforcement Learning,"Dealing with social tasks in robotic scenarios is difficult, as having humans in the learning loop is incompatible with most of the state-of-the-art machine learning algorithms. This is the case when exploring Incremental learning models, in particular the ones involving reinforcement learning. In this work, we discuss this problem and possible solutions by analysing a previous study on adaptive convolutional encoders for a social navigation task.",cs.RO,Robotics
Mobile Robot Path Planning in Static Environments using Particle Swarm Optimization,"Motion planning is a key element of robotics since it empowers a robot to navigate autonomously. Particle Swarm Optimization is a simple, yet a very powerful optimization technique which has been effectively used in many complex multi-dimensional optimization problems. This paper proposes a path planning algorithm based on particle swarm optimization for computing a shortest collision-free path for a mobile robot in environments populated with static convex obstacles. The proposed algorithm finds the optimal path by performing random sampling on grid lines generated between the robot start and goal positions. Functionality of the proposed algorithm is illustrated via simulation results for different scenarios.",cs.RO,Robotics
Unsupervised Feature Learning for Event Data: Direct vs Inverse Problem Formulation,"Event-based cameras record an asynchronous stream of per-pixel brightness changes. As such, they have numerous advantages over the standard frame-based cameras, including high temporal resolution, high dynamic range, and no motion blur. Due to the asynchronous nature, efficient learning of compact representation for event data is challenging. While it remains not explored the extent to which the spatial and temporal event ""information"" is useful for pattern recognition tasks. In this paper, we focus on single-layer architectures. We analyze the performance of two general problem formulations: the direct and the inverse, for unsupervised feature learning from local event data (local volumes of events described in space-time). We identify and show the main advantages of each approach. Theoretically, we analyze guarantees for an optimal solution, possibility for asynchronous, parallel parameter update, and the computational complexity. We present numerical experiments for object recognition. We evaluate the solution under the direct and the inverse problem and give a comparison with the state-of-the-art methods. Our empirical results highlight the advantages of both approaches for representation learning from event data. We show improvements of up to 9 % in the recognition accuracy compared to the state-of-the-art methods from the same class of methods.",cs.RO,Robotics
Data-Driven Distributed State Estimation and Behavior Modeling in Sensor Networks,"Nowadays, the prevalence of sensor networks has enabled tracking of the states of dynamic objects for a wide spectrum of applications from autonomous driving to environmental monitoring and urban planning. However, tracking real-world objects often faces two key challenges: First, due to the limitation of individual sensors, state estimation needs to be solved in a collaborative and distributed manner. Second, the objects' movement behavior is unknown, and needs to be learned using sensor observations. In this work, for the first time, we formally formulate the problem of simultaneous state estimation and behavior learning in a sensor network. We then propose a simple yet effective solution to this new problem by extending the Gaussian process-based Bayes filters (GP-BayesFilters) to an online, distributed setting. The effectiveness of the proposed method is evaluated on tracking objects with unknown movement behaviors using both synthetic data and data collected from a multi-robot platform.",cs.RO,Robotics
Behavioral Repertoires for Soft Tensegrity Robots,"Mobile soft robots offer compelling applications in fields ranging from urban search and rescue to planetary exploration. A critical challenge of soft robotic control is that the nonlinear dynamics imposed by soft materials often result in complex behaviors that are counterintuitive and hard to model or predict. As a consequence, most behaviors for mobile soft robots are discovered through empirical trial and error and hand-tuning. A second challenge is that soft materials are difficult to simulate with high fidelity -- leading to a significant reality gap when trying to discover or optimize new behaviors. In this work we employ a Quality Diversity Algorithm running model-free on a physical soft tensegrity robot that autonomously generates a behavioral repertoire with no a priori knowledge of the robot dynamics, and minimal human intervention. The resulting behavior repertoire displays a diversity of unique locomotive gaits useful for a variety of tasks. These results help provide a road map for increasing the behavioral capabilities of mobile soft robots through real-world automation.",cs.RO,Robotics
Evaluation of an indoor localization system for a mobile robot,"Although indoor localization has been a wide researched topic, obtained results may not fit the requirements that some domains need. Most approaches are not able to precisely localize a fast moving object even with a complex installation, which makes their implementation in the automated driving domain complicated. In this publication, common technologies were analyzed and a commercial product, called Marvelmind Indoor GPS, was chosen for our use case in which both ultrasound and radio frequency communications are used. The evaluation is given in a first moment on small indoor scenarios with static and moving objects. Further tests were done on wider areas, where the system is integrated within our Robotics Operating System (ROS)-based self-developed 'Smart PhysIcal Demonstration and evaluation Robot (SPIDER)' and the results of these outdoor tests are compared with the obtained localization by the installed GPS on the robot. Finally, the next steps to improve the results in further developments are discussed.",cs.RO,Robotics
Freetures: Localization in Signed Distance Function Maps,"Localization of a robotic system within a previously mapped environment is important for reducing estimation drift and for reusing previously built maps. Existing techniques for geometry-based localization have focused on the description of local surface geometry, usually using pointclouds as the underlying representation. We propose a system for geometry-based localization that extracts features directly from an implicit surface representation: the Signed Distance Function (SDF). The SDF varies continuously through space, which allows the proposed system to extract and utilize features describing both surfaces and free-space. Through evaluations on public datasets, we demonstrate the flexibility of this approach, and show an increase in localization performance over state-of-the-art handcrafted surfaces-only descriptors. We achieve an average improvement of ~12% on an RGB-D dataset and ~18% on a LiDAR-based dataset. Finally, we demonstrate our system for localizing a LiDAR-equipped MAV within a previously built map of a search and rescue training ground.",cs.RO,Robotics
SMAC: Symbiotic Multi-Agent Construction,"We present a novel concept of a heterogeneous, distributed platform for autonomous 3D construction. The platform is composed of two types of robots acting in a coordinated and complementary fashion: (i) A collection of communicating smart construction blocks behaving as a form of growable smart matter, and capable of planning and monitoring their own state and the construction progress; and (ii) A team of inchworm-shaped builder robots designed to navigate and modify the 3D structure, following the guidance of the smart blocks. We describe the design of the hardware and introduce algorithms for navigation and construction that support a wide class of 3D structures. We demonstrate the capabilities of our concept and characterize its performance through simulations and real-robot experiments.",cs.RO,Robotics
Social-VRNN: One-Shot Multi-modal Trajectory Prediction for Interacting Pedestrians,"Prediction of human motions is key for safe navigation of autonomous robots among humans. In cluttered environments, several motion hypotheses may exist for a pedestrian, due to its interactions with the environment and other pedestrians.
  Previous works for estimating multiple motion hypotheses require a large number of samples which limits their applicability in real-time motion planning. In this paper, we present a variational learning approach for interaction-aware and multi-modal trajectory prediction based on deep generative neural networks.
  Our approach can achieve faster convergence and requires significantly fewer samples comparing to state-of-the-art methods. Experimental results on real and simulation data show that our model can effectively learn to infer different trajectories. We compare our method with three baseline approaches and present performance results demonstrating that our generative model can achieve higher accuracy for trajectory prediction by producing diverse trajectories.",cs.RO,Robotics
A Systematic Approach to Computing the Manipulator Jacobian and Hessian using the Elementary Transform Sequence,"The elementary transform sequence (ETS) provides a universal method of describing the kinematics of any serial-link manipulator. The ETS notation is intuitive and easy to understand, while avoiding the complexity and limitations of Denvit-Hartenberg frame assignment. In this paper, we describe a systematic method for computing the manipulator Jacobian and Hessian (differential kinematics) using the ETS notation. Differential kinematics have many applications including numerical inverse kinematics, resolved-rate motion control and manipulability motion control. Furthermore, we provide an open-source Python library which implements our algorithm and can be interfaced with any serial-link manipulator (available at github.com/petercorke/robotics-toolbox-python).",cs.RO,Robotics
"Piecewise-Linear Motion Planning amidst Static, Moving, or Morphing Obstacles","We propose a novel method for planning shortest length piecewise-linear motions through complex environments punctured with static, moving, or even morphing obstacles. Using a moment optimization approach, we formulate a hierarchy of semidefinite programs that yield increasingly refined lower bounds converging monotonically to the optimal path length.
  For computational tractability, our global moment optimization approach motivates an iterative motion planner that outperforms competing sampling-based and nonlinear optimization baselines. Our method natively handles continuous time constraints without any need for time discretization, and has the potential to scale better with dimensions compared to popular sampling-based methods.",cs.RO,Robotics
ORangE: Operational Range Estimation for Mobile Robot Exploration on a Single Discharge Cycle,"This paper presents an approach for estimating the operational range for mobile robot exploration on a single battery discharge. Deploying robots in the wild usually requires uninterrupted energy sources to maintain the robot's mobility throughout the entire mission. However, for most endeavors into the unknown environments, recharging is usually not an option, due to the lack of pre-installed recharging stations or other mission constraints. In these cases, the ability to model the on-board energy consumption and estimate the operational range is crucial to prevent running out of battery in the wild. To this end, this work describes our recent findings that quantitatively break down the robot's on-board energy consumption and predict the operational range to guarantee safe mission completion on a single battery discharge cycle. Two range estimators with different levels of generality and model fidelity are presented, whose performances were validated on physical robot platforms in both indoor and outdoor environments. Model performance metrics are also presented as benchmarks.",cs.RO,Robotics
Wasserstein Distance guided Adversarial Imitation Learning with Reward Shape Exploration,"The generative adversarial imitation learning (GAIL) has provided an adversarial learning framework for imitating expert policy from demonstrations in high-dimensional continuous tasks. However, almost all GAIL and its extensions only design a kind of reward function of logarithmic form in the adversarial training strategy with the Jensen-Shannon (JS) divergence for all complex environments. The fixed logarithmic type of reward function may be difficult to solve all complex tasks, and the vanishing gradients problem caused by the JS divergence will harm the adversarial learning process. In this paper, we propose a new algorithm named Wasserstein Distance guided Adversarial Imitation Learning (WDAIL) for promoting the performance of imitation learning (IL). There are three improvements in our method: (a) introducing the Wasserstein distance to obtain more appropriate measure in the adversarial training process, (b) using proximal policy optimization (PPO) in the reinforcement learning stage which is much simpler to implement and makes the algorithm more efficient, and (c) exploring different reward function shapes to suit different tasks for improving the performance. The experiment results show that the learning procedure remains remarkably stable, and achieves significant performance in the complex continuous control tasks of MuJoCo.",cs.RO,Robotics
DeepMapping: Unsupervised Map Estimation From Multiple Point Clouds,"We propose DeepMapping, a novel registration framework using deep neural networks (DNNs) as auxiliary functions to align multiple point clouds from scratch to a globally consistent frame. We use DNNs to model the highly non-convex mapping process that traditionally involves hand-crafted data association, sensor pose initialization, and global refinement. Our key novelty is that ""training"" these DNNs with properly defined unsupervised losses is equivalent to solving the underlying registration problem, but less sensitive to good initialization than ICP. Our framework contains two DNNs: a localization network that estimates the poses for input point clouds, and a map network that models the scene structure by estimating the occupancy status of global coordinates. This allows us to convert the registration problem to a binary occupancy classification, which can be solved efficiently using gradient-based optimization. We further show that DeepMapping can be readily extended to address the problem of Lidar SLAM by imposing geometric constraints between consecutive point clouds. Experiments are conducted on both simulated and real datasets. Qualitative and quantitative comparisons demonstrate that DeepMapping often enables more robust and accurate global registration of multiple point clouds than existing techniques. Our code is available at https://ai4ce.github.io/DeepMapping/.",cs.RO,Robotics
Haptic Sketches on the Arm for Manipulation in Virtual Reality,"We propose a haptic system that applies forces or skin deformation to the user's arm, rather than at the fingertips, for believable interaction with virtual objects as an alternative to complex thimble devices. Such a haptic system would be able to convey information to the arm instead of the fingertips, even though the user manipulates virtual objects using their hands. We developed a set of haptic sketches to determine which directions of skin deformation are deemed more believable during a grasp and lift task. Subjective reports indicate that normal forces were the most believable feedback to represent this interaction.",cs.RO,Robotics
Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation,"Vision-based robotics often separates the control loop into one module for perception and a separate module for control. It is possible to train the whole system end-to-end (e.g. with deep RL), but doing it ""from scratch"" comes with a high sample complexity cost and the final result is often brittle, failing unexpectedly if the test environment differs from that of training.
  We study the effects of using mid-level visual representations (features learned asynchronously for traditional computer vision objectives), as a generic and easy-to-decode perceptual state in an end-to-end RL framework. Mid-level representations encode invariances about the world, and we show that they aid generalization, improve sample complexity, and lead to a higher final performance. Compared to other approaches for incorporating invariances, such as domain randomization, asynchronously trained mid-level representations scale better: both to harder problems and to larger domain shifts. In practice, this means that mid-level representations could be used to successfully train policies for tasks where domain randomization and learning-from-scratch failed. We report results on both manipulation and navigation tasks, and for navigation include zero-shot sim-to-real experiments on real robots.",cs.RO,Robotics
3-D Motion Capture of an Unmodified Drone with Single-chip Millimeter Wave Radar,"Accurate motion capture of aerial robots in 3-D is a key enabler for autonomous operation in indoor environments such as warehouses or factories, as well as driving forward research in these areas. The most commonly used solutions at present are optical motion capture (e.g. VICON) and Ultrawideband (UWB), but these are costly and cumbersome to deploy, due to their requirement of multiple cameras/sensors spaced around the tracking area. They also require the drone to be modified to carry an active or passive marker. In this work, we present an inexpensive system that can be rapidly installed, based on single-chip millimeter wave (mmWave) radar. Importantly, the drone does not need to be modified or equipped with any markers, as we exploit the Doppler signals from the rotating propellers. Furthermore, 3-D tracking is possible from a single point, greatly simplifying deployment. We develop a novel deep neural network and demonstrate decimeter level 3-D tracking at 10Hz, achieving better performance than classical baselines. Our hope is that this low-cost system will act to catalyse inexpensive drone research and increased autonomy.",cs.RO,Robotics
Learning Object Manipulation Skills via Approximate State Estimation from Real Videos,"Humans are adept at learning new tasks by watching a few instructional videos. On the other hand, robots that learn new actions either require a lot of effort through trial and error, or use expert demonstrations that are challenging to obtain. In this paper, we explore a method that facilitates learning object manipulation skills directly from videos. Leveraging recent advances in 2D visual recognition and differentiable rendering, we develop an optimization based method to estimate a coarse 3D state representation for the hand and the manipulated object(s) without requiring any supervision. We use these trajectories as dense rewards for an agent that learns to mimic them through reinforcement learning. We evaluate our method on simple single- and two-object actions from the Something-Something dataset. Our approach allows an agent to learn actions from single videos, while watching multiple demonstrations makes the policy more robust. We show that policies learned in a simulated environment can be easily transferred to a real robot.",cs.RO,Robotics
Foundations of the Socio-physical Model of Activities (SOMA) for Autonomous Robotic Agents,"In this paper, we present foundations of the Socio-physical Model of Activities (SOMA). SOMA represents both the physical as well as the social context of everyday activities. Such tasks seem to be trivial for humans, however, they pose severe problems for artificial agents. For starters, a natural language command requesting something will leave many pieces of information necessary for performing the task unspecified. Humans can solve such problems fast as we reduce the search space by recourse to prior knowledge such as a connected collection of plans that describe how certain goals can be achieved at various levels of abstraction. Rather than enumerating fine-grained physical contexts SOMA sets out to include socially constructed knowledge about the functions of actions to achieve a variety of goals or the roles objects can play in a given situation. As the human cognition system is capable of generalizing experiences into abstract knowledge pieces applicable to novel situations, we argue that both physical and social context need be modeled to tackle these challenges in a general manner. This is represented by the link between the physical and social context in SOMA where relationships are established between occurrences and generalizations of them, which has been demonstrated in several use cases that validate SOMA.",cs.RO,Robotics
MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry and Eye-Gaze,"As robots become more present in open human environments, it will become crucial for robotic systems to understand and predict human motion. Such capabilities depend heavily on the quality and availability of motion capture data. However, existing datasets of full-body motion rarely include 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze, which are all important when a robot needs to predict the movements of humans in close proximity. Hence, in this paper, we present a novel dataset of full-body motion for everyday manipulation tasks, which includes the above. The motion data was captured using a traditional motion capture system based on reflective markers. We additionally captured eye-gaze using a wearable pupil-tracking device. As we show in experiments, the dataset can be used for the design and evaluation of full-body motion prediction algorithms. Furthermore, our experiments show eye-gaze as a powerful predictor of human intent. The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed. It is available at https://humans-to-robots-motion.github.io/mogaze and is planned to be extended to collaborative tasks with two humans in the near future.",cs.RO,Robotics
Cost-to-Go Function Generating Networks for High Dimensional Motion Planning,"This paper presents c2g-HOF networks which learn to generate cost-to-go functions for manipulator motion planning. The c2g-HOF architecture consists of a cost-to-go function over the configuration space represented as a neural network (c2g-network) as well as a Higher Order Function (HOF) network which outputs the weights of the c2g-network for a given input workspace. Both networks are trained end-to-end in a supervised fashion using costs computed from traditional motion planners. Once trained, c2g-HOF can generate a smooth and continuous cost-to-go function directly from workspace sensor inputs (represented as a point cloud in 3D or an image in 2D). At inference time, the weights of the c2g-network are computed very efficiently and near-optimal trajectories are generated by simply following the gradient of the cost-to-go function. We compare c2g-HOF with traditional planning algorithms for various robots and planning scenarios. The experimental results indicate that planning with c2g-HOF is significantly faster than other motion planning algorithms, resulting in orders of magnitude improvement when including collision checking. Furthermore, despite being trained from sparsely sampled trajectories in configuration space, c2g-HOF generalizes to generate smoother, and often lower cost, trajectories. We demonstrate cost-to-go based planning on a 7 DoF manipulator arm where motion planning in a complex workspace requires only 0.13 seconds for the entire trajectory.",cs.RO,Robotics
R-AGNO-RPN: A LIDAR-Camera Region Deep Network for Resolution-Agnostic Detection,"Current neural networks-based object detection approaches processing LiDAR point clouds are generally trained from one kind of LiDAR sensors. However, their performances decrease when they are tested with data coming from a different LiDAR sensor than the one used for training, i.e., with a different point cloud resolution. In this paper, R-AGNO-RPN, a region proposal network built on fusion of 3D point clouds and RGB images is proposed for 3D object detection regardless of point cloud resolution. As our approach is designed to be also applied on low point cloud resolutions, the proposed method focuses on object localization instead of estimating refined boxes on reduced data. The resilience to low-resolution point cloud is obtained through image features accurately mapped to Bird's Eye View and a specific data augmentation procedure that improves the contribution of the RGB images. To show the proposed network's ability to deal with different point clouds resolutions, experiments are conducted on both data coming from the KITTI 3D Object Detection and the nuScenes datasets. In addition, to assess its performances, our method is compared to PointPillars, a well-known 3D detection network. Experimental results show that even on point cloud data reduced by $80\%$ of its original points, our method is still able to deliver relevant proposals localization.",cs.RO,Robotics
Wi-Fi Based Indoor Positioning System For Mobile Robots By Using Particle Filter,"Mobile robots have the capability to work in real-time autonomously. Autonomous behavior is strictly dependent on knowing the position of the mobile robot. The positioning of a mobile robot in an indoor area is a difficult task for only one sensor information is used. We proposed a system and method to locate the mobile robot via fusing signals from WIFI and odometer data via particle filter. In this study, the Particle filter is a well-known filter that is used for indoor positioning of mobile robots. The proposed system includes two parts that are RFKON system and evarobot for data collection and experiments. The Received Signal Strength (RSS) measurements of the WiFi access points that are located in any environment are used to locate a stationary mobile robot in one floor area via SIS Particle Filter. RSS measurements from the RFKON database are used and the average location error is 0.7606 and 0.1495 m for 300 and 1000 particles respectively.",cs.RO,Robotics
Efficient Online Trajectory Planning for Integrator Chain Dynamics using Polynomial Elimination,"Providing smooth reference trajectories can effectively increase performance and accuracy of tracking control applications while overshoot and unwanted vibrations are reduced. Trajectory planning computations can often be simplified significantly by transforming the system dynamics into decoupled integrator chains using methods such as feedback linearization, differential flatness or the controller canonical form. We present an efficient method to plan time optimal trajectories for integrator chains subject to derivative bound constraints. Therefore, an algebraic precomputation algorithm formulates the necessary conditions for time optimality in form of a set of polynomial systems, followed by a symbolic polynomial elimination using Grbner bases. A fast online algorithm then plans the trajectories by calculating the roots of the decomposed polynomial systems. These roots describe the switching time instants of the input signal and the full trajectory simply follows by multiple integration. This method presents a systematic way to compute time optimal trajectories exactly via algebraic calculations without numerical approximation iterations. It is applied to various trajectory types with different continuity order, asymmetric derivative bounds and non-rest initial and final states.",cs.RO,Robotics
Uniform Scattering of Robots on Alternate Nodes of a Grid,"In this paper, we propose a distributed algorithm to uniformly scatter the robots along a grid, with robots on alternate nodes of this grid distribution. These homogeneous, autonomous mobile robots place themselves equidistant apart on the grid, which can be required for guarding or covering a geographical area by the robots. The robots operate by executing cycles of the states ""look-compute-move"". In the look phase, it looks to see the position of the other robots; in the compute phase, it computes a destination to move to; and then in the move phase, it moves to that computed destination. They do not interact by message passing and can recollect neither the past actions nor the looked data from the previous cycle, i.e., oblivious. The robots are semi-synchronous, anonymous and have unlimited visibility. Eventually, the robots uniformly distribute themselves on alternate nodes of a grid, leaving the adjacent nodes of the grid vacant. The algorithm presented also assures no collision or deadlock among the robots.",cs.RO,Robotics
Deliberative and Conceptual Inference in Service Robots,"Service robots need to reason to support people in daily life situations. Reasoning is an expensive resource that should be used on demand whenever the expectations of the robot do not match the situation of the world and the execution of the task is broken down; in such scenarios the robot must perform the common sense daily life inference cycle consisting on diagnosing what happened, deciding what to do about it, and inducing and executing a plan, recurring in such behavior until the service task can be resumed. Here we examine two strategies to implement this cycle: (1) a pipe-line strategy involving abduction, decision-making and planning, which we call deliberative inference and (2) the use of the knowledge and preferences stored in the robot's knowledge-base, which we call conceptual inference. The former involves an explicit definition of a problem space that is explored through heuristic search, and the latter is based on conceptual knowledge including the human user preferences, and its representation requires a non-monotonic knowledge-based system. We compare the strengths and limitations of both approaches. We also describe a service robot conceptual model and architecture capable of supporting the daily life inference cycle during the execution of a robotics service task. The model is centered in the declarative specification and interpretation of robot's communication and task structure. We also show the implementation of this framework in the fully autonomous robot Golem-III. The framework is illustrated with two demonstration scenarios.",cs.RO,Robotics
Decision-Time Postponing Motion Planning for Combinatorial Uncertain Maneuvering,"Motion planning involves decision making among combinatorial maneuver variants in urban driving. A planner must consider uncertainties and associated risks of the maneuver variants, and subsequently select a maneuver alternative. In this paper we present a planning approach that considers the uncertainties in the prediction and, in case of high uncertainty, postpones the combinatorial decision making to a later time within the planning horizon. With our proposed approach, safe but at the same time not overconservative motion is planned.",cs.RO,Robotics
Crowd Vetting: Rejecting Adversaries via Collaboration--with Application to Multi-Robot Flocking,"We characterize the advantage of using a robot's neighborhood to find and eliminate adversarial robots in the presence of a Sybil attack. We show that by leveraging the opinions of its neighbors on the trustworthiness of transmitted data, robots can detect adversaries with high probability. We characterize a number of communication rounds required to achieve this result to be a function of the communication quality and the proportion of legitimate to malicious robots. This result enables increased resiliency of many multi-robot algorithms. Because our results are finite time and not asymptotic, they are particularly well-suited for problems with a time critical nature. We develop two algorithms, \emph{FindSpoofedRobots} that determines trusted neighbors with high probability, and \emph{FindResilientAdjacencyMatrix} that enables distributed computation of graph properties in an adversarial setting. We apply our methods to a flocking problem where a team of robots must track a moving target in the presence of adversarial robots. We show that by using our algorithms, the team of robots are able to maintain tracking ability of the dynamic target.",cs.RO,Robotics
Learning How to Trade-Off Safety with Agility Using Deep Covariance Estimation for Perception Driven UAV Motion Planning,"We investigate how to utilize predictive models for selecting appropriate motion planning strategies based on perception uncertainty estimation for agile unmanned aerial vehicle (UAV) navigation tasks. Although there are variety of motion planning and perception algorithms for such tasks, the impact of perception uncertainty is not explicitly handled in many of the current motion algorithms, which leads to performance loss in real-life scenarios where the measurement are often noisy due to external disturbances. We develop a novel framework for embedding perception uncertainty to high level motion planning management, in order to select the best available motion planning approach for the currently estimated perception uncertainty. We estimate the uncertainty in visual inputs using a deep neural network (CovNet) that explicitly predicts the covariance of the current measurements. Next, we train a high level machine learning model for predicting the lowest cost motion planning algorithm given the current estimate of covariance as well as the UAV states. We demonstrate on both real-life data and drone racing simulations that our approach, named uncertainty driven motion planning switcher (UDS) yields the safest and fastest trajectories among compared alternatives. Furthermore, we show that the developed approach learns how to trade-off safety with agility by switching to motion planners that leads to more agile trajectories when the estimated covariance is high and vice versa.",cs.RO,Robotics
Protective Policy Transfer,"Being able to transfer existing skills to new situations is a key capability when training robots to operate in unpredictable real-world environments. A successful transfer algorithm should not only minimize the number of samples that the robot needs to collect in the new environment, but also prevent the robot from damaging itself or the surrounding environment during the transfer process. In this work, we introduce a policy transfer algorithm for adapting robot motor skills to novel scenarios while minimizing serious failures. Our algorithm trains two control policies in the training environment: a task policy that is optimized to complete the task of interest, and a protective policy that is dedicated to keep the robot from unsafe events (e.g. falling to the ground). To decide which policy to use during execution, we learn a safety estimator model in the training environment that estimates a continuous safety level of the robot. When used with a set of thresholds, the safety estimator becomes a classifier for switching between the protective policy and the task policy. We evaluate our approach on four simulated robot locomotion problems and a 2D navigation problem and show that our method can achieve successful transfer to notably different environments while taking the robot's safety into consideration.",cs.RO,Robotics
Learning Multi-Arm Manipulation Through Collaborative Teleoperation,"Imitation Learning (IL) is a powerful paradigm to teach robots to perform manipulation tasks by allowing them to learn from human demonstrations collected via teleoperation, but has mostly been limited to single-arm manipulation. However, many real-world tasks require multiple arms, such as lifting a heavy object or assembling a desk. Unfortunately, applying IL to multi-arm manipulation tasks has been challenging -- asking a human to control more than one robotic arm can impose significant cognitive burden and is often only possible for a maximum of two robot arms. To address these challenges, we present Multi-Arm RoboTurk (MART), a multi-user data collection platform that allows multiple remote users to simultaneously teleoperate a set of robotic arms and collect demonstrations for multi-arm tasks. Using MART, we collected demonstrations for five novel two and three-arm tasks from several geographically separated users. From our data we arrived at a critical insight: most multi-arm tasks do not require global coordination throughout its full duration, but only during specific moments. We show that learning from such data consequently presents challenges for centralized agents that directly attempt to model all robot actions simultaneously, and perform a comprehensive study of different policy architectures with varying levels of centralization on our tasks. Finally, we propose and evaluate a base-residual policy framework that allows trained policies to better adapt to the mixed coordination setting common in multi-arm manipulation, and show that a centralized policy augmented with a decentralized residual model outperforms all other models on our set of benchmark tasks. Additional results and videos at https://roboturk.stanford.edu/multiarm .",cs.RO,Robotics
Development of Autonomous Quadcopter,"The main objective of this work is demonstrated through two main aspects. The first is the design of an adaptive neuro-fuzzy inference system (ANFIS) controller to develop the attitude and altitude of a quadcopter. The second is to establish the linearized mathematical model of the quadcopter in a simple and clear way. To show the effectiveness of the ANFIS approach, the performance of a well-trained ANFIS controller is compared to a classical proportional-derivative (PD) controller and a properly tuned fuzzy logic controller.",cs.RO,Robotics
Robotics Enabling the Workforce,"Robotics has the potential to magnify the skilled workforce of the nation by complementing our workforce with automation: teams of people and robots will be able to do more than either could alone. The economic engine of the U.S. runs on the productivity of our people. The rise of automation offers new opportunities to enhance the work of our citizens and drive the innovation and prosperity of our industries. Most critically, we need research to understand how future robot technologies can best complement our workforce to get the best of both human and automated labor in a collaborative team. Investments made in robotics research and workforce development will lead to increased GDP, an increased export-import ratio, a growing middle class of skilled workers, and a U.S.-based supply chain that can withstand global pandemics and other disruptions. In order to make the United States a leader in robotics, we need to invest in basic research, technology development, K-16 education, and lifelong learning.",cs.RO,Robotics
An exact solution in Markov decision process with multiplicative rewards as a general framework,"We develop an exactly solvable framework of Markov decision process with a finite horizon, and continuous state and action spaces. We first review the exact solution of conventional linear quadratic regulation with a linear transition and a Gaussian noise, whose optimal policy does not depend on the Gaussian noise, which is an undesired feature in the presence of significant noises. It motivates us to investigate exact solutions which depend on noise. To do so, we generalize the reward accumulation to be a general binary commutative and associative operation. By a new multiplicative accumulation, we obtain an exact solution of optimization assuming linear transitions with a Gaussian noise and the optimal policy is noise dependent in contrast to the additive accumulation. Furthermore, we also show that the multiplicative scheme is a general framework that covers the additive one with an arbitrary precision, which is a model-independent principle.",cs.RO,Robotics
Trajectory Planning Under Stochastic and Bounded Sensing Uncertainties Using Reachability Analysis,"Trajectory planning under uncertainty is an active research topic. Previous works predict state and state estimation uncertainties along trajectories to check for collision safety. They assume either stochastic or bounded sensing uncertainties. However, GNSS pseudoranges are typically modeled to contain stochastic uncertainties with additional biases in urban environments. Thus, given bounds for the bias, the planner needs to account for both stochastic and bounded sensing uncertainties. In our prior work we presented a reachability analysis to predict state and state estimation uncertainties under stochastic and bounded uncertainties. However, we ignored the correlation between these uncertainties, leading to an imperfect approximation of the state uncertainty. In this paper we improve our reachability analysis by predicting state uncertainty as a function of independent quantities. We design a metric for the predicted uncertainty to compare candidate trajectories during planning. Finally, we validate the planner for GNSS-based urban navigation of fixed-wing UAS.",cs.RO,Robotics
Collaborative Augmented Reality on Smartphones via Life-long City-scale Maps,"In this paper we present the first published end-to-end production computer-vision system for powering city-scale shared augmented reality experiences on mobile devices. In doing so we propose a new formulation for an experience-based mapping framework as an effective solution to the key issues of city-scale SLAM scalability, robustness, map updates and all-time all-weather performance required by a production system. Furthermore, we propose an effective way of synchronising SLAM systems to deliver seamless real-time localisation of multiple edge devices at the same time. All this in the presence of network latency and bandwidth limitations. The resulting system is deployed and tested at scale in San Francisco where it delivers AR experiences in a mapped area of several hundred kilometers. To foster further development of this area we offer the data set to the public, constituting the largest of this kind to date.",cs.RO,Robotics
STReSSD: Sim-To-Real from Sound for Stochastic Dynamics,"Sound is an information-rich medium that captures dynamic physical events. This work presents STReSSD, a framework that uses sound to bridge the simulation-to-reality gap for stochastic dynamics, demonstrated for the canonical case of a bouncing ball. A physically-motivated noise model is presented to capture stochastic behavior of the balls upon collision with the environment. A likelihood-free Bayesian inference framework is used to infer the parameters of the noise model, as well as a material property called the coefficient of restitution, from audio observations. The same inference framework and the calibrated stochastic simulator are then used to learn a probabilistic model of ball dynamics. The predictive capabilities of the dynamics model are tested in two robotic experiments. First, open-loop predictions anticipate probabilistic success of bouncing a ball into a cup. The second experiment integrates audio perception with a robotic arm to track and deflect a bouncing ball in real-time. We envision that this work is a step towards integrating audio-based inference for dynamic robotic tasks. Experimental results can be viewed at https://youtu.be/b7pOrgZrArk.",cs.RO,Robotics
Task-relevant Representation Learning for Networked Robotic Perception,"Today, even the most compute-and-power constrained robots can measure complex, high data-rate video and LIDAR sensory streams. Often, such robots, ranging from low-power drones to space and subterranean rovers, need to transmit high-bitrate sensory data to a remote compute server if they are uncertain or cannot scalably run complex perception or mapping tasks locally. However, today's representations for sensory data are mostly designed for human, not robotic, perception and thus often waste precious compute or wireless network resources to transmit unimportant parts of a scene that are unnecessary for a high-level robotic task. This paper presents an algorithm to learn task-relevant representations of sensory data that are co-designed with a pre-trained robotic perception model's ultimate objective. Our algorithm aggressively compresses robotic sensory data by up to 11x more than competing methods. Further, it achieves high accuracy and robust generalization on diverse tasks including Mars terrain classification with low-power deep learning accelerators, neural motion planning, and environmental timeseries classification.",cs.RO,Robotics
Capture Steps: Robust Walking for Humanoid Robots,"Stable bipedal walking is a key prerequisite for humanoid robots to reach their potential of being versatile helpers in our everyday environments. Bipedal walking is, however, a complex motion that requires the coordination of many degrees of freedom while it is also inherently unstable and sensitive to disturbances. The balance of a walking biped has to be constantly maintained. The most effective way of controlling balance are well timed and placed recovery steps -- capture steps -- that absorb the expense momentum gained from a push or a stumble. We present a bipedal gait generation framework that utilizes step timing and foot placement techniques in order to recover the balance of a biped even after strong disturbances. Our framework modifies the next footstep location instantly when responding to a disturbance and generates controllable omnidirectional walking using only very little sensing and computational power. We exploit the open-loop stability of a central pattern generated gait to fit a linear inverted pendulum model to the observed center of mass trajectory. Then, we use the fitted model to predict suitable footstep locations and timings in order to maintain balance while following a target walking velocity. Our experiments show qualitative and statistical evidence of one of the strongest push-recovery capabilities among humanoid robots to date.",cs.RO,Robotics
Learning a Centroidal Motion Planner for Legged Locomotion,"Whole-body optimizers have been successful at automatically computing complex dynamic locomotion behaviors. However they are often limited to offline planning as they are computationally too expensive to replan with a high frequency. Simpler models are then typically used for online replanning. In this paper we present a method to generate whole body movements in real-time for locomotion tasks. Our approach consists in learning a centroidal neural network that predicts the desired centroidal motion given the current state of the robot and a desired contact plan. The network is trained using an existing whole body motion optimizer. Our approach enables to learn with few training samples dynamic motions that can be used in a complete whole-body control framework at high frequency, which is usually not attainable with typical full-body optimizers. We demonstrate our method to generate a rich set of walking and jumping motions on a real quadruped robot.",cs.RO,Robotics
Neural fidelity warping for efficient robot morphology design,"We consider the problem of optimizing a robot morphology to achieve the best performance for a target task, under computational resource limitations. The evaluation process for each morphological design involves learning a controller for the design, which can consume substantial time and computational resources. To address the challenge of expensive robot morphology evaluation, we present a continuous multi-fidelity Bayesian Optimization framework that efficiently utilizes computational resources via low-fidelity evaluations. We identify the problem of non-stationarity over fidelity space. Our proposed fidelity warping mechanism can learn representations of learning epochs and tasks to model non-stationary covariances between continuous fidelity evaluations which prove challenging for off-the-shelf stationary kernels. Various experiments demonstrate that our method can utilize the low-fidelity evaluations to efficiently search for the optimal robot morphology, outperforming state-of-the-art methods.",cs.RO,Robotics
Privacy Interpretation of Behavioural-based Anomaly Detection Approaches,"This paper proposes the notion of 'Privacy-Anomaly Detection' and considers the question of whether behavioural-based anomaly detection approaches can have a privacy semantic interpretation and whether the detected anomalies can be related to the conventional (formal) definitions of privacy semantics such as k-anonymity. The idea is to learn the user's past querying behaviour in terms of privacy and then identifying deviations from past behaviour in order to detect privacy violations. Privacy attacks, violations of formal privacy definition, based on a sequence of SQL queries (query correlations) are also considered in the paper and it is shown that interactive querying settings are vulnerable to privacy attacks based on query sequences. Investigation on whether these types of privacy attacks can potentially manifest themselves as anomalies, specifically as privacy-anomalies was carried out. It is shown that in this paper that behavioural-based anomaly detection approaches have the potential to detect privacy attacks based on query sequences (violation of formal privacy definition) as privacy-anomalies.",cs.CR,Cybersecurity
Detecting Botnet Attacks in IoT Environments: An Optimized Machine Learning Approach,"The increased reliance on the Internet and the corresponding surge in connectivity demand has led to a significant growth in Internet-of-Things (IoT) devices. The continued deployment of IoT devices has in turn led to an increase in network attacks due to the larger number of potential attack surfaces as illustrated by the recent reports that IoT malware attacks increased by 215.7% from 10.3 million in 2017 to 32.7 million in 2018. This illustrates the increased vulnerability and susceptibility of IoT devices and networks. Therefore, there is a need for proper effective and efficient attack detection and mitigation techniques in such environments. Machine learning (ML) has emerged as one potential solution due to the abundance of data generated and available for IoT devices and networks. Hence, they have significant potential to be adopted for intrusion detection for IoT environments. To that end, this paper proposes an optimized ML-based framework consisting of a combination of Bayesian optimization Gaussian Process (BO-GP) algorithm and decision tree (DT) classification model to detect attacks on IoT devices in an effective and efficient manner. The performance of the proposed framework is evaluated using the Bot-IoT-2018 dataset. Experimental results show that the proposed optimized framework has a high detection accuracy, precision, recall, and F-score, highlighting its effectiveness and robustness for the detection of botnet attacks in IoT environments.",cs.CR,Cybersecurity
Effectiveness of SCADA System Security Used Within Critical Infrastructure,"Since the 1960s Supervisory Control and Data Acquisition (SCADA) systems have been used within industry. Referred to as critical infrastructure (CI), key installations such as power stations, water treatment and energy grids are controlled using SCADA. Existing literature reveals inherent security risks to CI and suggests this stems from the rise of interconnected networks, leading to the hypothesis that the rise of interconnectivity between corporate networks and SCADA system networks pose security risks to CI. The results from studies into previous global attacks involving SCADA and CI, with focus on two highly serious incidents in Iran and Ukraine, reveal that although interconnectivity is a major factor, isolated CIs are still highly vulnerable to attack due to risks within the SCADA controllers and protocols.",cs.CR,Cybersecurity
Privacy Enhanced DigiLocker using Ciphertext-Policy Attribute-Based Encryption,"Recently, Government of India has taken several initiatives to make India digitally strong such as to provide each resident a unique digital identity, referred to as Aadhaar, and to provide several online e-Governance services based on Aadhaar such as DigiLocker. DigiLocker is an online service which provides a shareable private storage space on public cloud to its subscribers. Although DigiLocker ensures traditional security such as data integrity and secure data access, privacy of e-documents are yet to addressed. Ciphertext-Policy Attribute-Based Encryption (CP-ABE) can improve data privacy but the right implementation of it has always been a challenge. This paper presents a scheme to implement privacy enhanced DigiLocker using CP-ABE.",cs.CR,Cybersecurity
General Domain Adaptation Through Proportional Progressive Pseudo Labeling,"Domain adaptation helps transfer the knowledge gained from a labeled source domain to an unlabeled target domain. During the past few years, different domain adaptation techniques have been published. One common flaw of these approaches is that while they might work well on one input type, such as images, their performance drops when applied to others, such as text or time-series. In this paper, we introduce Proportional Progressive Pseudo Labeling (PPPL), a simple, yet effective technique that can be implemented in a few lines of code to build a more general domain adaptation technique that can be applied on several different input types. At the beginning of the training phase, PPPL progressively reduces target domain classification error, by training the model directly with pseudo-labeled target domain samples, while excluding samples with more likely wrong pseudo-labels from the training set and also postponing training on such samples. Experiments on 6 different datasets that include tasks such as anomaly detection, text sentiment analysis and image classification demonstrate that PPPL can beat other baselines and generalize better.",cs.CR,Cybersecurity
Fuzzy Commitments Offer Insufficient Protection to Biometric Templates Produced by Deep Learning,"In this work, we study the protection that fuzzy commitments offer when they are applied to facial images, processed by the state of the art deep learning facial recognition systems. We show that while these systems are capable of producing great accuracy, they produce templates of too little entropy. As a result, we present a reconstruction attack that takes a protected template, and reconstructs a facial image. The reconstructed facial images greatly resemble the original ones. In the simplest attack scenario, more than 78% of these reconstructed templates succeed in unlocking an account (when the system is configured to 0.1% FAR). Even in the ""hardest"" settings (in which we take a reconstructed image from one system and use it in a different system, with different feature extraction process) the reconstructed image offers 50 to 120 times higher success rates than the system's FAR.",cs.CR,Cybersecurity
Neural Network Training With Homomorphic Encryption,"We introduce a novel method and implementation architecture to train neural networks which preserves the confidentiality of both the model and the data. Our method relies on homomorphic capability of lattice based encryption scheme. Our procedure is optimized for operations on packed ciphertexts in order to achieve efficient updates of the model parameters. Our method achieves a significant reduction of computations due to our way to perform multiplications and rotations on packed ciphertexts from a feedforward network to a back-propagation network. To verify the accuracy of the training model as well as the implementation feasibility, we tested our method on the Iris data set by using the CKKS scheme with Microsoft SEAL as a back end. Although our test implementation is for simple neural network training, we believe our basic implementation block can help the further applications for more complex neural network based use cases.",cs.CR,Cybersecurity
Implementation of Security Systems for Detection and Prevention of Data Loss/Leakage at Organization via Traffic Inspection,"Data Loss/Leakage Prevention (DLP) continues to be the main issue for many large organizations. There are multiple numbers of emerging security attach scenarios and a limitless number of overcoming solutions. Today's enterprises' major concern is to protect confidential information because a leakage that compromises confidential data means that sensitive information is in competitors' hands. Different data types need to be protected. However, our research is focused only on data in motion (DIM) i-e data transferred through the network. The research and scenarios in this paper demonstrate a recent survey on information and data leakage incidents, which reveals its importance and also proposed a model solution that will offer the combination of previous methodologies with a new way of pattern matching by advanced content checker based on the use of machine learning to protect data within an organization and then take actions accordingly. This paper also proposed a DLP deployment design on the gateway level that shows how data is moving through intermediate channels before reaching the final destination using the squid proxy server and ICAP server.",cs.CR,Cybersecurity
Towards Assessing Critical Infrastructures Cyber-Security Culture During Covid-19 Crisis: A Tailor-Made Survey,"This paper outlines the design and development of a survey targeting the cyber-security culture assessment of critical infrastructures during the COVID-19 crisis, when living routine was seriously disturbed and working reality fundamentally affected. Its foundations lie on a security culture framework consisted of 10 different security dimensions analysed into 52 domains examined under two different pillars: organizational and individual. In this paper, a detailed questionnaire building analysis is being presented while revealing the aims, goals and expected outcomes of each question. It concludes with the survey implementation and delivery plan following a number of pre-survey stages each serving a specific methodological purpose.",cs.CR,Cybersecurity
Secure Hot Path Crowdsourcing with Local Differential Privacy under Fog Computing Architecture,"Crowdsourcing plays an essential role in the Internet of Things (IoT) for data collection, where a group of workers is equipped with Internet-connected geolocated devices to collect sensor data for marketing or research purpose. In this paper, we consider crowdsourcing these worker's hot travel path. Each worker is required to report his real-time location information, which is sensitive and has to be protected. Encryption-based methods are the most direct way to protect the location, but not suitable for resource-limited devices. Besides, local differential privacy is a strong privacy concept and has been deployed in many software systems. However, the local differential privacy technology needs a large number of participants to ensure the accuracy of the estimation, which is not always the case for crowdsourcing. To solve this problem, we proposed a trie-based iterative statistic method, which combines additive secret sharing and local differential privacy technologies. The proposed method has excellent performance even with a limited number of participants without the need of complex computation. Specifically, the proposed method contains three main components: iterative statistics, adaptive sampling, and secure reporting. We theoretically analyze the effectiveness of the proposed method and perform extensive experiments to show that the proposed method not only provides a strict privacy guarantee, but also significantly improves the performance from the previous existing solutions.",cs.CR,Cybersecurity
MAVSec: Securing the MAVLink Protocol for Ardupilot/PX4 Unmanned Aerial Systems,"The MAVLink is a lightweight communication protocol between Unmanned Aerial Vehicles (UAVs) and ground control stations (GCSs). It defines a set of bi-directional messages exchanged between a UAV (aka drone) and a ground station. The messages carry out information about the UAV's states and control commands sent from the ground station. However, the MAVLink protocol is not secure and has several vulnerabilities to different attacks that result in critical threats and safety concerns. Very few studies provided solutions to this problem. In this paper, we discuss the security vulnerabilities of the MAVLink protocol and propose MAVSec, a security-integrated mechanism for MAVLink that leverages the use of encryption algorithms to ensure the protection of exchanged MAVLink messages between UAVs and GCSs. To validate MAVSec, we implemented it in Ardupilot and evaluated the performance of different encryption algorithms (i.e. AES-CBC, AES-CTR, RC4, and ChaCha20) in terms of memory usage and CPU consumption. The experimental results show that ChaCha20 has a better performance and is more efficient than other encryption algorithms. Integrating ChaCha20 into MAVLink can guarantee its messages confidentiality, without affecting its performance, while occupying less memory and CPU consumption, thus, preserving memory and saving the battery for the resource-constrained drone.",cs.CR,Cybersecurity
Generative Reversible Data Hiding by Image to Image Translation via GANs,"The traditional reversible data hiding technique is based on cover image modification which inevitably leaves some traces of rewriting that can be more easily analyzed and attacked by the warder. Inspired by the cover synthesis steganography based generative adversarial networks, in this paper, a novel generative reversible data hiding scheme (GRDH) by image translation is proposed. First, an image generator is used to obtain a realistic image, which is used as an input to the image-to-image translation model with CycleGAN. After image translation, a stego image with different semantic information will be obtained. The secret message and the original input image can be recovered separately by a well-trained message extractor and the inverse transform of the image translation. Experimental results have verified the effectiveness of the scheme.",cs.CR,Cybersecurity
Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems,"Thanks to recent advances in deep neural networks (DNNs), face recognition systems have become highly accurate in classifying a large number of face images. However, recent studies have found that DNNs could be vulnerable to adversarial examples, raising concerns about the robustness of such systems. Adversarial examples that are not restricted to small perturbations could be more serious since conventional certified defenses might be ineffective against them. To shed light on the vulnerability to such adversarial examples, we propose a flexible and efficient method for generating unrestricted adversarial examples using image translation techniques. Our method enables us to translate a source image into any desired facial appearance with large perturbations to deceive target face recognition systems. Our experimental results indicate that our method achieved about $90$ and $80\%$ attack success rates under white- and black-box settings, respectively, and that the translated images are perceptually realistic and maintain the identifiability of the individual while the perturbations are large enough to bypass certified defenses.",cs.CR,Cybersecurity
ARCHANGEL: Tamper-proofing Video Archives using Temporal Content Hashes on the Blockchain,"We present ARCHANGEL; a novel distributed ledger based system for assuring the long-term integrity of digital video archives. First, we describe a novel deep network architecture for computing compact temporal content hashes (TCHs) from audio-visual streams with durations of minutes or hours. Our TCHs are sensitive to accidental or malicious content modification (tampering) but invariant to the codec used to encode the video. This is necessary due to the curatorial requirement for archives to format shift video over time to ensure future accessibility. Second, we describe how the TCHs (and the models used to derive them) are secured via a proof-of-authority blockchain distributed across multiple independent archives. We report on the efficacy of ARCHANGEL within the context of a trial deployment in which the national government archives of the United Kingdom, Estonia and Norway participated.",cs.CR,Cybersecurity
Authenticated Key-Value Stores with Hardware Enclaves,"Authenticated data storage on an untrusted platform is an important computing paradigm for cloud applications ranging from big-data outsourcing, to cryptocurrency and certificate transparency log. These modern applications increasingly feature update-intensive workloads, whereas existing authenticated data structures (ADSs) designed with in-place updates are inefficient to handle such workloads. In this paper, we address this issue and propose a novel authenticated log-structured merge tree (eLSM) based key-value store by leveraging Intel SGX enclaves.
  We present a system design that runs the code of eLSM store inside enclave. To circumvent the limited enclave memory (128 MB with the latest Intel CPUs), we propose to place the memory buffer of the eLSM store outside the enclave and protect the buffer using a new authenticated data structure by digesting individual LSM-tree levels. We design protocols to support query authentication in data integrity, completeness (under range queries), and freshness. The proof in our protocol is made small by including only the Merkle proofs at selective levels.
  We implement eLSM on top of Google LevelDB and Facebook RocksDB with minimal code change and performance interference. We evaluate the performance of eLSM under the YCSB workload benchmark and show a performance advantage of up to 4.5X speedup.",cs.CR,Cybersecurity
A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency,"As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks.",cs.CR,Cybersecurity
Inference of Tampered Smart Meters with Validations from Feeder-Level Power Injections,"Tampering of metering infrastructure of an electrical distribution system can significantly cause customers' billing discrepancy. The large-scale deployment of smart meters may potentially be tampered by malware by propagating their agents to other IP-based meters. Such a possibility is to pivot through the physical perimeters of a smart meter. While this framework may help utilities to accurately energy consumption information on the regular basis, it is challenging to identify malicious meters when there is a large number of users that are exploited to vulnerability and kWh information being altered. This paper presents a reconfiguration switching scheme based on graph theory incorporating the concept of distributed generators to accelerate the anomaly localization process within an electrical distribution network. First, a data form transformation from a visualized grid topology to a graph with vertices and edges is presented. A conversion from the graph representation to machine recognized matrix representation is then performed. The connection of the grid topology is illustrated as an adjacency or incidence matrix for the following analysis. A switching procedure to change elements in the topological matrix is used to detect and localize the tampered node or cluster. The procedure has to meet the electrical and the temporary closed-loop operational constraints. The customer-level anomaly detection is then performed in accordance with probability derived from smart meter anomalies.",cs.CR,Cybersecurity
Selfish Mining and Dyck Words in Bitcoin and Ethereum Networks,"The main goal of this article is to present a direct approach for the formula giving the long-term apparent hashrates of Selfish Mining strategies using only elementary probabilities and combinatorics, more precisely, Dyck words. We can avoid computing stationary probabilities on Markov chain, nor stopping times for Poisson processes as in previous analysis. We do apply these techniques to other block withholding strategies in Bitcoin, and then, we consider also selfish mining in Ethereum.",cs.CR,Cybersecurity
Privacy-preserving Health Data Sharing for Medical Cyber-Physical Systems,"The recent spades of cyber security attacks have compromised end users' data safety and privacy in Medical Cyber-Physical Systems (MCPS). Traditional standard encryption algorithms for data protection are designed based on a viewpoint of system architecture rather than a viewpoint of end users. As such encryption algorithms are transferring the protection on the data to the protection on the keys, data safety and privacy will be compromised once the key is exposed. In this paper, we propose a secure data storage and sharing method consisted by a selective encryption algorithm combined with fragmentation and dispersion to protect the data safety and privacy even when both transmission media (e.g. cloud servers) and keys are compromised. This method is based on a user-centric design that protects the data on a trusted device such as end user's smartphone and lets the end user to control the access for data sharing. We also evaluate the performance of the algorithm on a smartphone platform to prove the efficiency.",cs.CR,Cybersecurity
On generating network traffic datasets with synthetic attacks for intrusion detection,"Most research in the area of intrusion detection requires datasets to develop, evaluate or compare systems in one way or another. In this field, however, finding suitable datasets is a challenge on to itself. Most publicly available datasets have negative qualities that limit their usefulness. In this article, we propose ID2T (Intrusion Detection Dataset Toolkit) to tackle this problem. ID2T facilitates the creation of labeled datasets by injecting synthetic attacks into background traffic. The injected synthetic attacks blend themselves with the background traffic by mimicking the background traffic's properties to eliminate any trace of ID2T's usage.
  This work has three core contribution areas. First, we present a comprehensive survey on intrusion detection datasets. In the survey, we propose a classification to group the negative qualities we found in the datasets. Second, the architecture of ID2T is revised, improved and expanded. The architectural changes enable ID2T to inject recent and advanced attacks such as the widespread EternalBlue exploit or botnet communication patterns. The toolkit's new functionality provides a set of tests, known as TIDED (Testing Intrusion Detection Datasets), that help identify potential defects in the background traffic into which attacks are injected. Third, we illustrate how ID2T is used in different use-case scenarios to evaluate the performance of anomaly and signature-based intrusion detection systems in a reproducible manner. ID2T is open source software and is made available to the community to expand its arsenal of attacks and capabilities.",cs.CR,Cybersecurity
"Please, do not decentralize the Internet with (permissionless) blockchains!","The old mantra of decentralizing the Internet is coming again with fanfare, this time around the blockchain technology hype. We have already seen a technology supposed to change the nature of the Internet: peer-to-peer. The reality is that peer-to-peer naming systems failed, peer-to-peer social networks failed, and yes, peer-to-peer storage failed as well. In this paper, we will review the research on distributed systems in the last few years to identify the limits of open peer-to-peer networks. We will address issues like system complexity, security and frailty, instability and performance. We will show how many of the aforementioned problems also apply to the recent breed of permissionless blockchain networks. The applicability of such systems to mature industrial applications is undermined by the same properties that make them so interesting for a libertarian audience: namely, their openness, their pseudo-anonymity and their unregulated cryptocurrencies. As such, we argue that permissionless blockchain networks are unsuitable to be the substrate for a decentralized Internet. Yet, there is still hope for more decentralization, albeit in a form somewhat limited with respect to the libertarian view of decentralized Internet: in cooperation rather than in competition with the superpowerful datacenters that dominate the world today. This is derived from the recent surge in interest in byzantine fault tolerance and permissioned blockchains, which opens the door to a world where use of trusted third parties is not the only way to arbitrate an ensemble of entities. The ability of establish trust through permissioned blockchains enables to move the control from the datacenters to the edge, truly realizing the promises of edge-centric computing.",cs.CR,Cybersecurity
Experimental Quantum-enhanced Cryptographic Remote Control,"The Internet of Things (IoT), as a cutting-edge integrated cross-technology, promises to informationize people's daily lives, while being threatened by continuous challenges of eavesdropping and tampering. The emerging quantum cryptography, harnessing the random nature of quantum mechanics, may also enable unconditionally secure control network, beyond the applications in secure communications. Here, we present a quantum-enhanced cryptographic remote control scheme that combines quantum randomness and one-time pad algorithm for delivering commands remotely. We experimentally demonstrate this on an unmanned aircraft vehicle (UAV) control system. We precharge quantum random number (QRN) into controller and controlee before launching UAV, instead of distributing QRN like standard quantum communication during flight. We statistically verify the randomness of both quantum keys and the converted ciphertexts to check the security capability. All commands in the air are found to be completely chaotic after encryption, and only matched keys on UAV can decipher those commands precisely. In addition, the controlee does not response to the commands that are not or incorrectly encrypted, showing the immunity against interference and decoy. Our work adds true randomness and quantum enhancement into the realm of secure control algorithm in a straightforward and practical fashion, providing a promoted solution for the security of artificial intelligence and IoT.",cs.CR,Cybersecurity
Analysis and Improvement of Adversarial Training in DQN Agents With Adversarially-Guided Exploration (AGE),"This paper investigates the effectiveness of adversarial training in enhancing the robustness of Deep Q-Network (DQN) policies to state-space perturbations. We first present a formal analysis of adversarial training in DQN agents and its performance with respect to the proportion of adversarial perturbations to nominal observations used for training. Next, we consider the sample-inefficiency of current adversarial training techniques, and propose a novel Adversarially-Guided Exploration (AGE) mechanism based on a modified hybrid of the $$-greedy algorithm and Boltzmann exploration. We verify the feasibility of this exploration mechanism through experimental evaluation of its performance in comparison with the traditional decaying $$-greedy and parameter-space noise exploration algorithms.",cs.CR,Cybersecurity
Adversarial Exploitation of Policy Imitation,"This paper investigates a class of attacks targeting the confidentiality aspect of security in Deep Reinforcement Learning (DRL) policies. Recent research have established the vulnerability of supervised machine learning models (e.g., classifiers) to model extraction attacks. Such attacks leverage the loosely-restricted ability of the attacker to iteratively query the model for labels, thereby allowing for the forging of a labeled dataset which can be used to train a replica of the original model. In this work, we demonstrate the feasibility of exploiting imitation learning techniques in launching model extraction attacks on DRL agents. Furthermore, we develop proof-of-concept attacks that leverage such techniques for black-box attacks against the integrity of DRL policies. We also present a discussion on potential solution concepts for mitigation techniques.",cs.CR,Cybersecurity
Sequential Triggers for Watermarking of Deep Reinforcement Learning Policies,"This paper proposes a novel scheme for the watermarking of Deep Reinforcement Learning (DRL) policies. This scheme provides a mechanism for the integration of a unique identifier within the policy in the form of its response to a designated sequence of state transitions, while incurring minimal impact on the nominal performance of the policy. The applications of this watermarking scheme include detection of unauthorized replications of proprietary policies, as well as enabling the graceful interruption or termination of DRL activities by authorized entities. We demonstrate the feasibility of our proposal via experimental evaluation of watermarking a DQN policy trained in the Cartpole environment.",cs.CR,Cybersecurity
Anonymity Network Tor and Performance Analysis of ARANEA; an IOT Based Privacy-Preserving Router,"There was a time when the word security was only confined to the physical protection of things that were valuable which must be guarded against all the odds. Today, in a world where people can do things virtually have emerged the necessity to protect the virtual world. Every single facet of our life is being controlled by the internet one way or another. There is no privacy in the cyberspace as the data which we are browsing on the internet is being monitored on the other side by someone. Each work we are doing on the internet is getting tracked or the data are getting leaked without consent. To browse the internet securely we developed a router named Aranea which relates to the browser Tor. Tor gives traffic anonymity and security. The Tor browser can be used in both positive and negative purpose. Tor encrypts data, it hides the location and identity of the user, it hides the IP address of the device, it hides the network traffic and many more. By using Tor browser each user can browse the internet safely in the cyber world. Our goal is to create an additional security bridge through the router Aranea for every user so that each user can simply browse the internet anonymously.",cs.CR,Cybersecurity
SPECCFI: Mitigating Spectre Attacks using CFI Informed Speculation,"Spectre attacks and their many subsequent variants are a new vulnerability class affecting modern CPUs. The attacks rely on the ability to misguide speculative execution, generally by exploiting the branch prediction structures, to execute a vulnerable code sequence speculatively. In this paper, we propose to use Control-Flow Integrity (CFI), a security technique used to stop control-flow hijacking attacks, on the committed path, to prevent speculative control-flow from being hijacked to launch the most dangerous variants of the Spectre attacks (Spectre-BTB and Spectre-RSB). Specifically, CFI attempts to constrain the possible targets of an indirect branch to a set of legal targets defined by a pre-calculated control-flow graph (CFG). As CFI is being adopted by commodity software (e.g., Windows and Android) and commodity hardware (e.g., Intel's CET and ARM's BTI), the CFI information becomes readily available through the hardware CFI extensions. With the CFI information, we apply CFI principles to also constrain illegal control-flow during speculative execution. Specifically, our proposed defense, SPECCFI, ensures that control flow instructions target legal destinations to constrain dangerous speculation on forward control-flow paths (indirect calls and branches). We augment this protection with a precise speculation-aware hardware stack to constrain speculation on backward control-flow edges (returns). We combine this solution with existing solutions against branch target predictor attacks (Spectre-PHT) to close all known non-vendor-specific Spectre vulnerabilities. We show that SPECCFI results in small overheads both in terms of performance and additional hardware complexity.",cs.CR,Cybersecurity
Privacy-preserving Crowd-guided AI Decision-making in Ethical Dilemmas,"With the rapid development of artificial intelligence (AI), ethical issues surrounding AI have attracted increasing attention. In particular, autonomous vehicles may face moral dilemmas in accident scenarios, such as staying the course resulting in hurting pedestrians or swerving leading to hurting passengers. To investigate such ethical dilemmas, recent studies have adopted preference aggregation, in which each voter expresses her/his preferences over decisions for the possible ethical dilemma scenarios, and a centralized system aggregates these preferences to obtain the winning decision. Although a useful methodology for building ethical AI systems, such an approach can potentially violate the privacy of voters since moral preferences are sensitive information and their disclosure can be exploited by malicious parties. In this paper, we report a first-of-its-kind privacy-preserving crowd-guided AI decision-making approach in ethical dilemmas. We adopt the notion of differential privacy to quantify privacy and consider four granularities of privacy protection by taking voter-/record-level privacy protection and centralized/distributed perturbation into account, resulting in four approaches VLCP, RLCP, VLDP, and RLDP. Moreover, we propose different algorithms to achieve these privacy protection granularities, while retaining the accuracy of the learned moral preference model. Specifically, VLCP and RLCP are implemented with the data aggregator setting a universal privacy parameter and perturbing the averaged moral preference to protect the privacy of voters' data. VLDP and RLDP are implemented in such a way that each voter perturbs her/his local moral preference with a personalized privacy parameter. Extensive experiments on both synthetic and real data demonstrate that the proposed approach can achieve high accuracy of preference aggregation while protecting individual voter's privacy.",cs.CR,Cybersecurity
DEMO: Extracting Physical-Layer BLE Advertisement Information from Broadcom and Cypress Chips,"Multiple initiatives propose utilizing Bluetooth Low Energy (BLE) advertisements for contact tracing and SARS-CoV-2 exposure notifications. This demo shows a research tool to analyze BLE advertisements; if universally enabled by the vendors, the uncovered features could improve exposure notifications for everyone. We reverse-engineer the firmware-internal implementation of BLE advertisements on Broadcom and Cypress chips and show how to extract further physical-layer information at the receiver. The analyzed firmware works on hundreds of millions of devices, such as all iPhones, the European Samsung Galaxy S series, and Raspberry Pis.",cs.CR,Cybersecurity
Algorithm Selection Framework for Cyber Attack Detection,"The number of cyber threats against both wired and wireless computer systems and other components of the Internet of Things continues to increase annually. In this work, an algorithm selection framework is employed on the NSL-KDD data set and a novel paradigm of machine learning taxonomy is presented. The framework uses a combination of user input and meta-features to select the best algorithm to detect cyber attacks on a network. Performance is compared between a rule-of-thumb strategy and a meta-learning strategy. The framework removes the conjecture of the common trial-and-error algorithm selection method. The framework recommends five algorithms from the taxonomy. Both strategies recommend a high-performing algorithm, though not the best performing. The work demonstrates the close connectedness between algorithm selection and the taxonomy for which it is premised.",cs.CR,Cybersecurity
Phishing and Spear Phishing: examples in Cyber Espionage and techniques to protect against them,"Phishing attacks have become the most used technique in the online scams, initiating more than 91% of cyberattacks, from 2012 onwards. This study reviews how Phishing and Spear Phishing attacks are carried out by the phishers, through 5 steps which magnify the outcome, increasing the chance of success. The focus will be also given on four different layers of protection against these social engineering attacks, showing their strengths and weaknesses; the first and second layers consist of automated tools and decision-aid tools. the third one is users' knowledge and expertise to deal with potential threats. The last layer, defined as ""external"", will underline the importance of having a Multi-factor authentication, an effective way to provide an enhanced security, creating a further layer of protection against Phishing and Spear Phishing.",cs.CR,Cybersecurity
Fast Execute-Only Memory for Embedded Systems,"Remote code disclosure attacks threaten embedded systems as they allow attackers to steal intellectual property or to find reusable code for use in control-flow hijacking attacks. Execute-only memory (XOM) prevents remote code disclosures, but existing XOM solutions either require a memory management unit that is not available on ARM embedded systems or incur significant overhead.
  We present PicoXOM: a fast and novel XOM system for ARMv7-M and ARMv8-M devices which leverages ARM's Data Watchpoint and Tracing unit along with the processor's simplified memory protection hardware. On average, PicoXOM incurs 0.33% performance overhead and 5.89% code size overhead on two benchmark suites and five real-world applications.",cs.CR,Cybersecurity
Distributed Attribute-Based Access Control System Using a Permissioned Blockchain,"Auditing provides an essential security control in computer systems, by keeping track of all access attempts, including both legitimate and illegal access attempts. This phase can be useful to the context of audits, where eventual misbehaving parties can be held accountable. Blockchain technology can provide trusted auditability required for access control systems. In this paper, we propose a distributed \ac{ABAC} system based on blockchain to provide trusted auditing of access attempts. Besides auditability, our system presents a level of transparency that both access requestors and resource owners can benefit from it. We present a system architecture with an implementation based on Hyperledger Fabric, achieving high efficiency and low computational overhead. The proposed solution is validated through a use case of independent digital libraries. Detailed performance analysis of our implementation is presented, taking into account different consensus mechanisms and databases. The experimental evaluation shows that our presented system can process 5,000 access control requests with the send rate of 200 per second and a latency of 0.3 seconds.",cs.CR,Cybersecurity
Contextualisation of Data Flow Diagrams for security analysis,"Data flow diagrams (DFDs) are popular for sketching systems for subsequent threat modelling. Their limited semantics make reasoning about them difficult, but enriching them endangers their simplicity and subsequent ease of take up. We present an approach for reasoning about tainted data flows in design-level DFDs by putting them in context with other complementary usability and requirements models. We illustrate our approach using a pilot study, where tainted data flows were identified without any augmentations to either the DFD or its complementary models.",cs.CR,Cybersecurity
Identifying Vulnerabilities of Industrial Control Systems using Evolutionary Multiobjective Optimisation,"In this paper we propose a novel methodology to assist in identifying vulnerabilities in a real-world complex heterogeneous industrial control systems (ICS) using two evolutionary multiobjective optimisation (EMO) algorithms, NSGA-II and SPEA2. Our approach is evaluated on a well known benchmark chemical plant simulator, the Tennessee Eastman (TE) process model. We identified vulnerabilities in individual components of the TE model and then made use of these to generate combinatorial attacks to damage the safety of the system, and to cause economic loss. Results were compared against random attacks, and the performance of the EMO algorithms were evaluated using hypervolume, spread and inverted generational distance (IGD) metrics. A defence against these attacks in the form of a novel intrusion detection system was developed, using a number of machine learning algorithms. Designed approach was further tested against the developed detection methods. Results demonstrate that EMO algorithms are a promising tool in the identification of the most vulnerable components of ICS, and weaknesses of any existing detection systems in place to protect the system. The proposed approach can be used by control and security engineers to design security aware control, and test the effectiveness of security mechanisms, both during design, and later during system operation.",cs.CR,Cybersecurity
A Security Policy Model Transformation and Verification Approach for Software Defined Networking,"Software defined networking (SDN) has been adopted to enforce the security of large-scale and complex networks because of its programmable, abstract, centralized intelligent control and global and real-time traffic view. However, the current SDN-based security enforcement mechanisms require network managers to fully understand the underlying configurations of network. Facing the increasingly complex and huge SDN networks, we urgently need a novel security policy management mechanism which can be completely transparent to any underlying information. That is it can permit network managers to define upper-level security policies without containing any underlying information of network, and by means of model transformation system, these upper-level security policies can be transformed into their corresponding lower-level policies containing underlying information automatically. Moreover, it should ensure system model updated by the generated lower-level policies can hold all of security properties defined in upper-level policies. Based on these insights, we propose a security policy model transformation and verification approach for SDN in this paper. We first present the formal definition of a security policy model (SPM) which can be used to specify the security policies used in SDN. Then, we propose a model transformation system based on SDN system model and mapping rules, which can enable network managers to convert SPM model into corresponding underlying network configuration policies automatically, i.e., flow table model (FTM). In order to verify SDN system model updated by the generated FTM models can hold the security properties defined in SPM models, we design a security policy verification system based on model checking. Finally, we utilize a comprehensive case to illustrate the feasibility of the proposed approach.",cs.CR,Cybersecurity
Efficient Privacy-Preserving Electricity Theft Detection with Dynamic Billing and Load Monitoring for AMI Networks,"In advanced metering infrastructure (AMI), smart meters (SMs) are installed at the consumer side to send fine-grained power consumption readings periodically to the system operator (SO) for load monitoring, energy management, billing, etc. However, fraudulent consumers launch electricity theft cyber-attacks by reporting false readings to reduce their bills illegally. These attacks do not only cause financial losses but may also degrade the grid performance because the readings are used for grid management. To identify these attackers, the existing schemes employ machine-learning models using the consumers' fine-grained readings, which violates the consumers' privacy by revealing their lifestyle. In this paper, we propose an efficient scheme that enables the SO to detect electricity theft, compute bills, and monitor load while preserving the consumers' privacy. The idea is that SMs encrypt their readings using functional encryption, and the SO uses the ciphertexts to (i) compute the bills following dynamic pricing approach, (ii) monitor the grid load, and (iii) evaluate a machine-learning model to detect fraudulent consumers, without being able to learn the individual readings to preserve consumers' privacy. We adapted a functional encryption scheme so that the encrypted readings are aggregated for billing and load monitoring and only the aggregated value is revealed to the SO. Also, we exploited the inner-product operations on encrypted readings to evaluate a machine-learning model to detect fraudulent consumers. Real dataset is used to evaluate our scheme, and our evaluations indicate that our scheme is secure and can detect fraudulent consumers accurately with low communication and computation overhead.",cs.CR,Cybersecurity
Securing Your Collaborative Jupyter Notebooks in the Cloud using Container and Load Balancing Services,"Jupyter has become the go-to platform for developing data applications but data and security concerns, especially when dealing with healthcare, have become paramount for many institutions and applications dealing with sensitive information. How then can we continue to enjoy the data analysis and machine learning opportunities provided by Jupyter and the Python ecosystem while guaranteeing auditable compliance with security and privacy concerns? We will describe the architecture and implementation of a cloud based platform based on Jupyter that integrates with Amazon Web Services (AWS) and uses containerized services without exposing the platform to the vulnerabilities present in Kubernetes and JupyterHub. This architecture addresses the HIPAA requirements to ensure both security and privacy of data. The architecture uses an AWS service to provide JSON Web Tokens (JWT) for authentication as well as network control. Furthermore, our architecture enables secure collaboration and sharing of Jupyter notebooks. Even though our platform is focused on Jupyter notebooks and JupyterLab, it also supports R-Studio and bespoke applications that share the same authentication mechanisms. Further, the platform can be extended to other cloud services other than AWS.",cs.CR,Cybersecurity
A New Chaos and Permutation Based Algorithm for Image and Video Encryption,"Images and video sequences carry large volumes of highly correlated and redundant data. Applications like military and telecommunication require encryption methods to protect the data from unwanted access. This requirement in most cases needs to be realized in real-time. In this paper, we propose a fast new Fiestal-structured approach for image and video encryption based on a chaotic random sequence generator and a Permutation-Inverse Permutation (PIP) pixel transform. This approach utilizes mathematical functions and transforms with low complexity. The algorithm at the same time, ensures no drastic pay off in terms of encryption quality. This renders the algorithm with promising scope for real time applications and easy hardware implementation. MATLAB simulation of the algorithm establishes its high quality of encryption in terms of elevated entropy values and negligible correlation of the encrypted data with the original. Simulation results also show high sensitivity to slight variation in keys ensuring high security.",cs.CR,Cybersecurity
DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks,"Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains. In this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy. We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.",cs.CR,Cybersecurity
A Large Scale Analysis of Android-Web Hybridization,"Many Android applications embed webpages via WebView components and execute JavaScript code within Android. Hybrid applications leverage dedicated APIs to load a resource and render it in a WebView. Furthermore, Android objects can be shared with the JavaScript world. However, bridging the interfaces of the Android and JavaScript world might also incur severe security threats: Potentially untrusted webpages and their JavaScript might interfere with the Android environment and its access to native features. No general analysis is currently available to assess the implications of such hybrid apps bridging the two worlds. To understand the semantics and effects of hybrid apps, we perform a large-scale study on the usage of the hybridization APIs in the wild. We analyze and categorize the parameters to hybridization APIs for 7,500 randomly selected and the 196 most popular applications from the Google Playstore as well as 1000 malware samples. Our results advance the general understanding of hybrid applications, as well as implications for potential program analyses, and the current security situation: We discovered thousands of flows of sensitive data from Android to JavaScript, the vast majority of which could flow to potentially untrustworthy code. Our analysis identified numerous web pages embedding vulnerabilities, which we exemplarily exploited. Additionally, we discovered a multitude of applications in which potentially untrusted JavaScript code may interfere with (trusted) Android objects, both in benign and malign applications.",cs.CR,Cybersecurity
Training DNN Model with Secret Key for Model Protection,"In this paper, we propose a model protection method by using block-wise pixel shuffling with a secret key as a preprocessing technique to input images for the first time. The protected model is built by training with such preprocessed images. Experiment results show that the performance of the protected model is close to that of non-protected models when the key is correct, while the accuracy is severely dropped when an incorrect key is given, and the proposed model protection is robust against not only brute-force attacks but also fine-tuning attacks, while maintaining almost the same performance accuracy as that of using a non-protected model.",cs.CR,Cybersecurity
Adv-watermark: A Novel Watermark Perturbation for Adversarial Examples,"Recent research has demonstrated that adding some imperceptible perturbations to original images can fool deep learning models. However, the current adversarial perturbations are usually shown in the form of noises, and thus have no practical meaning. Image watermark is a technique widely used for copyright protection. We can regard image watermark as a king of meaningful noises and adding it to the original image will not affect people's understanding of the image content, and will not arouse people's suspicion. Therefore, it will be interesting to generate adversarial examples using watermarks. In this paper, we propose a novel watermark perturbation for adversarial examples (Adv-watermark) which combines image watermarking techniques and adversarial example algorithms. Adding a meaningful watermark to the clean images can attack the DNN models. Specifically, we propose a novel optimization algorithm, which is called Basin Hopping Evolution (BHE), to generate adversarial watermarks in the black-box attack mode. Thanks to the BHE, Adv-watermark only requires a few queries from the threat models to finish the attacks. A series of experiments conducted on ImageNet and CASIA-WebFace datasets show that the proposed method can efficiently generate adversarial examples, and outperforms the state-of-the-art attack methods. Moreover, Adv-watermark is more robust against image transformation defense methods.",cs.CR,Cybersecurity
Multi-Stage Optimized Machine Learning Framework for Network Intrusion Detection,"Cyber-security garnered significant attention due to the increased dependency of individuals and organizations on the Internet and their concern about the security and privacy of their online activities. Several previous machine learning (ML)-based network intrusion detection systems (NIDSs) have been developed to protect against malicious online behavior. This paper proposes a novel multi-stage optimized ML-based NIDS framework that reduces computational complexity while maintaining its detection performance. This work studies the impact of oversampling techniques on the models' training sample size and determines the minimal suitable training sample size. Furthermore, it compares between two feature selection techniques, information gain and correlation-based, and explores their effect on detection performance and time complexity. Moreover, different ML hyper-parameter optimization techniques are investigated to enhance the NIDS's performance. The performance of the proposed framework is evaluated using two recent intrusion detection datasets, the CICIDS 2017 and the UNSW-NB 2015 datasets. Experimental results show that the proposed model significantly reduces the required training sample size (up to 74%) and feature set size (up to 50%). Moreover, the model performance is enhanced with hyper-parameter optimization with detection accuracies over 99% for both datasets, outperforming recent literature works by 1-2% higher accuracy and 1-2% lower false alarm rate.",cs.CR,Cybersecurity
Why are Developers Struggling to Put GDPR into Practice when Developing Privacy-Preserving Software Systems?,"The use of software applications is inevitable as they provide different services to users. The software applications collect, store users' data, and sometimes share with the third party, even without the user consent. One can argue that software developers do not implement privacy into the software applications they develop or take GDPR (General Data Protection Law) law into account. Failing to do this, may lead to software applications that open up privacy breaches (e.g. data breach). The GDPR law provides a set of guidelines for developers and organizations on how to protect user data when they are interacting with software applications. Previous research has attempted to investigate what hinders developers from embedding privacy into software systems. However, there has been no detailed investigation on why they cannot develop privacy-preserving systems taking GDPR into consideration, which is imperative to develop software applications that preserve privacy. Therefore, this paper investigates the issues that hinder software developers from implementing software applications taking GDPR law on-board. Our study findings revealed that developers are not familiar with GDPR principles. Even some of them are, they lack knowledge of the GDPR principles and their techniques to use when developing privacy-preserving software systems",cs.CR,Cybersecurity
ProblemChild: Discovering Anomalous Patterns based on Parent-Child Process Relationships,"It is becoming more common that adversary attacks consist of more than a standalone executable or script. Often, evidence of an attack includes conspicuous process heritage that may be ignored by traditional static machine learning models. Advanced attacker techniques, like ""living off the land"" that appear normal in isolation become more suspicious when observed in a parent-child context. The context derived from parent-child process chains can help identify and group malware families, as well as discover novel attacker techniques. Adversaries chain these techniques to achieve persistence, bypass defenses, and execute actions. Traditional heuristic-based detections often generate noise or disparate events that belong to what constitutes a single attack. ProblemChild is a graph-based framework designed to address these issues. ProblemChild applies a supervised learning classifier to derive a weighted graph used to identify communities of seemingly disparate events into larger attack sequences. ProblemChild applies conditional probability to automatically rank anomalous communities as well as suppress commonly occurring parent-child chains. In combination, this framework can be used by analysts to aid in the crafting or tuning of detectors and reduce false-positives over time. We evaluate ProblemChild against the 2018 MITRE ATT&CK(TM) emulation of APT3 attack to demonstrate its promise in identifying anomalous parent-child process chains.",cs.CR,Cybersecurity
Enhancing Robustness Against Adversarial Examples in Network Intrusion Detection Systems,"The increase of cyber attacks in both the numbers and varieties in recent years demands to build a more sophisticated network intrusion detection system (NIDS). These NIDS perform better when they can monitor all the traffic traversing through the network like when being deployed on a Software-Defined Network (SDN). Because of the inability to detect zero-day attacks, signature-based NIDS which were traditionally used for detecting malicious traffic are beginning to get replaced by anomaly-based NIDS built on neural networks. However, recently it has been shown that such NIDS have their own drawback namely being vulnerable to the adversarial example attack. Moreover, they were mostly evaluated on the old datasets which don't represent the variety of attacks network systems might face these days. In this paper, we present Reconstruction from Partial Observation (RePO) as a new mechanism to build an NIDS with the help of denoising autoencoders capable of detecting different types of network attacks in a low false alert setting with an enhanced robustness against adversarial example attack. Our evaluation conducted on a dataset with a variety of network attacks shows denoising autoencoders can improve detection of malicious traffic by up to 29% in a normal setting and by up to 45% in an adversarial setting compared to other recently proposed anomaly detectors.",cs.CR,Cybersecurity
Local Differential Privacy and Its Applications: A Comprehensive Survey,"With the fast development of Information Technology, a tremendous amount of data have been generated and collected for research and analysis purposes. As an increasing number of users are growing concerned about their personal information, privacy preservation has become an urgent problem to be solved and has attracted significant attention. Local differential privacy (LDP), as a strong privacy tool, has been widely deployed in the real world in recent years. It breaks the shackles of the trusted third party, and allows users to perturb their data locally, thus providing much stronger privacy protection. This survey provides a comprehensive and structured overview of the local differential privacy technology. We summarise and analyze state-of-the-art research in LDP and compare a range of methods in the context of answering a variety of queries and training different machine learning models. We discuss the practical deployment of local differential privacy and explore its application in various domains. Furthermore, we point out several research gaps, and discuss promising future research directions.",cs.CR,Cybersecurity
"Channel Leakage, Information-Theoretic Limitations of Obfuscation, and Optimal Privacy Mask Design for Streaming Data","In this paper, we first introduce the notion of channel leakage as the minimum mutual information between the channel input and channel output. As its name indicates, channel leakage quantifies the minimum information leakage to the malicious receiver. In a broad sense, it can be viewed as a dual concept of channel capacity, which characterizes the maximum information transmission to the targeted receiver. We obtain explicit formulas of channel leakage for the white Gaussian case, the colored Gaussian case, and the fading case. We then utilize this notion to investigate the fundamental limitations of obfuscation in terms of privacy-distortion tradeoffs (as well as privacy-power tradeoffs) for streaming data; particularly, we derive analytical tradeoff equations for the stationary case, the non-stationary case, and the finite-time case. Our results also indicate explicitly how to design the privacy masks in an optimal way.",cs.CR,Cybersecurity
Cyberattacks on Miniature Brain Implants to Disrupt Spontaneous Neural Signaling,"Brain-Computer Interfaces (BCI) arose as systems that merge computing systems with the human brain to facilitate recording, stimulation, and inhibition of neural activity. Over the years, the development of BCI technologies has shifted towards miniaturization of devices that can be seamlessly embedded into the brain and can target single neuron or small population sensing and control. We present a motivating example highlighting vulnerabilities of two promising micron-scale BCI technologies, demonstrating the lack of security and privacy principles in existing solutions. This situation opens the door to a novel family of cyberattacks, called neuronal cyberattacks, affecting neuronal signaling. This paper defines the first two neural cyberattacks, Neuronal Flooding (FLO) and Neuronal Scanning (SCA), where each threat can affect the natural activity of neurons. This work implements these attacks in a neuronal simulator to determine their impact over the spontaneous neuronal behavior, defining three metrics: number of spikes, percentage of shifts, and dispersion of spikes. Several experiments demonstrate that both cyberattacks produce a reduction of spikes compared to spontaneous behavior, generating a rise in temporal shifts and a dispersion increase. Mainly, SCA presents a higher impact than FLO in the metrics focused on the number of spikes and dispersion, where FLO is slightly more damaging, considering the percentage of shifts. Nevertheless, the intrinsic behavior of each attack generates a differentiation on how they alter neuronal signaling. FLO is adequate to generate an immediate impact on the neuronal activity, whereas SCA presents higher effectiveness for damages to the neural signaling in the long-term.",cs.CR,Cybersecurity
FPGA-Based Hardware Accelerator of Homomorphic Encryption for Efficient Federated Learning,"With the increasing awareness of privacy protection and data fragmentation problem, federated learning has been emerging as a new paradigm of machine learning. Federated learning tends to utilize various privacy preserving mechanisms to protect the transferred intermediate data, among which homomorphic encryption strikes a balance between security and ease of utilization. However, the complicated operations and large operands impose significant overhead on federated learning. Maintaining accuracy and security more efficiently has been a key problem of federated learning. In this work, we investigate a hardware solution, and design an FPGA-based homomorphic encryption framework, aiming to accelerate the training phase in federated learning. The root complexity lies in searching for a compact architecture for the core operation of homomorphic encryption, to suit the requirement of federated learning about high encryption throughput and flexibility of configuration. Our framework implements the representative Paillier homomorphic cryptosystem with high level synthesis for flexibility and portability, with careful optimization on the modular multiplication operation in terms of processing clock cycle, resource usage and clock frequency. Our accelerator achieves a near-optimal execution clock cycle, with a better DSP-efficiency than existing designs, and reduces the encryption time by up to 71% during training process of various federated learning models.",cs.CR,Cybersecurity
Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review,"This work provides the community with a timely comprehensive review of backdoor attacks and countermeasures on deep learning. According to the attacker's capability and affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into six categorizations: code poisoning, outsourcing, pretrained, data collection, collaborative learning and post-deployment. Accordingly, attacks under each categorization are combed. The countermeasures are categorized into four general classes: blind backdoor removal, offline backdoor inspection, online backdoor inspection, and post backdoor removal. Accordingly, we review countermeasures, and compare and analyze their advantages and disadvantages. We have also reviewed the flip side of backdoor attacks, which are explored for i) protecting intellectual property of deep learning models, ii) acting as a honeypot to catch adversarial example attacks, and iii) verifying data deletion requested by the data contributor.Overall, the research on defense is far behind the attack, and there is no single defense that can prevent all types of backdoor attacks. In some cases, an attacker can intelligently bypass existing defenses with an adaptive attack. Drawing the insights from the systematic review, we also present key areas for future research on the backdoor, such as empirical security evaluations from physical trigger attacks, and in particular, more efficient and practical countermeasures are solicited.",cs.CR,Cybersecurity
Addressing the Privacy Implications of Mixed Reality: A Regulatory Approach,"Mixed reality (MR) technologies are emerging into the mainstream with affordable devices like the Oculus Quest. These devices blend the physical and virtual in novel ways that blur the lines that exist in legal precedent, like those between speech and conduct. In this paper, we discuss the challenges of regulating immersive technologies, focusing on the potential for extensive data collection, and examine the trade-offs of three potential approaches to protecting data privacy in the context of mixed reality environments.",cs.CR,Cybersecurity
Certificate and Signature Free Anonymity for V2V Communications,"Anonymity is a desirable feature for vehicle-to-vehicle (V2V) communications, but it conflicts with other requirements such as non-repudiation and revocation. Existing, pseudonym-based V2V communications schemes rely on certificate generation and signature verification. These schemes require cumbersome key management, frequent updating of certificate chains and other costly procedures such as cryptographic pairings. In this paper, we present novel V2V communications schemes, that provide authentication, authorization, anonymity, non-repudiation, replay protection, pseudonym revocation, and forward secrecy without relying on traditional certificate generation and signature verification. Security and privacy of our schemes rely on hard problems in number theory. Furthermore, our schemes guarantee security and privacy in the presence of subsets of colluding malicious parties, provided that the cardinality of such sets is below a fixed threshold.",cs.CR,Cybersecurity
Corona-Warn-App: Tracing the Start of the Official COVID-19 Exposure Notification App for Germany,"On June 16, 2020, Germany launched an open-source smartphone contact tracing app (""Corona-Warn-App"") to help tracing SARS-CoV-2 (coronavirus) infection chains. It uses a decentralized, privacy-preserving design based on the Exposure Notification APIs in which a centralized server is only used to distribute a list of keys of SARS-CoV-2 infected users that is fetched by the app once per day. Its success, however, depends on its adoption. In this poster, we characterize the early adoption of the app using Netflow traces captured directly at its hosting infrastructure. We show that the app generated traffic from allover Germany---already on the first day. We further observe that local COVID-19 outbreaks do not result in noticeable traffic increases.",cs.CR,Cybersecurity
Network Intrusion Detection Using Wrapper-based Decision Tree for Feature Selection,"One of the key challenges of machine learning (ML) based intrusion detection system (IDS) is the expensive computational complexity which is largely due to redundant, incomplete, and irrelevant features contain in the IDS datasets. To overcome such challenge and ensure building an efficient and more accurate IDS models, many researchers utilize preprocessing techniques such as normalization and feature selection in a hybrid modeling approach. In this work, we propose a hybrid IDS modeling approach with an algorithm for feature selection (FS) and another for building an IDS. The FS algorithm is a wrapper-based with a decision tree as the feature evaluator. The propose FS method is used in combination with some selected ML algorithms to build IDS models using the UNSW-NB15 dataset. Some IDS models are built as a baseline in a single modeling approach using the full features of the dataset. We evaluate the effectiveness of our propose method by comparing it with the baseline models and also with state-of-the-art works. Our method achieves the best DR of 97.95% and shown to be quite effective in comparison to state-of-the-art works. We, therefore, recommend its usage especially in IDS modeling with the UNSW-NB15 dataset.",cs.CR,Cybersecurity
Multi-Party Private Set Intersection: An Information-Theoretic Approach,"We investigate the problem of multi-party private set intersection (MP-PSI). In MP-PSI, there are $M$ parties, each storing a data set $\mathcal{p}_i$ over $N_i$ replicated and non-colluding databases, and we want to calculate the intersection of the data sets $\cap_{i=1}^M \mathcal{p}_i$ without leaking any information beyond the set intersection to any of the parties. We consider a specific communication protocol where one of the parties, called the leader party, initiates the MP-PSI protocol by sending queries to the remaining parties which are called client parties. The client parties are not allowed to communicate with each other. We propose an information-theoretic scheme that privately calculates the intersection $\cap_{i=1}^M \mathcal{p}_i$ with a download cost of $D = \min_{t \in \{1, \cdots, M\}} \sum_{i \in \{1, \cdots M\}\setminus {t}} \left\lceil \frac{|\mathcal{p}_t|N_i}{N_i-1}\right\rceil$. Similar to the 2-party PSI problem, our scheme builds on the connection between the PSI problem and the multi-message symmetric private information retrieval (MM-SPIR) problem. Our scheme is a non-trivial generalization of the 2-party PSI scheme as it needs an intricate design of the shared common randomness. Interestingly, in terms of the download cost, our scheme does not incur any penalty due to the more stringent privacy constraints in the MP-PSI problem compared to the 2-party PSI problem.",cs.CR,Cybersecurity
Attributes affecting user decision to adopt a Virtual Private Network (VPN) app,"A Virtual Private Network (VPN) helps to mitigate security and privacy risks of data transmitting on unsecured network such as public Wi-Fi. However, despite awareness of public Wi-Fi risks becoming increasingly common, the use of VPN when using public Wi-Fi is low. To increase adoption, understanding factors driving user decision to adopt a VPN app is an important first step. This study is the first to achieve this objective using discrete choice experiments (DCEs) to elicit individual preferences of specific attributes of a VPN app. The experiments were run in the United Kingdom (UK) and Japan (JP). We first interviewed participants (15 UK, 17 JP) to identify common attributes of a VPN app which they considered important. The results were used to design and run a DCE in each country. Participants (149 UK, 94 JP) were shown a series of two hypothetical VPN apps, varying in features, and were asked to choose one which they preferred. Customer review rating, followed by price of a VPN app, significantly affected the decision to choose which VPN app to download and install. A change from a rating of 3 to 4-5 stars increased the probability of choosing an app by 33% in the UK and 14% in Japan. Unsurprisingly, price was a deterrent. Recommendations by friends, source of product reviews, and the presence of in-app ads also played a role but to a lesser extent. To actually use a VPN app, participants considered Internet speed, connection stability, battery level on mobile devices, and the presence of in-app ads as key drivers. Participants in the UK and in Japan prioritized these attributes differently, suggesting possible influences from cultural differences.",cs.CR,Cybersecurity
One-pixel Signature: Characterizing CNN Models for Backdoor Detection,"We tackle the convolution neural networks (CNNs) backdoor detection problem by proposing a new representation called one-pixel signature. Our task is to detect/classify if a CNN model has been maliciously inserted with an unknown Trojan trigger or not. Here, each CNN model is associated with a signature that is created by generating, pixel-by-pixel, an adversarial value that is the result of the largest change to the class prediction. The one-pixel signature is agnostic to the design choice of CNN architectures, and how they were trained. It can be computed efficiently for a black-box CNN model without accessing the network parameters. Our proposed one-pixel signature demonstrates a substantial improvement (by around 30% in the absolute detection accuracy) over the existing competing methods for backdoored CNN detection/classification. One-pixel signature is a general representation that can be used to characterize CNN models beyond backdoor detection.",cs.CR,Cybersecurity
Actor-based Risk Analysis for Blockchains in Smart Mobility,"Blockchain technology is a crypto-based secure ledger for data storage and transfer through decentralized, trustless peer-to-peer systems. Despite its advantages, previous studies have shown that the technology is not completely secure against cyber attacks. Thus, it is crucial to perform domain specific risk analysis to measure how viable the attacks are on the system, their impact and consequently the risk exposure. Specifically, in this paper, we carry out an analysis in terms of quantifying the risk associated to an operational multi-layered Blockchain framework for Smart Mobility Data-markets (BSMD). We conduct an actor-based analysis to determine the impact of the attacks. The analysis identified five attack goals and five types of attackers that violate the security of the blockchain system. In the case study of the public permissioned BSMD, we highlight the highest risk factors according to their impact on the victims in terms of monetary, privacy, integrity and trust. Four attack goals represent a risk in terms of economic losses and one attack goal contains many threats that represent a risk that is either unacceptable or undesirable.",cs.CR,Cybersecurity
A Comprehensive Survey of Aadhar and Security Issues,"The concept of Aadhaar came with the need for a unique identity for every individual. To implement this, the Indian government created the authority UIDAI to distribute and generate user identities for every individual based on their demographic and biometric data. After the implementation, came the security issues and challenges of Aadhaar and its authentication. So, our study focuses on the journey of Aadhaar from its history to the current condition. The paper also describes the authentication process, and the updates happened over time. We have also provided an analysis of the security attacks witnessed so far as well as the possible countermeasure and its classification. Our main aim is to cover all the security aspects related to Aadhaar to avoid possible security attacks. Also, we have included the current updates and news related to Aadhaar.",cs.CR,Cybersecurity
Secure Control in Partially Observable Environments to Satisfy LTL Specifications,"This paper studies the synthesis of control policies for an agent that has to satisfy a temporal logic specification in a partially observable environment, in the presence of an adversary. The interaction of the agent (defender) with the adversary is modeled as a partially observable stochastic game. The goal is to generate a defender policy to maximize satisfaction of a given temporal logic specification under any adversary policy. The search for policies is limited to the space of finite state controllers, which leads to a tractable approach to determine policies. We relate the satisfaction of the specification to reaching (a subset of) recurrent states of a Markov chain. We present an algorithm to determine a set of defender and adversary finite state controllers of fixed sizes that will satisfy the temporal logic specification, and prove that it is sound. We then propose a value-iteration algorithm to maximize the probability of satisfying the temporal logic specification under finite state controllers of fixed sizes. Lastly, we extend this setting to the scenario where the size of the finite state controller of the defender can be increased to improve the satisfaction probability. We illustrate our approach with an example.",cs.CR,Cybersecurity
Trust Infrastructures for Virtual Asset Service Providers,"Virtual asset service providers (VASPs) currently face a number of challenges, both from the technological and the regulatory perspectives. In the context of virtual asset transfers one key issue is the need for VASPs to securely exchange customer information to comply to the Travel Rule. We discuss a VASP information sharing network as one form of a trust infrastructure for VASP-to-VASP interactions. Related to this is the need for a trusted identity infrastructure for VASPs that would permit other entities to quickly ascertain the legal business status of a VASP. For regulated wallets, an attestation infrastructure may provide VASPs and insurance providers with better visibility into the state of wallets based on trusted hardware. Finally, for customers of VASPs there is a need for seamless integration between the VASP services with the existing consumer identity management infrastructure, providing a user-friendly experience for transferring virtual assets to other users.",cs.CR,Cybersecurity
Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis,"With the rapid growth of Android malware, many machine learning-based malware analysis approaches are proposed to mitigate the severe phenomenon. However, such classifiers are opaque, non-intuitive, and difficult for analysts to understand the inner decision reason. For this reason, a variety of explanation approaches are proposed to interpret predictions by providing important features. Unfortunately, the explanation results obtained in the malware analysis domain cannot achieve a consensus in general, which makes the analysts confused about whether they can trust such results. In this work, we propose principled guidelines to assess the quality of five explanation approaches by designing three critical quantitative metrics to measure their stability, robustness, and effectiveness. Furthermore, we collect five widely-used malware datasets and apply the explanation approaches on them in two tasks, including malware detection and familial identification. Based on the generated explanation results, we conduct a sanity check of such explanation approaches in terms of the three metrics. The results demonstrate that our metrics can assess the explanation approaches and help us obtain the knowledge of most typical malicious behaviors for malware analysis.",cs.CR,Cybersecurity
Scytale -- An Evolutionary Cryptosystem,"With the advent of quantum computing, and other advancements in computation and processing capabilities of modern systems, there arises a need to develop new trapdoor functions that will serve as the foundation for a new generation of encryption schemes. This paper explores the possibility of one such potential trapdoor function using concepts stemming from Reversible Cellular Automata (RCA) -- specifically, the Critter's Rule set up in a Margolus Neighborhood. The proposed block encryption algorithm discusses how sensitive data can be manipulated and converted efficiently into a two dimensional sequence of bits, that can be iteratively evolved using the rules of the RCA and a private key to achieve a desirable level of encryption within a reasonable runtime. The performance benchmark and analysis results exemplify how well the proposed encryption algorithm stands against different forms of attacks.",cs.CR,Cybersecurity
Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph,"Most of the existing text generative steganographic methods are based on coding the conditional probability distribution of each word during the generation process, and then selecting specific words according to the secret information, so as to achieve information hiding. Such methods have their limitations which may bring potential security risks. Firstly, with the increase of embedding rate, these models will choose words with lower conditional probability, which will reduce the quality of the generated steganographic texts; secondly, they can not control the semantic expression of the final generated steganographic text. This paper proposes a new text generative steganography method which is quietly different from the existing models. We use a Knowledge Graph (KG) to guide the generation of steganographic sentences. On the one hand, we hide the secret information by coding the path in the knowledge graph, but not the conditional probability of each generated word; on the other hand, we can control the semantic expression of the generated steganographic text to a certain extent. The experimental results show that the proposed model can guarantee both the quality of the generated text and its semantic expression, which is a supplement and improvement to the current text generation steganography.",cs.CR,Cybersecurity
Taxonomy and Practical Evaluation of Primality Testing Algorithms,"Modern cryptography algorithms are commonly used to ensure information security. Prime numbers are needed in many asymmetric cryptography algorithms. For example, RSA algorithm selects two large prime numbers and multiplies to each other to obtain a large composite number whose factorization is very difficult. Producing a prime number is not an easy task as they are not distributed regularly through integers. Primality testing algorithms are used to determine whether a particular number is prime or composite. In this paper, an intensive survey is thoroughly conducted among the several primality testing algorithms showing the pros and cons, the time complexity, and a brief summary of each algorithm. Besides, an implementation of these algorithms is accomplished using Java and Python as programming languages to evaluate the efficiency of both the algorithms and the programming languages.",cs.CR,Cybersecurity
Timely Detection and Mitigation of Stealthy DDoS Attacks via IoT Networks,"Internet of Things (IoT) networks consist of sensors, actuators, mobile and wearable devices that can connect to the Internet. With billions of such devices already in the market which have significant vulnerabilities, there is a dangerous threat to the Internet services and also some cyber-physical systems that are also connected to the Internet. Specifically, due to their existing vulnerabilities IoT devices are susceptible to being compromised and being part of a new type of stealthy Distributed Denial of Service (DDoS) attack, called Mongolian DDoS, which is characterized by its widely distributed nature and small attack size from each source. This study proposes a novel anomaly-based Intrusion Detection System (IDS) that is capable of timely detecting and mitigating this emerging type of DDoS attacks. The proposed IDS's capability of detecting and mitigating stealthy DDoS attacks with even very low attack size per source is demonstrated through numerical and testbed experiments.",cs.CR,Cybersecurity
A Novel Topology-Guided Attack and Its Countermeasure Towards Secure Logic Locking,"The outsourcing of the design and manufacturing of integrated circuits (ICs) in the current horizontal semiconductor integration flow has posed various security threats due to the presence of untrusted entities, such as overproduction of ICs, sale of out-of-specification/rejected ICs, and piracy of Intellectual Properties (IPs). Consequently, logic locking emerged as one of the prominent design for trust techniques. Unfortunately, these locking techniques are now inclined to achieve complete Boolean satisfiability (SAT) resiliency after the seminal work published in [47]. In this paper, we propose a novel oracle-less attack that is based on the topological analysis of the locked netlist even though it is SAT-resilient. The attack relies on identifying and constructing unit functions with a hypothesis key to be searched in the entire netlist to find its replica. The proposed graph search algorithm efficiently finds the duplicate functions in the netlist, making it a self-referencing attack. This proposed attack is extremely efficient and can determine the secret key within a few minutes. We have also proposed a countermeasure to make the circuit resilient against this topology-guided attack to progress towards a secure logic locking technique.",cs.CR,Cybersecurity
Adversarial Attacks and Detection on Reinforcement Learning-Based Interactive Recommender Systems,"Adversarial attacks pose significant challenges for detecting adversarial attacks at an early stage. We propose attack-agnostic detection on reinforcement learning-based interactive recommendation systems. We first craft adversarial examples to show their diverse distributions and then augment recommendation systems by detecting potential attacks with a deep learning-based classifier based on the crafted data. Finally, we study the attack strength and frequency of adversarial examples and evaluate our model on standard datasets with multiple crafting methods. Our extensive experiments show that most adversarial attacks are effective, and both attack strength and attack frequency impact the attack performance. The strategically-timed attack achieves comparative attack performance with only 1/3 to 1/2 attack frequency. Besides, our black-box detector trained with one crafting method has the generalization ability over several crafting methods.",cs.CR,Cybersecurity
Human Cognition through the Lens of Social Engineering Cyberattacks,"Social engineering cyberattacks are a major threat because they often prelude sophisticated and devastating cyberattacks. Social engineering cyberattacks are a kind of psychological attack that exploits weaknesses in human cognitive functions. Adequate defense against social engineering cyberattacks requires a deeper understanding of what aspects of human cognition are exploited by these cyberattacks, why humans are susceptible to these cyberattacks, and how we can minimize or at least mitigate their damage. These questions have received some amount of attention but the state-of-the-art understanding is superficial and scattered in the literature. In this paper, we review human cognition through the lens of social engineering cyberattacks. Then, we propose an extended framework of human cognitive functions to accommodate social engineering cyberattacks. We cast existing studies on various aspects of social engineering cyberattacks into the extended framework, while drawing a number of insights that represent the current understanding and shed light on future research directions. The extended framework might inspire future research endeavors towards a new sub-field that can be called Cybersecurity Cognitive Psychology, which tailors or adapts principles of Cognitive Psychology to the cybersecurity domain while embracing new notions and concepts that are unique to the cybersecurity domain.",cs.CR,Cybersecurity
Radium: Improving Dynamic PoW Targeting,"Most PoW blockchain protocols operate with a simple mechanism whereby a threshold is set for each block and miners generate block hashes until one of those values falls below the threshold. Although largely effective, this mechanism produces blocks at a highly variable rate and also leaves a blockchain susceptible to chain death, i.e. abandonment in the event that the threshold is set too high to attract any miners. A recent innovation called real-time block rate targeting, or RTT, fixes these problems by reducing the target throughout the mining interval. RTT exhibits much less variable block times and even features the ability to fully adjust the target after each block. However, as we show in this paper, RTT also suffers from a critical vulnerability whereby miners deviate form the protocol to increase their profits. We introduce the Radium protocol, which mitigates this vulnerability in RTT while retaining lower variance block times, responsive target adjustment, and lowering the risk of chain death. We also show that Radium's susceptibility to the doublespend attack and orphaned blocks remains similar to Bitcoin.",cs.CR,Cybersecurity
Hardware Implementation of Keyless Encryption Scheme for Internet of Things Based on Image of Memristors,"The Internet of Things (IoT) is rapidly increasing the number of connected devices. This causes new concerns towards solutions for authenticating numerous IoT devices. Most of these devices are resource-constrained. Therefore, the use of long-secret keys, in traditional cryptography schemes can be hard to implement. Also, the key generation, distribution, and storage are very complex. Moreover, the goal of many reported cyber-attacks is accessing the key. Therefore, researchers have shown an increased interest in designing keyless encryption schemes recently. In this report, we are going to explain the details of the implementation of the keyless protocol by taking advantage of known technology modules such as microcontrollers (MCU), and hash functions. Physical Unclonable Functions (PUFs) have been used in many cryptographic applications such as Password Management Systems, key exchange, Key Generation. In this report, we are going to explain the details of the hardware implementation of keyless encryption in the MCU. Different kinds of memristors have been used in the past. In this work, a look-up-table containing memristor cells value at the various current levels is used since the physical component is unavailable yet. The hardware that is used to implement the system is an evaluation-board of SAMV71 MCU, which is used to implement the control system and hardware hashing.",cs.CR,Cybersecurity
Understanding Object Detection Through An Adversarial Lens,"Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.",cs.CR,Cybersecurity
Approach for GDPR Compliant Detection of COVID-19 Infection Chains,"While prospect of tracking mobile devices' users is widely discussed all over European countries to counteract COVID-19 propagation, we propose a Bloom filter based construction providing users' location privacy and preventing mass surveillance. We apply a solution based on Bloom filters data structure that allows a third party, a government agency, to perform some privacy-preserving set relations on a mobile telco's access logfile. By computing set relations, the government agency, given the knowledge of two identified persons, has an instrument that provides a (possible) infection chain from the initial to the final infected user no matter at which location on a worldwide scale they are. The benefit of our approach is that intermediate possible infected users can be identified and subsequently contacted by the agency. With such approach, we state that solely identities of possible infected users will be revealed and location privacy of others will be preserved. To this extent, it meets General Data Protection Regulation (GDPR)requirements in this area.",cs.CR,Cybersecurity
MaxSAT Evaluation 2020 -- Benchmark: Identifying Maximum Probability Minimal Cut Sets in Fault Trees,This paper presents a MaxSAT benchmark focused on the identification of Maximum Probability Minimal Cut Sets (MPMCSs) in fault trees. We address the MPMCS problem by transforming the input fault tree into a weighted logical formula that is then used to build and solve a Weighted Partial MaxSAT problem. The benchmark includes 80 cases with fault trees of different size and composition as well as the optimal cost and solution for each case.,cs.CR,Cybersecurity
TurboCC: A Practical Frequency-Based Covert Channel With Intel Turbo Boost,"Covert channels are communication channels used by attackers to transmit information from a compromised system when the access control policy of the system does not allow doing so. Previous work has shown that CPU frequency scaling can be used as a covert channel to transmit information between otherwise isolated processes. Modern systems either try to save power or try to operate near their power limits in order to maximize performance, so they implement mechanisms to vary the frequency based on load. Existing covert channels based on this approach are either easily thwarted by software countermeasures or only work on completely idle systems. In this paper, we show how the automatic frequency scaling provided by Intel Turbo Boost can be used to construct a covert channel that is both hard to prevent without significant performance impact and can tolerate significant background system load. As Intel Turbo Boost selects the maximum CPU frequency based on the number of active cores, our covert channel modulates information onto the maximum CPU frequency by placing load on multiple additional CPU cores. Our prototype of the covert channel achieves a throughput of up to 61 bit/s on an idle system and up to 43 bit/s on a system with 25% utilization.",cs.CR,Cybersecurity
A Framework for Threats Analysis Using Software-Defined Networking,"The ability to analyze network threats is very important in security research. Traditional approaches, involving sandboxing technology are limited to simulating a single host, missing local network attacks. This issue is addressed by designing a threat analysis framework that uses software-defined networking for simulating arbitrary networks. The presented system offers flexibility, allowing a security researcher to define a virtual network that is able to capture malicious actions and to be restored to the initial state afterwards. Both the framework design and common usage scenarios are described. By providing this framework, we aim to ease the analysis effort in combating cyberthreats.",cs.CR,Cybersecurity
Vulnerability-Aware Resilient Networks: Software Diversity-based Network Adaptation,"By leveraging the principle of software polyculture to ensure security in a network, we proposed a vulnerability-based software diversity metric to determine how a network topology can be adapted to minimize security vulnerability while maintaining maximum network connectivity. Our proposed software diversity-based adaptation (SDA) scheme estimates a node's software diversity based on the vulnerabilities of software packages installed on other nodes on attack paths reachable to the node and employs it for edge adaptations, such as removing an edge with a neighboring node that exposes high security vulnerability because two connected nodes use the same software packages or a neighboring node may have high software vulnerability or adding an edge with another node with less or no security vulnerability because the two nodes use different software packages or have low vulnerabilities associated with them. To validate the proposed SDA scheme, we conducted extensive experiments comparing the proposed SDA scheme with counterpart baseline schemes in real networks. Our simulation experimental results proved the outperformance of our proposed SDA compared to the existing counterparts and provided insightful findings in terms of the effectiveness and efficiency of the proposed SDA scheme under three real network topologies with vastly different network density.",cs.CR,Cybersecurity
Identification and Correction of False Data Injection Attacks against AC State Estimation using Deep Learning,"recent literature has proposed various detection and identification methods for FDIAs, but few studies have focused on a solution that would prevent such attacks from occurring. However, great strides have been made using deep learning to detect attacks. Inspired by these advancements, we have developed a new methodology for not only identifying AC FDIAs but, more importantly, for correction as well. Our methodology utilizes a Long-Short Term Memory Denoising Autoencoder (LSTM-DAE) to correct attacked-estimated states based on the attacked measurements. The method was evaluated using the IEEE 30 system, and the experiments demonstrated that the proposed method was successfully able to identify the corrupted states and correct them with high accuracy.",cs.CR,Cybersecurity
A Survey of Distributed Denial of Service Attacks and Defenses,"A distributed denial-of-service (DDoS) attack is an attack wherein multiple compromised computer systems flood the bandwidth and/or resources of a target, such as a server, website or other network resource, and cause a denial of service for users of the targeted resource. The flood of incoming messages, connection requests or malformed packets to the target system forces it to slow down or even crash and shut down, thereby denying service to legitimate users or systems. This paper presents a literature review of DDoS attacks and the common defense mechanisms available. It also presents a literature review of the defenses for low-rate DDoS attacks that have not been handled effectively hitherto.",cs.CR,Cybersecurity
Blackbox Trojanising of Deep Learning Models : Using non-intrusive network structure and binary alterations,"Recent advancements in Artificial Intelligence namely in Deep Learning has heightened its adoption in many applications. Some are playing important roles to the extent that we are heavily dependent on them for our livelihood. However, as with all technologies, there are vulnerabilities that malicious actors could exploit. A form of exploitation is to turn these technologies, intended for good, to become dual-purposed instruments to support deviant acts like malicious software trojans. As part of proactive defense, researchers are proactively identifying such vulnerabilities so that protective measures could be developed subsequently. This research explores a novel blackbox trojanising approach using a simple network structure modification to any deep learning image classification model that would transform a benign model into a deviant one with a simple manipulation of the weights to induce specific types of errors. Propositions to protect the occurrence of such simple exploits are discussed in this research. This research highlights the importance of providing sufficient safeguards to these models so that the intended good of AI innovation and adoption may be protected.",cs.CR,Cybersecurity
On the Security of Networked Control Systems in Smart Vehicle and its Adaptive Cruise Control,"With the benefits of Internet of Vehicles (IoV) paradigm, come along unprecedented security challenges. Among many applications of inter-connected systems, vehicular networks and smart cars are examples that are already rolled out. Smart vehicles not only have networks connecting their internal components e.g. via Controller Area Network (CAN) bus, but also are connected to the outside world through road side units and other vehicles. In some cases, the internal and external network packets pass through the same hardware and are merely isolated by software defined rules. Any misconfiguration opens a window for the hackers to intrude into vehicles' internal components e.g. central lock system, Engine Control Unit (ECU), Anti-lock Braking System (ABS) or Adaptive Cruise Control (ACC) system. Compromise of any of these can lead to disastrous outcomes. In this paper, we study the security of smart vehicles' adaptive cruise control systems in the presence of covert attacks. We define two covert/stealth attacks in the context of cruise control and propose a novel intrusion detection and compensation method to disclose and respond to such attacks. More precisely, we focus on the covert cyber attacks that compromise the integrity of cruise controller and employ a neural network identifier in the IDS engine to estimate the system output dynamically and compare it against the ACC output. If any anomaly is detected, an embedded substitute controller kicks in and takes over the control. We conducted extensive experiments in MATLAB to evaluate the effectiveness of the proposed scheme in a simulated environment.",cs.CR,Cybersecurity
Computer and Network Security,"In the era of Internet of Things and with the explosive worldwide growth of electronic data volume, and associated need of processing, analysis and storage of such humongous volume of data, several new challenges are faced in protecting privacy of sensitive data and securing systems by designing novel schemes for secure authentication, integrity protection, encryption and non-repudiation. Lightweight symmetric key cryptography and adaptive network security algorithms are in demand for mitigating these challenges. This book presents some of the state-of-the-art research work in the field of cryptography and security in computing and communications. It is a valuable source of knowledge for researchers, engineers, practitioners, graduate and doctoral students who are working in the field of cryptography, network security and security and privacy issues in the Internet of Things (IoT), and machine learning application in security. It will also be useful for faculty members of graduate schools and universities.",cs.CR,Cybersecurity
Overview of Security of Virtual Mobile Networks,"5G is enabling different services over the same physical infrastructure through the concepts and technologies of virtualization, softwarization, slicing and cloud computing. Virtual Mobile Networks (VMNs), using these concepts, provide an opportunity to share the same physical infrastructure among multiple operators. Each VMN Operator (VMNO) can have own distinct operating and support systems. However, the technologies used to enable VMNs have their own explicit security challenges and solutions. The integrated environment built upon virtualization, softwarization, and cloudification, thus, will have complex security requirements and implications. In this vain, this article provides an overview of the security challenges and potential solutions for VMNs.",cs.CR,Cybersecurity
Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases,"When the training data are maliciously tampered, the predictions of the acquired deep neural network (DNN) can be manipulated by an adversary known as the Trojan attack (or poisoning backdoor attack). The lack of robustness of DNNs against Trojan attacks could significantly harm real-life machine learning (ML) systems in downstream applications, therefore posing widespread concern to their trustworthiness. In this paper, we study the problem of the Trojan network (TrojanNet) detection in the data-scarce regime, where only the weights of a trained DNN are accessed by the detector. We first propose a data-limited TrojanNet detector (TND), when only a few data samples are available for TrojanNet detection. We show that an effective data-limited TND can be established by exploring connections between Trojan attack and prediction-evasion adversarial attacks including per-sample attack as well as all-sample universal attack. In addition, we propose a data-free TND, which can detect a TrojanNet without accessing any data samples. We show that such a TND can be built by leveraging the internal response of hidden neurons, which exhibits the Trojan behavior even at random noise inputs. The effectiveness of our proposals is evaluated by extensive experiments under different model architectures and datasets including CIFAR-10, GTSRB, and ImageNet.",cs.CR,Cybersecurity
Measuring the Effectiveness of Privacy Policies for Voice Assistant Applications,"Voice Assistants (VA) such as Amazon Alexa and Google Assistant are quickly and seamlessly integrating into people's daily lives. The increased reliance on VA services raises privacy concerns such as the leakage of private conversations and sensitive information. Privacy policies play an important role in addressing users' privacy concerns and informing them about the data collection, storage, and sharing practices. VA platforms (both Amazon Alexa and Google Assistant) allow third-party developers to build new voice-apps and publish them to the app store. Voice-app developers are required to provide privacy policies to disclose their apps' data practices. However, little is known whether these privacy policies are informative and trustworthy or not on emerging VA platforms. On the other hand, many users invoke voice-apps through voice and thus there exists a usability challenge for users to access these privacy policies. In this paper, we conduct the first large-scale data analytics to systematically measure the effectiveness of privacy policies provided by voice-app developers on two mainstream VA platforms. We seek to understand the quality and usability issues of privacy policies provided by developers in the current app stores. We analyzed 64,720 Amazon Alexa skills and 2,201 Google Assistant actions. Our work also includes a user study to understand users' perspectives on VA's privacy policies. Our findings reveal a worrisome reality of privacy policies in two mainstream voice-app stores, where there exists a substantial number of problematic privacy policies. Surprisingly, Google and Amazon even have official voice-apps violating their own requirements regarding the privacy policy.",cs.CR,Cybersecurity
Multi-Armed Bandits with Local Differential Privacy,"This paper investigates the problem of regret minimization for multi-armed bandit (MAB) problems with local differential privacy (LDP) guarantee. In stochastic bandit systems, the rewards may refer to the users' activities, which may involve private information and the users may not want the agent to know. However, in many cases, the agent needs to know these activities to provide better services such as recommendations and news feeds. To handle this dilemma, we adopt differential privacy and study the regret upper and lower bounds for MAB algorithms with a given LDP guarantee. In this paper, we prove a lower bound and propose algorithms whose regret upper bounds match the lower bound up to constant factors. Numerical experiments also confirm our conclusions.",cs.CR,Cybersecurity
Smartphone Security Behavioral Scale: A New Psychometric Measurement for Smartphone Security,"Despite widespread use of smartphones, there is no measurement standard targeted at smartphone security behaviors. In this paper we translate a well-known cybersecurity behavioral scale into the smartphone domain and show that we can improve on this translation by following an established psychometrics approach surveying 1011 participants. We design a new 14-item Smartphone Security Behavioral Scale (SSBS) exhibiting high reliability and good fit to a two-component behavioural model based on technical versus social protection strategies. We then demonstrate how SSBS can be applied to measure the influence of mental health issues on smartphone security behavior intentions. We found significant correlations that predict SSBS profiles from three types of MHIs. Conversely, we are able to predict presence of MHIs using SSBS profiles.We obtain prediction AUCs of 72.1% for Internet addiction,75.8% for depression and 66.2% for insomnia.",cs.CR,Cybersecurity
"Smart Home, security concerns of IoT","The IoT (Internet of Things) has become widely popular in the domestic environments. People are renewing their homes into smart homes; however, the privacy concerns of owning many Internet connected devices with always-on environmental sensors remain insufficiently addressed. Default and weak passwords, cheap materials and hardware, and unencrypted communication are identified as the principal threats and vulnerabilities of IoT devices. Solutions and countermeasures are also provided: choosing a strong password, strong authentication mechanisms, check online databases of exposed or default credentials to mitigate the first threat; a selection of smart home devices from reputable companies and the implementation of the SDN for the Dos/DDoS threat; and finally IDS, HTTPS protocol and VPN for eavesdropping. The paper concludes dealing with a further challenge, ""the lack of technical support"", by which an auto-configuration approach should be analysed; this could both ease the installation/maintenance and enhance the security in the self configuration step of Smart Home devices.",cs.CR,Cybersecurity
Decentralized Lightweight Detection of Eclipse Attacks on Bitcoin Clients,"Clients of permissionless blockchain systems, like Bitcoin, rely on an underlying peer-to-peer network to send and receive transactions. It is critical that a client is connected to at least one honest peer, as otherwise the client can be convinced to accept a maliciously forked view of the blockchain. In such an eclipse attack, the client is unable to reliably distinguish the canonical view of the blockchain from the view provided by the attacker. The consequences of this can be catastrophic if the client makes business decisions based on a distorted view of the blockchain transactions. In this paper, we investigate the design space and propose two approaches for Bitcoin clients to detect whether an eclipse attack against them is ongoing. Each approach chooses a different trade-off between average attack detection time and network load. The first scheme is based on the detection of suspicious block timestamps. The second scheme allows blockchain clients to utilize their natural connections to the Internet (i.e., standard web activity) to gossip about their blockchain views with contacted servers and their other clients. Our proposals improve upon previously proposed eclipse attack countermeasures without introducing any dedicated infrastructure or changes to the Bitcoin protocol and network, and we discuss an implementation. We demonstrate the effectiveness of the gossip-based schemes through rigorous analysis using original Internet traffic traces and real-world deployment. The results indicate that our protocol incurs a negligible overhead and detects eclipse attacks rapidly with high probability, and is well-suited for practical deployment.",cs.CR,Cybersecurity
Steroids for DOPed Applications: A Compiler for Automated Data-Oriented Programming,"The wide-spread adoption of system defenses such as the randomization of code, stack, and heap raises the bar for code-reuse attacks. Thus, attackers utilize a scripting engine in target programs like a web browser to prepare the code-reuse chain, e.g., relocate gadget addresses or perform a just-in-time gadget search. However, many types of programs do not provide such an execution context that an attacker can use. Recent advances in data-oriented programming (DOP) explored an orthogonal way to abuse memory corruption vulnerabilities and demonstrated that an attacker can achieve Turing-complete computations without modifying code pointers in applications. As of now, constructing DOP exploits requires a lot of manual work.
  In this paper, we present novel techniques to automate the process of generating DOP exploits. We implemented a compiler called Steroids that compiles our high-level language SLANG into low-level DOP data structures driving malicious computations at run time. This enables an attacker to specify her intent in an application- and vulnerability-independent manner to maximize reusability. We demonstrate the effectiveness of our techniques and prototype implementation by specifying four programs of varying complexity in SLANG that calculate the Levenshtein distance, traverse a pointer chain to steal a private key, relocate a ROP chain, and perform a JIT-ROP attack. Steroids compiles each of those programs to low-level DOP data structures targeted at five different applications including GStreamer, Wireshark, and ProFTPd, which have vastly different vulnerabilities and DOP instances. Ultimately, this shows that our compiler is versatile, can be used for both 32- and 64-bit applications, works across bug classes, and enables highly expressive attacks without conventional code-injection or code-reuse techniques in applications lacking a scripting engine.",cs.CR,Cybersecurity
Scalable Role-based Access Control Using The EOS Blockchain,"Role-based access control (RBAC) policies represent the rights of subjects in terms of roles to access resources. This research proposes a scalable, flexible and auditable RBAC system using the EOS blockchain platform to meet the security requirements of organizations. The EOS blockchain platform for developing smart contract and decentralized applications (DAPPs) aims to address the scalability problem found in existing blockchain platforms. This smart contract platform aims to eliminate transaction fees while conducting millions of transactions per second. In our proposed approach, the EOS blockchain transparently stores RBAC policies. Administrative roles control access to resources at a higher level according to the way organisations perform operations. An organisation creates roles, role hierarchies and constraints to regulate user actions. Therefore, once an RBAC framework is established, the administrative user (issuer) only needs to grant and revoke roles to support changes in the organisational structure. Our proposed blockchain-based RBAC supports delegation capabilities using gaseless transactions which makes it adoptable and appealing in a large number of application scenarios. Our proposed solution is application-agnostic and well-suited for diverse use cases. Existing state-of-the art security frameworks are not suitable due to the difficulty of scale, higher cost and single point of failure. Consequently, organisations demand a scalable, cost-effective and lightweight access control solution which can better protect their privacy as well. A proof of concept implementation is developed based on the EOS blockchain. Our experimental results and analysis clearly show that our EOS blockchain-based RBAC outperforms existing blockchain platforms in terms of cost, latency, block generation time, contract execution time and throughput.",cs.CR,Cybersecurity
Overview of digital health surveillance system during COVID-19 pandemic: public health issues and misapprehensions,"Without proper medication and vaccination for the COVID-19, many governments are using automated digital healthcare surveillance system to prevent and control the spread. There is not enough literature explaining the concerns and privacy issues; hence, we have briefly explained the topics in this paper. We focused on digital healthcare surveillance system's privacy concerns and different segments. Further research studies should be conducted in different sectors. This paper provides an overview based on the published articles, which are not focusing on the privacy issues that much. Artificial intelligence and 5G networks combine the advanced digital healthcare surveillance system; whereas Bluetooth-based contact tracing systems have fewer privacy concerns. More studies are required to find the appropriate digital healthcare surveillance system, which would be ideal for monitoring, controlling, and predicting the COVID-19 trajectory.",cs.CR,Cybersecurity
Digital Surveillance Systems for Tracing COVID-19: Privacy and Security Challenges with Recommendations,"Coronavirus disease 2019, i.e. COVID-19 has imposed the public health measure of keeping social distancing for preventing mass transmission of COVID-19. For monitoring the social distancing and keeping the trace of transmission, we are obligated to develop various types of digital surveillance systems, which include contact tracing systems and drone-based monitoring systems. Due to the inconvenience of manual labor, traditional contact tracing systems are gradually replaced by the efficient automated contact tracing applications that are developed for smartphones. However, the commencement of automated contact tracing applications introduces the inevitable privacy and security challenges. Nevertheless, unawareness and/or lack of smartphone usage among mass people lead to drone-based monitoring systems. These systems also invite unwelcomed privacy and security challenges. This paper discusses the recently designed and developed digital surveillance system applications with their protocols deployed in several countries around the world. Their privacy and security challenges are discussed as well as analyzed from the viewpoint of privacy acts. Several recommendations are suggested separately for automated contact tracing systems and drone-based monitoring systems, which could further be explored and implemented afterwards to prevent any possible privacy violation and protect an unsuspecting person from any potential cyber attack.",cs.CR,Cybersecurity
A Proposed Access Control-Based Privacy Preservation Model to Share Healthcare Data in Cloud,"Healthcare data in cloud computing facilitates the treatment of patients efficiently by sharing information about personal health data between the healthcare providers for medical consultation. Furthermore, retaining the confidentiality of data and patients' identity is a another challenging task. This paper presents the concept of an access control-based (AC) privacy preservation model for the mutual authentication of users and data owners in the proposed digital system. The proposed model offers a high-security guarantee and high efficiency. The proposed digital system consists of four different entities, user, data owner, cloud server, and key generation center (KGC). This approach makes the system more robust and highly secure, which has been verified with multiple scenarios. Besides, the proposed model consisted of the setup phase, key generation phase, encryption phase, validation phase, access control phase, and data sharing phase. The setup phases are run by the data owner, which takes input as a security parameter and generates the system master key and security parameter. Then, in the key generation phase, the private key is generated by KGC and is stored in the cloud server. After that, the generated private key is encrypted. Then, the session key is generated by KGC and granted to the user and cloud server for storing, and then, the results are verified in the validation phase using validation messages. Finally, the data is shared with the user and decrypted at the user-end. The proposed model outperforms other methods with a maximal genuine data rate of 0.91.",cs.CR,Cybersecurity
Tempered Sigmoid Activations for Deep Learning with Differential Privacy,"Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. Instead, we propose that model architectures are chosen ab initio explicitly for privacy-preserving training.
  To provide guarantees under the gold standard of differential privacy, one must bound as strictly as possible how individual training points can possibly affect model updates. In this paper, we are the first to observe that the choice of activation function is central to bounding the sensitivity of privacy-preserving deep learning. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis.",cs.CR,Cybersecurity
Adversarial Privacy-preserving Filter,"While widely adopted in practical applications, face recognition has been critically discussed regarding the malicious use of face images and the potential privacy problems, e.g., deceiving payment system and causing personal sabotage. Online photo sharing services unintentionally act as the main repository for malicious crawler and face recognition applications. This work aims to develop a privacy-preserving solution, called Adversarial Privacy-preserving Filter (APF), to protect the online shared face images from being maliciously used.We propose an end-cloud collaborated adversarial attack solution to satisfy requirements of privacy, utility and nonaccessibility. Specifically, the solutions consist of three modules: (1) image-specific gradient generation, to extract image-specific gradient in the user end with a compressed probe model; (2) adversarial gradient transfer, to fine-tune the image-specific gradient in the server cloud; and (3) universal adversarial perturbation enhancement, to append image-independent perturbation to derive the final adversarial noise. Extensive experiments on three datasets validate the effectiveness and efficiency of the proposed solution. A prototype application is also released for further evaluation.We hope the end-cloud collaborated attack framework could shed light on addressing the issue of online multimedia sharing privacy-preserving issues from user side.",cs.CR,Cybersecurity
Privacy-Preserving Resilience of Cyber-Physical Systems to Adversaries,"A cyber-physical system (CPS) is expected to be resilient to more than one type of adversary. In this paper, we consider a CPS that has to satisfy a linear temporal logic (LTL) objective in the presence of two kinds of adversaries. The first adversary has the ability to tamper with inputs to the CPS to influence satisfaction of the LTL objective. The interaction of the CPS with this adversary is modeled as a stochastic game. We synthesize a controller for the CPS to maximize the probability of satisfying the LTL objective under any policy of this adversary. The second adversary is an eavesdropper who can observe labeled trajectories of the CPS generated from the previous step. It could then use this information to launch other kinds of attacks. A labeled trajectory is a sequence of labels, where a label is associated to a state and is linked to the satisfaction of the LTL objective at that state. We use differential privacy to quantify the indistinguishability between states that are related to each other when the eavesdropper sees a labeled trajectory. Two trajectories of equal length will be differentially private if they are differentially private at each state along the respective trajectories. We use a skewed Kantorovich metric to compute distances between probability distributions over states resulting from actions chosen according to policies from related states in order to quantify differential privacy. Moreover, we do this in a manner that does not affect the satisfaction probability of the LTL objective. We validate our approach on a simulation of a UAV that has to satisfy an LTL objective in an adversarial environment.",cs.CR,Cybersecurity
BlockFLow: An Accountable and Privacy-Preserving Solution for Federated Learning,"Federated learning enables the development of a machine learning model among collaborating agents without requiring them to share their underlying data. However, malicious agents who train on random data, or worse, on datasets with the result classes inverted, can weaken the combined model. BlockFLow is an accountable federated learning system that is fully decentralized and privacy-preserving. Its primary goal is to reward agents proportional to the quality of their contribution while protecting the privacy of the underlying datasets and being resilient to malicious adversaries. Specifically, BlockFLow incorporates differential privacy, introduces a novel auditing mechanism for model contribution, and uses Ethereum smart contracts to incentivize good behavior. Unlike existing auditing and accountability methods for federated learning systems, our system does not require a centralized test dataset, sharing of datasets between the agents, or one or more trusted auditors; it is fully decentralized and resilient up to a 50% collusion attack in a malicious trust model. When run on the public Ethereum blockchain, BlockFLow uses the results from the audit to reward parties with cryptocurrency based on the quality of their contribution. We evaluated BlockFLow on two datasets that offer classification tasks solvable via logistic regression models. Our results show that the resultant auditing scores reflect the quality of the honest agents' datasets. Moreover, the scores from dishonest agents are statistically lower than those from the honest agents. These results, along with the reasonable blockchain costs, demonstrate the effectiveness of BlockFLow as an accountable federated learning system.",cs.CR,Cybersecurity
Green-PoW: An Energy-Efficient Blockchain Proof-of-Work Consensus Algorithm,"This paper opts to mitigate the energy-inefficiency of the Blockchain Proof-of-Work (PoW) consensus algorithm by rationally repurposing the power spent during the mining process. The original PoW mining scheme is designed to consider one block at a time and assign a reward to the first place winner of a computation race. To reduce the mining-related energy consumption, we propose to compensate the computation effort of the runner(s)-up of a mining round, by granting them exclusivity of solving the upcoming block in the next round. This will considerably reduce the number of competing nodes in the next round and consequently, the consumed energy. Our proposed scheme divides time into epochs, where each comprises two mining rounds; in the first one, all network nodes can participate in the mining process, whereas in the second round only runners-up can take part. Thus, the overall mining energy consumption can be reduced to nearly $50\%$. To the best of our knowledge, our proposed scheme is the first to considerably improve the energy consumption of the original PoW algorithm. Our analysis demonstrates the effectiveness of our scheme in reducing energy consumption, the probability of fork occurrences, the level of mining centralization presented in the original PoW algorithm, and the effect of transaction censorship attack.",cs.CR,Cybersecurity
Automated Multi-Architectural Discovery of CFI-Resistant Code Gadgets,"Memory corruption vulnerabilities are still a severe threat for software systems. To thwart the exploitation of such vulnerabilities, many different kinds of defenses have been proposed in the past. Most prominently, Control-Flow Integrity (CFI) has received a lot of attention recently. Several proposals were published that apply coarse-grained policies with a low performance overhead. However, their security remains questionable as recent attacks have shown.
  To ease the assessment of a given CFI implementation, we introduce a framework to discover code gadgets for code-reuse attacks that conform to coarse-grained CFI policies. For this purpose, binary code is extracted and transformed to a symbolic representation in an architecture-independent manner. Additionally, code gadgets are verified to provide the needed functionality for a security researcher. We show that our framework finds more CFI-compatible gadgets compared to other code gadget discovery tools. Furthermore, we demonstrate that code gadgets needed to bypass CFI solutions on the ARM architecture can be discovered by our framework as well.",cs.CR,Cybersecurity
Epidemic Exposure Notification with Smartwatch: A Proximity-Based Privacy-Preserving Approach,"Businesses planning for the post-pandemic world are looking for innovative ways to protect the health and welfare of their employees and customers. Wireless technologies can play a key role in assisting contact tracing to quickly halt a local infection outbreak and prevent further spread. In this work, we present a wearable proximity and exposure notification solution based on a smartwatch that also promotes safe physical distancing in business, hospitality, or recreational facilities. Our proximity-based privacy-preserving contact tracing (P$^3$CT) leverages the Bluetooth Low Energy (BLE) technology for reliable proximity sensing, and an ambient signature protocol for preserving identity. Proximity sensing exploits the received signal strength (RSS) to detect the user's interaction and thus classifying them into low- or high-risk with respect to a patient diagnosed with an infectious disease. More precisely, a user is notified of their exposure based on their interactions, in terms of distance and time, with a patient. Our privacy-preserving protocol uses the ambient signatures to ensure that users' identities be anonymized. We demonstrate the feasibility of our proposed solution through extensive experimentation.",cs.CR,Cybersecurity
Short Paper: Privacy Comparison of Contact Tracing Mobile Applications for COVID-19,"With the COVID-19 pandemic, quarantines took place across the globe. In the aim of stopping or slowing the progression of the COVID-19 contamination, many countries have deployed a contact tracing system to notify persons that be in contact with a COVID-positive person. The contact tracing system is implemented in a mobile application and leverages technologies such as Bluetooth to trace interactions between persons. This paper discusses different smart-phone applications based on contact tracing system from privacy point of view.",cs.CR,Cybersecurity
Two attacks and counterattacks on the mutual semi-quantum key agreement protocol using Bell states,"Recently, a mutual semi-quantum key agreement protocol using Bell states is proposed by Yan et al. (Mod. Phys. Lett. A, 34, 1950294, 2019). The proposed protocol tries to help a quantum participant share a key with a classical participant who just has limited quantum capacities. Yan et al. claimed that both the participants have the same influence on the final shared key. However, this study points out that the classical participant can manipulate the final shared key by himself/herself without being detected. To solve this problem, an improved method is proposed here.",cs.CR,Cybersecurity
Concealed Communication in Online Social Networks,"Online social networks are used frequently by many people: Staying in contact with friends and sharing experiences with them is very important. However, users are increasingly concerned that their data will end up in the hands of strangers or that personal data may even be misused. Secure OSNs can help. These often use different types of encryption to keep the communication between the participants incomprehensible to outsiders. However, participants in such social networks cannot be sure that their data is secure. Various approaches show that even harmless-looking metadata, such as the number of contacts of users, can be evaluated to draw conclusions about the users and their communication. These attack methods are analyzed, and existing secure OSNs are examined, whether these attack methods can be utilized to violate the user's privacy. To prevent these privacy attacks, protocols for a secure centralized OSN are developed. Metadata is obscured in the presented OSM and end-to-end encryption is used for secure communication between clients. Additionally, communication channels are concealed using mix networks such that adversaries cannot determine which user is accessing which data or which user is communicating with whom even with access to the server.",cs.CR,Cybersecurity
Adversarial Patch Attacks on Monocular Depth Estimation Networks,"Thanks to the excellent learning capability of deep convolutional neural networks (CNN), monocular depth estimation using CNNs has achieved great success in recent years. However, depth estimation from a monocular image alone is essentially an ill-posed problem, and thus, it seems that this approach would have inherent vulnerabilities. To reveal this limitation, we propose a method of adversarial patch attack on monocular depth estimation. More specifically, we generate artificial patterns (adversarial patches) that can fool the target methods into estimating an incorrect depth for the regions where the patterns are placed. Our method can be implemented in the real world by physically placing the printed patterns in real scenes. We also analyze the behavior of monocular depth estimation under attacks by visualizing the activation levels of the intermediate layers and the regions potentially affected by the adversarial attack.",cs.CR,Cybersecurity
Fundamental Limits of Obfuscation for Linear Gaussian Dynamical Systems: An Information-Theoretic Approach,"In this paper, we study the fundamental limits of obfuscation in terms of privacy-distortion tradeoffs for linear Gaussian dynamical systems via an information-theoretic approach. Particularly, we obtain analytical formulas that capture the fundamental privacy-distortion tradeoffs when privacy masks are to be added to the outputs of the dynamical systems, while indicating explicitly how to design the privacy masks in an optimal way: The privacy masks should be colored Gaussian with power spectra shaped specifically based upon the system and noise properties.",cs.CR,Cybersecurity
Towards a certified reference monitor of the Android 10 permission system,"Android is a platform for mobile devices that captures more than 85% of the total market-share. Currently, mobile devices allow people to develop multiple tasks in different areas. Regrettably, the benefits of using mobile devices are counteracted by increasing security risks. The important and critical role of these systems makes them a prime target for formal verification. In our previous work (LNCS 10855, https://doi.org/10.1007/978-3-319-94460-9_16), we exhibited a formal specification of an idealized formulation of the permission model of version \texttt{6} of Android. In this paper we present an enhanced version of the model in the proof-assistant Coq, including the most relevant changes concerning the permission system introduced on versions Nougat, Oreo, Pie and 10. The properties that we had proved earlier for the security model has been either revalidated or refuted, and new ones have been formulated and proved. Additionally, we make observations on the security of the most recent versions of Android. Using the programming language of Coq we have developed a functional implementation of a reference validation mechanism and certified its correctness. The formal development is about 23k LOC of Coq, including proofs.",cs.CR,Cybersecurity
Unsupervised Intrusion Detection System for Unmanned Aerial Vehicle with Less Labeling Effort,"Along with the importance of safety, an IDS has become a significant task in the real world. Prior studies proposed various intrusion detection models for the UAV. Past rule-based approaches provided a concrete baseline IDS model, and the machine learning-based method achieved a precise intrusion detection performance on the UAV with supervised learning models. However, previous methods have room for improvement to be implemented in the real world. Prior methods required a large labeling effort on the dataset, and the model could not identify attacks that were not trained before. To jump over these hurdles, we propose an IDS with unsupervised learning. As unsupervised learning does not require labeling, our model let the practitioner not to label every type of attack from the flight data. Moreover, the model can identify an abnormal status of the UAV regardless of the type of attack. We trained an autoencoder with the benign flight data only and checked the model provides a different reconstruction loss at the benign flight and the flight under attack. We discovered that the model produces much higher reconstruction loss with the flight under attack than the benign flight; thus, this reconstruction loss can be utilized to recognize an intrusion to the UAV. With consideration of the computation overhead and the detection performance in the wild, we expect our model can be a concrete and practical baseline IDS on the UAV.",cs.CR,Cybersecurity
You Do (Not) Belong Here: Detecting DPI Evasion Attacks with Context Learning,"As Deep Packet Inspection (DPI) middleboxes become increasingly popular, a spectrum of adversarial attacks have emerged with the goal of evading such middleboxes. Many of these attacks exploit discrepancies between the middlebox network protocol implementations, and the more rigorous/complete versions implemented at end hosts. These evasion attacks largely involve subtle manipulations of packets to cause different behaviours at DPI and end hosts, to cloak malicious network traffic that is otherwise detectable. With recent automated discovery, it has become prohibitively challenging to manually curate rules for detecting these manipulations. In this work, we propose CLAP, the first fully-automated, unsupervised ML solution to accurately detect and localize DPI evasion attacks. By learning what we call the packet context, which essentially captures inter-relationships across both (1) different packets in a connection; and (2) different header fields within each packet, from benign traffic traces only, CLAP can detect and pinpoint packets that violate the benign packet contexts (which are the ones that are specially crafted for evasion purposes). Our evaluations with 73 state-of-the-art DPI evasion attacks show that CLAP achieves an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.963, an Equal Error Rate (EER) of only 0.061 in detection, and an accuracy of 94.6% in localization. These results suggest that CLAP can be a promising tool for thwarting DPI evasion attacks.",cs.CR,Cybersecurity
Total Eclipse of the Heart -- Disrupting the InterPlanetary File System,"Peer-to-peer networks are an attractive alternative to classical client-server architectures in several fields of application such as voice-over-IP telephony and file sharing. Recently, a new peer-to-peer solution called the InterPlanetary File System (IPFS) has attracted attention, which promises to re-decentralise the Web. Being increasingly used as a stand-alone application, IPFS has also emerged as the technical backbone of various other decentralised solutions and was even used to evade censorship. Decentralised applications serving millions of users rely on IPFS as one of their crucial building blocks. This popularity makes IPFS attractive for large-scale attacks. We have identified a conceptual issue in one of IPFS's core libraries and demonstrate their exploitation by means of a successful end-to-end attack. We evaluated this attack against the IPFS reference implementation on the public IPFS network, which is used by the average user to share and consume IPFS content. Results obtained from mounting this attack on live IPFS nodes show that arbitrary IPFS nodes can be eclipsed, i.e. isolated from the network, with moderate effort and limited resources. Compared to similar works, we show that our attack scales linearly even beyond current network sizes and can disrupt the entire public IPFS network with alarmingly low effort. The vulnerability set described in this paper has been assigned CVE-2020-10937. Responsible disclosure procedures are currently being carried out and have led to mitigations being deployed, with additional fixes to be rolled out in future releases.",cs.CR,Cybersecurity
"Computing Power, Key Length and Cryptanalysis. An Unending Battle?","There are several methods to measure computing power. On the other hand, Bit Length (BL) can be considered a metric to measure the strength of an asymmetric encryption method. We review here ways to determine the security, given an span of time, of a factoring-based encryption method, such as RSA, by establishing a relation between the processing power needed to break a given encryption and the given bit length used in the encryption. This relation would help us provide an estimation of the time span that an encryption method for a given BL will be secure from attacks.",cs.CR,Cybersecurity
MAD-VAE: Manifold Awareness Defense Variational Autoencoder,"Although deep generative models such as Defense-GAN and Defense-VAE have made significant progress in terms of adversarial defenses of image classification neural networks, several methods have been found to circumvent these defenses. Based on Defense-VAE, in our research we introduce several methods to improve the robustness of defense models. The methods introduced in this paper are straight forward yet show promise over the vanilla Defense-VAE. With extensive experiments on MNIST data set, we have demonstrated the effectiveness of our algorithms against different attacks. Our experiments also include attacks on the latent space of the defensive model. We also discuss the applicability of existing adversarial latent space attacks as they may have a significant flaw.",cs.CR,Cybersecurity
A Scalable Approach for Privacy-Preserving Collaborative Machine Learning,"We consider a collaborative learning scenario in which multiple data-owners wish to jointly train a logistic regression model, while keeping their individual datasets private from the other parties. We propose COPML, a fully-decentralized training framework that achieves scalability and privacy-protection simultaneously. The key idea of COPML is to securely encode the individual datasets to distribute the computation load effectively across many parties and to perform the training computations as well as the model updates in a distributed manner on the securely encoded data. We provide the privacy analysis of COPML and prove its convergence. Furthermore, we experimentally demonstrate that COPML can achieve significant speedup in training over the benchmark protocols. Our protocol provides strong statistical privacy guarantees against colluding parties (adversaries) with unbounded computational power, while achieving up to $16\times$ speedup in the training time against the benchmark protocols.",cs.CR,Cybersecurity
Verification of the IBOS Browser Security Properties in Reachability Logic,"This paper presents a rewriting logic specification of the Illinois Browser Operating System (IBOS) and defines several security properties, including the same-origin policy (SOP) in reachability logic. It shows how these properties can be deductively verified using our constructor-based reachability logic theorem prover. This paper also highlights the reasoning techniques used in the proof and three modularity principles that have been crucial to scale up and complete the verification effort.",cs.CR,Cybersecurity
SoK: Blockchain Solutions for Forensics,"As the digitization of information-intensive processes gains momentum in nowadays, the concern is growing about how to deal with the ever-growing problem of cybercrime. To this end, law enforcement officials and security firms use sophisticated digital forensics techniques for analyzing and investigating cybercrimes. However, multi-jurisdictional mandates, interoperability issues, the massive amount of evidence gathered (multimedia, text etc.) and multiple stakeholders involved (law enforcement agencies, security firms etc.) are just a few among the various challenges that hinder the adoption and implementation of sound digital forensics schemes. Blockchain technology has been recently proposed as a viable solution for developing robust digital forensics mechanisms. In this paper, we provide an overview and classification of the available blockchain-based digital forensic tools, and we further describe their main features. We also offer a thorough analysis of the various benefits and challenges of the symbiotic relationship between blockchain technology and the current digital forensics approaches, as proposed in the available literature. Based on the findings, we identify various research gaps, and we suggest future research directions that are expected to be of significant value both for academics and practitioners in the field of digital forensics.",cs.CR,Cybersecurity
Premium Access to Convolutional Neural Networks,"Neural Networks (NNs) are today used for all our daily tasks; for instance, in mobile phones. We here want to show how to restrict their access to privileged users. Our solution relies on a degraded implementation which can be corrected thanks to a PIN. We explain how to select a few parameters in an NN so as to maximize the gap in the accuracy between the premium and the degraded modes. We report experiments on an implementation of our proposal on a deep NN to prove its practicability.",cs.CR,Cybersecurity
Privacy-preserving Medical Treatment System through Nondeterministic Finite Automata,"In this paper, we propose a privacy-preserving medical treatment system using nondeterministic finite automata (NFA), hereafter referred to as P-Med, designed for the remote medical environment. P-Med makes use of the nondeterministic transition characteristic of NFA to flexibly represent the medical model, which includes illness states, treatment methods and state transitions caused by exerting different treatment methods. A medical model is encrypted and outsourced to the cloud to deliver telemedicine services. Using P-Med, patient-centric diagnosis and treatment can be made on-the-fly while protecting the confidentiality of a patient's illness states and treatment recommendation results. Moreover, a new privacy-preserving NFA evaluation method is given in P-Med to get a confidential match result for the evaluation of an encrypted NFA and an encrypted data set, which avoids the cumbersome inner state transition determination. We demonstrate that P-Med realizes treatment procedure recommendation without privacy leakage to unauthorized parties. We conduct extensive experiments and analyses to evaluate efficiency.",cs.CR,Cybersecurity
Adversarial Robustness of Deep Convolutional Candlestick Learner,"Deep learning (DL) has been applied extensively in a wide range of fields. However, it has been shown that DL models are susceptible to a certain kinds of perturbations called \emph{adversarial attacks}. To fully unlock the power of DL in critical fields such as financial trading, it is necessary to address such issues. In this paper, we present a method of constructing perturbed examples and use these examples to boost the robustness of the model. Our algorithm increases the stability of DL models for candlestick classification with respect to perturbations in the input data.",cs.CR,Cybersecurity
Unique properties of adversarially trained linear classifiers on Gaussian data,"Machine learning models are vulnerable to adversarial perturbations, that when added to an input, can cause high confidence misclassifications. The adversarial learning research community has made remarkable progress in the understanding of the root causes of adversarial perturbations. However, most problems that one may consider important to solve for the deployment of machine learning in safety critical tasks involve high dimensional complex manifolds that are difficult to characterize and study. It is common to develop adversarially robust learning theory on simple problems, in the hope that insights will transfer to `real world datasets'. In this work, we discuss a setting where this approach fails. In particular, we show with a linear classifier, it is always possible to solve a binary classification problem on Gaussian data under arbitrary levels of adversarial corruption during training, and that this property is not observed with non-linear classifiers on the CIFAR-10 dataset.",cs.CR,Cybersecurity
SCARL: Side-Channel Analysis with Reinforcement Learning on the Ascon Authenticated Cipher,"Existing side-channel analysis techniques require a leakage model, in the form of a prior knowledge or a set of training data, to establish a relationship between the secret data and the measurements. We introduce side-channel analysis with reinforcement learning (SCARL) capable of extracting data-dependent features of the measurements in an unsupervised learning approach without requiring a prior knowledge on the leakage model. SCARL consists of an auto-encoder to encode the information of power measurements into an internal representation, and a reinforcement learning algorithm to extract information about the secret data. We employ a reinforcement learning algorithm with actor-critic networks, to identify the proper leakage model that results in maximum inter-cluster separation of the auto-encoder representation. SCARL assumes that the lower order components of a generic non-linear leakage model have larger contribution to the leakage of sensitive data. On a lightweight implementation of the Ascon authenticated cipher on the Artix-7 FPGA, SCARL is able to recover the secret key using 24K power traces during the key insertion, or Initialization Stage, of the cipher. We also demonstrate that classical techniques such as DPA and CPA fail to identify the correct key using traditional linear leakage models and more than 40K power traces.",cs.CR,Cybersecurity
Performance Evaluation and Modeling of Cryptographic Libraries for MPI Communications,"In order for High-Performance Computing (HPC) applications with data security requirements to execute in the public cloud, the cloud infrastructure must ensure the privacy and integrity of data. To meet this goal, we consider incorporating encryption in the Message Passing Interface (MPI) library. We empirically evaluate four contemporary cryptographic libraries, OpenSSL, BoringSSL, Libsodium, and CryptoPP using micro-benchmarks and NAS parallel benchmarks on two different networking technologies, 10Gbps Ethernet and 40Gbps InfiniBand. We also develop accurate models that allow us to reason about the performance of encrypted MPI communication in different situations and give guidance on how to improve encrypted MPI performance.",cs.CR,Cybersecurity
COVID-19 Imaging Data Privacy by Federated Learning Design: A Theoretical Framework,"To address COVID-19 healthcare challenges, we need frequent sharing of health data, knowledge and resources at a global scale. However, in this digital age, data privacy is a big concern that requires the secure embedding of privacy assurance into the design of all technological solutions that use health data. In this paper, we introduce differential privacy by design (dPbD) framework and discuss its embedding into the federated machine learning system. To limit the scope of our paper, we focus on the problem scenario of COVID-19 imaging data privacy for disease diagnosis by computer vision and deep learning approaches. We discuss the evaluation of the proposed design of federated machine learning systems and discuss how differential privacy by design (dPbD) framework can enhance data privacy in federated learning systems with scalability and robustness. We argue that scalable differentially private federated learning design is a promising solution for building a secure, private and collaborative machine learning model such as required to combat COVID19 challenge.",cs.CR,Cybersecurity
S3ML: A Secure Serving System for Machine Learning Inference,"We present S3ML, a secure serving system for machine learning inference in this paper. S3ML runs machine learning models in Intel SGX enclaves to protect users' privacy. S3ML designs a secure key management service to construct flexible privacy-preserving server clusters and proposes novel SGX-aware load balancing and scaling methods to satisfy users' Service-Level Objectives. We have implemented S3ML based on Kubernetes as a low-overhead, high-available, and scalable system. We demonstrate the system performance and effectiveness of S3ML through extensive experiments on a series of widely-used models.",cs.CR,Cybersecurity
CrypTFlow2: Practical 2-Party Secure Inference,"We present CrypTFlow2, a cryptographic framework for secure inference over realistic Deep Neural Networks (DNNs) using secure 2-party computation. CrypTFlow2 protocols are both correct -- i.e., their outputs are bitwise equivalent to the cleartext execution -- and efficient -- they outperform the state-of-the-art protocols in both latency and scale. At the core of CrypTFlow2, we have new 2PC protocols for secure comparison and division, designed carefully to balance round and communication complexity for secure inference tasks. Using CrypTFlow2, we present the first secure inference over ImageNet-scale DNNs like ResNet50 and DenseNet121. These DNNs are at least an order of magnitude larger than those considered in the prior work of 2-party DNN inference. Even on the benchmarks considered by prior work, CrypTFlow2 requires an order of magnitude less communication and 20x-30x less time than the state-of-the-art.",cs.CR,Cybersecurity
Security and Privacy Considerations for Machine Learning Models Deployed in the Government and Public Sector (white paper),"As machine learning becomes a more mainstream technology, the objective for governments and public sectors is to harness the power of machine learning to advance their mission by revolutionizing public services. Motivational government use cases require special considerations for implementation given the significance of the services they provide. Not only will these applications be deployed in a potentially hostile environment that necessitates protective mechanisms, but they are also subject to government transparency and accountability initiatives which further complicates such protections.
  In this paper, we describe how the inevitable interactions between a user of unknown trustworthiness and the machine learning models, deployed in governments and public sectors, can jeopardize the system in two major ways: by compromising the integrity or by violating the privacy. We then briefly overview the possible attacks and defense scenarios, and finally, propose recommendations and guidelines that once considered can enhance the security and privacy of the provided services.",cs.CR,Cybersecurity
Improved Fault Analysis on SIMECK Ciphers,"The advances of the Internet of Things (IoT) have had a fundamental impact and influence in sharping our rich living experiences. However, since IoT devices are usually resource-constrained, lightweight block ciphers have played a major role in serving as a building block for secure IoT protocols. In CHES 2015, SIMECK, a family of block ciphers, was designed for resource-constrained IoT devices. Since its publication, there have been many analyses on its security. In this paper, under the one bit-flip model, we propose a new efficient fault analysis attack on SIMECK ciphers. Compared to those previously reported attacks, our attack can recover the full master key by injecting faults into only a single round of all SIMECK family members. This property is crucial, as it is infeasible for an attacker to inject faults into different rounds of a SIMECK implementation on IoT devices in the real world. Specifically, our attack is characterized by exercising a deep analysis of differential trail between the correct and faulty immediate ciphertexts. Extensive simulation evaluations are conducted, and the results demonstrate the effectiveness and correctness of our proposed attack.",cs.CR,Cybersecurity
MMH* with arbitrary modulus is always almost-universal,"Universal hash functions, discovered by Carter and Wegman in 1979, are of great importance in computer science with many applications. MMH$^*$ is a well-known $\triangle$-universal hash function family, based on the evaluation of a dot product modulo a prime. In this paper, we introduce a generalization of MMH$^*$, that we call GMMH$^*$, using the same construction as MMH$^*$ but with an arbitrary integer modulus $n>1$, and show that GMMH$^*$ is $\frac{1}{p}$-almost-$\triangle$-universal, where $p$ is the smallest prime divisor of $n$. This bound is tight.",cs.CR,Cybersecurity
Scalable Attack-Resistant Obfuscation of Logic Circuits,"Hardware IP protection has been one of the most critical areas of research in the past years. Recently, attacks on hardware IPs (such as reverse engineering or cloning) have evolved as attackers have developed sophisticated techniques. Therefore, hardware obfuscation has been introduced as a powerful tool to protect IPs against piracy attacks. However, many recent attempts to break existing obfuscation methods have been successful in unlocking the IP and restoring its functionality. In this paper, we propose SARO, a Scalable Attack-Resistant Obfuscation that provides a robust functional and structural design transformation process. SARO treats the target circuit as a graph, and performs a partitioning algorithm to produce a set of sub-graphs, then applies our novel Truth Table Transformation (T3) process to each partition. We also propose the $T3_{metric}$, which is developed to quantify the structural and functional design transformation level caused by the obfuscation process. We evaluate SARO on ISCAS85 and EPFL benchmarks, and provide full security and performance analysis of our proposed framework.",cs.CR,Cybersecurity
Low Latency Cross-Shard Transactions in Coded Blockchain,"Although blockchain, the supporting technology of Bitcoin and various cryptocurrencies, has offered a potentially effective framework for numerous applications, it still suffers from the adverse affects of the impossibility triangle. Performance, security, and decentralization of blockchains normally do not scale simultaneously with the number of participants in the network. The recent introduction of error correcting codes in sharded blockchain by Li et al. partially settles this trilemma, boosting throughput without compromising security and decentralization. In this paper, we improve the coded sharding scheme in three ways. First, we propose a novel 2-Dimensional Sharding strategy, which inherently supports cross-shard transactions, alleviating the need for complicated inter-shard communication protocols. Second, we employ distributed storage techniques in the propagation of blocks, improving latency under restricted bandwidth. Finally, we incorporate polynomial cryptographic primitives of low degree, which brings coded blockchain techniques into the realm of feasible real-world parameters.",cs.CR,Cybersecurity
Differentially Private ADMM Algorithms for Machine Learning,"In this paper, we study efficient differentially private alternating direction methods of multipliers (ADMM) via gradient perturbation for many machine learning problems. For smooth convex loss functions with (non)-smooth regularization, we propose the first differentially private ADMM (DP-ADMM) algorithm with performance guarantee of $(,)$-differential privacy ($(,)$-DP). From the viewpoint of theoretical analysis, we use the Gaussian mechanism and the conversion relationship between Rnyi Differential Privacy (RDP) and DP to perform a comprehensive privacy analysis for our algorithm. Then we establish a new criterion to prove the convergence of the proposed algorithms including DP-ADMM. We also give the utility analysis of our DP-ADMM. Moreover, we propose an accelerated DP-ADMM (DP-AccADMM) with the Nesterov's acceleration technique. Finally, we conduct numerical experiments on many real-world datasets to show the privacy-utility tradeoff of the two proposed algorithms, and all the comparative analysis shows that DP-AccADMM converges faster and has a better utility than DP-ADMM, when the privacy budget $$ is larger than a threshold.",cs.CR,Cybersecurity
Improving Bitcoin Transaction Propagation by Leveraging Unreachable Nodes,"The Bitcoin P2P network is at the core of all communications between clients. The reachable part of this network has been explored and analyzed by numerous studies. Unreachable nodes, however, are, in most part, overlooked. Nonetheless, they are a relevant part of the network and play an essential role in the propagation of messages. In this paper, we focus on transaction propagation and show that increasing the participation of unreachable nodes can potentially improve the robustness and efficiency of the network. In order to do that, we propose a few changes to the network protocol. Additionally, we design a novel transaction propagation protocol that explicitly involves unreachable nodes to provide better protection against deanonymization attacks. Our solutions are simple to implement and can effectively bring immediate benefits to the Bitcoin network.",cs.CR,Cybersecurity
How to Not Get Caught When You Launder Money on Blockchain?,"The number of blockchain users has tremendously grown in recent years. As an unintended consequence, e-crime transactions on blockchains has been on the rise. Consequently, public blockchains have become a hotbed of research for developing AI tools to detect and trace users and transactions that are related to e-crime.
  We argue that following a few select strategies can make money laundering on blockchain virtually undetectable with most of the existing tools and algorithms. As a result, the effective combating of e-crime activities involving cryptocurrencies requires the development of novel analytic methodology in AI.",cs.CR,Cybersecurity
Differential Privacy and Natural Language Processing to Generate Contextually Similar Decoy Messages in Honey Encryption Scheme,"Honey Encryption is an approach to encrypt the messages using low min-entropy keys, such as weak passwords, OTPs, PINs, credit card numbers. The ciphertext is produces, when decrypted with any number of incorrect keys, produces plausible-looking but bogus plaintext called ""honey messages"". But the current techniques used in producing the decoy plaintexts do not model human language entirely. A gibberish, random assortment of words is not enough to fool an attacker; that will not be acceptable and convincing, whether or not the attacker knows some information of the genuine source.
  In this paper, I focus on the plaintexts which are some non-numeric informative messages. In order to fool the attacker into believing that the decoy message can actually be from a certain source, we need to capture the empirical and contextual properties of the language. That is, there should be no linguistic difference between real and fake message, without revealing the structure of the real message. I employ natural language processing and generalized differential privacy to solve this problem. Mainly I focus on machine learning methods like keyword extraction, context classification, bags-of-words, word embeddings, transformers for text processing to model privacy for text documents. Then I prove the security of this approach with e-differential privacy.",cs.CR,Cybersecurity
Garou: An Efficient and Secure Off-Blockchain Multi-Party Payment Hub,"To mitigate the scalability problem of decentralized cryptocurrencies such as Bitcoin and Ethereum, the payment channel, which allows two parties to perform secure coin transfers without involving the blockchain, has been proposed. The payment channel increases the transaction throughput of two parties to a level that is only limited by their network bandwidth. Recent proposals focus on extending the two-party payment channel to the N-party payment hub. Unfortunately, none of them can achieve efficiency, flexibility in the absence of a trusted third-party. In this paper, we propose Garou, a secure N-party payment hub that allows multiple parties to perform secure off-chain coin transfers. Except in the case of disputes, participants within the payment hub can make concurrent and direct coin transfers with each other without the involvement of the blockchain or any third-party intermediaries. This allows Garou to achieve both high-performance and flexibility. Garou also guarantees that an honest party always maintains its balance security against strong adversarial capabilities. To demonstrate the feasibility of the Garou protocol, we develop a proof of concept prototype for the Ethereum network. Our evaluation results show that the maximum transaction throughput of Garou is 20 times higher than that of state-of-art payment hubs.",cs.CR,Cybersecurity
EnCoD: Distinguishing Compressed and Encrypted File Fragments,"Reliable identification of encrypted file fragments is a requirement for several security applications, including ransomware detection, digital forensics, and traffic analysis. A popular approach consists of estimating high entropy as a proxy for randomness. However, many modern content types (e.g. office documents, media files, etc.) are highly compressed for storage and transmission efficiency. Compression algorithms also output high-entropy data, thus reducing the accuracy of entropy-based encryption detectors. Over the years, a variety of approaches have been proposed to distinguish encrypted file fragments from high-entropy compressed fragments. However, these approaches are typically only evaluated over a few, select data types and fragment sizes, which makes a fair assessment of their practical applicability impossible. This paper aims to close this gap by comparing existing statistical tests on a large, standardized dataset. Our results show that current approaches cannot reliably tell apart encryption and compression, even for large fragment sizes. To address this issue, we design EnCoD, a learning-based classifier which can reliably distinguish compressed and encrypted data, starting with fragments as small as 512 bytes. We evaluate EnCoD against current approaches over a large dataset of different data types, showing that it outperforms current state-of-the-art for most considered fragment sizes and data types.",cs.CR,Cybersecurity
Federated Learning in Adversarial Settings,"Federated Learning enables entities to collaboratively learn a shared prediction model while keeping their training data locally. It prevents data collection and aggregation and, therefore, mitigates the associated privacy risks. However, it still remains vulnerable to various security attacks where malicious participants aim at degrading the generated model, inserting backdoors, or inferring other participants' training data. This paper presents a new federated learning scheme that provides different trade-offs between robustness, privacy, bandwidth efficiency, and model accuracy. Our scheme uses biased quantization of model updates and hence is bandwidth efficient. It is also robust against state-of-the-art backdoor as well as model degradation attacks even when a large proportion of the participant nodes are malicious. We propose a practical differentially private extension of this scheme which protects the whole dataset of participating entities. We show that this extension performs as efficiently as the non-private but robust scheme, even with stringent privacy requirements but are less robust against model degradation and backdoor attacks. This suggests a possible fundamental trade-off between Differential Privacy and robustness.",cs.CR,Cybersecurity
"""Healthy surveillance"": Designing a concept for privacy-preserving mask recognition AI in the age of pandemics","The obligation to wear masks in times of pandemics reduces the risk of spreading viruses. In case of the COVID-19 pandemic in 2020, many governments recommended or even obligated their citizens to wear masks as an effective countermeasure. In order to continuously monitor the compliance of this policy measure in public spaces like restaurants or tram stations by public authorities, one scalable and automatable option depicts the application of surveillance systems, i.e., CCTV. However, large-scale monitoring of mask recognition does not only require a well-performing Artificial Intelligence, but also ensure that no privacy issues are introduced, as surveillance is a deterrent for citizens and regulations like General Data Protection Regulation (GDPR) demand strict regulations of such personal data. In this work, we show how a privacy-preserving mask recognition artifact could look like, demonstrate different options for implementation and evaluate performances. Our conceptual deep-learning based Artificial Intelligence is able to achieve detection performances between 95% and 99% in a privacy-friendly setting. On that basis, we elaborate on the trade-off between the level of privacy preservation and Artificial Intelligence performance, i.e. the ""price of privacy"".",cs.CR,Cybersecurity
Contact Tracing Made Un-relay-able,"Automated contact tracing is a key solution to control the spread of airborne transmittable diseases: it traces contacts among individuals in order to alert people about their potential risk of being infected. The current SARS-CoV-2 pandemic put a heavy strain on the healthcare system of many countries. Governments chose different approaches to face the spread of the virus and the contact tracing apps were considered the most effective ones. In particular, by leveraging on the Bluetooth Low-Energy technology, mobile apps allow to achieve a privacy-preserving contact tracing of citizens. While researchers proposed several contact tracing approaches, each government developed its own national contact tracing app.
  In this paper, we demonstrate that many popular contact tracing apps (e.g., the ones promoted by the Italian, French, Swiss government) are vulnerable to relay attacks. Through such attacks people might get misleadingly diagnosed as positive to SARS-CoV-2, thus being enforced to quarantine and eventually leading to a breakdown of the healthcare system. To tackle this vulnerability, we propose a novel and lightweight solution that prevents relay attacks, while providing the same privacy-preserving features as the current approaches. To evaluate the feasibility of both the relay attack and our novel defence mechanism, we developed a proof of concept against the Italian contact tracing app (i.e., Immuni). The design of our defence allows it to be integrated into any contact tracing app.",cs.CR,Cybersecurity
Private Outsourced Bayesian Optimization,"This paper presents the private-outsourced-Gaussian process-upper confidence bound (PO-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our PO-GP-UCB algorithm. We empirically evaluate the performance of our PO-GP-UCB algorithm with synthetic and real-world datasets.",cs.CR,Cybersecurity
The Benefits of Deploying Smart Contracts on Trusted Third Parties,"The hype about Bitcoin has overrated the potential of smart contracts deployed on-blockchains (on-chains) and underrated the potential of smart contracts deployed on-Trusted Third Parties (on-TTPs). As a result, current research and development in this field is focused mainly on smart contract applications that use on-chain smart contracts. We argue that there is a large class of smart contract applications where on-TTP smart contracts are a better alternative. The problem with on-chain smart contracts is that the fully decentralised model and indelible append-only data model followed by blockchains introduces several engineering problems that are hard to solve. In these situations, the inclusion of a TTP (assuming that the application can tolerate its inconveniences) instead of a blockchain to host the smart contract simplifies the problems and offers pragmatic solutions. The intention and contribution of this paper is to shed some light on this issue. We use a hypothetical use case of a car insurance application to illustrate technical problems that are easier to solve with on-TTP smart contracts than with on-chain smart contracts.",cs.CR,Cybersecurity
Are Adversarial Examples Created Equal? A Learnable Weighted Minimax Risk for Robustness under Non-uniform Attacks,"Adversarial Training is proved to be an efficient method to defend against adversarial examples, being one of the few defenses that withstand strong attacks. However, traditional defense mechanisms assume a uniform attack over the examples according to the underlying data distribution, which is apparently unrealistic as the attacker could choose to focus on more vulnerable examples. We present a weighted minimax risk optimization that defends against non-uniform attacks, achieving robustness against adversarial examples under perturbed test data distributions. Our modified risk considers importance weights of different adversarial examples and focuses adaptively on harder examples that are wrongly classified or at higher risk of being classified incorrectly. The designed risk allows the training process to learn a strong defense through optimizing the importance weights. The experiments show that our model significantly improves state-of-the-art adversarial accuracy under non-uniform attacks without a significant drop under uniform attacks.",cs.CR,Cybersecurity
DualNet: Locate Then Detect Effective Payload with Deep Attention Network,"Network intrusion detection (NID) is an essential defense strategy that is used to discover the trace of suspicious user behaviour in large-scale cyberspace, and machine learning (ML), due to its capability of automation and intelligence, has been gradually adopted as a mainstream hunting method in recent years. However, traditional ML based network intrusion detection systems (NIDSs) are not effective to recognize unknown threats and their high detection rate often comes with the cost of high false alarms, which leads to the problem of alarm fatigue. To address the above problems, in this paper, we propose a novel neural network based detection system, DualNet, which is constructed with a general feature extraction stage and a crucial feature learning stage. DualNet can rapidly reuse the spatial-temporal features in accordance with their importance to facilitate the entire learning process and simultaneously mitigate several optimization problems occurred in deep learning (DL). We evaluate the DualNet on two benchmark cyber attack datasets, NSL-KDD and UNSW-NB15. Our experiment shows that DualNet outperforms classical ML based NIDSs and is more effective than existing DL methods for NID in terms of accuracy, detection rate and false alarm rate.",cs.CR,Cybersecurity
Knowledge Discovery in Cryptocurrency Transactions: A Survey,"Cryptocurrencies gain trust in users by publicly disclosing the full creation and transaction history. In return, the transaction history faithfully records the whole spectrum of cryptocurrency user behaviors. This article analyzes and summarizes the existing research on knowledge discovery in the cryptocurrency transactions using data mining techniques. Specifically, we classify the existing research into three aspects, i.e., transaction tracings and blockchain address linking, the analyses of collective user behaviors, and the study of individual user behaviors. For each aspect, we present the problems, summarize the methodologies, and discuss major findings in the literature. Furthermore, an enumeration of transaction data parsing and visualization tools and services is also provided. Finally, we outline several future directions in this research area, such as the current rapid development of Decentralized Finance (De-Fi) and digital fiat money.",cs.CR,Cybersecurity
DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles,"Recent research finds CNN models for image classification demonstrate overlapped adversarial vulnerabilities: adversarial attacks can mislead CNN models with small perturbations, which can effectively transfer between different models trained on the same dataset. Adversarial training, as a general robustness improvement technique, eliminates the vulnerability in a single model by forcing it to learn robust features. The process is hard, often requires models with large capacity, and suffers from significant loss on clean data accuracy. Alternatively, ensemble methods are proposed to induce sub-models with diverse outputs against a transfer adversarial example, making the ensemble robust against transfer attacks even if each sub-model is individually non-robust. Only small clean accuracy drop is observed in the process. However, previous ensemble training methods are not efficacious in inducing such diversity and thus ineffective on reaching robust ensemble. We propose DVERGE, which isolates the adversarial vulnerability in each sub-model by distilling non-robust features, and diversifies the adversarial vulnerability to induce diverse outputs against a transfer attack. The novel diversity metric and training procedure enables DVERGE to achieve higher robustness against transfer attacks comparing to previous ensemble methods, and enables the improved robustness when more sub-models are added to the ensemble. The code of this work is available at https://github.com/zjysteven/DVERGE",cs.CR,Cybersecurity
Differentially Private Representation for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness,"It has been demonstrated that hidden representation learned by a deep model can encode private information of the input, hence can be exploited to recover such information with reasonable accuracy. To address this issue, we propose a novel approach called Differentially Private Neural Representation (DPNR) to preserve the privacy of the extracted representation from text. DPNR utilises Differential Privacy (DP) to provide a formal privacy guarantee. Further, we show that masking words via dropout can further enhance privacy. To maintain utility of the learned representation, we integrate DP-noisy representation into a robust training process to derive a robust target model, which also helps for model fairness over various demographic variables. Experimental results on benchmark datasets under various parameter settings demonstrate that DPNR largely reduces privacy leakage without significantly sacrificing the main task performance.",cs.CR,Cybersecurity
Secure Collaborative Training and Inference for XGBoost,"In recent years, gradient boosted decision tree learning has proven to be an effective method of training robust models. Moreover, collaborative learning among multiple parties has the potential to greatly benefit all parties involved, but organizations have also encountered obstacles in sharing sensitive data due to business, regulatory, and liability concerns.
  We propose Secure XGBoost, a privacy-preserving system that enables multiparty training and inference of XGBoost models. Secure XGBoost protects the privacy of each party's data as well as the integrity of the computation with the help of hardware enclaves. Crucially, Secure XGBoost augments the security of the enclaves using novel data-oblivious algorithms that prevent access side-channel attacks on enclaves induced via access pattern leakage.",cs.CR,Cybersecurity
Why Older Adults (Don't) Use Password Managers,"Password managers (PMs) are considered highly effective tools for increasing security, and a recent study by Pearman et al. (SOUPS'19) highlighted the motivations and barriers to adopting PMs. We expand these findings by replicating Pearman et al.'s protocol and interview instrument applied to a sample of strictly older adults (>60 years of age), as the prior work focused on a predominantly younger cohort. We conducted n=26 semi-structured interviews with PM users, built-in browser/operating system PM users, and non-PM users. The average participant age was 70.4 years. Using the same codebook from Pearman et al., we showcase differences and similarities in PM adoption between the samples, including fears of a single point of failure and the importance of having control over one's private information. Meanwhile, older adults were found to have higher mistrust of cloud storage of passwords and cross-device synchronization. We also highlight PM adoption motivators for older adults, including the power of recommendations from family members and the importance of education and outreach to improve familiarity.",cs.CR,Cybersecurity
FaceLeaks: Inference Attacks against Transfer Learning Models via Black-box Queries,"Transfer learning is a useful machine learning framework that allows one to build task-specific models (student models) without significantly incurring training costs using a single powerful model (teacher model) pre-trained with a large amount of data. The teacher model may contain private data, or interact with private inputs. We investigate if one can leak or infer such private information without interacting with the teacher model directly. We describe such inference attacks in the context of face recognition, an application of transfer learning that is highly sensitive to personal privacy.
  Under black-box and realistic settings, we show that existing inference techniques are ineffective, as interacting with individual training instances through the student models does not reveal information about the teacher. We then propose novel strategies to infer from aggregate-level information. Consequently, membership inference attacks on the teacher model are shown to be possible, even when the adversary has access only to the student models.
  We further demonstrate that sensitive attributes can be inferred, even in the case where the adversary has limited auxiliary information. Finally, defensive strategies are discussed and evaluated. Our extensive study indicates that information leakage is a real privacy threat to the transfer learning framework widely used in real-life situations.",cs.CR,Cybersecurity
Lattice-based IBE with Equality Test Supporting Flexible Authorization in the Standard Model,"Identity-based encryption with equality test supporting flexible authorization (IBEET-FA) allows the equality test of underlying messages of two ciphertexts while strengthens privacy protection by allowing users (identities) to control the comparison of their ciphertexts with others. IBEET by itself has a wide range of useful applicable domain such as keyword search on encrypted data, database partitioning for efficient encrypted data management, personal health record systems, and spam filtering in encrypted email systems. The flexible authorization will enhance privacy protection of IBEET. In this paper, we propose an efficient construction of IBEET-FA system based on the hardness of learning with error (LWE) problem. Our security proof holds in the standard model.",cs.CR,Cybersecurity
Generalized Iris Presentation Attack Detection Algorithm under Cross-Database Settings,"Presentation attacks are posing major challenges to most of the biometric modalities. Iris recognition, which is considered as one of the most accurate biometric modality for person identification, has also been shown to be vulnerable to advanced presentation attacks such as 3D contact lenses and textured lens. While in the literature, several presentation attack detection (PAD) algorithms are presented; a significant limitation is the generalizability against an unseen database, unseen sensor, and different imaging environment. To address this challenge, we propose a generalized deep learning-based PAD network, MVANet, which utilizes multiple representation layers. It is inspired by the simplicity and success of hybrid algorithm or fusion of multiple detection networks. The computational complexity is an essential factor in training deep neural networks; therefore, to reduce the computational complexity while learning multiple feature representation layers, a fixed base model has been used. The performance of the proposed network is demonstrated on multiple databases such as IIITD-WVU MUIPA and IIITD-CLI databases under cross-database training-testing settings, to assess the generalizability of the proposed algorithm.",cs.CR,Cybersecurity
Geo-Graph-Indistinguishability: Location Privacy on Road Networks Based on Differential Privacy,"In recent years, concerns about location privacy are increasing with the spread of location-based services (LBSs). Many methods to protect location privacy have been proposed in the past decades. Especially, perturbation methods based on Geo-Indistinguishability (Geo-I), which randomly perturb a true location to a pseudolocation, are getting attention due to its strong privacy guarantee inherited from differential privacy. However, Geo-I is based on the Euclidean plane even though many LBSs are based on road networks (e.g. ride-sharing services). This causes unnecessary noise and thus an insufficient tradeoff between utility and privacy for LBSs on road networks. To address this issue, we propose a new privacy notion, Geo-Graph-Indistinguishability (GG-I), for locations on a road network to achieve a better tradeoff. We propose Graph-Exponential Mechanism (GEM), which satisfies GG-I. Moreover, we formalize the optimization problem to find the optimal GEM in terms of the tradeoff. However, the computational complexity of a naive method to find the optimal solution is prohibitive, so we propose a greedy algorithm to find an approximate solution in an acceptable amount of time. Finally, our experiments show that our proposed mechanism outperforms a Geo-I's mechanism with respect to the tradeoff.",cs.CR,Cybersecurity
Protocol Analysis with Time,"We present a framework suited to the analysis of cryptographic protocols that make use of time in their execution. We provide a process algebra syntax that makes time information available to processes, and a transition semantics that takes account of fundamental properties of time. Additional properties can be added by the user if desirable. This timed protocol framework can be implemented either as a simulation tool or as a symbolic analysis tool in which time references are represented by logical variables, and in which the properties of time are implemented as constraints on those time logical variables. These constraints are carried along the symbolic execution of the protocol. The satisfiability of these constraints can be evaluated as the analysis proceeds, so attacks that violate the laws of physics can be rejected as impossible. We demonstrate the feasibility of our approach by using the Maude-NPA protocol analyzer together with an SMT solver that is used to evaluate the satisfiability of timing constraints. We provide a sound and complete protocol transformation from our timed process algebra to the Maude-NPA syntax and semantics, and we prove its soundness and completeness. We then use the tool to analyze Mafia fraud and distance hijacking attacks on a suite of distance-bounding protocols.",cs.CR,Cybersecurity
Fair Proof-of-Stake using VDF+VRF Consensus,"We propose a new Proof-of-Stake consensus protocol constructed with a verifiable random function (VRF) and a verifiable delay function (VDF) that has the following properties: a) all addresses with positive stake can participate; b) is fair because the coin stake is proportional to the distribution of rewards; c) is resistant to several classic blockchain attacks such as Sybil attacks, ""Nothing-at-stake"" attacks and ""Winner-takes-all"" attacks. We call it Vixify Consensus.",cs.CR,Cybersecurity
Flow-based detection and proxy-based evasion of encrypted malware C2 traffic,"State of the art deep learning techniques are known to be vulnerable to evasion attacks where an adversarial sample is generated from a malign sample and misclassified as benign. Detection of encrypted malware command and control traffic based on TCP/IP flow features can be framed as a learning task and is thus vulnerable to evasion attacks. However, unlike e.g. in image processing where generated adversarial samples can be directly mapped to images, going from flow features to actual TCP/IP packets requires crafting the sequence of packets, with no established approach for such crafting and a limitation on the set of modifiable features that such crafting allows. In this paper we discuss learning and evasion consequences of the gap between generated and crafted adversarial samples. We exemplify with a deep neural network detector trained on a public C2 traffic dataset, white-box adversarial learning, and a proxy-based approach for crafting longer flows. Our results show 1) the high evasion rate obtained by using generated adversarial samples on the detector can be significantly reduced when using crafted adversarial samples; 2) robustness against adversarial samples by model hardening varies according to the crafting approach and corresponding set of modifiable features that the attack allows for; 3) incrementally training hardened models with adversarial samples can produce a level playing field where no detector is best against all attacks and no attack is best against all detectors, in a given set of attacks and detectors. To the best of our knowledge this is the first time that level playing field feature set- and iteration-hardening are analyzed in encrypted C2 malware traffic detection.",cs.CR,Cybersecurity
Homomorphic Encryption for Quantum Annealing with Spin Reversal Transformations,"Homomorphic encryption has been an area of study in classical computing for decades. The fundamental goal of homomorphic encryption is to enable (untrusted) Oscar to perform a computation for Alice without Oscar knowing the input to the computation or the output from the computation. Alice encrypts the input before sending it to Oscar, and Oscar performs the computation directly on the encrypted data, producing an encrypted result. Oscar then sends the encrypted result of the computation back to Alice, who can decrypt it. We describe an approach to homomorphic encryption for quantum annealing based on spin reversal transformations and show that it comes with little or no performance penalty. This is in contrast to approaches to homomorphic encryption for classical computing, which incur a significant additional computational cost. This implies that the performance gap between quantum annealing and classical computing is reduced when both paradigms use homomorphic encryption. Further, homomorphic encryption is critical for quantum annealing because quantum annealers are native to the cloud -- a third party (such as untrusted Oscar) performs the computation. If sensitive information, such as health-related data subject to the Health Insurance Portability and Accountability Act, is to be processed with quantum annealers, such a technique could be useful.",cs.CR,Cybersecurity
Differentially Private Adversarial Robustness Through Randomized Perturbations,"Deep Neural Networks, despite their great success in diverse domains, are provably sensitive to small perturbations on correctly classified examples and lead to erroneous predictions. Recently, it was proposed that this behavior can be combatted by optimizing the worst case loss function over all possible substitutions of training examples. However, this can be prone to weighing unlikely substitutions higher, limiting the accuracy gain. In this paper, we study adversarial robustness through randomized perturbations, which has two immediate advantages: (1) by ensuring that substitution likelihood is weighted by the proximity to the original word, we circumvent optimizing the worst case guarantees and achieve performance gains; and (2) the calibrated randomness imparts differentially-private model training, which additionally improves robustness against adversarial attacks on the model outputs. Our approach uses a novel density-based mechanism based on truncated Gumbel noise, which ensures training on substitutions of both rare and dense words in the vocabulary while maintaining semantic similarity for model robustness.",cs.CR,Cybersecurity
Evasive Windows Malware: Impact on Antiviruses and Possible Countermeasures,"The perpetual opposition between antiviruses and malware leads both parties to evolve continuously. On the one hand, antiviruses put in place solutions that are more and more sophisticated and propose more complex detection techniques in addition to the classic signature analysis. This sophistication leads antiviruses to leave more traces of their presence on the machine they protect. To remain undetected as long as possible, malware can avoid executing within such environments by hunting down the modifications left by the antiviruses. This paper aims at determining the possibilities for malware to detect the antiviruses and then evaluating the efficiency of these techniques on a panel of antiviruses that are the most used nowadays. We then collect samples showing this kind of behavior and propose to evaluate a countermeasure that creates false artifacts, thus forcing malware to evade.",cs.CR,Cybersecurity
Taxonomy of Centralization in Public Blockchain Systems: A Systematic Literature Review,"Bitcoin introduced delegation of control over a monetary system from a select few to all who participate in that system. This delegation is known as the decentralization of controlling power and is a powerful security mechanism for the ecosystem. After the introduction of Bitcoin, the field of cryptocurrency has seen widespread attention from industry and academia, so much so that the original novel contribution of Bitcoin i.e. decentralization, may be overlooked, due to decentralizations assumed fundamental existence for the functioning of such cryptoassets. However recent studies have observed a trend of increased centralization in cryptocurrencies such as Bitcoin and Ethereum. As this increased centralization has an impact the security of the blockchain, it is crucial that it is measured, towards adequate control. This research derives an initial taxonomy of centralization present in decentralized blockchains through rigorous synthesis using a systematic literature review. This is followed by iterative refinement through expert interviews. We systematically analyzed 89 research papers published between 2009 and 2019. Our study contributes to the existing body of knowledge by highlighting the multiple definitions and measurements of centralization in the literature. We identify different aspects of centralization and propose an encompassing taxonomy of centralization concerns. This taxonomy is based on empirically observable and measurable characteristics. It consists of 13 aspects of centralization classified over six architectural layers Governance Network Consensus Incentive Operational and Application. We also discuss how the implications of centralization can vary depending on the aspects studied. We believe that this review and taxonomy provides a comprehensive overview of centralization in decentralized blockchains involving various conceptualizations and measures.",cs.CR,Cybersecurity
Deep Learning based Covert Attack Identification for Industrial Control Systems,"Cybersecurity of Industrial Control Systems (ICS) is drawing significant concerns as data communication increasingly leverages wireless networks. A lot of data-driven methods were developed for detecting cyberattacks, but few are focused on distinguishing them from equipment faults. In this paper, we develop a data-driven framework that can be used to detect, diagnose, and localize a type of cyberattack called covert attacks on smart grids. The framework has a hybrid design that combines an autoencoder, a recurrent neural network (RNN) with a Long-Short-Term-Memory (LSTM) layer, and a Deep Neural Network (DNN). This data-driven framework considers the temporal behavior of a generic physical system that extracts features from the time series of the sensor measurements that can be used for detecting covert attacks, distinguishing them from equipment faults, as well as localize the attack/fault. We evaluate the performance of the proposed method through a realistic simulation study on the IEEE 14-bus model as a typical example of ICS. We compare the performance of the proposed method with the traditional model-based method to show its applicability and efficacy.",cs.CR,Cybersecurity
CoVer: Collaborative Light-Node-Only Verification and Data Availability for Blockchains,"Validating a blockchain incurs heavy computation, communication, and storage costs. As a result, clients with limited resources, called light nodes, cannot verify transactions independently and must trust full nodes, making them vulnerable to security attacks. Motivated by this problem, we ask a fundamental question: can light nodes securely validate without any full nodes? We answer affirmatively by proposing CoVer, a decentralized protocol that allows a group of light nodes to collaboratively verify blocks even under a dishonest majority, achieving the same level of security for block validation as full nodes while only requiring a fraction of the work. In particular, work per node scales down proportionally with the number of participants (up to a log factor), resulting in computation, communication, and storage requirements that are sublinear in block size. Our main contributions are light-node-only protocols for fraud proofs and data availability.",cs.CR,Cybersecurity
The General Law Principles for Protection the Personal Data and their Importance,"Rapid technological change and globalization have created new challenges when it comes to the protection and processing of personal data. In 2018, Brazil presented a new law that has the proposal to inform how personal data should be collected and treated, to guarantee the security and integrity of the data holder. The purpose of this paper is to emphasize the principles of the General Law on Personal Data Protection, informing real cases of leakage of personal data and thus obtaining an understanding of the importance of gains that meet the interests of Internet users on the subject and its benefits to the entire Brazilian society.",cs.CR,Cybersecurity
"XDA: Accurate, Robust Disassembly with Transfer Learning","Accurate and robust disassembly of stripped binaries is challenging. The root of the difficulty is that high-level structures, such as instruction and function boundaries, are absent in stripped binaries and must be recovered based on incomplete information. Current disassembly approaches rely on heuristics or simple pattern matching to approximate the recovery, but these methods are often inaccurate and brittle, especially across different compiler optimizations.
  We present XDA, a transfer-learning-based disassembly framework that learns different contextual dependencies present in machine code and transfers this knowledge for accurate and robust disassembly. We design a self-supervised learning task motivated by masked Language Modeling to learn interactions among byte sequences in binaries. The outputs from this task are byte embeddings that encode sophisticated contextual dependencies between input binaries' byte tokens, which can then be finetuned for downstream disassembly tasks.
  We evaluate XDA's performance on two disassembly tasks, recovering function boundaries and assembly instructions, on a collection of 3,121 binaries taken from SPEC CPU2017, SPEC CPU2006, and the BAP corpus. The binaries are compiled by GCC, ICC, and MSVC on x86/x64 Windows and Linux platforms over 4 optimization levels. XDA achieves 99.0% and 99.7% F1 score at recovering function boundaries and instructions, respectively, surpassing the previous state-of-the-art on both tasks. It also maintains speed on par with the fastest ML-based approach and is up to 38x faster than hand-written disassemblers like IDA Pro. We release the code of XDA at https://github.com/CUMLSec/XDA.",cs.CR,Cybersecurity
Oblivious Sampling Algorithms for Private Data Analysis,"We study secure and privacy-preserving data analysis based on queries executed on samples from a dataset. Trusted execution environments (TEEs) can be used to protect the content of the data during query computation, while supporting differential-private (DP) queries in TEEs provides record privacy when query output is revealed. Support for sample-based queries is attractive due to \emph{privacy amplification} since not all dataset is used to answer a query but only a small subset. However, extracting data samples with TEEs while proving strong DP guarantees is not trivial as secrecy of sample indices has to be preserved. To this end, we design efficient secure variants of common sampling algorithms. Experimentally we show that accuracy of models trained with shuffling and sampling is the same for differentially private models for MNIST and CIFAR-10, while sampling provides stronger privacy guarantees than shuffling.",cs.CR,Cybersecurity
Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities,"Artificial intelligence (AI) and machine learning (ML) have become increasingly vital in the development of novel defense and intelligence capabilities across all domains of warfare. An adversarial AI (A2I) and adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is imperative that AI/ML models can defend against these attacks. A2I/AML defenses will help provide the necessary assurance of these advanced capabilities that use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research and development of assured AI/ML capabilities via new A2I/AML defenses by fostering a collaborative environment across the U.S. Department of Defense and U.S. Intelligence Community. The A2IWG aims to identify specific challenges that it can help solve or address more directly, with initial focus on three topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture Vulnerabilities.",cs.CR,Cybersecurity
Target Privacy Threat Modeling for COVID-19 Exposure Notification Systems,"The adoption of digital contact tracing (DCT) technology during the COVID-19pandemic has shown multiple benefits, including helping to slow the spread of infectious disease and to improve the dissemination of accurate information. However, to support both ethical technology deployment and user adoption, privacy must be at the forefront. With the loss of privacy being a critical threat, thorough threat modeling will help us to strategize and protect privacy as digital contact tracing technologies advance. Various threat modeling frameworks exist today, such as LINDDUN, STRIDE, PASTA, and NIST, which focus on software system privacy, system security, application security, and data-centric risk, respectively. When applied to the exposure notification system (ENS) context, these models provide a thorough view of the software side but fall short in addressing the integrated nature of hardware, humans, regulations, and software involved in such systems. Our approach addresses ENSsas a whole and provides a model that addresses the privacy complexities of a multi-faceted solution. We define privacy principles, privacy threats, attacker capabilities, and a comprehensive threat model. Finally, we outline threat mitigation strategies that address the various threats defined in our model",cs.CR,Cybersecurity
Differentially-Private Federated Linear Bandits,"The rapid proliferation of decentralized learning systems mandates the need for differentially-private cooperative learning. In this paper, we study this in context of the contextual linear bandit: we consider a collection of agents cooperating to solve a common contextual bandit, while ensuring that their communication remains private. For this problem, we devise \textsc{FedUCB}, a multiagent private algorithm for both centralized and decentralized (peer-to-peer) federated learning. We provide a rigorous technical analysis of its utility in terms of regret, improving several results in cooperative bandit learning, and provide rigorous privacy guarantees as well. Our algorithms provide competitive performance both in terms of pseudoregret bounds and empirical benchmark performance in various multi-agent settings.",cs.CR,Cybersecurity
Selection of the optimal embedding positions of digital audio watermarking in wavelet domain,"This work studied embedding positions of digital audio watermarking in wavelet domain, to make beginners understand the nature of watermarking in a short time. Based on the theory of wavelet transform, this paper analyzed statistical distributions of each level after transformation and the features of watermark embedded in different transform levels. Through comparison and analysis, we found that watermark was suitable for embedding into the coefficients of the first four levels of wavelet transform. In current state-of-art approaches, the embedding algorithms were always to replace the coefficient values of the embedded positions. In contrast this paper proposed an embedding algorithm of selfadaptive interpolation to achieve a better imperceptibility. In order to reduce the computational complexity, we took a pseudo random sequence with a length of 31 bits as the watermark. In the experiments, watermark was embedded in different locations, including different transform levels, high-frequency coefficients and low-frequency coefficients, high-energy regions and low-frequency regions. Results showed that the imperceptibility was better than traditional embedding algorithms. The bit error rates of the extracted watermark were calculated and we analyzed the robustness and fragility of each embedded signal. At last we concluded the best embedding positions of watermark for different applications and our future work.",cs.CR,Cybersecurity
MixCon: Adjusting the Separability of Data Representations for Harder Data Recovery,"To address the issue that deep neural networks (DNNs) are vulnerable to model inversion attacks, we design an objective function, which adjusts the separability of the hidden data representations, as a way to control the trade-off between data utility and vulnerability to inversion attacks. Our method is motivated by the theoretical insights of data separability in neural networking training and results on the hardness of model inversion. Empirically, by adjusting the separability of data representation, we show that there exist sweet-spots for data separability such that it is difficult to recover data during inference while maintaining data utility.",cs.CR,Cybersecurity
Amnesiac Machine Learning,"The Right to be Forgotten is part of the recently enacted General Data Protection Regulation (GDPR) law that affects any data holder that has data on European Union residents. It gives EU residents the ability to request deletion of their personal data, including training records used to train machine learning models. Unfortunately, Deep Neural Network models are vulnerable to information leaking attacks such as model inversion attacks which extract class information from a trained model and membership inference attacks which determine the presence of an example in a model's training data. If a malicious party can mount an attack and learn private information that was meant to be removed, then it implies that the model owner has not properly protected their user's rights and their models may not be compliant with the GDPR law. In this paper, we present two efficient methods that address this question of how a model owner or data holder may delete personal data from models in such a way that they may not be vulnerable to model inversion and membership inference attacks while maintaining model efficacy. We start by presenting a real-world threat model that shows that simply removing training data is insufficient to protect users. We follow that up with two data removal methods, namely Unlearning and Amnesiac Unlearning, that enable model owners to protect themselves against such attacks while being compliant with regulations. We provide extensive empirical analysis that show that these methods are indeed efficient, safe to apply, effectively remove learned information about sensitive data from trained models while maintaining model efficacy.",cs.CR,Cybersecurity
On Differentially Private Stochastic Convex Optimization with Heavy-tailed Data,"In this paper, we consider the problem of designing Differentially Private (DP) algorithms for Stochastic Convex Optimization (SCO) on heavy-tailed data. The irregularity of such data violates some key assumptions used in almost all existing DP-SCO and DP-ERM methods, resulting in failure to provide the DP guarantees. To better understand this type of challenges, we provide in this paper a comprehensive study of DP-SCO under various settings. First, we consider the case where the loss function is strongly convex and smooth. For this case, we propose a method based on the sample-and-aggregate framework, which has an excess population risk of $\tilde{O}(\frac{d^3}{n^4})$ (after omitting other factors), where $n$ is the sample size and $d$ is the dimensionality of the data. Then, we show that with some additional assumptions on the loss functions, it is possible to reduce the \textit{expected} excess population risk to $\tilde{O}(\frac{ d^2}{ n^2 })$. To lift these additional conditions, we also provide a gradient smoothing and trimming based scheme to achieve excess population risks of $\tilde{O}(\frac{ d^2}{n^2})$ and $\tilde{O}(\frac{d^\frac{2}{3}}{(n^2)^\frac{1}{3}})$ for strongly convex and general convex loss functions, respectively, \textit{with high probability}. Experiments suggest that our algorithms can effectively deal with the challenges caused by data irregularity.",cs.CR,Cybersecurity
Non-Stochastic Private Function Evaluation,"We consider private function evaluation to provide query responses based on private data of multiple untrusted entities in such a way that each cannot learn something substantially new about the data of others. First, we introduce perfect non-stochastic privacy in a two-party scenario. Perfect privacy amounts to conditional unrelatedness of the query response and the private uncertain variable of other individuals conditioned on the uncertain variable of a given entity. We show that perfect privacy can be achieved for queries that are functions of the common uncertain variable, a generalization of the common random variable. We compute the closest approximation of the queries that do not take this form. To provide a trade-off between privacy and utility, we relax the notion of perfect privacy. We define almost perfect privacy and show that this new definition equates to using conditional disassociation instead of conditional unrelatedness in the definition of perfect privacy. Then, we generalize the definitions to multi-party function evaluation (more than two data entities). We prove that uniform quantization of query responses, where the quantization resolution is a function of privacy budget and sensitivity of the query (cf., differential privacy), achieves function evaluation privacy.",cs.CR,Cybersecurity
Multi-Dimensional Randomized Response,"In our data world, a host of not necessarily trusted controllers gather data on individual subjects. To preserve her privacy and, more generally, her informational self-determination, the individual has to be empowered by giving her agency on her own data. Maximum agency is afforded by local anonymization, that allows each individual to anonymize her own data before handing them to the data controller. Randomized response (RR) is a local anonymization approach able to yield multi-dimensional full sets of anonymized microdata that are valid for exploratory analysis and machine learning. This is so because an unbiased estimate of the distribution of the true data of individuals can be obtained from their pooled randomized data. Furthermore, RR offers rigorous privacy guarantees. The main weakness of RR is the curse of dimensionality when applied to several attributes: as the number of attributes grows, the accuracy of the estimated true data distribution quickly degrades. We propose several complementary approaches to mitigate the dimensionality problem. First, we present two basic protocols, separate RR on each attribute and joint RR for all attributes, and discuss their limitations. Then we introduce an algorithm to form clusters of attributes so that attributes in different clusters can be viewed as independent and joint RR can be performed within each cluster. After that, we introduce an adjustment algorithm for the randomized data set that repairs some of the accuracy loss due to assuming independence between attributes when using RR separately on each attribute or due to assuming independence between clusters in cluster-wise RR. We also present empirical work to illustrate the proposed methods.",cs.CR,Cybersecurity
Toward Smart Security Enhancement of Federated Learning Networks,"As traditional centralized learning networks (CLNs) are facing increasing challenges in terms of privacy preservation, communication overheads, and scalability, federated learning networks (FLNs) have been proposed as a promising alternative paradigm to support the training of machine learning (ML) models. In contrast to the centralized data storage and processing in CLNs, FLNs exploit a number of edge devices (EDs) to store data and perform training distributively. In this way, the EDs in FLNs can keep training data locally, which preserves privacy and reduces communication overheads. However, since the model training within FLNs relies on the contribution of all EDs, the training process can be disrupted if some of the EDs upload incorrect or falsified training results, i.e., poisoning attacks. In this paper, we review the vulnerabilities of FLNs, and particularly give an overview of poisoning attacks and mainstream countermeasures. Nevertheless, the existing countermeasures can only provide passive protection and fail to consider the training fees paid for the contributions of the EDs, resulting in a unnecessarily high training cost. Hence, we present a smart security enhancement framework for FLNs. In particular, a verify-before-aggregate (VBA) procedure is developed to identify and remove the non-benign training results from the EDs. Afterward, deep reinforcement learning (DRL) is applied to learn the behaving patterns of the EDs and to actively select the EDs that can provide benign training results and charge low training fees. Simulation results reveal that the proposed framework can protect FLNs effectively and efficiently.",cs.CR,Cybersecurity
A Formally Verified Protocol for Log Replication with Byzantine Fault Tolerance,"Byzantine fault tolerant protocols enable state replication in the presence of crashed, malfunctioning, or actively malicious processes. Designing such protocols without the assistance of verification tools, however, is remarkably error-prone. In an adversarial environment, performance and flexibility come at the cost of complexity, making the verification of existing protocols extremely difficult. We take a different approach and propose a formally verified consensus protocol designed for a specific use case: secure logging. Our protocol allows each node to propose entries in a parallel subroutine, and guarantees that correct nodes agree on the set of all proposed entries, without leader election. It is simple yet practical, as it can accommodate the workload of a logging system such as Certificate Transparency. We show that it is optimal in terms of both required rounds and tolerable faults. Using Isabelle/HOL, we provide a fully machine-checked security proof based upon the Heard-Of model, which we extend to support signatures. We also present and evaluate a prototype implementation.",cs.CR,Cybersecurity
I-SiamIDS: an improved Siam-IDS for handling class imbalance in network-based intrusion detection systems,"NIDSs identify malicious activities by analyzing network traffic. NIDSs are trained with the samples of benign and intrusive network traffic. Training samples belong to either majority or minority classes depending upon the number of available instances. Majority classes consist of abundant samples for the normal traffic as well as for recurrent intrusions. Whereas, minority classes include fewer samples for unknown events or infrequent intrusions. NIDSs trained on such imbalanced data tend to give biased predictions against minority attack classes, causing undetected or misclassified intrusions. Past research works handled this class imbalance problem using data-level approaches that either increase minority class samples or decrease majority class samples in the training data set. Although these data-level balancing approaches indirectly improve the performance of NIDSs, they do not address the underlying issue in NIDSs i.e. they are unable to identify attacks having limited training data only. This paper proposes an algorithm-level approach called I-SiamIDS, which is a two-layer ensemble for handling class imbalance problem. I-SiamIDS identifies both majority and minority classes at the algorithm-level without using any data-level balancing techniques. The first layer of I-SiamIDS uses an ensemble of b-XGBoost, Siamese-NN and DNN for hierarchical filtration of input samples to identify attacks. These attacks are then sent to the second layer of I-SiamIDS for classification into different attack classes using m-XGBoost. As compared to its counterparts, I-SiamIDS showed significant improvement in terms of Accuracy, Recall, Precision, F1-score and values of AUC for both NSL-KDD and CIDDS-001 datasets. To further strengthen the results, computational cost analysis was also performed to study the acceptability of the proposed I-SiamIDS.",cs.CR,Cybersecurity
Lic-Sec: an enhanced AppArmor Docker security profile generator,"Along with the rapid development of cloud computing technology, containerization technology has drawn much attention from both industry and academia. In this paper, we perform a comparative measurement analysis of Docker-sec, which is a Linux Security Module proposed in 2018, and a new AppArmor profile generator called Lic-Sec, which combines Docker-sec with a modified version of LiCShield, which is also a Linux Security Module proposed in 2015. Docker-sec and LiCShield can be used to enhance Docker container security based on mandatory access control and allows protection of the container without manually configurations. Lic-Sec brings together their strengths and provides stronger protection. We evaluate the effectiveness and performance of Docker-sec and Lic-Sec by testing them with real-world attacks. We generate an exploit database with 42 exploits effective on Docker containers selected from the latest 400 exploits on Exploit-db. We launch these exploits on containers spawned with Docker-sec and Lic-Sec separately. Our evaluations show that for demanding images, Lic-Sec gives protection for all privilege escalation attacks for which Docker-sec failed to give protection.",cs.CR,Cybersecurity
Differentially Private Secure Multi-Party Computation for Federated Learning in Financial Applications,"Federated Learning enables a population of clients, working with a trusted server, to collaboratively learn a shared machine learning model while keeping each client's data within its own local systems. This reduces the risk of exposing sensitive data, but it is still possible to reverse engineer information about a client's private data set from communicated model parameters. Most federated learning systems therefore use differential privacy to introduce noise to the parameters. This adds uncertainty to any attempt to reveal private client data, but also reduces the accuracy of the shared model, limiting the useful scale of privacy-preserving noise. A system can further reduce the coordinating server's ability to recover private client information, without additional accuracy loss, by also including secure multiparty computation. An approach combining both techniques is especially relevant to financial firms as it allows new possibilities for collaborative learning without exposing sensitive client data. This could produce more accurate models for important tasks like optimal trade execution, credit origination, or fraud detection. The key contributions of this paper are: We present a privacy-preserving federated learning protocol to a non-specialist audience, demonstrate it using logistic regression on a real-world credit card fraud data set, and evaluate it using an open-source simulation platform which we have adapted for the development of federated learning systems.",cs.CR,Cybersecurity
Decentralized and Secure Generation Maintenance with Differential Privacy,"Decentralized methods are gaining popularity for data-driven models in power systems as they offer significant computational scalability while guaranteeing full data ownership by utility stakeholders. However, decentralized methods still require sharing information about network flow estimates over public facing communication channels, which raises privacy concerns. In this paper we propose a differential privacy driven approach geared towards decentralized formulations of mixed integer operations and maintenance optimization problems that protects network flow estimates. We prove strong privacy guarantees by leveraging the linear relationship between the phase angles and the flow. To address the challenges associated with the mixed integer and dynamic nature of the problem, we introduce an exponential moving average based consensus mechanism to enhance convergence, coupled with a control chart based convergence criteria to improve stability. Our experimental results obtained on the IEEE 118 bus case demonstrate that our privacy preserving approach yields solution qualities on par with benchmark methods without differential privacy. To demonstrate the computational robustness of our method, we conduct experiments using a wide range of noise levels and operational scenarios.",cs.CR,Cybersecurity
Mischief: A Simple Black-Box Attack Against Transformer Architectures,"We introduce Mischief, a simple and lightweight method to produce a class of human-readable, realistic adversarial examples for language models. We perform exhaustive experimentations of our algorithm on four transformer-based architectures, across a variety of downstream tasks, as well as under varying concentrations of said examples. Our findings show that the presence of Mischief-generated adversarial samples in the test set significantly degrades (by up to $20\%$) the performance of these models with respect to their reported baselines. Nonetheless, we also demonstrate that, by including similar examples in the training set, it is possible to restore the baseline scores on the adversarial test set. Moreover, for certain tasks, the models trained with Mischief set show a modest increase on performance with respect to their original, non-adversarial baseline.",cs.CR,Cybersecurity
DOOM: A Novel Adversarial-DRL-Based Op-Code Level Metamorphic Malware Obfuscator for the Enhancement of IDS,"We designed and developed DOOM (Adversarial-DRL based Opcode level Obfuscator to generate Metamorphic malware), a novel system that uses adversarial deep reinforcement learning to obfuscate malware at the op-code level for the enhancement of IDS. The ultimate goal of DOOM is not to give a potent weapon in the hands of cyber-attackers, but to create defensive-mechanisms against advanced zero-day attacks. Experimental results indicate that the obfuscated malware created by DOOM could effectively mimic multiple-simultaneous zero-day attacks. To the best of our knowledge, DOOM is the first system that could generate obfuscated malware detailed to individual op-code level. DOOM is also the first-ever system to use efficient continuous action control based deep reinforcement learning in the area of malware generation and defense. Experimental results indicate that over 67% of the metamorphic malware generated by DOOM could easily evade detection from even the most potent IDS. This achievement gains significance, as with this, even IDS augment with advanced routing sub-system can be easily evaded by the malware generated by DOOM.",cs.CR,Cybersecurity
Parametric non-interference in timed automata,"We consider a notion of non-interference for timed automata (TAs) that allows to quantify the frequency of an attack; that is, we infer values of the minimal time between two consecutive actions of the attacker, so that (s)he disturbs the set of reachable locations. We also synthesize valuations for the timing constants of the TA (seen as parameters) guaranteeing non-interference. We show that this can reduce to reachability synthesis in parametric timed automata. We apply our method to a model of the Fischer mutual exclusion protocol and obtain preliminary results.",cs.CR,Cybersecurity
Multi-Shard Private Transactions for Permissioned Blockchains,"Traditionally, blockchain systems involve sharing transaction information across all blockchain network participants. Clearly, this introduces barriers to the adoption of the technology by the enterprise world, where preserving the privacy of the business data is a necessity. Previous efforts to bring privacy and blockchains together either still leak partial information, are restricted in their functionality or use costly mechanisms like zk-SNARKs. In this paper, we propose the Multi-Shard Private Transaction (MSPT) protocol, a novel privacy-preserving protocol for permissioned blockchains, which relies only on simple cryptographic primitives and targeted dissemination of information to achieve atomicity and high performances.",cs.CR,Cybersecurity
Fundamental Properties of the Layer Below a Payment Channel Network (Extended Version),"Payment channel networks are a highly discussed approach for improving scalability of cryptocurrencies such as Bitcoin. As they allow processing transactions off-chain, payment channel networks are referred to as second layer technology, while the blockchain is the first layer. We uncouple payment channel networks from blockchains and look at them as first-class citizens. This brings up the question what model payment channel networks require as first layer. In response, we formalize a model (called RFL Model) for a first layer below a payment channel network. While transactions are globally made available by a blockchain, the RFL Model only provides the reduced property that a transaction is delivered to the users being affected by a transaction. We show that the reduced model's properties still suffice to implement payment channels. By showing that the RFL Model can not only be instantiated by the Bitcoin blockchain but also by trusted third parties like banks, we show that the reduction widens the design space for the first layer. Further, we show that the stronger property provided by blockchains allows for optimizations that can be used to reduce the time for locking collateral during payments over multiple hops in a payment channel network.",cs.CR,Cybersecurity
(How) Do people change their passwords after a breach?,"To protect against misuse of passwords compromised in a breach, consumers should promptly change affected passwords and any similar passwords on other accounts. Ideally, affected companies should strongly encourage this behavior and have mechanisms in place to mitigate harm. In order to make recommendations to companies about how to help their users perform these and other security-enhancing actions after breaches, we must first have some understanding of the current effectiveness of companies' post-breach practices. To study the effectiveness of password-related breach notifications and practices enforced after a breach, we examine---based on real-world password data from 249 participants---whether and how constructively participants changed their passwords after a breach announcement.
  Of the 249 participants, 63 had accounts on breached domains; only 33% of the 63 changed their passwords and only 13% (of 63) did so within three months of the announcement. New passwords were on average 1.3x stronger than old passwords (when comparing log10-transformed strength), though most were weaker or of equal strength. Concerningly, new passwords were overall more similar to participants' other passwords, and participants rarely changed passwords on other sites even when these were the same or similar to their password on the breached domain. Our results highlight the need for more rigorous password-changing requirements following a breach and more effective breach notifications that deliver comprehensive advice.",cs.CR,Cybersecurity
CacheOut: Leaking Data on Intel CPUs via Cache Evictions,"Recent transient-execution attacks, such as RIDL, Fallout, and ZombieLoad, demonstrated that attackers can leak information while it transits through microarchitectural buffers. Named Microarchitectural Data Sampling (MDS) by Intel, these attacks are likened to ""drinking from the firehose"", as the attacker has little control over what data is observed and from what origin. Unable to prevent the buffers from leaking, Intel issued countermeasures via microcode updates that overwrite the buffers when the CPU changes security domains.
  In this work we present CacheOut, a new microarchitectural attack that is capable of bypassing Intel's buffer overwrite countermeasures. We observe that as data is being evicted from the CPU's L1 cache, it is often transferred back to the leaky CPU buffers where it can be recovered by the attacker. CacheOut improves over previous MDS attacks by allowing the attacker to choose which data to leak from the CPU's L1 cache, as well as which part of a cache line to leak. We demonstrate that CacheOut can leak information across multiple security boundaries, including those between processes, virtual machines, user and kernel space, and from SGX enclaves.",cs.CR,Cybersecurity
Smart Contracts for Multiagent Plan Execution in Untrusted Cyber-physical Systems,"Intelligent Cyber-physical systems can be modelled as multi-agent systems with planning capability to impart adaptivity for changing contexts. In such multi-agent systems, the protocol for plan execution must result in the proper completion and ordering of actions in spite of their distributed execution. However, in untrusted scenarios, there is a possibility of agents not respecting the protocol either due to faults or due to malicious reasons thereby resulting in plan failure. In order to prevent such situations, we propose to implement the execution of agents through smart contracts. This points to a generic architecture seamlessly integrating intelligent planning-based CPS and smart-contracts.",cs.CR,Cybersecurity
PhishZip: A New Compression-based Algorithm for Detecting Phishing Websites,"Phishing has grown significantly in the past few years and is predicted to further increase in the future. The dynamics of phishing introduce challenges in implementing a robust phishing detection system and selecting features which can represent phishing despite the change of attack. In this paper, we propose PhishZip which is a novel phishing detection approach using a compression algorithm to perform website classification and demonstrate a systematic way to construct the word dictionaries for the compression models using word occurrence likelihood analysis. PhishZip outperforms the use of best-performing HTML-based features in past studies, with a true positive rate of 80.04%. We also propose the use of compression ratio as a novel machine learning feature which significantly improves machine learning based phishing detection over previous studies. Using compression ratios as additional features, the true positive rate significantly improves by 30.3% (from 51.47% to 81.77%), while the accuracy increases by 11.84% (from 71.20% to 83.04%).",cs.CR,Cybersecurity
A GAN-based Approach for Mitigating Inference Attacks in Smart Home Environment,"The proliferation of smart, connected, always listening devices have introduced significant privacy risks to users in a smart home environment. Beyond the notable risk of eavesdropping, intruders can adopt machine learning techniques to infer sensitive information from audio recordings on these devices, resulting in a new dimension of privacy concerns and attack variables to smart home users. Techniques such as sound masking and microphone jamming have been effectively used to prevent eavesdroppers from listening in to private conversations. In this study, we explore the problem of adversaries spying on smart home users to infer sensitive information with the aid of machine learning techniques. We then analyze the role of randomness in the effectiveness of sound masking for mitigating sensitive information leakage. We propose a Generative Adversarial Network (GAN) based approach for privacy preservation in smart homes which generates random noise to distort the unwanted machine learning-based inference. Our experimental results demonstrate that GANs can be used to generate more effective sound masking noise signals which exhibit more randomness and effectively mitigate deep learning-based inference attacks while preserving the semantics of the audio samples.",cs.CR,Cybersecurity
Information-theoretically secure data origin authentication with quantum and classical resources,"In conventional cryptography, information-theoretically secure message authentication can be achieved by means of universal hash functions, and requires that the two legitimate users share a random secret key, which is twice as long as the message. We address the question as of whether quantum resources can offer any advantage over classical unconditionally secure message authentication codes. It is shown that passive prepare-and-measure quantum message-authentication schemes cannot do better than their classical counterparts. Subsequently we present an interactive entanglement-assisted scheme, which ideally allows for the authentication of classical messages with a classical key, which is as long as the message.",cs.CR,Cybersecurity
On the Serverless Nature of Blockchains and Smart Contracts,"Although historically the term serverless was also used in the context of peer-to-peer systems, it is more frequently associated with the architectural style for developing cloud-native applications. From the developer's perspective, serverless architectures allow reducing management efforts since applications are composed using provider-managed components, e.g., Database-as-a-Service (DBaaS) and Function-as-a-Service (FaaS) offerings. Blockchains are distributed systems designed to enable collaborative scenarios involving multiple untrusted parties. It seems that the decentralized peer-to-peer nature of blockchains makes it interesting to consider them in serverless architectures, since resource allocation and management tasks are not required to be performed by users. Moreover, considering their useful properties of ensuring transaction's immutability and facilitating accountable interactions, blockchains might enhance the overall guarantees and capabilities of serverless architectures. Therefore, in this work, we analyze how the blockchain technology and smart contracts fit into the serverless picture and derive a set of scenarios in which they act as different component types in serverless architectures. Furthermore, we formulate the implementation requirements that have to be fulfilled to successfully use blockchains and smart contracts in these scenarios. Finally, we investigate which existing technologies enable these scenarios, and analyze their readiness and suitability to fulfill the formulated requirements.",cs.CR,Cybersecurity
Validity and Reliability of the Scale Internet Users' Information Privacy Concern (IUIPC) [Extended Version],"Internet Users' Information Privacy Concerns (IUIPC-10) is one of the most endorsed privacy concern scales. It is widely used in the evaluation of human factors of PETs and the investigation of the privacy paradox. Even though its predecessor Concern For Information Privacy (CFIP) has been evaluated independently and the instrument itself seen some scrutiny, we are still missing a dedicated confirmation of IUIPC-10, itself. We aim at closing this gap by systematically analyzing IUIPC's construct validity and reliability. We obtained three mutually independent samples with a total of $N = 1031$ participants. We conducted a confirmatory factor analysis (CFA) on our main sample. Having found weaknesses, we established further factor analyses to assert the dimensionality of IUIPC-10. We proposed a respecified instrument IUIPC-8 with improved psychometric properties. Finally, we validated our findings on a validation sample. While we could confirm the overall three-dimensionality of IUIPC-10, we found that IUIPC-10 consistently failed construct validity and reliability evaluations, calling into question the unidimensionality of its sub-scales Awareness and Control. Our respecified scale IUIPC-8 offers a statistically significantly better model and outperforms IUIPC-10's construct validity and reliability. The disconfirming evidence on the construct validity raises doubts how well IUIPC-10 measures the latent variable information privacy concern. The sub-par reliability could yield spurious and erratic results as well as attenuate relations with other latent variables, such as behavior. Thereby, the instrument could confound studies of human factors of PETs or the privacy paradox, in general.",cs.CR,Cybersecurity
Research Challenges in Designing Differentially Private Text Generation Mechanisms,"Accurately learning from user data while ensuring quantifiable privacy guarantees provides an opportunity to build better Machine Learning (ML) models while maintaining user trust. Recent literature has demonstrated the applicability of a generalized form of Differential Privacy to provide guarantees over text queries. Such mechanisms add privacy preserving noise to vectorial representations of text in high dimension and return a text based projection of the noisy vectors. However, these mechanisms are sub-optimal in their trade-off between privacy and utility. This is due to factors such as a fixed global sensitivity which leads to too much noise added in dense spaces while simultaneously guaranteeing protection for sensitive outliers. In this proposal paper, we describe some challenges in balancing the tradeoff between privacy and utility for these differentially private text mechanisms. At a high level, we provide two proposals: (1) a framework called LAC which defers some of the noise to a privacy amplification step and (2), an additional suite of three different techniques for calibrating the noise based on the local region around a word. Our objective in this paper is not to evaluate a single solution but to further the conversation on these challenges and chart pathways for building better mechanisms.",cs.CR,Cybersecurity
Towards Secure and Leak-Free Workflows Using Microservice Isolation,"Data leaks and breaches are on the rise. They result in huge losses of money for businesses like the movie industry, as well as a loss of user privacy for businesses dealing with user data like the pharmaceutical industry. Preventing data exposures is challenging, because the causes for such events are various, ranging from hacking to misconfigured databases. Alongside the surge in data exposures, the recent rise of microservices as a paradigm brings the need to not only secure traffic at the border of the network, but also internally, pressing the adoption of new security models such as zero-trust to secure business processes.
  Business processes can be modeled as workflows, where the owner of the data at risk interacts with contractors to realize a sequence of tasks on this data. In this paper, we show how those workflows can be enforced while preventing data exposure. Following the principles of zero-trust, we develop an infrastructure using the isolation provided by a microservice architecture, to enforce owner policy. We show that our infrastructure is resilient to the set of attacks considered in our security model. We implement a simple, yet realistic, workflow with our infrastructure in a publicly available proof of concept. We then verify that the specified policy is correctly enforced by testing the deployment for policy violations, and estimate the overhead cost of authorization.",cs.CR,Cybersecurity
EMRs with Blockchain : A distributed democratised Electronic Medical Record sharing platform,"Medical data sharing needs to be done with the utmost respect for privacy and security. It contains intimate data of the patient and any access to it must be highly regulated. With the emergence of vertical solutions in healthcare institutions, interoperability across organisations has been hindered. The authors of this paper propose a blockchain based medical-data sharing solution, utilising Hyperledger Fabric to regulate access to medical data, and using the InterPlanatory File System for its storage. We believe that the combination of these two distributed solutions can enable patients to access their medical records across healthcare institutions while ensuring non-repudiation, immutability and providing data-ownership. It would enable healthcare practitioners to access all previous medical records in a single location, empowering them with the data required for the effective diagnosis and treatment of patients. Making it safe and straightforward, it would also enable patients to share medical data with research institutions, leading to the creation of reliable data sets, laying the groundwork required for the creation of personalised medicine.",cs.CR,Cybersecurity
Data Storage in the Decentralized World: Blockchain and Derivatives,"We have entered an era where the importance of decentralized solutions has become more obvious. Blockchain technology and its derivatives are distributed ledger technologies that keep the registry of data between peers of a network. This ledger is secured within a successive over looping cryptographic chain. The accomplishment of the Bitcoin cryptocurrency proved that blockchain technology and its derivatives could be used to eliminate intermediaries and provide security for cyberspace. However, there are some challenges in the implementation of blockchain technology. This chapter first explains the concept of blockchain technology and the data that we can store therein. The main advantage of blockchain is the security services that it provides. This section continues by describing these services.. The challenges of blockchain; blockchain anomalies, energy consumption, speed, scalability, interoperability, privacy and cryptology in the age of quantum computing are described. Selected solutions for these challenges are given. Remarkable derivatives of blockchain, which use different solutions (directed acyclic graph, distributed hash table, gossip consensus protocol) to solve some of these challenges are described. Then the data storage in blockchain and evolving data solutions are explained. The comparison of decentralized solutions with the lcentralized database systems is given. A multi-platform interoperable scalable architecture (MPISA) is proposed. In the conclusion we include the evolution assumptions of data storage in a decentralized world.",cs.CR,Cybersecurity
Identification of Metallic Objects using Spectral MPT Signatures: Object Characterisation and Invariants,"The early detection of terrorist threats, such as guns and knives, through improved metal detection, has the potential to reduce the number of attacks and improve public safety and security. To achieve this, there is considerable potential to use the fields applied and measured by a metal detector to discriminate between different shapes and different metals since, hidden within the field perturbation, is object characterisation information. The magnetic polarizability tensor (MPT) offers an economical characterisation of metallic objects that can be computed for different threat and non-threat objects and has an established theoretical background, which shows that the induced voltage is a function of the hidden object's MPT coefficients. In this paper, we describe the additional characterisation information that measurements of the induced voltage over a range of frequencies offer compared to measurements at a single frequency. We call such object characterisations its MPT spectral signature. Then, we present a series of alternative rotational invariants for the purpose of classifying hidden objects using MPT spectral signatures. Finally, we include examples of computed MPT spectral signature characterisations of realistic threat and non-threat objects that can be used to train machine learning algorithms for classification purposes.",cs.CR,Cybersecurity
Suspicious Massive Registration Detection via Dynamic Heterogeneous Graph Neural Networks,"Massive account registration has raised concerns on risk management in e-commerce companies, especially when registration increases rapidly within a short time frame. To monitor these registrations constantly and minimize the potential loss they might incur, detecting massive registration and predicting their riskiness are necessary. In this paper, we propose a Dynamic Heterogeneous Graph Neural Network framework to capture suspicious massive registrations (DHGReg). We first construct a dynamic heterogeneous graph from the registration data, which is composed of a structural subgraph and a temporal subgraph. Then, we design an efficient architecture to predict suspicious/benign accounts. Our proposed model outperforms the baseline models and is computationally efficient in processing a dynamic heterogeneous graph constructed from a real-world dataset. In practice, the DHGReg framework would benefit the detection of suspicious registration behaviors at an early stage.",cs.CR,Cybersecurity
Localization Attack by Precoder Feedback Overhearing in 5G Networks and Countermeasures,"In fifth-generation (5G) cellular networks, users feed back to the base station the index of the precoder (from a codebook) to be used for downlink transmission. The precoder is strongly related to the user channel and in turn to the user position within the cell. We propose a method by which an external attacker determines the user position by passively overhearing this unencrypted layer-2 feedback signal. The attacker first builds a map of fed back precoder indices in the cell. Then, by overhearing the precoder index fed back by the victim user, the attacker finds its position on the map. We focus on the type-I single-panel codebook, which today is the only mandatory solution in the 3GPP standard. We analyze the attack and assess the obtained localization accuracy against various parameters. We analyze the localization error of a simplified precoder feedback model and describe its asymptotic localization precision. We also propose a mitigation against our attack, wherein the user randomly selects the precoder among those providing the highest rate. Simulations confirm that the attack can achieve a high localization accuracy, which is significantly reduced when the mitigation solution is adopted, at the cost of a negligible rate degradation.",cs.CR,Cybersecurity
Quantum Request-Answer Game with Buffer Model for Online Algorithms,"We consider online algorithms as a request-answer game. An adversary that generates input requests, and an online algorithm answers. We consider a generalized version of the game that has a buffer of limited size. The adversary loads data to the buffer, and the algorithm has random access to elements of the buffer. We consider quantum and classical (deterministic or randomized) algorithms for the model.
  In the paper, we provide a specific problem (The Most Frequent Keyword Problem) and a quantum algorithm that works better than any classical (deterministic or randomized) algorithm in terms of competitive ratio. At the same time, for the problem, classical online algorithms in the standard model are equivalent to the classical algorithms in the request-answer game with buffer model.",quant-ph,Quantum Physics
Maximising Dynamic Nuclear Polarisation via Selective Hyperfine Tuning,"Dynamic nuclear polarisation (DNP) refers to a class of techniques used to increase the signal in nuclear magnetic resonance measurements by transferring spin polarisation from ensembles of highly polarised electrons to target nuclear analytes. These techniques, however, require the application of strong magnetic fields to maximise electron spin polarisation, limiting pathways for electron-nuclear (hyperfine) spin coupling and transfer. In this work we show that, for systems of electronic spin $S\geq1$ possessing an intrinsic zero-field splitting, a separate class of stronger hyperfine interactions based on lab-frame cross relaxation may be utilised to improve DNP efficiency and yield, whilst operating at moderate fields. We analytically review existing methods, and determine that this approach increases the rate of polarisation transfer to the nuclear ensemble by up to an order of magnitude over existing techniques. This result is demonstrated experimentally at room temperature using the optically polarisable $S=1$ electron spin system of the nitrogen vacancy (NV) defect in diamond as the source of electron spin polarisation. Finally we assess the utility of these NV-based approaches for the polarisation of macroscopic quantities of molecular spins external to the diamond for NMR and MRI applications.",quant-ph,Quantum Physics
Analytical Solutions of the Schrodinger Equation for Hua Potential within the Framework of two Approximations Scheme,"In this paper, we solve analytically the Schrodinger equation for s-wave and arbitrary angular momenta with the Hua potential is investigated respectively. The wave function as well as energy equation are obtained in an exact analytical manner via the Nikiforov Uvarov method using two approximations scheme. Some special cases of this potentials are also studied.",quant-ph,Quantum Physics
Comment on 'Semi-Quantum Private Comparison Based on Bell States',"This study points out a semi-quantum protocol for private comparison using Bell states (SQPC) suffering from the double C-NOT attack and the malicious agent attack. The attacker can easily obtain information through these attacks. An improved protocol is proposed, which can effectively resist both of these attacks.",quant-ph,Quantum Physics
Discrimination of symmetric states in operational probabilistic theory,"A state discrimination problem in an operational probabilistic theory (OPT) is investigated in diagrammatic terms. It is well-known that, in the case of quantum theory, if a state set has a certain symmetry, then there exists a minimum-error measurement having the same type of symmetry. However, to our knowledge, it is not yet clear whether this property also holds in a more general OPT. We show that it also holds in OPTs, i.e., for a symmetric state set, there exists a minimum-error measurement that has the same type of symmetry. It is also shown that this result can be utilized to optimize over a restricted class of measurements, such as sequential or separable measurements.",quant-ph,Quantum Physics
Electromagnetically Induced Transparencies with Two Transverse Bose-Einstein Condensates in a Four-Mirror Cavity,"We investigate electromagnetically induced transparencies with two transverse Bose-Einstein condensates in four-mirror optical cavity, driven by a strong pump laser and a weak probe laser. The cavity mode, after getting split from beam splitter, interacts with two independent Bose-Einstein Condensates transversely trapped in the arms of the cavity along $x$-axis and $y$-axis. The interaction of intra-cavity optical mode excites momentum side modes in Bose-Einstein Condensates, which then mimic as two atomic mirrors coupled through cavity field. We show that the probe field photons transition through the atomic mirrors yields to two coupled electromagnetically induced transparency windows, which only exist when both atomic states are coupled with the cavity. Further, the strength of these novel electromagnetically induced transparencies gets increased with an increase in atom-cavity coupling. Furthermore, we investigate the behavior of Fano resonances and dynamics of fast and slow light. We illustrate that the Fano line shapes and dynamics of slow light can be enhanced by strengthening the interaction between atomic states and cavity mode. Our findings not only contribute to the quantum nonlinear optics of complex systems but also provide a platform to test multi-dimensional atomic states in a single system.",quant-ph,Quantum Physics
Entanglement preserving local thermalization,"We investigate whether entanglement can survive the thermalization of subsystems. We present two equivalent formulations of this problem: (1) Can two isolated agents, accessing only pre-shared randomness, locally thermalize arbitrary input states while maintaining some entanglement? (2) Can thermalization with local heat baths, which may be classically correlated but do not exchange information, locally thermalize arbitrary input states while maintaining some entanglement? We answer these questions in the positive at every nonzero temperature and provide bounds on the amount of preserved entanglement. We provide explicit protocols and discuss their thermodynamic interpretation: we suggest that the underlying mechanism is a speed-up of the subsystem thermalization process. We also present extensions to multipartite systems. Our findings show that entanglement can survive locally performed thermalization processes accessing only classical correlations as a resource. They also suggest a broader study of the channel's ability to preserve resources and of the compatibility between global and local dynamics.",quant-ph,Quantum Physics
Ballistic transport and boundary resistances in inhomogeneous quantum spin chains,"Transport phenomena are central to physics, and transport in the many-body and fully-quantum regime is attracting an increasing amount of attention. It has been recently revealed that some quantum spin chains support ballistic transport of excitations at all energies. However, when joining two semi-infinite ballistic parts, such as the XX and XXZ spin-1/2 models, our understanding suddenly becomes less established. Employing a matrix-product-state ansatz of the wavefunction, we study the relaxation dynamics in this latter case. Here we show that it takes place inside a light cone, within which two qualitatively different regions coexist: an inner one with a strong tendency towards thermalization, and an outer one supporting ballistic transport. We comment on the possibility that even at infinite time the system supports stationary currents and displays a non-zero Kapitza boundary resistance. Our study paves the way to the analysis of the interplay between transport, integrability, and local defects.",quant-ph,Quantum Physics
Explicit derivation of the Pauli spin matrices from the Jones vector,"Using dyadic representations elaborated from vectors of Jones, and calculating relations of anti-commutation of these tensorial forms, we obtain in shape explicit the Pauli spin matrices.",quant-ph,Quantum Physics
Coherently driving a single quantum two-level system with dichromatic laser pulses,"Efficient excitation of a single two-level system usually requires that the driving field is at the same frequency as the atomic transition. However, the scattered laser light in solid-state implementations can dominate over the single photons, imposing an outstanding challenge to perfect single-photon sources. Here, we propose a background-free method using a phase-locked dichromatic electromagnetic field with no spectral overlap with the optical transition for a coherent control of a two-level system, and we demonstrate this method experimentally with a single quantum dot embedded in a micropillar. Single photons generated by pi excitation show a purity of 0.988(1) and indistinguishability of 0.962(6). Further, the phase-coherent nature of the two-color excitation is captured by the resonance-fluorescence intensity dependence on the relative phase between the two pulses. Our two-color excitation method adds a useful toolbox to the study of atom-photon interaction, and the generation of spectrally isolated indistinguishable single photons.",quant-ph,Quantum Physics
Emergent limit cycles and time crystal dynamics in an atom-cavity system,"We propose an experimental realization of a time crystal using an atomic Bose-Einstein condensate in a high finesse optical cavity pumped with laser light detuned to the blue side of the relevant atomic resonance. By mapping out the dynamical phase diagram, we identify regions in parameter space showing stable limit cycle dynamics. Since the model describing the system is time-independent, the emergence of a limit cycle phase indicates the breaking of continuous time translation symmetry. Employing a semiclassical analysis to demonstrate the robustness of the limit cycles against perturbations and quantum fluctuations, we establish the emergence of a time crystal.",quant-ph,Quantum Physics
A 10-qubit solid-state spin register with quantum memory up to one minute,"Spins associated to single defects in solids provide promising qubits for quantum information processing and quantum networks. Recent experiments have demonstrated long coherence times, high-fidelity operations and long-range entanglement. However, control has so far been limited to a few qubits, with entangled states of three spins demonstrated. Realizing larger multi-qubit registers is challenging due to the need for quantum gates that avoid crosstalk and protect the coherence of the complete register. In this paper, we present novel decoherence-protected gates that combine dynamical decoupling of an electron spin with selective phase-controlled driving of nuclear spins. We use these gates to realize a 10-qubit quantum register consisting of the electron spin of a nitrogen-vacancy center and 9 nuclear spins in diamond. We show that the register is fully connected by generating entanglement between all 45 possible qubit pairs, and realize genuine multipartite entangled states with up to 7 qubits. Finally, we investigate the register as a multi-qubit memory. We show coherence times up to 63(2) seconds - the longest reported for a single solid-state qubit - and demonstrate that two-qubit entangled states can be stored for over 10 seconds. Our results enable the control of large quantum registers with long coherence times and therefore open the door to advanced quantum algorithms and quantum networks with solid-state spin qubits.",quant-ph,Quantum Physics
Atomic-scale imaging of a 27-nuclear-spin cluster using a single-spin quantum sensor,"Nuclear magnetic resonance (NMR) is a powerful method for determining the structure of molecules and proteins. While conventional NMR requires averaging over large ensembles, recent progress with single-spin quantum sensors has created the prospect of magnetic imaging of individual molecules. As an initial step towards this goal, isolated nuclear spins and spin pairs have been mapped. However, large clusters of interacting spins - such as found in molecules - result in highly complex spectra. Imaging these complex systems is an outstanding challenge due to the required high spectral resolution and efficient spatial reconstruction with sub-angstrom precision. Here we develop such atomic-scale imaging using a single nitrogen-vacancy (NV) centre as a quantum sensor, and demonstrate it on a model system of $27$ coupled $^{13}$C nuclear spins in a diamond. We present a new multidimensional spectroscopy method that isolates individual nuclear-nuclear spin interactions with high spectral resolution ($< 80\,$mHz) and high accuracy ($2$ mHz). We show that these interactions encode the composition and inter-connectivity of the cluster, and develop methods to extract the 3D structure of the cluster with sub-angstrom resolution. Our results demonstrate a key capability towards magnetic imaging of individual molecules and other complex spin systems.",quant-ph,Quantum Physics
On the Entanglement Cost of One-Shot Compression,"We revisit the task of visible compression of an ensemble of quantum states with entanglement assistance in the one-shot setting. The protocols achieving the best compression use many more qubits of shared entanglement than the number of qubits in the states in the ensemble. Other compression protocols, with potentially larger communication cost, have entanglement cost bounded by the number of qubits in the given states. This motivates the question as to whether entanglement is truly necessary for compression, and if so, how much of it is needed.
  Motivated by questions in communication complexity, we lift certain restrictions that are placed on compression protocols in tasks such as state-splitting and channel simulation. We show that an ensemble of the form designed by Jain, Radhakrishnan, and Sen (ICALP'03) saturates the known bounds on the sum of communication and entanglement costs, even with the relaxed compression protocols we study.
  The ensemble and the associated one-way communication protocol have several remarkable properties. The ensemble is incompressible by more than a constant number of qubits without shared entanglement, even when constant error is allowed. Moreover, in the presence of shared entanglement, the communication cost of compression can be arbitrarily smaller than the entanglement cost. The quantum information cost of the protocol can thus be arbitrarily smaller than the cost of compression without shared entanglement. The ensemble can also be used to show the impossibility of reducing, via compression, the shared entanglement used in two-party protocols for computing Boolean functions.",quant-ph,Quantum Physics
"Multifractal dimensions for random matrices, chaotic quantum maps, and many-body systems","Multifractal dimensions allow for characterizing the localization properties of states in complex quantum systems. For ergodic states the finite-size versions of fractal dimensions converge to unity in the limit of large system size. However, the approach to the limiting behavior is remarkably slow. Thus, an understanding of the scaling and finite-size properties of fractal dimensions is essential. We present such a study for random matrix ensembles, and compare with two chaotic quantum systems --- the kicked rotor and a spin chain. For random matrix ensembles we analytically obtain the finite-size dependence of the mean behavior of the multifractal dimensions, which provides a lower bound to the typical (logarithmic) averages. We show that finite statistics has remarkably strong effects, so that even random matrix computations deviate from analytic results (and show strong sample-to-sample variation), such that restoring agreement requires exponentially large sample sizes. For the quantized standard map (kicked rotor) the multifractal dimensions are found to follow the random matrix predictions closely, with the same finite statistics effects. For a XXZ spin-chain we find significant deviations from the random matrix prediction --- the large-size scaling follows a system-specific path towards unity. This suggests that local many-body Hamiltonians are ""weakly ergodic"", in the sense that their eigenfunction statistics deviate from random matrix theory.",quant-ph,Quantum Physics
Demonstration of Adiabatic Variational Quantum Computing with a Superconducting Quantum Coprocessor,"Adiabatic quantum computing enables the preparation of many-body ground states. This is key for applications in chemistry, materials science, and beyond. Realisation poses major experimental challenges: Direct analog implementation requires complex Hamiltonian engineering, while the digitised version needs deep quantum gate circuits. To bypass these obstacles, we suggest an adiabatic variational hybrid algorithm, which employs short quantum circuits and provides a systematic quantum adiabatic optimisation of the circuit parameters. The quantum adiabatic theorem promises not only the ground state but also that the excited eigenstates can be found. We report the first experimental demonstration that many-body eigenstates can be efficiently prepared by an adiabatic variational algorithm assisted with a multi-qubit superconducting coprocessor. We track the real-time evolution of the ground and exited states of transverse-field Ising spins with a fidelity up that can reach about 99%.",quant-ph,Quantum Physics
Majorana dimers and holographic quantum error-correcting codes,"Holographic quantum error-correcting codes have been proposed as toy models that describe key aspects of the AdS/CFT correspondence. In this work, we introduce a versatile framework of Majorana dimers capturing the intersection of stabilizer and Gaussian Majorana states. This picture allows for an efficient contraction with a simple diagrammatic interpretation and is amenable to analytical study of holographic quantum error-correcting codes. Equipped with this framework, we revisit the recently proposed hyperbolic pentagon code (HyPeC). Relating its logical code basis to Majorana dimers, we efficiently compute boundary state properties even for the non-Gaussian case of generic logical input. The dimers characterizing these boundary states coincide with discrete bulk geodesics, leading to a geometric picture from which properties of entanglement, quantum error correction, and bulk/boundary operator mapping immediately follow. We also elaborate upon the emergence of the Ryu-Takayanagi formula from our model, which realizes many of the properties of the recent bit thread proposal. Our work thus elucidates the connection between bulk geometry, entanglement, and quantum error correction in AdS/CFT, and lays the foundation for new models of holography.",quant-ph,Quantum Physics
Quantum optics of single electrons in quantum liquid and solid helium-4,"Single electrons can be conceived as the simplest quantum nodes in a quantum network. Between electrons, single photons can act as quantum channels to exchange quantum information. Despite this appealing picture, in conventional materials, it is extremely difficult to make individual electrons and photons coherently interact with each other at the visible-infrared wavelengths suitable for long-distance communication. Here we theoretically demonstrate that the self-confined single-electron structure in condensed helium-4 can be a fascinating candidate for single-electron quantum nodes. Each electron in helium forms a bubble of 1 to 2 nm radius and coherently interacts with mid-infrared photons. A parametrically amplified femtosecond laser can drive the electrons into any superposition between the ground and excited states. An electron inside a slot-waveguide cavity can strongly couple with cavity photons and exhibits vacuum Rabi oscillations. Two electrons in the cavity naturally generate entanglement through their respective coupling to the lossy cavity. The electron-in-helium system offers unique insight in understanding nonequilibrium quantum dynamics.",quant-ph,Quantum Physics
"Classical dynamics, arrow of time, and genesis of the Heisenberg commutation relations","Based on the assumption that time evolves only in one direction and mechanical systems can be described by Lagrangeans, a dynamical C*-algebra is presented for non-relativistic particles at atomic scales. Without presupposing any quantization scheme, this algebra is inherently non-commutative and comprises a large set of dynamics. In contrast to other approaches, the generating elements of the algebra are not interpreted as observables, but as operations on the underlying system; they describe the impact of temporary perturbations caused by the surroundings. In accordance with the doctrine of Nils Bohr, the operations carry individual names of classical significance. Without stipulating from the outset their `quantization', their concrete implementation in the quantum world emerges from the inherent structure of the algebra. In particular, the Heisenberg commutation relations for position and velocity measurements are derived from it. Interacting systems can be described within the algebraic setting by a rigorous version of the interaction picture. It is shown that Hilbert space representations of the algebra lead to the conventional formalism of quantum mechanics, where operations on states are described by time-ordered exponentials of interaction potentials. It is also discussed how the familiar statistical interpretation of quantum mechanics can be recovered from operations.",quant-ph,Quantum Physics
Stationary Bipartite Entanglement in Hybrid Optomechanical cavities,"We investigate the stationary bipartite entanglement is a useful hybrid optomechanical system, which is constituted of two coupled-cavity optomechanics through a photon hopping process and both are driven by squeezed light. The transfer of correlations from an entangled light source to optomechanical cavities is explored. It is found that the generation of bipartite entanglement and entanglement transfer depend strongly on photon hopping strength and the matching of the input squeezed modes to the cavity modes. It is revealed that the generated stationary bipartite entanglement due to squeezed light that drives the cavities is robust against the thermal fluctuations. The fidelity of a coherent state of the optical modes is explored and it is shown that it offered interesting conditions on the stability of the system, which is the same for entanglement generation.",quant-ph,Quantum Physics
Nonclassicality and entanglement for wavepackets,"Mode-entanglement based criteria and measures become insufficient for broadband emission, e.g. from spasers (plasmonic nano-lasers). We introduce criteria and measures for the (i) total entanglement of two wavepackets, (ii) entanglement of a wavepacket with an ensemble and (iii) total nonclassicality of a wavepacket~(WP). We discuss these criteria in the context of (i) entanglement of two WPs emitted from two initially entangled cavities (or two initially entangled atoms) and (ii) entanglement of an emitted WP with the ensemble/atom for the spontaneous emission and the single-photon superradiance. We also show that, (iii) when the two constituent modes of a WP are entangled, this creates nonclassicality in the WP as a noise reduction below the standard quantum limit. The criteria we introduce are, all, compatible with near-field detectors.",quant-ph,Quantum Physics
Establishing the Quantum Supremacy Frontier with a 281 Pflop/s Simulation,"Noisy Intermediate-Scale Quantum (NISQ) computers are entering an era in which they can perform computational tasks beyond the capabilities of the most powerful classical computers, thereby achieving ""Quantum Supremacy"", a major milestone in quantum computing. NISQ Supremacy requires comparison with a state-of-the-art classical simulator. We report HPC simulations of hard random quantum circuits (RQC), which have been recently used as a benchmark for the first experimental demonstration of Quantum Supremacy, sustaining an average performance of 281 Pflop/s (true single precision) on Summit, currently the fastest supercomputer in the World. These simulations were carried out using qFlex, a tensor-network-based classical high-performance simulator of RQCs. Our results show an advantage of many orders of magnitude in energy consumption of NISQ devices over classical supercomputers. In addition, we propose a standard benchmark for NISQ computers based on qFlex.",quant-ph,Quantum Physics
State convertibility in the von Neumann algebra framework,"We establish a generalisation of the fundamental state convertibility theorem in quantum information to the context of bipartite quantum systems modelled by commuting semi-finite von Neumann algebras. Namely, we establish a generalisation to this setting of Nielsen's theorem on the convertibility of quantum states under local operations and classical communication (LOCC) schemes. Along the way, we introduce an appropriate generalisation of LOCC operations and connect the resulting notion of approximate convertibility to the theory of singular numbers and majorisation in von Neumann algebras. As an application of our result in the setting of $II_1$-factors, we show that the entropy of the singular value distribution relative to the unique tracial state is an entanglement monotone in the sense of Vidal, thus yielding a new way to quantify entanglement in that context. Building on previous work in the infinite-dimensional setting, we show that trace vectors play the role of maximally entangled states for general $II_1$-factors. Examples are drawn from infinite spin chains, quasi-free representations of the CAR, and discretised versions of the CCR.",quant-ph,Quantum Physics
Quantum Control with Quantum Light of Molecular Nonadiabaticity,Coherent control experiments in molecules are often done with shaped laser fields. The electric field is described classically and control over the time evolution of the system is achieved by shaping the laser pulses in the time or frequency domain. Moving on from a classical to a quantum description of the light field allows to engineer the quantum state of light to steer chemical processes. The quantum field description of the photon mode allows to manipulate the light-matter interaction directly in phase-space. In this paper we will demonstrate the basic principle of coherent control with quantum light on the avoided crossing in lithium fluoride. Using a quantum description of light together with the nonadiabatic couplings and vibronic degrees of freedoms opens up new perspective on quantum control. We show the deviations from control with purely classical light field and how back-action of the light field becomes important in a few photon regime.,quant-ph,Quantum Physics
Building quantum neural networks based on swap test,"Artificial neural network, consisting of many neurons in different layers, is an important method to simulate humain brain. Usually, one neuron has two operations: one is linear, the other is nonlinear. The linear operation is inner product and the nonlinear operation is represented by an activation function. In this work, we introduce a kind of quantum neuron whose inputs and outputs are quantum states. The inner product and activation operator of the quantum neurons can be realized by quantum circuits. Based on the quantum neuron, we propose a model of quantum neural network in which the weights between neurons are all quantum states. We also construct a quantum circuit to realize this quantum neural network model. A learning algorithm is proposed meanwhile. We show the validity of learning algorithm theoretically and demonstrate the potential of the quantum neural network numerically.",quant-ph,Quantum Physics
Foundations of quantum physics IV. More on the thermal interpretation,"This paper continues the discussion of the thermal interpretation of quantum physics. While Part II and Part III of this series of papers explained and justified the reasons for the departure from tradition, the present Part IV summarizes the main features and adds intuitive explanations and new technical developments.
  It is shown how the spectral features of quantum systems and an approximate classical dynamics arise under appropriate conditions.
  Evidence is given for how, in the thermal interpretation, the measurement of a qubit by a pointer q-expectation may result in a binary detection event with probabilities given by the diagonal entries of the reduced density matrix of the prepared qubit.
  Differences in the conventions about measurement errors in the thermal interpretation and in traditional interpretations are discussed in detail.
  Several standard experiments, the double slit, Stern--Gerlach, and particle decay are described from the perspective of the thermal interpretation.",quant-ph,Quantum Physics
When can quantum decoherence be mimicked by classical noise?,"Quantum decoherence arises due to uncontrollable entanglement between a system with its environment. However the effects of decoherence are often thought of and modeled through a simpler picture in which the role of the environment is to introduce classical noise in the system's degrees of freedom. Here we establish necessary conditions that the classical noise models need to satisfy to quantitatively model the decoherence. Specifically, for pure-dephasing processes we identify well-defined statistical properties for the noise that are determined by the quantum many-point time correlation function of the environmental operators that enter into the system-bath interaction. In particular, for the exemplifying spin-boson problem with a Lorentz-Drude spectral density we show that the high-temperature quantum decoherence is quantitatively mimicked by colored Gaussian noise. In turn, for dissipative environments we show that classical noise models cannot describe decoherence effects due to spontaneous emission induced by a dissipative environment. These developments provide a rigorous platform to assess the validity of classical noise models of decoherence.",quant-ph,Quantum Physics
Universality classes of non-Hermitian random matrices,"Non-Hermitian random matrices have been utilized in such diverse fields as dissipative and stochastic processes, mesoscopic physics, nuclear physics, and neural networks. However, the only known universal level-spacing statistics is that of the Ginibre ensemble characterized by complex-conjugation symmetry. Here we report our discovery of two other distinct universality classes characterized by transposition symmetry. We find that transposition symmetry alters repulsive interactions between two neighboring eigenvalues and deforms their spacing distribution. Such alteration is not possible with other symmetries including Ginibre's complex-conjugation symmetry which can affect only nonlocal correlations. Our results complete the non-Hermitian counterpart of Wigner-Dyson's threefold universal statistics of Hermitian random matrices and serve as a basis for characterizing nonintegrability and chaos in open quantum systems with symmetry.",quant-ph,Quantum Physics
Experimental Quantum-enhanced Cryptographic Remote Control,"The Internet of Things (IoT), as a cutting-edge integrated cross-technology, promises to informationize people's daily lives, while being threatened by continuous challenges of eavesdropping and tampering. The emerging quantum cryptography, harnessing the random nature of quantum mechanics, may also enable unconditionally secure control network, beyond the applications in secure communications. Here, we present a quantum-enhanced cryptographic remote control scheme that combines quantum randomness and one-time pad algorithm for delivering commands remotely. We experimentally demonstrate this on an unmanned aircraft vehicle (UAV) control system. We precharge quantum random number (QRN) into controller and controlee before launching UAV, instead of distributing QRN like standard quantum communication during flight. We statistically verify the randomness of both quantum keys and the converted ciphertexts to check the security capability. All commands in the air are found to be completely chaotic after encryption, and only matched keys on UAV can decipher those commands precisely. In addition, the controlee does not response to the commands that are not or incorrectly encrypted, showing the immunity against interference and decoy. Our work adds true randomness and quantum enhancement into the realm of secure control algorithm in a straightforward and practical fashion, providing a promoted solution for the security of artificial intelligence and IoT.",quant-ph,Quantum Physics
Do psi-ontology theorems prove that the wave function is not epistemic?,"As a counterexample to $$-ontology theorems we consider a $$-epistemic interpretation of the wave function in the configuration space representation with a configuration space trajectory defining the ontology. This shows that $$-ontology theorems appear unable to prove that the wave function cannot be interpreted as epistemic. We identify the criterion used to decide if $$ is epistemic or ontological as the misleading part. Different states having overlaps is only sufficient for being epistemic. In an epistemic interpretation the information which completely identifies the wave function also has an objective base in reality - the preparation procedure, which has to be really executed to prepare a state. Naming the $\in $ ""hidden variables"" and considering them as part of the state of the system we also identify as misleading, because it hides the possibility that they may be visible and external to the system. This leads to further misconceptions about the consequences of the theorems.",quant-ph,Quantum Physics
An optimization problem for finite point interaction families,"We consider the spectral problem for a family of $N$ point interactions of the same strength confined to a manifold with a rotational symmetry, a circle or a sphere, and ask for configurations that optimize the ground state energy of the corresponding singular Schrdinger operator. In case of the circle the principal eigenvalue is sharply maximized if the point interactions are distributed at equal distances. The analogous question for the sphere is much harder and reduces to a modification of Thomson problem; we have been able to indicate the unique maximizer configurations for $N=2,\,3,\,4,\,6,\,12$. We also discuss the optimization for one-dimensional point interactions on an interval with periodic boundary conditions. We show that the equidistant distributions give rise to maximum ground state eigenvalue if the interactions are attractive, in the repulsive case we get the same result for weak and strong coupling and we conjecture that it is valid generally.",quant-ph,Quantum Physics
Phase diagram of bipartite entanglement,We investigate the features of the entanglement spectrum (distribution of the eigenvalues of the reduced density matrix) of a large quantum system in a pure state. We consider all Rnyi entropies and recover purity and von Neumann entropy as particular cases. We construct the phase diagram of the theory and unveil the presence of two critical lines.,quant-ph,Quantum Physics
Pair State Transfer,"Let $L$ denote the Laplacian matrix of a graph $G$. We study continuous quantum walks on $G$ defined by the transition matrix $U(t)=\exp\left(itL\right)$. The initial state is of the pair state form, $e_a-e_b$ with $a,b$ being any two vertices of $G$. We provide two ways to construct infinite families of graphs that have perfect pair transfer. We study a ""transitivity"" phenomenon which cannot occur in vertex state transfer. We characterize perfect pair state transfer on paths and cycles. We also study the case when quantum walks are generated by the unsigned Laplacians of underlying graphs and the initial states are of the plus state form, $e_a+e_b$. When the underlying graphs are bipartite, plus state transfer is equivalent to pair state transfer.",quant-ph,Quantum Physics
Wormhole Traversability via Quantum Random Walks,"In this work we seek to generalize the connection between traversable wormholes and quantum channels. We do this via a connection between traversable wormholes and graph geometries on which we can perform quantum random walks for signal transmission. As a proof of principle, we give an example of a geometry for which quantum random walk signal traversal occurs exponentially faster than a standard ballistic classical traversal. We then generalize this connection to superpositions over holographic states, and argue that the quantum random walk/graph geometry picture works to study quantum channel properties of such states.",quant-ph,Quantum Physics
"Comment on ""Fully device-independent conference key agreement"" [Phys. Rev. A 97, 022307 (2018)]","In this manuscript we discuss the device-independent conference key agreement (DICKA) protocol [Phys. Rev. A 97, 022307 (2018)]. We show that the suggested honest implementation fails, because perfect correlated measurement results and the required Bell-inequality violation cannot be achieved simultaneously, in contradiction to what is claimed. We further show via semidefinite programming that there cannot exist any suitable honest implementation in the tripartite setting, rendering the DICKA protocol incomplete.",quant-ph,Quantum Physics
Entanglement in Higher-Radix Quantum Systems,"Entanglement is an important phenomenon that enables quantum information processing algorithms and quantum communications protocols. Although entangled quantum states are often described in radix-2, higher-radix qudits can become entangled as well. In this work, we both introduce partial entanglement, a concept that does not exist for radix-2 quantum systems, and differentiate between partial and maximal entanglement within non-binary quantum information processing systems. We also develop and present higher-radix maximal entanglement generator circuits that are analogous to the well-known Bell state generator for binary quantum systems. Because higher-dimensioned qudits can be subjected to entangling processes that result in either partially or maximally entangled states, we demonstrate how higher-radix qudit circuits can be composed to generate these varying degrees of partial quantum entanglement. Theoretical results are provided for the general case where the radix is greater than two, and specific results based on a pair of radix-4 qudits are described.",quant-ph,Quantum Physics
Dynamical Scaling Laws of Out-of-Time-Ordered Correlators,"The out-of-time-ordered correlator (OTOC) is central to the understanding of information scrambling in quantum many-body systems. In this work, we show that the OTOC in a quantum many-body system close to its critical point obeys dynamical scaling laws which are specified by a few universal critical exponents of the quantum critical point. Such scaling laws of the OTOC imply a universal form for the butterfly velocity of a chaotic system in the quantum critical region and allow one to locate the quantum critical point and extract all universal critical exponents of the quantum phase transitions. We numerically confirm the universality of the butterfly velocity in a chaotic model, namely the transverse axial next-nearest-neighbor Ising model, and show the feasibility of extracting the critical properties of quantum phase transitions from OTOC using the Lipkin-Meshkov-Glick (LMG) model.",quant-ph,Quantum Physics
Reduced density matrix of nonlocal identical particles,"We probe the theoretical connection among three different approaches to analyze the entanglement of identical particles, i.e., the first quantization language (1QL), elementary-symmetric/exterior products (which has the mathematical equivalence to no-labeling approaches), and the algebraic approach based on the GNS construction. Among several methods to quantify the entanglement of identical particles, we focus on the computation of reduced density matrices, which can be achieved by the concept of \emph{symmetrized partial trace} defined in 1QL. We show that the symmetrized partial trace corresponds to the interior product in symmetric and exterior algebra (SEA), which also corresponds to the subalgebra restriction in the algebraic approach based on GNS representation. Our research bridges different viewpoints for understanding the quantum correlation of identical particles in a consistent manner.",quant-ph,Quantum Physics
Non-Hermitian phase transition and eigenstate localization induced by asymmetric coupling,"We investigate a uniformly coupled non-Hermitian system with asymmetric coupling amplitude. The asymmetric coupling equals to a symmetric coupling threaded by an imaginary gauge field. In a closed configuration, the imaginary gauge field leads to an imaginary magnetic flux, which induces a non-Hermitian phase transition. For an open boundary, the imaginary gauge field results in an eigenstate localization. The eigenstates under Dirac and biorthogonal norms and the scaling laws are quantitatively investigated to show the affect of asymmetric coupling induced one-way amplification. However, the imaginary magnetic flux does not inevitably induce the non-Hermitian phase transition for systems without translation invariance, this is elucidated from the non-Hermitian phase transition in the non-Hermitian ring with a single coupling defect. Our findings provide insights into the non-Hermitian phase transition and one-way localization.",quant-ph,Quantum Physics
Quantum renewal processes,"We introduce a general construction of master equations with memory kernel whose solutions are given by completely positive trace preserving maps. These dynamics going beyond the Lindblad paradigm are obtained with reference to classical renewal processes, so that they are termed quantum renewal processes. They can be described by means of semigroup dynamics interrupted by jumps, separated by independently distributed time intervals, following suitable waiting time distributions. In this framework, one can further introduce modified processes, in which the first few events follow different distributions. A crucial role, marking an important difference with respect to the classical case, is played by operator ordering. Indeed, for the same choice of basic quantum transformations, different quantum dynamics arise. In particular, for the case of modified processes, it is natural to consider the time inverted operator ordering, in which the last few events are distributed differently.",quant-ph,Quantum Physics
Experimental Demonstration of Force Driven Quantum Harmonic Oscillator in IBM Quantum Computer,"Though algorithms for quantum simulation of Quantum Harmonic Oscillator (QHO) have been proposed, still they have not yet been experimentally verified. Here, for the first time, we demonstrate a quantum simulation of QHO in the presence of both time-varying and constant force field for both one and two dimensional case. New quantum circuits are developed to simulate both the one and two-dimensional QHO and are implemented on the real quantum chip ""ibmqx4"". Experimental data, clearly illustrating the dynamics of QHO in the presence of time-dependent force field, are presented in graphs for different frequency parameters in the Hamiltonian picture.",quant-ph,Quantum Physics
Spintronics meets nonadiabatic molecular dynamics: Geometric spin torque and damping on noncollinear classical magnetism due to electronic open quantum system,"We analyze a quantum-classical hybrid system of steadily precessing slow classical localized magnetic moments, forming a head-to-head domain wall, embedded into an open quantum system of fast nonequilibrium electrons. The electrons reside within a metallic wire connected to macroscopic reservoirs. The model captures the essence of dynamical noncollinear and noncoplanar magnetic textures in spintronics, while making it possible to obtain the exact time-dependent nonequilibrium density matrix of electronic system and split it into four contributions. The Fermi surface contribution generates dissipative (or damping-like in spintronics terminology) spin torque on the moments, and one of the two Fermi sea contributions generates geometric torque dominating in the adiabatic regime. When the coupling to the reservoirs is reduced, the geometric torque is the only nonzero contribution. Locally it has both nondissipative (or field-like in spintronics terminology) and damping-like components, but with the sum of latter being zero, which act as the counterparts of geometric magnetism force and electronic friction in nonadiabatic molecular dynamics. Such current-independent geometric torque is absent from widely used micromagnetics or atomistic spin dynamics modeling of magnetization dynamics based on the Landau-Lifshitz-Gilbert equation, where previous analysis of Fermi surface-type torque has severely underestimated its magnitude.",quant-ph,Quantum Physics
Non-Markovian quantum dynamics: Extended correlated projection superoperators,"The correlated projection superoperator techniques provide a better understanding about how correlations lead to strong non-Markovian effects in open quantum systems. Their superoperators are independent of initial state, which may not be suitable for some cases. To improve this, we develop a new approach, that is extending the composite system before use the correlated projection superoperator techniques. Such approach allows choosing different superoperators for different initial states. We apply these techniques to a simple model to illustrate the general approach. The numerical simulations of the full Schrdinger equation of the model reveal the power and efficiency of the method.",quant-ph,Quantum Physics
Dissipative dynamical Casimir effect in terms of the complex spectral analysis in the symplectic-Floquet space,"Dynamical Casimir effect of the optomechanical cavity interacting with one-dimensional photonic crystal is theoretically investigated in terms of the complex spectral analysis of Floquet-Liouvillian in the symplectic-Floquet space. The quantum vacuum fluctuation of the intra-cavity mode is parametrically amplified by a periodic motion of the mirror boundary, and the amplified photons are spontaneously emitted to the photonic band. We have derived the non-Hermitian effective Floquet-Liouvillian from the total system Liouvillian with the use of the Brillouin-Wigner-Feshbach projection method in the symplectic-Floquet space. The microscopic dissipation process of the photon emission from the cavity has been taken into account by the energy-dependent self-energy. We have obtained the discrete eigenmodes of the total system by non-perturbatively solving the nonlinear complex eigenvalue problem of the effective Floquet-Liouvillian, where the eigenmodes are represented by the multimode Bogoliubov transformation. Based on the microscopic dynamics, the nonequilibrium stationary eigenmodes are identified as the eigenmodes with vanishing values of their imaginary parts due to the balance between the parametric amplification and dissipation effects. We have found that the nonlocal stationary eigenmode appears when the mixing between the cavity mode and the photonic band is caused by the indirect virtual transition, where the external field frequency to cause the DCE can be largely reduced by using the finite bandwidth photonic band.",quant-ph,Quantum Physics
Edge states of the long-range Kitaev chain: an analytical study,"We analyze the properties of the edge states of the one-dimensional Kitaev model with long-range anisotropic pairing and tunneling. Tunneling and pairing are assumed to decay algebraically with exponents $$ and $$, respectively, and $,>1$. We determine analytically the decay of the edges modes. We show that the decay is exponential for $=$ and when the coefficients scaling tunneling and pairing terms are equal. Otherwise, the decay is exponential at sufficiently short distances and then algebraic at the asymptotics. We show that the exponent of the algebraic tail is determined by the smallest exponent between $$ and $$. Our predictions are in agreement with numerical results found by exact diagonalization and in the literature.",quant-ph,Quantum Physics
Shareability of Quantum Steering and its Relation with Entanglement,"Steerability is a characteristic of quantum correlations lying in between entanglement and Bell nonlocality. Understanding how these steering correlations can be shared between different parties has profound applications in ensuring security of quantum communication protocols. Here we show that at most two bipartite reduced states of a three qubit state can violate the three settings CJWR linear steering inequality contrary to two settings linear steering inequality. This result explains that quantum steering correlations have limited shareability properties and can sometimes even be nonmonogamous. In contrast to the two setting measurement scenario, three setting scenario turns out to be more useful to develop deeper understanding of shareability of tripartite steering correlations. Apart from distribution of steering correlations, several relations between reduced bipartite steering, different measures of bipartite entanglement of reduced states and genuine tripartite entanglement are presented here. The results enable detection of different kind of tripartite entanglement.",quant-ph,Quantum Physics
"Comment on ""Observing the ""quantum Cheshire cat"" effect with noninvasive weak measurement''","In a very recent work [arXiv:2004.07451], Kim et al claimed to have made the first genuine experimental observation of the Quantum Cheshire Cat effect. We dispute this claim on the ground that the setup employed is not adequate for making the weak measurements that define this interferometric effect. Half of the necessary weak values are not observed, and the other half is obtained indirectly by combining results measured with distinct setups.",quant-ph,Quantum Physics
Superhyperfine induced photon-echo collapse of erbium in Y$_2$SiO$_5$,We investigate the decoherence of Er$^{3+}$ in Y$_2$SiO$_5$ at low magnetic fields using the photon-echo technique. We reproduce accurately a variety of the decay curves with a unique coherence time by considering the so-called superhyperfine modulation induced by a large number of neighbouring spins. There is no need to invoke any characteristic time of the spin fluctuations to reproduce very different decay curves. The number of involved nuclei increases when the magnetic is lowered. The experiment is compared with a model associating 100 surrounding ions with their exact positions in the crystal frame. We also derive an approximate spherical model (angular averaging) to interpret the main feature the observed decay curves close to zero-field.,quant-ph,Quantum Physics
Long-distance free-space measurement-device-independent quantum key distribution,"Measurement-device-independent quantum key distribution (MDI-QKD), based on two-photon interference, is immune to all attacks against the detection system and allows a QKD network with untrusted relays. Since the MDI-QKD protocol was proposed, fibre-based implementations have been rapidly developed towards longer distance, higher key rates, and network verification. However, owing to the effect of atmospheric turbulence, MDI-QKD over free-space channel remains experimentally challenging. Here, by developing the robust adaptive optics system, high precision time synchronization and frequency locking between independent photon sources located far apart, we realised the first free-space MDI-QKD over a 19.2-km urban atmospheric channel, which well exceeds the effective atmospheric thickness. Our experiment takes the first step towards satellite-based MDI-QKD. Moreover, the technology developed here opens the way to quantum experiments in free space involving long-distance interference of independent single photons.",quant-ph,Quantum Physics
An Ensemble Approach for Compressive Sensing with Quantum,"We leverage the idea of a statistical ensemble to improve the quality of quantum annealing based binary compressive sensing. Since executing quantum machine instructions on a quantum annealer can result in an excited state, rather than the ground state of the given Hamiltonian, we use different penalty parameters to generate multiple distinct quadratic unconstrained binary optimization (QUBO) functions whose ground state(s) represent a potential solution of the original problem. We then employ the attained samples from minimizing all corresponding (different) QUBOs to estimate the solution of the problem of binary compressive sensing. Our experiments, on a D-Wave 2000Q quantum processor, demonstrated that the proposed ensemble scheme is notably less sensitive to the calibration of the penalty parameter that controls the trade-off between the feasibility and sparsity of recoveries.",quant-ph,Quantum Physics
Quantum Optical Response of a Hybrid Optomechanical Device embedded with a Qubit,"We theoretically investigate the optical response in a hybrid quantum optomechanical system consisting of two optically coupled micro-cavities in which a two-level system (qubit) is embedded on a movable membrane. The qubit can either be a defect which interacts with the mechanical oscillator via the linear Jaynes-Cummings interaction or a superconducting charge qubit coupled with the mechanical mode via nonlinear interaction. We find that coherent perfect transmission (CPT), coherent perfect synthesis (CPS) and optomechanically induced absorption (OMIA) can be generated by suitably adjusting the system parameters. We find that the qubit and its interaction with the mechanical oscillator emerges as a new handle to control these quantum optical properties. The presence of the qubit results in four points where CPT and CPS can be realized compared to the pure optomechanical case (i.e. in the absence of qubit) where only three points are attained. This shows that the presence of the qubit gives us more flexibility in choosing the appropriate parameter regime where CPT and CPS can be attained and controlled. We also find that OMIA shows three distinct peaks both in the linear and nonlinear cases. In the absence of the qubit, OMIA is converted to optomechanically induced transmission (OMIT). An increase in in the qubit decay rate also shows a transition from OMIA to OMIT. Our study reveals that the optical response of the nonlinear case is relatively rapid (more sensitive) compared to the linear case to changes in the system parameters. This demonstrates the potential use of this hybrid system in designing tunable all-optical-switch and photon-router both of which forms an important element of a quantum information network.",quant-ph,Quantum Physics
Why does the Kerr-Newman black hole have the same gyromagnetic ratio as the electron?,"We have recently proposed a deterministic matrix dynamics at the Planck scale, for gravity coupled to Dirac fermions, evolving in the so-called Connes time. By coarse-graining this dynamics over time intervals much larger than Planck time, we derived the space-time manifold, quantum theory, and classical general relativity, as low energy emergent approximations to the underlying matrix dynamics. In the present article, we show how to include Yang-Mills gauge fields in this Planck scale matrix dynamics. We do this by appropriately modifying the fundamental action for the previously introduced `atom' of space-time-matter [which we now call an `aikyon']. This is achieved by modifying the Dirac operator to include a `potential' for the Yang-Mills aspect, and a `current' for the Yang-Mills charge. Our work opens up an avenue for unification of gravity with gauge-fields and Dirac fermions. We show how spontaneous localisation in the matrix dynamics gives rise to general relativity coupled to gauge-fields and relativistic point particles, in the classical limit. We use this formalism to explain the remarkable fact that the Kerr-Newman black hole has the same value for the gyromagnetic ratio as that for a Dirac fermion, both being twice the classical value.",quant-ph,Quantum Physics
Sharp negative differential resistance from vibrational mode softening in molecular junctions,"We unravel the critical role of vibrational mode softening in single-molecule electronic devices at high bias. Our theoretical analysis is carried out with a minimal model for molecular junctions, with mode softening arising due to quadratic electron-vibration couplings, and by developing a mean-field approach. We discover that the negative sign of the quadratic electron-vibration coupling coefficient can realize at high voltage a sharp negative differential resistance (NDR) effect with a large peak-to-valley ratio. Calculated current-voltage characteristics, obtained based on ab initio parameters for a nitro-substituted oligo(phenylene ethynylene) junction, agree very well with measurements. Our results establish that vibrational mode softening is a crucial effect at high voltage, underlying NDR, a substantial diode effect, and the breakdown of current-carrying molecular junctions.",quant-ph,Quantum Physics
"Beyond the Lindblad Master Equation: Heat, Work and Energy Currents in Boundary Driven Spin Chains","We consider the accurate investigation of the energy current and its components, heat and work, in some boundary driven quantum spin systems. The expressions for the currents, as well as the associated Lindblad master equation, are obtained via a repeated interaction scheme. We consider small systems in order to analytically compute the steady distribution to study the current in the steady state. Asymmetrical XXZ and quantum Ising models are detailed analyzed. For the XXZ chain we present cases in which different compositions of heat and work currents, obtained via the repeated interaction protocol, lead to the same energy current, which may be obtained via the Lindblad master equation. For the quantum Ising chain, we describe a case of zero energy current and novanishing heat and work currents. Our findings make clear that to talk about heat in these boundary driven spin quantum systems we must go beyond an investigation involving only the Lindblad master equation.",quant-ph,Quantum Physics
Persistent dark states in anisotropic central spin models,"Long-lived dark states, in which an experimentally accessible qubit is not in thermal equilibrium with a surrounding spin bath, are pervasive in solid-state systems. We explain the ubiquity of dark states in a large class of inhomogenous central spin models using the proximity to integrable lines with exact dark eigenstates. At numerically accessible sizes, dark states persist as eigenstates at large deviations from integrability, and the qubit retains memory of its initial polarization at long times. Although the eigenstates of the system are chaotic, exhibiting exponential sensitivity to small perturbations, they do not satisfy the eigenstate thermalization hypothesis. Rather, we predict long relaxation times that increase exponentially with system size. We propose that this intermediate chaotic but non-ergodic regime characterizes mesoscopic quantum dot and diamond defect systems, as we see no numerical tendency towards conventional thermalization with a finite relaxation time.",quant-ph,Quantum Physics
Microscopic models for Kitaev's sixteenfold way of anyon theories,"In two dimensions, the topological order described by $\mathbb{Z}_2$ gauge theory coupled to free or weakly interacting fermions with a nonzero spectral Chern number $$ is classified by $\; \mathrm{mod}\; 16$ as predicted by Kitaev [Ann. Phys. 321, 2 (2006)]. Here we provide a systematic and complete construction of microscopic models realizing this so-called sixteenfold way of anyon theories. These models are defined by $$ matrices satisfying the Clifford algebra, enjoy a global $\mathrm{SO}()$ symmetry, and live on either square or honeycomb lattices depending on the parity of $$. We show that all these models are exactly solvable by using a Majorana representation and characterize the topological order by calculating the topological spin of an anyonic quasiparticle and the ground-state degeneracy. The possible relevance of the $=2$ and $=3$ models to materials with Kugel-Khomskii-type spin-orbital interactions is discussed.",quant-ph,Quantum Physics
Thermoelectric current in a graphene Cooper pair splitter,"Thermoelectric effect generating electricity from thermal gradient and vice versa appears in numerous generic applications. Recently, an original prospect of thermoelectricity arising from the nonlocal Cooper pair splitting (CPS) and the elastic co-tunneling (EC) in hybrid normal metal-superconductor-normal metal (NSN) structures was foreseen. Here we demonstrate experimentally the existence of non-local Seebeck effect in a graphene-based CPS device comprising two quantum dots connected to an aluminum superconductor and theoretically validate the observations. This non-local Seebeck effect offers an efficient tool for producing entangled electrons.",quant-ph,Quantum Physics
Witnessing the non-classical nature of gravity in the presence of unknown interactions,"General relativity as a classical field theory does not predict gravitationally induced entanglement, as such, recent proposals seek an empirical demonstration of this feature which would represent a significant milestone for physics. We introduce improvements to a spin witness protocol that reduce the highly challenging experimental requirements. After rigorously assessing approximations from the original proposal [S. Bose et al. Phys. Rev. Lett. 119, 240401 (2017)], we focus on entanglement witnessing. We propose a new witness which greatly reduces the required interaction time, thereby making the experiment feasible for higher decoherence rates, and we show how statistical analysis can separate the gravitational contribution from other possibly dominant and ill-known interactions. We point out a potential loophole and show how it can be closed using state tomography.",quant-ph,Quantum Physics
Optical Coherence Tomography with a nonlinear interferometer in the high parametric gain regime,"We demonstrate optical coherence tomography based on an SU(1,1) nonlinear interferometer with high-gain parametric down-conversion. For imaging and sensing applications, this scheme promises to outperform previous experiments working at low parametric gain, since higher photon fluxes provide lower integration times for obtaining high-quality images. In this way one can avoid using single-photon detectors or CCD cameras with very high sensitivities, and standard spectrometers can be used instead. Other advantages are: higher sensitivity to small loss and amplification before detection, so that the detected light power considerably exceeds the probing one.",quant-ph,Quantum Physics
The guidance theorem of de Broglie,"We review some aspects of the double solution theory proposed by de Broglie at the beginning of the quantum era (i.e., in the period 1924-28). We specifically analyze and rederive the so called guidance theorem which is a key element of the full theory. We compare the double solution approach to the most known pilot-wave interpretation, also known as de Broglie-Bohm or Bohmian mechanics. We explain why de Broglie rejected the pilot wave interpretation and advocated the double solution. We also discuss some philosophical issues related to difference of strategies between de Broglie on the one side and Bohm on the other side.",quant-ph,Quantum Physics
Transient Work Function Gating: A New Photoemission Regime,"We present the theoretical basis for a new photoemission regime, transient work function gating (TWFG), that temporally and energetically gates photoemission and produces near-threshold photoelectrons with thermally limited emittance, percent-level quantum efficiency, and control over temporal coherence. The technique consists of actively gating the work function of a generalized photocathode using non-ionizing long-wavelength optical field to produce an adiabatic modulation of the carrier density at their surface. We examine TWFG as a means to circumvent the long-standing trade-off between low emittance and high quantum efficiency, untethered to particle source or photocathode specifics. TWFG promises new opportunities in photoemission physics for next generation electron and accelerator-based x-ray photon sources.",quant-ph,Quantum Physics
Spectral statistics of Toeplitz matrices,Spectral statistics of hermitian random Toeplitz matrices with independent identically distributed elements is investigated numerically. It is found that the eigenvalue statistics of complex Toeplitz matrices is surprisingly well approximated by the semi-Poisson distribution belonging to intermediate-type statistics observed in certain pseudo-integrable billiards. The origin of intermediate behaviour could be attributed to the fact that Fourier transformed random Toeplitz matrices have the same slow decay outside the main diagonal as critical random matrix ensembles. The statistical properties of the full spectrum of real random Toeplitz matrices with i.i.d. elements are close to the Poisson distribution but each of their constituted sub-spectra is again well described by the semi-Poisson distribution. The findings open new perspective in intermediate statistics.,quant-ph,Quantum Physics
Scalable quantum processor noise characterization,"Measurement fidelity matrices (MFMs) (also called error kernels) are a natural way to characterize state preparation and measurement errors in near-term quantum hardware. They can be employed in post processing to mitigate errors and substantially increase the effective accuracy of quantum hardware. However, the feasibility of using MFMs is currently limited as the experimental cost of determining the MFM for a device grows exponentially with the number of qubits. In this work we present a scalable way to construct approximate MFMs for many-qubit devices based on cumulant expansion. Our method can also be used to characterize various types of correlation error.",quant-ph,Quantum Physics
Quantum state preparation with multiplicative amplitude transduction,"Quantum state preparation is an important class of quantum algorithms that is employed as a black-box subroutine in many algorithms, or used by itself to generate arbitrary probability distributions. We present a novel state preparation method that utilizes less quantum computing resource than the existing methods. Two variants of the algorithm with different emphases are introduced. One variant uses fewer qubits and no controlled gates, while the other variant potentially requires fewer gates overall. A general analysis is given to estimate the number of qubits necessary to achieve a desired precision in the amplitudes of the computational basis states. The validity of the algorithm is demonstrated using a prototypical problem of generating Ising model spin configurations according to its Boltzmann distribution.",quant-ph,Quantum Physics
Angle-Resolved Attosecond Streaking of Twisted Attosecond Pulses,"The present work focuses on the characterisation of the amount of orbital angular momentum (OAM) encoded in the twisted attosecond pulses via energy- and angle-resolved attosecond streaking in pump-probe setup. It is found that the photoelectron spectra generated by the linearly polarised twisted pulse with different OAM values exhibit angular modulations, whereas circularly polarised twisted pulse yields angular isotropic spectra. It is demonstrated that the energy- and angle-resolved streaking spectra are sensitive to the OAM values of the twisted pulse. Moreover, the different combinations of the polarisation of the twisted pump pulse and strong infrared probe pulse influence the streaking spectra differently. The characterisation of the OAM carrying twisted attosecond pulses opens up the possibility to explore helical light-matter interaction on attosecond timescale.",quant-ph,Quantum Physics
Investigating the Chinese Postman Problem on a Quantum Annealer,"The recent availability of quantum annealers has fueled a new area of information technology where such devices are applied to address practically motivated and computationally difficult problems with hardware that exploits quantum mechanical phenomena. D-Wave annealers are promising platforms to solve these problems in the form of quadratic unconstrained binary optimization. Here we provide a formulation of the Chinese postman problem that can be used as a tool for probing the local connectivity of graphs and networks. We treat the problem classically with a tabu algorithm and using a D-Wave device. We systematically analyze computational parameters associated with the specific hardware. Our results clarify how the interplay between the embedding due to limited connectivity of the Chimera graph, the definition of logical qubits, and the role of spin-reversal controls the probability of reaching the expected solution.",quant-ph,Quantum Physics
Are quantum spins but small perturbations of ontological Ising spins?,"The dynamics-from-permutations of classical Ising spins is generalized here for an arbitrarily long chain. This serves as an ontological model with discrete dynamics generated by pairwise exchange interactions defining the unitary update operator. The model incorporates a finite signal velocity and resembles in many aspects a discrete free field theory. We deduce the corresponding Hamiltonian operator and show that it generates an exact terminating Baker-Campbell-Hausdorff formula. Motivation for this study is provided by the Cellular Automaton Interpretation of Quantum Mechanics. We find that our ontological model, which is classical and deterministic, appears as if of quantum mechanical kind in an appropriate formal description. However, it is striking that (in principle arbitrarily) small deformations of the model turn it into a genuine quantum theory. This supports the view that quantum mechanics stems from an epistemic approach handling physical phenomena.",quant-ph,Quantum Physics
Cavity magnon polariton based precision magnetometry,"A photon-magnon hybrid system can be realised by coupling the electron spin resonance of a magnetic material to a microwave cavity mode. The quasiparticles associated with the system dynamics are the cavity magnon polaritons, which arise from the mixing of strongly coupled magnons and photons. We illustrate how these particles can be used to probe the magnetisation of a sample with a remarkable sensitivity, devising suitable spin-magnetometers which ultimately can be used to directly assess oscillating magnetic fields. Specifically, the capability of cavity magnon polaritons of converting magnetic excitations to electromagnetic ones, allows for translating to magnetism the quantum-limited sensitivity reached by state-of-the-art electronics. Here we employ hybrid systems composed of microwave cavities and ferrimagnetic spheres, to experimentally implement two types of novel spin-magnetometers.",quant-ph,Quantum Physics
Scattering of Spin-$\frac{1}{2}$ Particles from a $\cal PT$-symmetric Complex Potential,"In this letter, we study the scattering of spin-$\frac{1}{2}$ particles from a spin-independent parity time ($\cal PT$)-symmetric complex potential, and for the first time, theoretically demonstrate the coexistence of $\cal PT$-symmetric and $\cal PT$-broken phases for broadband energy spectra in this system. We also show the existence of anisotropic transmission resonances, accessible through the tuning of energy. Our results are promising for applications in spintronics, semiconductor-based devices, and a better understanding of the topological surface states.",quant-ph,Quantum Physics
Quantum dynamics in low-dimensional topological systems,"The discovery of topological matter has revolutionized the field of condensed matter physics giving rise to many interesting phenomena, and fostering the development of new quantum technologies. In this thesis we study the quantum dynamics that take place in low dimensional topological systems, specifically 1D and 2D lattices that are instances of topological insulators. First, we study the dynamics of doublons, bound states of two fermions that appear in systems with strong Hubbard-like interactions. We also include the effect of periodic drivings and investigate how the interplay between interaction and driving produces novel phenomena. Prominent among these are the disappearance of topological edge states in the SSH-Hubbard model, the sublattice confinement of doublons in certain 2D lattices, and the long-range transfer of doublons between the edges of any finite lattice. Then, we apply our insights about topological insulators to a rather different setup: quantum emitters coupled to the photonic analogue of the SSH model. In this setup we compute the dynamics of the emitters, regarding the photonic SSH model as a collective structured bath. We find that the topological nature of the bath reflects itself in the photon bound states and the effective dipolar interactions between the emitters. Also, the topology of the bath affects the single-photon scattering properties. Finally, we peek into the possibility of using these kind of setups for the simulation of spin Hamiltonians and discuss the different ground states that the system supports.",quant-ph,Quantum Physics
A Sparse Model of Quantum Holography,"We study a sparse version of the Sachdev-Ye-Kitaev (SYK) model defined on random hypergraphs constructed either by a random pruning procedure or by randomly sampling regular hypergraphs. The resulting model has a new parameter, $k$, defined as the ratio of the number of terms in the Hamiltonian to the number of degrees of freedom, with the sparse limit corresponding to the thermodynamic limit at fixed $k$. We argue that this sparse SYK model recovers the interesting global physics of ordinary SYK even when $k$ is of order unity. In particular, at low temperature the model exhibits a gravitational sector which is maximally chaotic. Our argument proceeds by constructing a path integral for the sparse model which reproduces the conventional SYK path integral plus gapped fluctuations. The sparsity of the model permits larger scale numerical calculations than previously possible, the results of which are consistent with the path integral analysis. Additionally, we show that the sparsity of the model considerably reduces the cost of quantum simulation algorithms. This makes the sparse SYK model the most efficient currently known route to simulate a holographic model of quantum gravity. We also define and study a sparse supersymmetric SYK model, with similar conclusions to the non-supersymmetric case. Looking forward, we argue that the class of models considered here constitute an interesting and relatively unexplored sparse frontier in quantum many-body physics.",quant-ph,Quantum Physics
Balanced k-Means Clustering on an Adiabatic Quantum Computer,"Adiabatic quantum computers are a promising platform for approximately solving challenging optimization problems. We present a quantum approach to solving the balanced $k$-means clustering training problem on the D-Wave 2000Q adiabatic quantum computer. Existing classical approaches scale poorly for large datasets and only guarantee a locally optimal solution. We show that our quantum approach better targets the global solution of the training problem, while achieving better theoretic scalability on large datasets. We test our quantum approach on a number of small problems, and observe clustering performance similar to the best classical algorithms.",quant-ph,Quantum Physics
Controlled not connectivity in the Clifford group,"The Clifford group is the set of gates generated by the CZ gate, and the two local gates: the Hadamard and the Pi/2 phase shift gate. It is known that, for a two qubit system, the Clifford group C2 is a subgroup of order 92160 of the group of 4 by 4 unitary matrices. It is also known that the local Clifford gates LC2 is a subgroup of order 4608 of the group C2. In order to better understand the set C2, we make two matrices U1 and U2 in C2 equivalent if U_1=VU_2 for some V in LC2. We show that this equivalence relation splits C2 into 20 orbits, O1, ..., O20, each with 4608 elements. Moreover, for each orbit Oi, CZOi intersects 9 different orbits Oi1, ...,Oi9. Moreover, the intersection of Oij and CZOi has 512 matrices for each j=1,2, ..., ,9. The link https://www.youtube.com/watch?v=lcYtB2tnXFw&t=685s leads you to a YouTube video that explains the most important results in this paper.",quant-ph,Quantum Physics
Modification of quantum many-body relaxation by perturbations exhibiting a banded matrix structure,"We investigate how the observable relaxation behavior of an isolated quantum many-body system is modified in response to weak-to-moderate perturbations within a nonperturbative typicality framework. A key role is played by the so-called perturbation profile, which characterizes the dependence of the perturbation matrix elements in the eigenbasis of the unperturbed Hamiltonian on the difference of the corresponding energy eigenvalues. In particular, a banded matrix structure is quantitatively captured by a perturbation profile which approaches zero for large energy differences. The temporal modification of the relaxation is linked to the perturbation profile via a nonlinear integral equation, which admits approximate analytical solutions for sufficiently weak and strong perturbations, and for which we work out a numerical solution scheme in the general case. As an example, we consider a spin lattice model with a pronounced banded matrix structure, and we find very good agreement of the numerics with our analytical predictions without any free fit parameter.",quant-ph,Quantum Physics
Bias-free source-independent quantum random number generator,"A bias-free source-independent quantum random number generator scheme based on the measurement of vacuum fluctuation is proposed to realize the effective elimination of system bias and common mode noise introduced by the local oscillator. Optimal parameter settings are derived to avoid the system recording two canonically conjugate quadratures simultaneously in each measurement. In particular, it provides a new approach to investigate the performance difference between measuring two quadratures of equal and unequal intensity. It is experimentally demonstrated that the system supports 4.2 Gbps bias-free source-independent random number generation, where its common mode rejection ratio reaches 61.17 dB. Furthermore, the scheme offers an all-optical method facilitating the integration of source-independent quantum random number generators into compact chips.",quant-ph,Quantum Physics
QUANTIFY: A framework for resource analysis and design verification of quantum circuits,"Quantum resource analysis is crucial for designing quantum circuits as well as assessing the viability of arbitrary (error-corrected) quantum computations. To this end, we introduce QUANTIFY, which is an open-source framework for the quantitative analysis of quantum circuits. It is based on Google Cirq and is developed with Clifford+T circuits in mind, and it includes the necessary methods to handle Toffoli+H and more generalised controlled quantum gates, too. Key features of QUANTIFY include: (1) analysis and optimisation methods which are compatible with the surface code, (2) choice between different automated (mixed polarity) Toffoli gate decompositions, (3) semi-automatic quantum circuit rewriting and quantum gate insertion methods that take into account known gate commutation rules, and (4) novel optimiser types that can be combined with different verification methods (e.g. truth table or circuit invariants like number of wires). For benchmarking purposes QUANTIFY includes quantum memory and quantum arithmetic circuits. Experimental results show that the framework's performance scales to circuits with thousands of qubits.",quant-ph,Quantum Physics
Heralding Quantum Entanglement between Two Room-Temperature Atomic Ensembles,"Establishing quantum entanglement between individual nodes is crucial for building large-scale quantum networks, enabling secure quantum communication, distributed quantum computing, enhanced quantum metrology and fundamental tests of quantum mechanics. However, the shared entanglements have been merely observed in either extremely low-temperature or well-isolated systems, which limits the quantum networks for the real-life applications. Here, we report the realization of heralding quantum entanglement between two atomic ensembles at room temperature, where each of them contains billions of motional atoms. By measuring the mapped-out entangled state with quantum interference, concurrence and correlation, we strongly verify the existence of a single excitation delocalized in two atomic ensembles. Remarkably, the heralded quantum entanglement of atomic ensembles can be operated with the feature of delay-choice, which illustrates the essentiality of the built-in quantum memory. The demonstrated building block paves the way for constructing quantum networks and distributing entanglement across multiple remote nodes at ambient conditions.",quant-ph,Quantum Physics
Many-body quantum dynamics slows down at low density,"We study quantum many-body systems with a global U(1) conservation law, focusing on a theory of $N$ interacting fermions with charge conservation, or $N$ interacting spins with one conserved component of total spin. We define an effective operator size at finite chemical potential through suitably regularized out-of-time-ordered correlation functions. The growth rate of this density-dependent operator size vanishes algebraically with charge density; hence we obtain new bounds on Lyapunov exponents and butterfly velocities in charged systems at a given density, which are parametrically stronger than any Lieb-Robinson bound. We argue that the density dependence of our bound on the Lyapunov exponent is saturated in the charged Sachdev-Ye-Kitaev model. We also study random automaton quantum circuits and Brownian Sachdev-Ye-Kitaev models, each of which exhibit a different density dependence for the Lyapunov exponent, and explain the discrepancy. We propose that our results are a cartoon for understanding Planckian-limited energy-conserving dynamics at finite temperature.",quant-ph,Quantum Physics
Catalyzed entanglement concentration of qubit pairs,"We analytically obtain the maximum probability of converting a finite number of copies of an arbitrary two-qubit pure state to a single copy of a maximally entangled two-qubit pure state via entanglement assisted local operations and classical communications using a two-qubit catalyst state. We show that the optimal catalyst for this transformation is always more entangled than the initial state but any two-qubit state can act as a (non-optimal) catalyst. Interestingly, the entanglement of the optimal two-qubit catalyst state is shown to decrease with that of the initial state. Entanglement assisted strategies for obtaining multiple Bell states are discussed.",quant-ph,Quantum Physics
Density profile of a semi-infinite one-dimensional Bose gas and bound states of the impurity,"We study the effect of the boundary on a system of weakly interacting bosons in one dimension. It strongly influences the boson density which is completely suppressed at the boundary position. Away from it, the density is depleted over the distances on the order of the healing length at the mean-field level. Quantum fluctuations modify the density profile considerably. The local density approaches the average one as an inverse square of the distance from the boundary. We calculate an analytic expression for the density profile at arbitrary separations from the boundary. We then consider the problem of localization of a foreign quantum particle (impurity) in the potential created by the inhomogeneous boson density. At the mean-field level, we find exact results for the energy spectrum of the bound states, the corresponding wave functions, and the condition for interaction-induced localization. The quantum contribution to the boson density gives rise to small corrections of the bound state energy levels. However, it is fundamentally important for the existence of a long-range Casimir-like interaction between the impurity and the boundary.",quant-ph,Quantum Physics
Highly-efficient generation of coherent light at 2128 nm via degenerate optical-parametric oscillation,"Cryogenic operation in conjunction with new test-mass materials promises to reduce the sensitivity limitations from thermal noise in gravitational-wave detectors. The currently most advanced materials under discussion are crystalline silicon as a substrate with amorphous silicon-based coatings. They require, however, operational wavelengths around 2 $\mathrm$m to avoid laser absorption. Here, we present a light source at 2128 nm based on a degenerate optical parametric oscillator (DOPO) to convert light from a 1064 nm non-planar ring-oscillator (NPRO). We achieve an external conversion efficiency of $(88.3\,\pm\,1.4)\,\%$ at a pump power of 52 mW in PPKTP (periodically-poled potassium titanyl phosphate, internal efficiency was 94 %), from which we infer an effective non-linearity of $(4.75\,\pm\,0.18)\,\mathrm{pm/V}$. With our approach, light from the established and existing laser sources can be efficiently converted to the 2 $\mathrm$m regime, while retaining the excellent stability properties.",quant-ph,Quantum Physics
Higher-order non-Hermitian skin effect,"The non-Hermitian skin effect is a unique feature of non-Hermitian systems, in which an extensive number of boundary modes appear under the open boundary conditions. Here, we discover higher-order counterparts of the non-Hermitian skin effect that exhibit new boundary physics. In two-dimensional systems with the system size $L \times L$, while the conventional (first-order) skin effect accompanies $O\,( L^{2} )$ skin modes, the second-order skin effect accompanies $O\,( L )$ corner skin modes. This also contrasts with Hermitian second-order topological insulators, in which only $O\,( 1 )$ corner zero modes appear. Moreover, for the third-order skin effect in three dimensions, $O\,( L )$ corner skin modes appear from all $O\,( L^{3} )$ modes. We demonstrate that the higher-order skin effect originates from intrinsic non-Hermitian topology protected by spatial symmetry. We also show that it accompanies the modification of the non-Bloch band theory in higher dimensions.",quant-ph,Quantum Physics
Parameter-dependent unitary transformation approach for quantum Rabi model,"Quantum Rabi model has been exactly solved by employing the parameter-dependent unitary transformation method in both the occupation number representation and the Bargmann space. The analytical expressions for the complete energy spectrum consisting of two double-fold degenerate sub-energy spectra are presented in the whole range of all the physical parameters. Each energy level is determined by a parameter in the unitary transformation, which obeys a highly nonlinear equation. The corresponding eigenfunction is a convergent infinite series in terms of the physical parameters. Due to the level crossings between the neighboring eigenstates at certain physical parameter values, such the degeneracies could lead to novel physical phenomena in the two-level system with the light-matter interaction.",quant-ph,Quantum Physics
Certified quantum gates,"High quality, fully-programmable quantum processors are available with small numbers (<1000) of qubits, and the scientific potential of these near term machines is not well understood. If the small number of physical qubits precludes practical quantum error correction, how can these error-susceptible processors be used to perform useful tasks? We present a strategy for developing quantum error detection for certain gate imperfections that utilizes additional internal states and does not require additional physical qubits. Examples for adding error detection are provided for a universal gate set in the trapped ion platform. Error detection can be used to certify individual gate operations against certain errors, and the irreversible nature of the detection allows a result of a complex computation to be checked at the end for error flags.",quant-ph,Quantum Physics
Quantum algorithm for alchemical optimization in material design,"The development of tailored materials for specific applications is an active field of research in chemistry, material science and drug discovery. The number of possible molecules that can be obtained from a set of atomic species grow exponentially with the size of the system, limiting the efficiency of classical sampling algorithms. On the other hand, quantum computers can provide an efficient solution to the sampling of the chemical compound space for the optimization of a given molecular property. In this work we propose a quantum algorithm for addressing the material design problem with a favourable scaling. The core of this approach is the representation of the space of candidate structures as a linear superposition of all possible atomic compositions. The corresponding `alchemical' Hamiltonian drives then the optimization in both the atomic and electronic spaces leading to the selection of the best fitting molecule, which optimizes a given property of the system, e.g., the interaction with an external potential in drug design. The quantum advantage resides in the efficient calculation of the electronic structure properties together with the sampling of the exponentially large chemical compound space. We demonstrate both in simulations and in IBM Quantum hardware the efficiency of our scheme and highlight the results in a few test cases. These preliminary results can serve as a basis for the development of further material design quantum algorithms for near-term quantum computers.",quant-ph,Quantum Physics
QuAlg -- A symbolic algebra package for quantum information,"We introduce QuAlg, an open-source symbolic algebra package for quantum information. QuAlg supports working with qubit-states, qudit-states, fock-states and even wave-functions in infinite dimenional Hilbert spaces. States can have amplitudes which are pure complex numbers or more complicated symbolic expressions involving for example delta-distributions. These symbolic expressions can be automatically simplified and integrated. Furthermore, arbitrary operators can be defined, even transformations between different Hilbert spaces. Operators can be applied to states or used to defined measurement operators.",quant-ph,Quantum Physics
Superradiance paradox in waveguide lattices,"Recently, it has been suggested that the collective radiative decay of two point-like quantum emitters coupled to a waveguide, separated by a distance comparable to the coherence length of a spontaneously emitted photon, leads to an apparent $^{\prime}$superradiance paradox$^{\prime}$ by which one cannot decide whether independent or collective emission occurs. The resolution of the paradox stems from the strong non-Markovian dynamics arising from the delayed field-mediated atom interaction. Here we suggest an integrated optics platform to emulate the superradiance paradox, based on photon escape dynamics in waveguide lattices. Remarkably, Markovian decay dynamics and independent photon emission can be restored by frequent (Zeno-like) observation of the system.",quant-ph,Quantum Physics
$k$-Forrelation Optimally Separates Quantum and Classical Query Complexity,"Aaronson and Ambainis (SICOMP `18) showed that any partial function on $N$ bits that can be computed with an advantage $$ over a random guess by making $q$ quantum queries, can also be computed classically with an advantage $/2$ by a randomized decision tree making ${O}_q(N^{1-\frac{1}{2q}}^{-2})$ queries. Moreover, they conjectured the $k$-Forrelation problem -- a partial function that can be computed with $q = \lceil k/2 \rceil$ quantum queries -- to be a suitable candidate for exhibiting such an extremal separation.
  We prove their conjecture by showing a tight lower bound of $\widetilde(N^{1-1/k})$ for the randomized query complexity of $k$-Forrelation, where the advantage $= 2^{-O(k)}$. By standard amplification arguments, this gives an explicit partial function that exhibits an $O_(1)$ vs $(N^{1-})$ separation between bounded-error quantum and randomized query complexities, where $>0$ can be made arbitrarily small. Our proof also gives the same bound for the closely related but non-explicit $k$-Rorrelation function introduced by Tal (FOCS `20).
  Our techniques rely on classical Gaussian tools, in particular, Gaussian interpolation and Gaussian integration by parts, and in fact, give a more general statement. We show that to prove lower bounds for $k$-Forrelation against a family of functions, it suffices to bound the $\ell_1$-weight of the Fourier coefficients between levels $k$ and $(k-1)k$. We also prove new interpolation and integration by parts identities that might be of independent interest in the context of rounding high-dimensional Gaussian vectors.",quant-ph,Quantum Physics
Merged-element transmon,"Transmon qubits are ubiquitous in the pursuit of quantum computing using superconducting circuits. However, they have some drawbacks that still need to be addressed. Most importantly, the scalability of transmons is limited by the large device footprint needed to reduce the participation of the lossy capacitive parts of the circuit. In this work, we investigate and evaluate losses in an alternative device geometry, namely, the merged-element transmon (mergemon). To this end, we replace the large external shunt capacitor of a traditional transmon with the intrinsic capacitance of a Josephson junction (JJ) and achieve an approximately 100 times reduction in qubit dimensions. We report the implementation of the mergemon using a sputtered Nb--amorphous-Si--Nb trilayer film. In an experiment below 10 mK, the frequency of the readout resonator, capacitively coupled to the mergemon, exhibits a qubit-state dependent shift in the low power regime. The device also demonstrates the single- and multi-photon transitions that represent a weakly anharmonic system in the two-tone spectroscopy. The transition spectra are explained well with master-equation simulations. A participation ratio analysis identifies the dielectric loss of the a-Si tunnel barrier and its interfaces as the dominant source for qubit relaxation. We expect the mergemon to achieve high coherence in relatively small device dimensions when implemented using a low-loss, epitaxially-grown, and lattice-matched trilayer.",quant-ph,Quantum Physics
Geometric aspects of analog quantum search evolutions,"We use geometric concepts originally proposed by Anandan and Aharonov to show that the Farhi-Gutmann time optimal analog quantum search evolution between two orthogonal quantum states is characterized by unit efficiency dynamical trajectories traced on a projective Hilbert space. In particular, we prove that these optimal dynamical trajectories are the shortest geodesic paths joining the initial and the final states of the quantum evolution. In addition, we verify they describe minimum uncertainty evolutions specified by an uncertainty inequality that is tighter than the ordinary time-energy uncertainty relation. We also study the effects of deviations from the time optimality condition from our proposed Riemannian geometric perspective. Furthermore, after pointing out some physically intuitive aspects offered by our geometric approach to quantum searching, we mention some practically relevant physical insights that could emerge from the application of our geometric analysis to more realistic time-dependent quantum search evolutions. Finally, we briefly discuss possible extensions of our work to the geometric analysis of the efficiency of thermal trajectories of relevance in quantum computing tasks.",quant-ph,Quantum Physics
Predicting toxicity by quantum machine learning,"In recent years, parameterized quantum circuits have been regarded as machine learning models within the framework of the hybrid quantum-classical approach. Quantum machine learning (QML) has been applied to binary classification problems and unsupervised learning. However, practical quantum application to nonlinear regression tasks has received considerably less attention. Here, we develop QML models designed for predicting the toxicity of 221 phenols on the basis of quantitative structure activity relationship. The results suggest that our data encoding enhanced by quantum entanglement provided more expressive power than the previous ones, implying that quantum correlation could be beneficial for the feature map representation of classical data. Our QML models performed significantly better than the multiple linear regression method. Furthermore, our simulations indicate that the QML models were comparable to those obtained using radial basis function networks, while improving the generalization performance. The present study implies that QML could be an alternative approach for nonlinear regression tasks such as cheminformatics.",quant-ph,Quantum Physics
From megahertz to terahertz qubits encoded in molecular ions: theoretical analysis of dipole-forbidden spectroscopic transitions in N$\mathbf{_2^+}$,"Recent advances in quantum technologies have enabled the precise control of single trapped molecules on the quantum level. Exploring the scope of these new technologies, we studied theoretically the implementation of qubits and clock transitions in the spin, rotational, and vibrational degrees of freedom of molecular nitrogen ions including the effects of magnetic fields. The relevant spectroscopic transitions span six orders of magnitude in frequency illustrating the versatility of the molecular spectrum for encoding quantum information. We identified two types of magnetically insensitive qubits with very low (""stretched""-state qubits) or even zero (""magic"" magnetic-field qubits) linear Zeeman shifts. The corresponding spectroscopic transitions are predicted to shift by as little as a few mHz for an amplitude of magnetic-field fluctuations on the order of a few mG translating into Zeeman-limited coherence times of tens of minutes encoded in the rotations and vibrations of the molecule. We also found that the Q(0) line of the fundamental vibrational transition is magnetic-dipole allowed by interaction with the first excited electronic state of the molecule. The Q(0) transitions, which benefit from small systematic shifts for clock operation and high sensitivity to a possible variation in the proton-to-electron mass ratio, were so far not considered in single-photon spectra. Finally, we explored possibilities to coherently control the nuclear-spin configuration of N$_2^+$ through the magnetically enhanced mixing of nuclear-spin states.",quant-ph,Quantum Physics
Quantum Anonymity for Quantum Networks,"We present the first quantum anonymous notification (QAN) protocol that introduces anonymity and paves the way for anonymous secure quantum communication in quantum networks. QAN protocol has applications ranging from multiparty quantum computation to quantum internet. We utilize the QAN protocol to propose an anonymous quantum private comparison protocol in an $n$-node quantum network. This protocol can compare private information of any $2 \leq k \leq n$ parties with the help of the remaining $n-k$ parties and a semi-honest third party. These protocols feature a traceless property, i.e., encoding operations cannot be traced back to their originating sources. Security analysis shows that this protocol is robust against external adversaries and malicious participants.",quant-ph,Quantum Physics
Bit-Slicing the Hilbert Space: Scaling Up Accurate Quantum Circuit Simulation to a New Level,"Quantum computing is greatly advanced in recent years and is expected to transform the computation paradigm in the near future. Quantum circuit simulation plays a key role in the toolchain for the development of quantum hardware and software systems. However, due to the enormous Hilbert space of quantum states, simulating quantum circuits with classical computers is extremely challenging despite notable efforts have been made. In this paper, we enhance quantum circuit simulation in two dimensions: accuracy and scalability. The former is achieved by using an algebraic representation of complex numbers; the latter is achieved by bit-slicing the number representation and replacing matrix-vector multiplication with symbolic Boolean function manipulation. Experimental results demonstrate that our method can be superior to the state-of-the-art for various quantum circuits and can simulate certain benchmark families with up to tens of thousands of qubits.",quant-ph,Quantum Physics
"Sub-ms, nondestructive, time-resolved quantum-state readout of a single, trapped neutral atom","We achieve fast, nondestructive quantum-state readout via fluorescence detection of a single $^{87}$Rb atom in the 5$S_{1/2}$ ($F=2$) ground state held in an optical dipole trap. The atom is driven by linearly-polarized readout laser beams, making the scheme insensitive to the distribution of atomic population in the magnetic sub-levels. We demonstrate a readout fidelity of $97.6\pm0.2\%$ in a readout time of $160\pm20$ $$s with the atom retained in $>97\%$ of the trials, representing an advancement over other magnetic-state-insensitive techniques. We demonstrate that the $F=2$ state is partially protected from optical pumping by the distribution of the dipole matrix elements for the various transitions and the AC-Stark shifts from the optical trap. Our results are likely to find application in neutral-atom quantum computing and simulation.",quant-ph,Quantum Physics
Procedural generation using quantum computation,"Quantum computation is an emerging technology that promises to be a powerful tool in many areas. Though some years likely still remain until significant quantum advantage is demonstrated, the development of the technology has led to a range of valuable resources. These include publicly available prototype quantum hardware, advanced simulators for small quantum programs and programming frameworks to test and develop quantum software. In this provocation paper we seek to demonstrate that these resources are sufficient to provide the first useful results in the field of procedural generation. This is done by introducing a proof-of-principle method: a quantum generalization of a blurring process, in which quantum interference is used to provide a unique effect. Through this we hope to show that further developments in the technology are not required before it becomes useful for procedural generation. Rather, fruitful experimentation with this new technology can begin now.",quant-ph,Quantum Physics
The rotating harmonic oscillator revisited,We analyze the distribution of the eigenvalues of the quantum-mechanical rotating harmonic oscillator by means of the Frobenius method. A suitable ansatz leads to a three-term recurrence relation for the expansion coefficients. Truncation of the series yields some particular eigenvalues and eigenfunctions in exact analytical form. The former can be organized in such a way that one obtains suitable information about the whole spectrum of the model.,quant-ph,Quantum Physics
Large and robust mechanical squeezing of optomechanical systems in a highly unresolved sideband regime via Duffing nonlinearity and intracavity squeezed light,"We propose a scheme to generate strong and robust mechanical squeezing in an optomechanical system in the highly unresolved sideband (HURSB) regime with the help of the Duffing nonlinearity and intracavity squeezed light. The system is formed by a standard optomechanical system with the Duffing nonlinearity (mechanical nonlinearity) and a second-order nonlinear medium (optical nonlinearity). In the resolved sideband regime, the second-order nonlinear medium may play a destructive role in the generation of mechanical squeezing. However, it can significantly increase the mechanical squeezing (larger than 3dB) in the HURSB regime. Finally, we show the mechanical squeezing is robust against thermal fluctuations of the mechanical resonator. The generation of large and robust mechanical squeezing in the HURSB regime is a combined effect of the mechanical and optical nonlinearities.",quant-ph,Quantum Physics
Measurement-dependence cost for Bell nonlocality: causal vs retrocausal models,"Device independent protocols based on Bell nonlocality, such as quantum key distribution and randomness generation, must ensure no adversary can have prior knowledge of the measurement outcomes. This requires a measurement independence assumption: that the choice of measurement is uncorrelated with any other underlying variables that influence the measurement outcomes. Conversely, relaxing measurement independence allows for a fully `causal' simulation of Bell nonlocality. We construct the most efficient such simulation, as measured by the mutual information between the underlying variables and the measurement settings, for the Clauser-Horne-Shimony-Holt (CHSH) scenario, and find that the maximal quantum violation requires a mutual information of just $\sim 0.080$ bits. Any physical device built to implement this simulation allows an adversary to have full knowledge of a cryptographic key or `random' numbers generated by a device independent protocol based on violation of the CHSH inequality. We also show that a previous model for the CHSH scenario, requiring only $\sim 0.046$ bits to simulate the maximal quantum violation, corresponds to the most efficient `retrocausal' simulation, in which future measurement settings necessarily influence earlier source variables. This may be viewed either as an unphysical limitation of the prior model, or as an argument for retrocausality on the grounds of its greater efficiency. Causal and retrocausal models are also discussed for maximally entangled two-qubit states, as well as superdeterministic, one-sided and zigzag causal models.",quant-ph,Quantum Physics
Correlation Plenoptic Imaging between Arbitrary Planes,"We propose a novel method to perform plenoptic imaging at the diffraction limit by measuring second-order correlations of light between two reference planes, arbitrarily chosen, within the tridimensional scene of interest. We show that for both chaotic light and entangled-photon illumination, the protocol enables to change the focused planes, in post-processing, and to achieve an unprecedented combination of image resolution and depth of field. In particular, the depth of field results larger by a factor 3 with respect to previous correlation plenoptic imaging protocols, and by an order of magnitude with respect to standard imaging, while the resolution is kept at the diffraction limit. The results lead the way towards the development of compact designs for correlation plenoptic imaging devices based on chaotic light, as well as high-SNR plenoptic imaging devices based on entangled photon illumination, thus contributing to make correlation plenoptic imaging effectively competitive with commercial plenoptic devices.",quant-ph,Quantum Physics
Dissipative flow equations,We generalize the theory of flow equations to open quantum systems focusing on Lindblad master equations. We introduce and discuss three different generators of the flow that transform a linear non-Hermitian operator into a diagonal one. We first test our dissipative flow equations on a generic matrix and on a physical problem with a driven-dissipative single fermionic mode. We then move to problems with many fermionic modes and discuss the interplay between coherent (disordered) dynamics and localized losses. Our method can also be applied to non-Hermitian Hamiltonians.,quant-ph,Quantum Physics
"Comment on ""How to observe and quantify quantum-discord states via correlations""","The protocol proposed in the commented-upon article can certify zero discord; however, it fails to quantify non-zero discord for some states. Further, the protocol is less efficient than calculating discord via performing full quantum state tomography.",quant-ph,Quantum Physics
Honeycomb structures in magnetic fields,"We consider reduced-dimensionality models of honeycomb lattices in magnetic fields and report results about the spectrum, the density of states, self-similarity, and metal/insulator transitions under disorder. We perform a spectral analysis by which we discover a fractal Cantor spectrum for irrational magnetic flux through a honeycomb, prove the existence of zero energy Dirac cones for each rational flux, obtain an explicit expansion of the density of states near the conical points, and show the existence of mobility edges under Anderson-type disorder. Our results give a precise description of de Haas-van Alphen and Quantum Hall effects, and provide quantitative estimates on transport properties. In particular, our findings explain experimentally observed asymmetry phenomena by going beyond the perfect cone approximation.",quant-ph,Quantum Physics
Coherence and path indistinguishability for the interference of multiple single-mode fields,"A well known result for the interference of two single-mode fields is that the degree of coherence and the degree of indistinguishablity are same when we consider the detection of a single photon. In this article we present the relation between degree of coherence, path indistinguishability and the fringe visibility considering interference of multiple number of single-mode fields while being interested in the detection of a single photon only . We will also mention how Born's rule of interference for multiple sources is reflected in these results.",quant-ph,Quantum Physics
Reconstructing quantum states with quantum reservoir networks,"Reconstructing quantum states is an important task for various emerging quantum technologies. The process of reconstructing the density matrix of a quantum state is known as quantum state tomography. Conventionally, tomography of arbitrary quantum states is challenging as the paradigm of efficient protocols has remained in applying specific techniques for different types of quantum states. Here we introduce a quantum state tomography platform based on the framework of reservoir computing. It forms a quantum neural network, and operates as a comprehensive device for reconstructing an arbitrary quantum state (finite dimensional or continuous variable). This is achieved with only measuring the average occupation numbers in a single physical setup, without the need of any knowledge of optimum measurement basis or correlation measurements.",quant-ph,Quantum Physics
"Kullback-Leibler divergence between quantum distributions, and its upper-bound","This work presents an upper-bound to value that the Kullback-Leibler (KL) divergence can reach for a class of probability distributions called quantum distributions (QD). The aim is to find a distribution $U$ which maximizes the KL divergence from a given distribution $P$ under the assumption that $P$ and $U$ have been generated by distributing a given discrete quantity, a quantum. Quantum distributions naturally represent a wide range of probability distributions that are used in practical applications. Moreover, such a class of distributions can be obtained as an approximation of any probability distribution. The retrieving of an upper-bound for the entropic divergence is here shown to be possible under the condition that the compared distributions are quantum distributions over the same quantum value, thus they become comparable. Thus, entropic divergence acquires a more powerful meaning when it is applied to comparable distributions. This aspect should be taken into account in future developments of divergences. The theoretical findings are used for proposing a notion of normalized KL divergence that is empirically shown to behave differently from already known measures.",quant-ph,Quantum Physics
Discussions about the landscape of possibilities for treatments of cosmic inflation involving continuous spontaneous localization models,"In this work we consider a wide variety of alternatives opened when applying the continuous spontaneous localization (CSL) dynamical collapse theory to the inflationary era. The definitive resolution of many of the issues discussed here will have to await, not only for a general relativistic CSL theory, but for a fully workable theory of quantum gravity. Our concern here is to explore these issues, and to warn against premature conclusions. This exploration includes: two different approaches to deal with quantum field theory and gravitation, the identification of the collapse-generating operator and the general nature and values of the parameters of the CSL theory. All the choices connected with these issues have the potential to dramatically alter the conclusions one can draw. We also argue that the incompatibilities found in a recent paper, between the CSL parameter values and the CMB observational data, are associated with specific choices made for the extrapolation to the cosmological context of the CSL theory (as it is known to work in non-relativistic laboratory situations) which do not represent the most natural ones.",quant-ph,Quantum Physics
Continuity equations for entanglement,We introduce a complex purity density and its associated current for pure states of continuous variable systems. The scheme is constructed by analogy with the notions of probability density and probability current. Taking advantage of the formal continuity equations obtained this way we can introduce an entanglement subdynamics. We suggest the use of the dimensionality of this subdynamics as a potential measure of the complexity of entanglement. The scheme also provides insights into the relation between the global and local aspects of quantum correlations.,quant-ph,Quantum Physics
Wigner and Bell Inequalities relationships and Kolmogorov's Axioms,"In this work, we show that Bell's inequality violation of arise from the fact that the condition imposed upon the development of inequality is not respected when it is applied in the idealized experiment. Such a condition is that the quantities taken by probability must be non negative, and such a condition is represented by $|\cal{P(w)}|=\cal{P(w)}$. We will also show that, when trying to define the values of the joint probabilities of $ (Z_1, Z_2, Z_3) $, through the values obtained from the $ (Z_j, Z_k) $ pairs, we find that these values are negative, so not Kolmogorov's axiom is respected: $\cal{P(w)}\geq0$ in cases where Bell's inequality is violated, and we also show that only such violation is possible if Wigner's inequality, in a certain arrangement, is violated, and that both violations are related to the violation of one of Kolmogorov's axioms. At the end of the paper, we suggest a new interpretation of the probabilities involved, in order to avoid the situation of negative probabilities and the violation of Bell's inequality and, consequently, Wigner's inequality.",quant-ph,Quantum Physics
Detection and control of single proton spins in a thin layer of diamond grown by chemical vapor deposition,"We report detection and coherent control of a single proton nuclear spin using an electronic spin of the nitrogen-vacancy (NV) center in diamond as a quantum sensor. In addition to determining the NV-proton hyperfine parameters by employing multipulse sequences, we polarize and coherently rotate the single proton spin, and detect an induced free precession. Observation of free induction decays is an essential ingredient for high resolution proton nuclear magnetic resonance, and the present work extends it to the atomic scale. We also discuss the origin of the proton as incorporation during chemical vapor deposition growth, which provides an opportunity to use protons in diamond as built-in quantum memories coupled with the NV center.",quant-ph,Quantum Physics
Spin coherence and depths of single nitrogen-vacancy centers created by ion implantation into diamond via screening masks,"We characterize single nitrogen-vacancy (NV) centers created by 10-keV N+ ion implantation into diamond via thin SiO$_2$ layers working as screening masks. Despite the relatively high acceleration energy compared with standard ones (< 5 keV) used to create near-surface NV centers, the screening masks modify the distribution of N$^+$ ions to be peaked at the diamond surface [Ito et al., Appl. Phys. Lett. 110, 213105 (2017)]. We examine the relation between coherence times of the NV electronic spins and their depths, demonstrating that a large portion of NV centers are located within 10 nm from the surface, consistent with Monte Carlo simulations. The effect of the surface on the NV spin coherence time is evaluated through noise spectroscopy, surface topography, and X-ray photoelectron spectroscopy.",quant-ph,Quantum Physics
On the evolution of quantum non-equilibrium in expanding systems,"We consider a particle confined in a uniformly expanding two-dimensional square box from the point of the view of the de Broglie-Bohm pilot-wave theory. In particular we study quantum ensembles in which the Born Law is initially violated (quantum non-equilibrium). We show examples of such ensembles that start close to quantum equilibrium, as measured by the standard coarse-grained H-function, but diverge from it with time. We give an explanation of this result and discuss the possibilities that it opens.",quant-ph,Quantum Physics
"Quantum teleportation, entanglement, and Bell nonlocality in Unruh channel","Decoherence is an unavoidable phenomenon that results from the interaction of the system with its surroundings. The study of decoherence due to the relativistic effects has the fundamental importance. The Unruh effect is observed by the relativistically accelerator observer. The unruh effect acts as a quantum channel and we call it the Unruh Channel. The Unruh channel can be characterize by providing its Kraus representation. We consider the bipartite scheme in which the quantum information is shared between an inertial observer (Alice) and an accelerated observer (Rob) in the case of Dirac field. We will show that this channel reduces the common quantum information between the two observers. In this work we will study the effects of the Unruh channel on various facets of quantum correlations, such as the quantum teleportation, entanglement, and Bell inequality violations for a Dirac field mode.",quant-ph,Quantum Physics
Symmetry assisted preparation of entangled many-body states on a quantum computer,"Starting from the Quantum-Phase-Estimate (QPE) algorithm, a method is proposed to construct entangled states that describe correlated many-body systems on quantum computers. Using operators for which the discrete set of eigenvalues is known, the QPE approach is followed by measurements that serve as projectors on the entangled states. These states can then be used as inputs for further quantum or hybrid quantum-classical processing. When the operator is associated to a symmetry of the Hamiltonian, the approach can be seen as a quantum--computer formulation of symmetry breaking followed by symmetry restoration. The method proposed in this work, called Discrete Spectra Assisted (DSA), is applied to superfluid systems. By using the blocking technique adapted to qubits, the full spectra of a pairing Hamiltonian is obtained.",quant-ph,Quantum Physics
Nonequilibrium Thermodynamics of Quantum Friction,"Thermodynamic principles are often deceptively simple and yet surprisingly powerful. We show how a simple rule, such as the net flow of energy in and out of a moving atom under nonequilibrium steady state condition, can expose the shortcomings of many popular theories of quantum friction. Our thermodynamic approach provides a conceptual framework in guiding atom-optical experiments, thereby highlighting the importance of fluctuation-dissipation relations and long-time correlations between subsystems. Our results introduce consistency conditions for (numerical) models of nonequilibrium dynamics of open quantum systems.",quant-ph,Quantum Physics
"Graph model overview, events scales structure and chains of events","We present a graph model for a background independent, relational approach to spacetime emergence. The general idea and the graph main features, detailed in [1], are discussed. This is a combinatorial (dynamical) metric graph, colored on vertexes, endowed with a classical distribution of colors probability on the graph vertexes. The graph coloring determines the graph structure in clusters of graph vertices (events) that can be monochromatic (homogeneous loops) or polychromatic (inhomogeneous loops). The probability is conserved after the graph conformal expansion from an initial seed graph state to higher (conformally expanded) graph states. The emerging structure has self-similar characteristics on different scales (states). From the coloring, different levels of vertices and thus graph levels arise as new aggregates of colored vertices. In this second (derived) graphs level, the derived graph vertices correspond to the polychromatic edges (with differently colored vertices) of the initial graph. Vertex aggregates are related, as some levels (graph states) to plexors and twistors (involving Clifford statistics). Two metric levels are defined on the colored graph, the first level is a natural metric defined on the graph, the second level emerges from the first and related, due to symmetries. Metric structure reflects the graph colored structure under conformal transformations evolving with its states under conformal expansion. In some special cases vertices/events chains could be related to strings generalizations.",quant-ph,Quantum Physics
Stokes--anti-Stokes light scattering process: -- A photon-wave-function approach,"The Photon wave function Formalism provides an alternative description of some quantum optical phenomena in a more intuitive way. We use this formalism to describe the process of correlated Stokes--anti-Stokes Raman scattering. In this process, two photons from a laser beam are inelastically scattered by a phonon created by the first photon (Stokes processes) and annihilated by the second photon (anti-Stokes process), producing a Stokes--anti-Sokes (SaS) photon pair. We arrive at an expression for the two-photon wave function of the scattered SaS photon pair, which is in agreement with a number of experimental results.",quant-ph,Quantum Physics
Optical response of a topological-insulator--quantum-dot hybrid interacting with a probe electric field,"We study the interaction between a topological insulator nanoparticle and a quantum dot subject to an applied electric field. The electromagnetic response of the topological insulator is derived from axion electrodynamics in the quasistatic approximation. Localized modes are quantized in terms of dipolar bosonic modes, which couples dipolarly to the quantum dot. Hence, we treat the hybrid as a two-level system interacting with a single bosonic mode, where the coupling strength encodes the information concerning the nontrivial topology of the nanoparticle. The interaction of the hybrid with the environment is implemented through the coupling with a continuum reservoir of radiative output modes and a reservoir of phonon modes. In particular, we use the method of Zubarev's Green functions to derive an expression for the optical absorption spectrum of the system. We apply our results to a realistic system which consists of a topological insulator nanoparticle made of TlBiSe$_{2}$ interacting with a cadmium selenide quantum dot, both immersed in a polymer layer such as poly(methyl methacrylate). The optical absorption spectrum exhibits Fano resonances with a line shape that strongly depends on the polarization of the electric field as well as on the topological magnetoelectric polarizability $$. Our results and methods can also be applied to nontopological magnetoelectric materials such as Cr$_{2}$O$_{3}$.",quant-ph,Quantum Physics
Time-resolved Photoluminescence in Terahertz-driven Hybrid Systems of Plasmons and Excitons,"Ultrafast pump-probe technique is a powerful tool to understand and manipulate properties of materials for designing novel quantum devices. An intense, single cycle terahertz pulse can change the intrinsic properties of semiconductor quantum dots to have different luminescence. In a hybrid system of plasmon and exciton, the coherence and coupling between these two degrees of freedom play an important role on their optical properties. Therefore, we consider a terahertz pump optical probe experiment in the hybrid systems where the terahertz pump pulse couples to the exciton degrees of freedom on the quantum dot. The time resolved photoluminescence of the hybrid system shows that the response of the characteristic frequency shifts according to the overlap between the pump and probe pulses. Furthermore, the resonance between the exciton and plasmons can be induced by the terahertz pump pulse in some parameter regimes. Our results show the terahertz driven hybrid system can be a versatile tool for manipulating the material properties and open a new route to design modern optical devices.",quant-ph,Quantum Physics
"The rule of conditional probability is valid in quantum theory [Comment on Gelman & Yao's ""Holes in Bayesian Statistics""]","In a recent manuscript, Gelman & Yao (2020) claim that ""the usual rules of conditional probability fail in the quantum realm"" and that ""probability theory isn't true (quantum physics)"" and purport to support these statements with the example of a quantum double-slit experiment. The present comment recalls some relevant literature in quantum theory and shows that (i) Gelman & Yao's statements are false; in fact, the quantum example confirms the rules of probability theory; (ii) the particular inequality found in the quantum example can be shown to appear also in very non-quantum examples, such as drawing from an urn; thus there is nothing peculiar to quantum theory in this matter. A couple of wrong or imprecise statements about quantum theory in the cited manuscript are also corrected.",quant-ph,Quantum Physics
Single Particle Thermodynamics with Levitated Nanoparticles,"This chapter is intended as a pedagogical introduction to the dynamics of optically levitated nanoparticles with a focus on the study of single particle thermodynamics. Much of the work studying thermodynamics with nano- and micro-particles has taken place in liquid, and this chapter will avoid reviewing this impressive body of work, focussing instead on studies of thermodynamics with nanoparticles levitated in a gas. For a recent literature review we refer the reader to Gieseler & Millen Entropy 20, 326 (2018). The authors will discuss extensions into the quantum regime where relevant throughout the chapter.",quant-ph,Quantum Physics
Capacity-approaching quantum repeaters for quantum communications,"In present-day quantum communications, one of the main problems is the lack of a quantum repeater design that can simultaneously secure high rates and long distances. Recent literature has established the end-to-end capacities that are achievable by the most general protocols for quantum and private communication within a quantum network, encompassing the case of a quantum repeater chain. However, whether or not a physical design exists to approach such capacities remains a challenging objective. Driven by this motivation, in this work, we put forward a design for continuous-variable quantum repeaters and show that it can actually achieve the feat. We also show that even in a noisy regime our rates surpass the Pirandola-Laurenza-Ottaviani-Banchi (PLOB) bound. Our repeater setup is developed upon using noiseless linear amplifiers, quantum memories, and continuous-variable Bell measurements. We, furthermore, propose a non-ideal model for continuous-variable quantum memories that we make use of in our design. We then show that potential quantum communications rates would deviate from the theoretical capacities, as one would expect, if the quantum link is too noisy and/or low-quality quantum memories and amplifiers are employed.",quant-ph,Quantum Physics
Stable nonlinear modes sustained by gauge fields,"We reveal the universal effect of gauge fields on the existence, evolution, and stability of solitons in the spinor multidimensional nonlinear Schrdinger equation. Focusing on the two-dimensional case, we show that when gauge field can be split in a pure gauge and a \rtext{non-pure gauge} generating \rtext{effective potential}, the roles of these components in soliton dynamics are different: the \btext{localization characteristics} of emerging states are determined by the curvature, while pure gauge affects the stability of the modes. Respectively the solutions can be exactly represented as the envelopes independent of the pure gauge, modulating stationary carrier-mode states, which are independent of the curvature. Our central finding is that nonzero curvature can lead to the existence of unusual modes, in particular, enabling stable localized self-trapped fundamental and vortex-carrying states in media with constant repulsive interactions without additional external confining potentials and even in the expulsive external traps.",quant-ph,Quantum Physics
Multiphoton pulses interacting with multiple emitters in a one-dimensional waveguide,"We derive a generalized master equation for multiphoton pulses interacting with multiple emitters in a waveguide-quantum electrodynamics system where the emitter frequency can be modulated and the effects of non-guided modes can also be considered. Based on this theory, we can calculate the real-time dynamics of an array of interacting emitters driven by an incident photon pulse which can be vacuum, a coherent state, a Fock state or their superpositions. Moreover, we also derive generalized input-output relations to calculate the reflectivity and transmissivity of this system. We can also calculate the output photon pulse shapes. Our theory can find important applications in the study of waveguide-based quantum systems.",quant-ph,Quantum Physics
Deterministic Generation of Loss-Tolerant Photonic Cluster States with a Single Quantum Emitter,"A photonic cluster state with a tree-type entanglement structure constitutes an efficient resource for quantum error correction of photon loss. But the generation of a tree cluster state with an arbitrary size is notoriously difficult. Here, we propose a protocol to deterministically generate photonic tree states of arbitrary size by using only a single quantum emitter. Photonic entanglement is established through both emission and re-scattering from the same emitter, enabling fast and resource-efficient entanglement generation. The same protocol can also be extended to generate more general tree-type entangled states.",quant-ph,Quantum Physics
Entanglement degradation of cavity modes due to the dynamical Casimir effect,"We study the entanglement dynamics between two cavities when one of them is harmonically shaken in the context of quantum information theory. We find four different regimes depending on the frequency of the motion and the spectrum of the moving cavity. If the moving cavity is three dimensional only two modes inside get coupled and the entanglement can either degrade asymptotically with time or oscillate depending on the driving. On the other hand, if the cavity has an equidistant spectrum the entanglement can either vanish asymptotically if it is driven with its fundamental frequency or have a sudden death if it is driven with an uneven harmonic frequency.",quant-ph,Quantum Physics
A gauge redundancy-free formulation of compact QED with dynamical matter for quantum and classical computations,"We introduce a way to express compact quantum electrodynamics with dynamical matter on two- and three-dimensional spatial lattices in a gauge redundancy-free manner while preserving translational invariance. By transforming to a rotating frame, where the matter is decoupled from the gauge constraints, we can express the gauge field operators in terms of dual operators. In two space dimensions, the dual representation is completely free of any local constraints. In three space dimensions, local constraints among the dual operators remain but involve only the gauge field degrees of freedom (and not the matter degrees of freedom). These formulations, which reduce the required Hilbert space dimension, could be useful for both numerical (classical) Hamiltonian computations and quantum simulation or computation.",quant-ph,Quantum Physics
Completely real? A critical note on the claims by Colbeck and Renner,"In a series of papers Colbeck and Renner claim to have shown that the quantum state provides a complete description for the prediction of future measurement outcomes. In this paper I argue that thus far no solid satisfactory proof has been presented to support this claim. Building on the earlier work of Leifer, Landsman and Leegwater, I present and prove two results that only partially support this claim. I then discuss the arguments by Colbeck, Renner and Leegwater concerning how these results are to generalize to the full claim. This argument turns out to hinge on the implicit use of an assumption concerning the way unitary evolution is to be represented in any possible completion of quantum mechanics. I argue that this assumption is unsatisfactory and that possible attempts to validate it based on measurement theory also do not succeed.",quant-ph,Quantum Physics
Eigenstate thermalization for observables that break Hamiltonian symmetries and its counterpart in interacting integrable systems,"We study the off-diagonal matrix elements of observables that break the translational symmetry of a spin-chain Hamiltonian, and as such connect energy eigenstates from different total quasimomentum sectors. We consider quantum-chaotic and interacting integrable points of the Hamiltonian, and focus on average energies at the center of the spectrum. In the quantum-chaotic model, we find that there is eigenstate thermalization; specifically, the matrix elements are Gaussian distributed with a variance that is a smooth function of $=E_-E_$ ({$E_$} are the eigenenergies) and scales as $1/D$ ($D$ is the Hilbert space dimension). In the interacting integrable model, we find that the matrix elements exhibit a skewed log-normal-like distribution and have a variance that is also a smooth function of $$ that scales as $1/D$. We study in detail the low-frequency behavior of the variance of the matrix elements to unveil the regimes in which it exhibits diffusive or ballistic scaling. We show that in the quantum-chaotic model the behavior of the variance is qualitatively similar for matrix elements that connect eigenstates from the same versus different quasimomentum sectors. We also show that this is not the case in the interacting integrable model for observables whose translationally invariant counterpart does not break integrability if added as a perturbation to the Hamiltonian.",quant-ph,Quantum Physics
Approximation of point interactions by geometric perturbations in two-dimensional domains,"We present a new type of approximation of a second-order elliptic operator in a planar domain with a point interaction. It is of a geometric nature, the approximating family consists of operators with the same symbol and regular coefficients on the domain with a small hole. At the boundary of it Robin condition is imposed with the coefficient which depends on the linear size of a hole. We show that as the hole shrinks to a point and the parameter in the boundary condition is scaled in a suitable way, nonlinear and singular, the indicated family converges in the norm-resolvent sense to the operator with the point interaction. This resolvent convergence is established with respect to several operator norms and order-sharp estimates of the convergence rates are provided.",quant-ph,Quantum Physics
dc to ac Josephson transition in a dc atom superconducting quantum interference device,"We analyze the effect of the barrier motion on the Bose-Hubbard Hamiltonian of a ring-shaped Bose-Einstein condensate interrupted by a pair of Josephson junctions, a configuration which is the cold atom analog of the well-known dc superconducting quantum interference device (SQUID). Such an effect is also shown to modify the Heisenberg equation of motion of the boson field operator in the two-mode approximation, where a hysteretic contribution that could affect the dynamics for accelerated or overlapping barriers is identified. By studying the energy landscape as a function of order and control parameters, we determine the diagram with the location of the dc and ac Josephson regimes, along with the critical points that are shown to depend on the junctions position. We analyze the dc to ac Josephson transition for adiabatic barrier trajectories that lead to a final uniform velocity, or which perform symmetric velocity paths. We show that such symmetric trajectories may induce, when reaching the critical point, highly hysteretic oscillating return paths within the dc regime, similar to the underdamped hysteresis loops arising from the action of a resistive flow in the ac regime. We also consider nonequilibrium initial conditions resulting from a finite phase difference on either side of the junctions, along with the critical features of such a parameter. An excellent agreement between the Gross-Pitaevskii simulations and the two-mode results is found in all cases.",quant-ph,Quantum Physics
Qudits and high-dimensional quantum computing,"Qudit is a multi-level computational unit alternative to the conventional 2-level qubit. Compared to qubit, qudit provides a larger state space to store and process information, and thus can provide reduction of the circuit complexity, simplification of the experimental setup and enhancement of the algorithm efficiency. This review provides an overview of qudit-based quantum computing covering a variety of topics ranging from circuit building, algorithm design, to experimental methods. We first discuss the qudit gate universality and a variety of qudit gates including the pi/8 gate, the SWAP gate, and the multi-level-controlled gate. We then present the qudit version of several representative quantum algorithms including the Deutsch-Jozsa algorithm, the quantum Fourier transform, and the phase estimation algorithm. Finally we discuss various physical realizations for qudit computation such as the photonic platform, iron trap, and nuclear magnetic resonance.",quant-ph,Quantum Physics
Quantum and semiclassical dynamics as fluid theories where gauge matters,"The family of trajectories-based approximations employed in computational quantum physics and chemistry is very diverse. For instance, Bohmian and Heller's frozen Gaussian semiclassical trajectories seem to have nothing in common. Based on a hydrodynamic analogy to quantum mechanics, we furnish the unified gauge theory of all such models. In the light of this theory, currently known methods are just a tip of the iceberg, and there exists an infinite family of yet unexplored trajectory-based approaches. Specifically, we show that each definition for a semiclassical trajectory corresponds to a specific hydrodynamic analogy, where a quantum system is mapped to an effective probability fluid in the phase space. We derive the continuity equation for the effective fluid representing dynamics of an arbitrary open bosonic many-body system. We show that unlike in conventional fluid, the flux of the effective fluid is defined up to Skodje's gauge [R. T. Skodje et. al. Phys. Rev. A 40, 2894 (1989)]. We prove that the Wigner, Husimi and Bohmian representations of quantum mechanics are particular cases of our generic hydrodynamic analogy, and all the differences among them reduce to the gauge choice. Infinitely many gauges are possible, each leading to a distinct quantum hydrodynamic analogy and a definition for semiclassical trajectories. We propose a scheme for identifying practically useful gauges and apply it to improve a semiclassical initial value representation employed in quantum many-body simulations.",quant-ph,Quantum Physics
Restrain the losses of the entanglement and the non-local advantage of quantum coherence for accelerated quantum systems,"We examined the possibility of recovering the losses of entanglement and the non-local advantage by using the local symmetric operations. The improvement efficiency may be increased by applying the symmetric operations on both qubits.
  The recovering process of both phenomenon is exhibited clearly when only one qubit is accelerated and the symmetric operations is applied on both qubits. It is shown that, for large acceleration, the non-local coherent advantage may be re-birthed by using these symmetric operations.",quant-ph,Quantum Physics
Epistemic Horizons: This Sentence is $\frac{1}{\sqrt{2}}(|True\rangle + |False\rangle)$,"In [Found. Phys. 48.12 (2018): 1669], the notion of 'epistemic horizon' was introduced as an explanation for many of the puzzling features of quantum mechanics. There, it was shown that Lawvere's theorem, which forms the categorical backdrop to phenomena such as Gdelian incompleteness, Turing undecidability, Russell's paradox and others, applied to a measurement context, yields bounds on the maximum knowledge that can be obtained about a system, which produces many paradigmatically quantum phenomena. We give a brief presentation of the framework, and then demonstrate how it naturally yields Bell inequality violations. We then study the argument due to Einstein, Podolsky, and Rosen, and show how the counterfactual inference needed to conclude the incompleteness of the quantum formalism is barred by the epistemic horizon. Similarly, the paradoxes due to Hardy and Frauchiger-Renner are discussed, and found to turn on an inconsistent combination of information from incompatible contexts.",quant-ph,Quantum Physics
On Representing (Anti)Symmetric Functions,"Permutation-invariant, -equivariant, and -covariant functions and anti-symmetric functions are important in quantum physics, computer vision, and other disciplines. Applications often require most or all of the following properties: (a) a large class of such functions can be approximated, e.g. all continuous function, (b) only the (anti)symmetric functions can be represented, (c) a fast algorithm for computing the approximation, (d) the representation itself is continuous or differentiable, (e) the architecture is suitable for learning the function from data. (Anti)symmetric neural networks have recently been developed and applied with great success. A few theoretical approximation results have been proven, but many questions are still open, especially for particles in more than one dimension and the anti-symmetric case, which this work focusses on. More concretely, we derive natural polynomial approximations in the symmetric case, and approximations based on a single generalized Slater determinant in the anti-symmetric case. Unlike some previous super-exponential and discontinuous approximations, these seem a more promising basis for future tighter bounds. We provide a complete and explicit universality proof of the Equivariant MultiLayer Perceptron, which implies universality of symmetric MLPs and the FermiNet.",quant-ph,Quantum Physics
Time and space resolved first order optical interference between distinguishable photon paths,"Interference between different photons occurs and has been observed under diverse experimental conditions. A necessary condition in order to obtain interference fringes is the existence of at least two possible paths and unknown which-path information. If the photon beams have different frequencies, stability of the sources and fast enough detectors are also required.First order interference between two truly independent CW laser sources is observed. Contrary to what is expected, interference is observed although the photon beams are distinguishable and the path is unequivocally known for each photon beam. Segments of the CW wavetrains are selected with an acousto optic modulator. Temporal and spatial interference are integrated in a single combined phenomenon via streak camera detection. The fringes displacement in the time-space interferograms reveal the trajectories of the labeled photons. These results indicate that in non-degenerate frequency schemes, the ontology has to be refined and the which path criterion must be precisely stated. If reference is made to the frequency labeled photons, the path of each photon is known, whereas if the query is stated in terms of the detected photons, the path is unknown.",quant-ph,Quantum Physics
Non-Markovianity of Quantum Brownian Motion,"We study quantum non-Markovian dynamics of the Caldeira-Leggett model, a prototypical model for quantum Brownian motion describing a harmonic oscillator linearly coupled to a reservoir of harmonic oscillators. Employing the exact analytical solution of this model one can determine the size of memory effects for arbitrary couplings, temperatures and frequency cutoffs. Here, quantum non-Markovianity is defined in terms of the flow of information between the open system and its environment, which is quantified through the Bures metric as distance measure for quantum states. This approach allows us to discuss quantum memory effects in the whole range from weak to strong dissipation for arbitrary Gaussian initial states. A comparison of our results with the corresponding results for the spin-boson problem show a remarkable similarity in the structure of non-Markovian behavior of the two paradigmatic models.",quant-ph,Quantum Physics
Quantum fluctuations hinder finite-time information erasure near the Landauer limit,"Information is physical but information is also processed in finite time. Where computing protocols are concerned, finite-time processing in the quantum regime can dynamically generate coherence. Here we show that this can have significant thermodynamic implications. We demonstrate that quantum coherence generated in the energy eigenbasis of a system undergoing a finite-time information erasure protocol yields rare events with extreme dissipation. These fluctuations are of purely quantum origin. By studying the full statistics of the dissipated heat in the slow driving limit, we prove that coherence provides a non-negative contribution to all statistical cumulants. Using the simple and paradigmatic example of single bit erasure, we show that these extreme dissipation events yield distinct, experimentally distinguishable signatures.",quant-ph,Quantum Physics
On Actual Preparation of Dicke State on a Quantum Computer,"The exact number of CNOT and single qubit gates needed to implement a Quantum Algorithm in a given architecture is one of the central problems of Quantum Computation. In this work we study the importance of concise realizations of Partially defined Unitary Transformations for better circuit construction using the case study of Dicke State Preparation. The Dicke States $(\left|D^n_k \right>)$ are an important class of entangled states with uses in many branches of Quantum Information. In this regard we provide the most efficient Deterministic Dicke State Preparation Circuit in terms of CNOT and single qubit gate counts in comparison to existing literature. We further observe that our improvements also reduce architectural constraints of the circuits. We implement the circuit for preparing $\left| D^4_2 \right>$ on the ""ibmqx2"" machine of the IBM QX service and observe that the error induced due to noise in the system is lesser in comparison to the existing circuit descriptions. We conclude by describing the CNOT map of the generic $\left| D^n_k \right>$ preparation circuit and analyze different ways of distributing the CNOT gates in the circuit and its affect on the induced error.",quant-ph,Quantum Physics
Note on a Floquet/Bloch-band fusion phenomenon in scattering by truncated periodic multi-well potentials,"A transmission phenomenon for a quantal particle scattered through a multi-well potential in one dimension is observed by means of an amplitude-phase method. The potential model consists of $N$ identical potential cells, each containing a symmetric well. Typical transmission bands contain $N-1$ possible energies of total transmission. It is found that certain band types contain $n$ energies of total transmission. A fusion phenomenon of this type of band with a typical neigboring band is also found. As the transmission gap between them collapse and disappear, a resulting fused single band is seen to contain $2N-1$ energy peaks of total transmission.",quant-ph,Quantum Physics
On quantum bound states in equiperiodic multi-well potentials I: $-$ Locally periodic potential sequences and Floquet/Bloch bands,"Two connected equiperiodic one-dimensional multi-well potentials of different well depths are studied. Floquet/Bloch energy bands for respective multi-well potential are found to be relevant for understanding level structures. Althoug energies are classically allowed in both multi-well potentials, a band gap of one multi-well potential makes this potential quantum-mechanically 'forbidden'. All energy levels are located in the union of the band regions.",quant-ph,Quantum Physics
Clarifying multiple-tip effects on Scanning Tunneling Microscopy imaging of 2D periodic objects and crystallographic averaging in the spatial frequency domain,Crystallographic image processing (CIP) techniques may be utilized in scanning probe microscopy (SPM) to glean information that has been obscured by signals from multiple probe tips. This may be of particular importance for scanning tunneling microscopy (STM) and requires images from a sample that is periodic in two dimensions. The image-forming current for multiple tips in STM is derived in a more straightforward manner than prior approaches. The Fourier spectrum of the current for p4mm Bloch surface wave functions and a pair of delta function tips reveals the tip-separation dependence of various types of image obscurations. In particular our analyses predict that quantum interference should be visible on a macroscopic scale in the form of bands quite distinct from the basket-weave patterns a purely classical model would create at the same periodic double STM tip separations. A surface wave function that models the essential character of highly (0001) oriented pyrolytic graphite (technically known as HOPG) is introduced and used for a similar tip-separation analysis. Using a bonding H_2 tip wave function with significant spatial extent instead of this pair of infinitesimal Dirac delta function tips does not affect these outcomes in any observable way. This is explained by Pierre Curie's well known symmetry principle. Classical simulations of multiple tip effects in STM images may be understood as modeling multiple tip effects in images that were recorded with other types of SPMs). Our analysis clarifies why CIP and crystallographic averaging work well in removing the effects of a blunt SPM tip (that consist of multiple mini-tips) from the recorded 2D periodic images and also outlines the limitations of this image processing techniques for certain spatial separations of STM mini-tips.,quant-ph,Quantum Physics
A switching approach for perfect state transfer over a scalable and routing enabled network architecture with superconducting qubits,"We propose a hypercube switching architecture for the perfect state transfer (PST) where we prove that it is always possible to find an induced hypercube in any given hypercube of any dimension such that PST can be performed between any two given vertices of the original hypercube. We then generalise this switching scheme over arbitrary number of qubits where also this routing feature of PST between any two vertices is possible. It is shown that this is optimal and scalable architecture for quantum computing with the feature of routing. This allows for a scalable and growing network of qubits. We demonstrate this switching scheme to be experimentally realizable using superconducting transmon qubits with tunable couplings. We also propose a PST assisted quantum computing model where we show the computational advantage of using PST against the conventional resource expensive quantum swap gates. In addition, we present the numerical study of signed graphs under Corona product of graphs and show few examples where PST is established, in contrast to pre-existing results in the literature for disproof of PST under Corona product. We also report an error in pre-existing research for qudit state transfer over Bosonic Hamiltonian where unitarity is violated.",quant-ph,Quantum Physics
Star product representation of coherent state path integrals,"In this paper, we determine the star product representation of coherent path integrals. By employing the properties of generalized delta functions with complex arguments, the Glauber-Sudarshan P-function corresponding to a non-diagonal density operator is obtained. Then, we compute the Husimi-Kano Q-representation of the time evolution operator in terms of the normal star product. Finally, the optical equivalence theorem allows us to express the coherent state path integral as a star exponential of the Hamiltonian function for the normal product.",quant-ph,Quantum Physics
Evolution equations for quantum semi-Markov dynamics,"Using a newly introduced connection between the local and non-local description of open quantum system dynamics, we investigate the relationship between these two characterisations in the case of quantum semi-Markov processes. This class of quantum evolutions, which is a direct generalisation of the corresponding classical concept, guarantees mathematically well-defined master equations, while accounting for a wide range of phenomena, possibly in the non-Markovian regime. In particular, we analyse the emergence of a dephasing term when moving from one type of master equation to the other, by means of several examples. We also investigate the corresponding Redfield-like approximated dynamics, which are obtained after a coarse graining in time. Relying on general properties of the associated classical random process, we conclude that such an approximation always leads to a Markovian evolution for the considered class of dynamics.",quant-ph,Quantum Physics
Dynamic modulation of phonon-assisted transitions in quantum defects in monolayer transition-metal dichalcogenide semiconductors,"Quantum localization via atomic point defects in semiconductors is of significant fundamental and technological importance. Quantum defects in monolayer transition-metal dichalcogenide semiconductors have been proposed as stable and scalable optically-addressable spin qubits. Yet, the impact of strong spin-orbit coupling on their dynamical response, for example under optical excitation, has remained elusive. In this context, we study the effect of spin-orbit coupling on the electron-phonon interaction in a single chalcogen vacancy defect in monolayer transition metal dichalcogenides, molybdenum disulfide (MoS$_2$) and tungsten disulfide (WS$_2$). From ab initio electronic structure theory calculations, we find that spin-orbit interactions tune the magnitude of the electron-phonon coupling in both optical and charge-state transitions of the defect, modulating their respective efficiencies. This observation opens up a promising scheme of dynamically modulating material properties to tune the local behavior of a quantum defect.",quant-ph,Quantum Physics
Entropy production dynamics in quench protocols of a driven-dissipative critical system,"Driven-dissipative phase transitions are currently a topic of intense research due to the prospect of experimental realizations in quantum optical setups. The most paradigmatic model presenting such a transition is the Kerr model, which predicts the phenomenon of optical bistability, where the system may relax to two different steady-states for the same driving condition. These states, however, are inherently out-of-equilibrium and are thus characterized by the continuous production of irreversible entropy, a key quantifier in thermodynamics. In this paper we study the dynamics of the entropy production rate in a quench scenario of the Kerr model, where the external pump is abruptly changed. This is accomplished using a recently developed formalism, based on the Husimi $Q$-function, which is particularly tailored for driven-dissipative and non-Gaussian bosonic systems [Phys. Rev. Res. 2, 013136 (2020)]. Within this framework the entropy production can be split into two contributions, one being extensive with the drive and describing classical irreversibility, and the other being intensive and directly related to quantum fluctuations. The latter, in particular, is found to reveal the high degree of non-adiabaticity, for quenches between different metastable states.",quant-ph,Quantum Physics
Dynamically Evolving Bond-Dimensions within the one-site Time-Dependent-Variational-Principle method for Matrix Product States: Towards efficient simulation of non-equilibrium open quantum dynamics,"Understanding the emergent system-bath correlations in non-Markovian and non-perturbative open systems is a theoretical challenge that has benefited greatly from the application of Matrix Product State (MPS) methods. Here, we propose an autonmously adapative variant of the one-site Time-Dependent-Variational-Principle (1TDVP) method for many-body MPS wave-functions in which the local bond-dimensions can evolve to capture growing
  entanglement 'on the fly'. We achieve this by efficiently examining the effect of increasing each MPS bond-dimension in advance of each dynamic timestep, resulting in an MPS that can dynamically and inhomogeneously restructure itself as the complexity of the dynamics grows across time and space. This naturally leads to more efficient simulations, oviates the need for multiple convergence runs, and, as we demonstrate, is ideally suited to the typical, finite-temperature 'impurity' problems that describe open quantum system connected to multiple environments.",quant-ph,Quantum Physics
The role of the multiple excitation manifold in a driven quantum simulator of an antenna complex,"Biomolecular light-harvesting antennas operate as nanoscale devices in a regime where the coherent interactions of individual light, matter and vibrational quanta are non-perturbatively strong. The complex behaviour arising from this could, if fully understood, be exploited for myriad energy applications. However, non-perturbative dynamics are computationally challenging to simulate, and experiments on biomaterials explore very limited regions of the non-perturbative parameter space. So-called `quantum simulators' of light-harvesting models could provide a solution to this problem, and here we employ the hierarchical equations of motion technique to investigate recent superconducting experiments of Poto{}nik $\it{et}$ $\it{al.}$ (Nat. Com. 9, 904 (2018)) used to explore excitonic energy capture. By explicitly including the role of optical driving fields, non-perturbative dephasing noise and the full multi-excitation Hilbert space of a three-qubit quantum circuit, we predict the measureable impact of these factors on transfer efficiency. By analysis of the eigenspectrum of the network, we uncover a structure of energy levels that allows the network to exploit optical `dark' states and excited state absorption for energy transfer. We also confirm that time-resolvable coherent oscillations could be experimentally observed, even under strong, non-additive action of the driving and optical fields.",quant-ph,Quantum Physics
Finite-size scaling analysis of two-dimensional deformed Affleck-Kennedy-Lieb-Tasaki states,"Using tensor network methods, we perform finite-size scaling analysis to study the parameter-induced phase transitions of two-dimensional deformed Affleck-Kennedy-Lieb-Tasaki states. We use higher-order tensor renormalization group method to evaluate the moments and the correlations. Then, the critical point and critical exponents are determined simultaneously by collapsing the data. Alternatively, the crossing points of the dimensionless ratios are used to determine the critical point, and the scaling at the critical point is used to determine the critical exponents. For the transition between the disordered AKLT phase and the ferromagnetic ordered phase, we demonstrate that both the critical point and the exponents can be determined accurately. Furthermore, the values of the exponents confirm that the AKLT-FM transition belongs to the 2D Ising universality class. We also investigate the Berezinskii-Kosterlitz-Thouless transition from the AKLT phase to the critical XY phase. In this case we show that the critical point can be located by the crossing point of the correlation ratio.",quant-ph,Quantum Physics
Modeling Linear Inequality Constraints in Quadratic Binary Optimization for Variational Quantum Eigensolver,"This paper introduces the use of tailored variational forms for variational quantum eigensolver that have properties of representing certain constraints on the search domain of a linear constrained quadratic binary optimization problem solution. Four constraints that usually appear in several optimization problems are modeled. The main advantage of the proposed methodology is that the number of parameters on the variational form remain constant and depend on the number of variables that appear on the constraints. Moreover, this variational form always produces feasible solutions for the represented constraints differing from penalization techniques commonly used to translate constrained problems into unconstrained one. The methodology is implemented in a real quantum computer for two known optimization problems: the Facility Location Problem and the Set Packing Problem. The results obtained for this two problems with VQE using 2-Local variational form and a general QAOA implementation are compared, and indicate that less quantum gates and parameters were used, leading to a faster convergence.",quant-ph,Quantum Physics
"Bohm, Penrose, and the Search for non-Local Causality","Before they met, David Bohm and Roger Penrose each puzzled over the paradox of the arrow of time. After they met, the case for projective physical space became clearer.",quant-ph,Quantum Physics
Non-Equilibrium Steady State of the Lieb-Liniger model: exact treatment of the Tonks Girardeau limit,"Aiming at studying the emergence of Non-Equilibrium Steady States (NESS) in quantum integrable models by means of an exact analytical method, we focus on the Tonks-Girardeau or hard-core boson limit of the Lieb-Liniger model. We consider the abrupt expansion of a gas from one half to the entire confining box, a prototypical case of inhomogeneous quench, also known as ""geometric quench"". Based on the exact calculation of quench overlaps, we develop an analytical method for the derivation of the NESS by rigorously treating the thermodynamic and large time and distance limit. Our method is based on complex analysis tools for the derivation of the asymptotics of the many-body wavefunction, does not make essential use of the effectively non-interacting character of the hard-core boson gas and is sufficiently robust for generalisation to the genuinely interacting case.",quant-ph,Quantum Physics
Possibility to generate any Gaussian cluster state by a multi-mode squeezing transformation,"Gaussian cluster states are ideal infinitely squeezed states. In practice it is possible to construct only approximated version of them with finite squeezing. Here we show how to determine the specific multi-mode squeezing transformation, which generates a faithful approximation of any given Gaussian cluster state.",quant-ph,Quantum Physics
Spin-momentum entanglement in a Bose-Einstein condensate,"Entanglement is at the core of quantum information processing and may prove essential for quantum speed-up. Inspired by both theoretical and experimental studies of spin-momentum coupling in systems of ultra-cold atoms, we investigate the entanglement between the spin and momentum degrees of freedom of an optically trapped BEC of $^{87}$Rb atoms. We consider entanglement that arises due to the coupling of these degrees of freedom induced by Raman and radio-frequency fields and examine its dependence on the coupling parameters by evaluating von Neumann entropy as well as concurrence as measures of the entanglement attained. Our calculations reveal that under proper experimental conditions significant spin-momentum entanglement can be obtained, with von Neumann entropy of 80% of the maximum attainable value. Our analysis sheds some light on the prospects of using BECs for quantum information applications.",quant-ph,Quantum Physics
Purcell enhanced and indistinguishable single-photon generation from quantum dots coupled to on-chip integrated ring resonators,"Integrated photonic circuits provide a versatile toolbox of functionalities for advanced quantum optics applications. Here, we demonstrate an essential component of such a system in the form of a Purcell enhanced single-photon source based on a quantum dot coupled to a robust on-chip integrated resonator. For that, we develop GaAs monolithic ring cavities based on distributed Bragg reflector ridge waveguides. Under resonant excitation conditions, we observe an over twofold spontaneous emission rate enhancement using Purcell effect and gain a full coherent optical control of a QD-two-level system via Rabi oscillations. Furthermore, we demonstrate an on-demand single-photon generation with strongly suppressed multi-photon emission probability as low as 1% and two-photon interference with visibility up to 95%. This integrated single-photon source can be readily scaled up, promising a realistic pathway for scalable on-chip linear optical quantum simulation, quantum computation and quantum networks.",quant-ph,Quantum Physics
Nonstationary force sensing under dissipative mechanical quantum squeezing,"We study the stationary and nonstationary measurement of a classical force driving a mechanical oscillator coupled to an electromagnetic cavity under two-tone driving. For this purpose, we develop a theoretical framework based on the signal-to-noise ratio to quantify the sensitivity of linear spectral measurements. Then, we consider stationary force sensing and study the necessary conditions to minimise the added force noise. We find that imprecision noise and back-action noise can be arbitrarily suppressed by manipulating the amplitudes of the input coherent fields, however, the force noise power spectral density cannot be reduced below the level of thermal fluctuations. Therefore, we consider a nonstationary protocol that involves non-thermal dissipative state preparation followed by a finite time measurement, which allows one to perform measurements with a signal-to-noise much greater than the maximum possible in a stationary measurement scenario. We analyse two different measurement schemes in the nonstationary transient regime, a back-action evading measurement, which implies modifying the drive asymmetry configuration upon arrival of the force, and a nonstationary measurement that leaves the drive asymmetry configuration unchanged. Conditions for optimal force noise sensitivity are determined, and the corresponding force noise power spectral densities are calculated.",quant-ph,Quantum Physics
Proposal to measure out-of-time-ordered correlations using Bell states,"We present a protocol to experimentally measure the infinite-temperature out-of-time-ordered correlation (OTOC) -- which is a probe of quantum information scrambling in a system -- for systems with a Hamiltonian which has either a chiral symmetry or a particle-hole symmetry. We show that the OTOC can be obtained by preparing two entangled systems, evolving them with the Hamiltonian, and measuring appropriate local observables. At the cost of requiring two copies of the system and putting restrictions on the Hamiltonian's symmetries, we show that our method provides some advantages over existing methods -- it can be implemented without reversing the sign of the Hamiltonian, it requires fewer measurements than schemes based on implementing the SWAP operator, and it is robust to imperfections like some earlier methods. Our ideas can be implemented in currently available quantum platforms.",quant-ph,Quantum Physics
Polarization-gradient cooling of 1D and 2D ion Coulomb crystals,We present experiments on polarization gradient cooling of Ca$^+$ multi-ion Coulomb crystals in a linear Paul trap. Polarization gradient cooling of the collective modes of motion whose eigenvectors have overlap with the symmetry axis of the trap is achieved by two counter-propagating laser beams with mutually orthogonal linear polarizations that are blue-detuned from the S$_{1/2}$ to P$_{1/2}$ transition. We demonstrate cooling of linear chains of up to 51 ions and 2D-crystals in zig-zag configuration with 22 ions. The cooling results are compared with numerical simulations and the predictions of a simple model of cooling in a moving polarization gradient.,quant-ph,Quantum Physics
Experimental implementation of leakage elimination operators,"Decoherence-induced leakage errors can potentially damage physical or logical qubits by coupling them to other system levels. Here we report the first experimental implementation of Leakage Elimination Operators (LEOs) that aims to reduce this undermining, and that can be applied alongside universal quantum computing. Using IBM's cloud quantum computer, we have studied three potentially applicable examples of subspaces in two- and three-qubit Hilbert spaces and found that the LEOs significantly suppress leakage.",quant-ph,Quantum Physics
Charging a quantum battery via non equilibrium heat current,"When a quantum system is subject to a thermal gradient it may sustain a steady non-equilibrium heat current, by entering into a so-called non equilibrium steady state (NESS). Here we show that NESS constitute a thermodynamic resource that can be exploited to fuel a quantum heat engine. This adds to the list of recently reported sources available at the nano-scale, such as coherence, entanglement and quantum measurements. We elucidate this concept by showing analytic and numerical studies of a two-qubits quantum battery that is alternatively charged by a thermal gradient and discharged by application of a properly chosen unitary gate. The presence of a NESS for the charging step guarantees steady operation with positive power output. Decreasing the duration of the charging step results in a time periodic steady state accompanied by increased efficiency and output power. The device is amenable to implementation with different nanotechnology platforms.",quant-ph,Quantum Physics
Adjoint-optimized nanoscale light extractor for nitrogen-vacancy centers in diamond,"We designed a nanoscale light extractor (NLE) for efficient outcoupling and beaming of broadband light emitted by shallow, negatively charged nitrogen-vacancy (NV) centers in bulk diamond. The NLE consists of a patterned silicon layer on diamond and requires no etching of the diamond surface. Our design process is based on adjoint optimization using broadband time-domain simulations and yields structures that are inherently robust to positioning and fabrication errors. Our NLE functions like a transmission antenna for the NV center, enhancing the optical power extracted from an NV center positioned 10 nm below the diamond surface by a factor of more than 35, and beaming the light into a +/-30 cone in the far field. This approach to light extraction can be readily adapted to other solid-state color centers.",quant-ph,Quantum Physics
Quantum paraelectric varactors for radio-frequency measurements at mK temperatures,"Radio-frequency reflectometry allows for fast and sensitive electrical readout of charge and spin qubits hosted in quantum dot devices coupled to resonant circuits. Optimizing readout, however, requires frequency tuning of the resonators and impedance matching. This is difficult to achieve using conventional semiconductor or ferroelectric-based varactors in the detection circuit as their performance degrades significantly in the mK temperature range relevant for solid-state quantum devices. Here we explore a different type of material, strontium titanate, a quantum paraelectric with exceptionally large field-tunable permittivity at low temperatures. Using strontium titanate varactors we demonstrate perfect impedance matching and resonator frequency tuning at 6 mK and characterize the varactors at this temperature in terms of their capacitance tunability, dissipative losses and magnetic field sensitivity. We show that this allows us to optimize the radio-frequency readout signal-to-noise ratio of carbon nanotube quantum dot devices to achieve a charge sensitivity of 4.8 $$e/Hz$^{1/2}$ and capacitance sensitivity of 0.04 aF/Hz$^{1/2}$.",quant-ph,Quantum Physics
Levy flights in steep potential wells: Langevin modeling versus direct response to energy landscapes,"We investigate the non-Langevin relative of the Lvy-driven Langevin random system, under an assumption that both systems share a common (asymptotic, stationary, steady-state) target pdf. The relaxation to equilibrium in the fractional Langevin-Fokker-Planck scenario results from an impact of confining conservative force fields on the random motion. A non-Langevin alternative has a built-in direct response of jump intensities to energy (potential) landscapes in which the process takes place. We revisit the problem of Lvy flights in superharmonic potential wells, with a focus on the extremally steep well regime, and address the issue of its (spectral) ""closeness"" to the Lvy jump-type process confined in a finite enclosure with impenetrable (in particular reflecting) boundaries. The pertinent random system ""in a box/interval"" is expected to have a fractional Laplacian with suitable boundary conditions as a legitimate motion generator. The problem is, that in contrast to amply studied Dirichlet boundary problems, a concept of reflecting boundary conditions and the path-wise implementation of the pertinent random process in the vicinity of (or sharply at) reflecting boundaries are not unequivocally settled for Lvy processes. This ambiguity extends to fractional motion generators, for which nonlocal analogs of Neumann conditions are not associated with path-wise reflection scenarios at the boundary, respecting the impenetrability assumption.",quant-ph,Quantum Physics
Single- versus two-parameter Fisher information in quantum interferometry,"In this paper we reconsider the single parameter quantum Fisher information (QFI) and compare it with the two-parameter one. We find simple relations connecting the single parameter QFI (both in the asymmetric and symmetric phase shift cases) to the two parameter Fisher matrix coefficients. Following some clarifications about the role of an external phase [Phys. Rev. A 85, 011801(R) (2012)], the single-parameter QFI and its over-optimistic predictions have been disregarded in the literature. We show in this paper that both the single- and two-parameter QFI have physical meaning and their predicted quantum Cramr-Rao bounds are often attainable with the appropriate experimental setup. Moreover, we give practical situations of interest in quantum metrology, where the phase sensitivities of a number of input states approach the quantum Cramr-Rao bound induced by the single-parameter QFI, outperforming the two-parameter QFI.",quant-ph,Quantum Physics
Two attacks and counterattacks on the mutual semi-quantum key agreement protocol using Bell states,"Recently, a mutual semi-quantum key agreement protocol using Bell states is proposed by Yan et al. (Mod. Phys. Lett. A, 34, 1950294, 2019). The proposed protocol tries to help a quantum participant share a key with a classical participant who just has limited quantum capacities. Yan et al. claimed that both the participants have the same influence on the final shared key. However, this study points out that the classical participant can manipulate the final shared key by himself/herself without being detected. To solve this problem, an improved method is proposed here.",quant-ph,Quantum Physics
Paving Through the Vacuum,"Using the model in which the vacuum is filled with virtual fermion pairs we propose an effective description of photon propagation compatible with the wave-particle duality and the quantum field theory.
  In this model the origin of the vacuum permittivity and permeability appear naturally in the statistical description of the gas of the virtual pairs. Assuming virtual gas in thermal equilibrium at temperature corresponding to the Higgs field vacuum expectation value, $kT\approx246.22\, \rm{GeV}$, the deduced value of the vacuum magnetic permeability (magnetic constant) appears on the same order of the experimental value. One of the features that makes this model attractive is the expected fluctuation of the speed of light propagation that is at the level of $\approx 1.9\,\mathrm{as\,m}^{-1/2}$. This non-classical light propagation property is reachable with the available technologies.",quant-ph,Quantum Physics
Decoherence: A Numerical Study,"We study quantum decoherence numerically in a system consisting of a relativistic quantum field theory coupled to a measuring device that is itself coupled to an environment. The measuring device and environment are treated as quantum, non-relativistic particles. We solve the Schrdinger equation for the wave function of this tripartite system using exact diagonalization. Although computational limitations on the size of the Hilbert space prevent us from exploring the regime where the device and environment consist of a truly macroscopic number of degrees of freedom, we nevertheless see clear evidence of decoherence: after tracing out the environment, the density matrix describing the system and measuring device evolves quickly towards a matrix that is close to diagonal in a subspace of pointer states.",quant-ph,Quantum Physics
Atom-light entanglement for precise field sensing in the optical domain,"Macroscopic arrays of cold atoms trapped in optical cavities can reach the strong atom-light collective coupling regime thanks to the simultaneous interactions of the cavity mode with the atomic ensemble. In a recent work we reported a protocol that takes advantage of the strong and collective atom-light interactions in cavity QED systems for precise electric field sensing in the optical domain. We showed that it can provide between $10$-$20$~dB of metrological gain over the standard quantum limit in current cavity QED experiments operating with long-lived alkaline-earth atoms. Here, we give a more in depth discussion of the protocol using both exact analytical calculations and numerical simulations, and describe the precise conditions under which the predicted enhancement holds after thoroughly accounting for both photon loss and spontaneous emission, natural decoherence mechanisms in current experiments. The analysis presented here not only serves to benchmark the protocol and its utility in cavity QED arrays but also sets the conditions required for its applicability in other experimental platforms such as arrays of trapped ions.",quant-ph,Quantum Physics
Comment on `Energy spectrum of a Dirac particle with position-dependent mass under the influence of the Aharonov-Casher effect',"It is shown that the paper `Energy spectrum of a Dirac particle with position-dependent mass under the influence of the Aharonov-Casher effect', by Oliveira, Borges and Sousa [Braz. J. Phys. 49, 801 (2019)], is based on a series of ingredients clearly incorrect.",quant-ph,Quantum Physics
Mapping the charge-dyon system into the position-dependent effective mass background via Pauli equation,"This work aims to reproduce a quantum system composed of a charged spin - $1/2$ fermion interacting with a dyon with an opposite electrical charge (charge-dyon system), utilizing a position-dependent effective mass (PDM) background in the non-relativistic regime via the PDM free Pauli equation. To investigate whether there is a PDM quantum system with the same physics (analogous model) that a charge-dyon system (target system), we resort to the PDM free Pauli equation itself. We proceed with replacing the exact bi-spinor of the target system into this equation, obtaining an uncoupled system of non-linear partial differential equations for the mass distribution $M$. We were able to solve them numerically for $M$ considering a radial dependence only, i.e., $M=M(r)$, fixing $_0$, and considering specific values of $$ and $m$ satisfying a certain condition. We present the solutions graphically, and from them, we determine the respective effective potentials, which actually represent our analogous models. We study the mapping for eigenvalues starting from the minimal value $j = - 1/2$.",quant-ph,Quantum Physics
"Picking Efficient Portfolios from 3,171 US Common Stocks with New Quantum and Classical Solvers","We analyze 3,171 US common stocks to create an efficient portfolio based on the Chicago Quantum Net Score (CQNS) and portfolio optimization. We begin with classical solvers and incorporate quantum annealing. We add a simulated bifurcator as a new classical solver and the new D-Wave Advantage(TM) quantum annealing computer as our new quantum solver.",quant-ph,Quantum Physics
Piggybacking on Quantum Streams,"This paper shows that it is possible to piggyback classical information on a stream of qubits protected by quantum error correcting codes. The piggyback channel can be created by introducing intentional errors corresponding to a controlled sequence of syndromes. These syndromes are further protected, when quantum noise is present, by classical error correcting codes according to a performance-delay trade-off. Classical information can thus be added and extracted at arbitrary epochs without consuming additional quantum resources and without disturbing the quantum stream.",quant-ph,Quantum Physics
Low-frequency behavior of off-diagonal matrix elements in the integrable XXZ chain and in a locally perturbed quantum-chaotic XXZ chain,"We study the matrix elements of local operators in the eigenstates of the integrable XXZ chain and of the quantum-chaotic model obtained by locally perturbing the XXZ chain with a magnetic impurity. We show that, at frequencies that are polynomially small in the system size, the behavior of the variances of the off-diagonal matrix elements can be starkly different depending on the operator. In the integrable model we find that, as the frequency $\rightarrow0$, the variances are either nonvanishing (generic behavior) or vanishing (for a special class of operators). In the quantum-chaotic model, on the other hand, we find the variances to be nonvanishing as $\rightarrow0$ and to indicate diffusive dynamics. We highlight which properties of the matrix elements of local operators are different between the integrable and quantum-chaotic models independently of the specific operator selected.",quant-ph,Quantum Physics
Visible stripe phases in spin-orbital-angular-momentum coupled Bose-Einstein condensates,"Recently, stripe phases in spin-orbit coupled Bose-Einstein condensates (BECs) have attracted much attention since they are identified as supersolid phases. In this paper, we exploit experimentally reachable parameters and show theoretically that annular stripe phases with large stripe spacing and high stripe contrast can be achieved in spin-orbital-angular-momentum coupled (SOAMC) BECs. In addition to using Gross-Pitaevskii numerical simulations, we develop a variational ansatz that captures the essential interaction effects to first order, which are not present in the ansatz employed in previous literature. Our work should open the possibility toward directly observing stripe phases in SOAMC BECs in experiments.",quant-ph,Quantum Physics
Microcavity Polaritons for Quantum simulation,"Quantum simulations are one of the pillars of quantum technologies. These simulations provide insight in fields as varied as high energy physics, many-body physics, or cosmology to name only a few. Several platforms, ranging from ultracold-atoms to superconducting circuits through trapped ions have been proposed as quantum simulators. This article reviews recent developments in another well established platform for quantum simulations: polaritons in semiconductor microcavities. These quasiparticles obey a nonlinear Schrdigner equation (NLSE), and their propagation in the medium can be understood in terms of quantum hydrodynamics. As such, they are considered as ""fluids of light"". The challenge of quantum simulations is the engineering of configurations in which the potential energy and the nonlinear interactions in the NLSE can be controlled. Here, we revisit some landmark experiments with polaritons in microcavities, discuss how the various properties of these systems may be used in quantum simulations, and highlight the richness of polariton systems to explore non-equilibrium physics",quant-ph,Quantum Physics
A minimalist's view of quantum mechanics,"We analyse a proposition which considers quantum theory as a mere tool for calculating probabilities for sequences of outcomes of observations made by an Observer, who him/herself remains outside the scope of the theory. Predictions are possible, provided a sequence includes at least two such observations. Complex valued probability amplitudes, each defined for an entire sequence of outcomes, are attributed to Observer's reasoning, and the problem of wave function's collapse is dismissed as a purely semantic one. Our examples include quantum ""weak values"", and a simplified version of the ""delayed quantum eraser"".",quant-ph,Quantum Physics
Efficient Routing for Quantum Key Distribution Networks,"As quantum key distribution becomes increasingly practical, questions of how to effectively employ it in large-scale networks and over large distances becomes increasingly important. To that end, in this work, we model the performance of the E91 entanglement based QKD protocol when operating in a network consisting of both quantum repeaters and trusted nodes. We propose a number of routing protocols for this network and compare their performance under different usage scenarios. Through our modeling, we investigate optimal placement and number of trusted nodes versus repeaters depending on device performance (e.g., quality of the repeater's measurement devices). Along the way we discover interesting lessons determining what are the important physical aspects to improve for upcoming quantum networks in order to improve secure communication rates.",quant-ph,Quantum Physics
Experimental evaluation of quantum Bayesian networks on IBM QX hardware,"Bayesian Networks (BN) are probabilistic graphical models that are widely used for uncertainty modeling, stochastic prediction and probabilistic inference. A Quantum Bayesian Network (QBN) is a quantum version of the Bayesian network that utilizes the principles of quantum mechanical systems to improve the computational performance of various analyses. In this paper, we experimentally evaluate the performance of QBN on various IBM QX hardware against Qiskit simulator and classical analysis. We consider a 4-node BN for stock prediction for our experimental evaluation. We construct a quantum circuit to represent the 4-node BN using Qiskit, and run the circuit on nine IBM quantum devices: Yorktown, Vigo, Ourense, Essex, Burlington, London, Rome, Athens and Melbourne. We will also compare the performance of each device across the four levels of optimization performed by the IBM Transpiler when mapping a given quantum circuit to a given device. We use the root mean square percentage error as the metric for performance comparison of various hardware.",quant-ph,Quantum Physics
Remarks on Black Hole Complexity Puzzle,"Recently a certain conceptual puzzle in the AdS/CFT correspondence, concerning the growth of quantum circuit complexity and the wormhole volume, has been identified by Bouland-Fefferman-Vazirani and Susskind. In this note, we propose a resolution of the puzzle and save the quantum Extended Church-Turing thesis by arguing that there is no computational shortcut in measuring the volume due to gravitational backreaction from bulk observers. A certain strengthening of the firewall puzzle from the computational complexity perspective, as well as its potential resolution, is also presented.",quant-ph,Quantum Physics
Mermin's Inequalities of Multiple qubits with Orthogonal Measurements on IBM Q 53-qubit system,Entanglement properties of IBM Q 53 qubit quantum computer are carefully examined with the noisy intermediate-scale quantum (NISQ) technology. We study GHZ-like states with multiple qubits (N=2 to N=7) on IBM Rochester and compare their maximal violation values of Mermin polynomials with analytic results. A rule of N-qubits orthogonal measurements is taken to further justify the entanglement less than maximal values of local realism (LR). The orthogonality of measurements is another reliable criterion for entanglement except the maximal values of LR. Our results indicate that the entanglement of IBM 53-qubits is reasonably good when N <= 4 while for the longer entangle chains the entanglement is only valid for some special connectivity.,quant-ph,Quantum Physics
Non-Markovian memory in a measurement-based quantum computer,"We study the exact open system dynamics of single qubit gates during a measurement-based quantum computation considering non-Markovian environments. We obtain analytical solutions for the average gate fidelities and analyze it for amplitude damping and dephasing channels. We show that the average fidelity is identical for the X-gate and Z-gate and that neither fast application of the projective measurements necessarily implies high gate fidelity, nor slow application necessarily implies low gate fidelity. Indeed, for highly non-Markovian environments, it is of utmost importance to know the best time to perform the measurements, since a huge variation in the gate fidelity may occur given this scenario. Furthermore, we show that while for the amplitude damping the knowledge of the dissipative map is sufficient to determine the best measurement times, i.e. the best times in which measures are taken, the same is not necessarily true for the phase damping. To the later, the time of the set of measures becomes crucial since a phase error in one qubit can fix the phase error that takes place in another.",quant-ph,Quantum Physics
Detecting Fractional Chern Insulators in Optical Lattices through Quantized Displacement,"The realization of interacting topological states of matter such as fractional Chern insulators (FCIs) in cold atom systems has recently come within experimental reach due to the engineering of optical lattices with synthetic gauge fields providing the required topological band structures. However, detecting their occurrence might prove difficult since transport measurements akin to those in solid state systems are challenging to perform in cold atom setups and alternatives have to be found. We show that for a $= 1/2$ FCI state realized in the lowest band of a Harper-Hofstadter model of interacting bosons confined by a harmonic trapping potential, the fractionally quantized Hall conductivity $_{xy}$ can be accurately determined by the displacement of the atomic cloud under the action of a constant force which provides a suitable experimentally measurable signal for detecting the topological nature of the state. Using matrix-product state algorithms, we show that, in both cylinder and square geometries, the movement of the particle cloud in time under the application of a constant force field on top of the confining potential is proportional to $_{xy}$ for an extended range of field strengths.",quant-ph,Quantum Physics
Anomalous in-gap edge states in two-dimensional pseudospin-1 Dirac insulators,"Quantum materials that host a flat band, such as pseudospin-1 lattices and magic-angle twisted bilayer graphene, can exhibit drastically new physical phenomena including unconventional superconductivity, orbital ferromagnetism, and Chern insulating behaviors. We report a surprising class of electronic in-gap edge states in pseudospin-1 materials without the conventional need of band-inversion topological phase transitions or introducing magnetism via an external magnetic type of interactions. In particular, we find that, in two-dimensional gapped (insulating) Dirac systems of massive spin-1 quasiparticles, in-gap edge modes can emerge through only an {\em electrostatic potential} applied to a finite domain. Associated with these unconventional edge modes are spontaneous formation of pronounced domain-wall spin textures, which exhibit the feature of out-of-plane spin-angular momentum locking on both sides of the domain boundary and are quite robust against boundary deformations and impurities despite a lack of an explicit topological origin. The in-gap modes are formally three-component evanescent wave solutions, akin to the Jackiw-Rebbi type of bound states. Such modes belong to a distinct class due to the following physical reasons: three-component spinor wave function, unusual boundary conditions, and a shifted flat band induced by the external scalar potential. Not only is the finding of fundamental importance, but it also paves the way for generating highly controllable in-gap edge states with emergent spin textures using the traditional semiconductor gate technology. Results are validated using analytic calculations of a continuum Dirac-Weyl model and tight-binding simulations of realistic materials through characterizations of local density of state spectra and resonant tunneling conductance.",quant-ph,Quantum Physics
Lieb Robinson bounds and out of time order correlators in a long range spin chain,"Lieb Robinson bounds quantify the maximal speed of information spreading in nonrelativistic quantum systems. We discuss the relation of Lieb Robinson bounds to out of time order correlators, which correspond to different norms of commutators $C(r,t) = [A_i(t),B_{i+r}]$ of local operators. Using an exact Krylov space time evolution technique, we calculate these two different norms of such commutators for the spin 1/2 Heisenberg chain with interactions decaying as a power law $1/r^$ with distance $r$. Our numerical analysis shows that both norms (operator norm and normalized Frobenius norm) exhibit the same asymptotic behavior, namely a linear growth in time at short times and a power law decay in space at long distance, leading asymptotically to power law light cones for $<1$ and to linear light cones for $>1$. The asymptotic form of the tails of $C(r,t)\propto t/r^$ is described by short time perturbation theory which is valid at short times and long distances.",quant-ph,Quantum Physics
Fault-tolerant quantum speedup from constant depth quantum circuits,"A defining feature in the field of quantum computing is the potential of a quantum device to outperform its classical counterpart for a specific computational task. By now, several proposals exist showing that certain sampling problems can be done efficiently quantumly, but are not possible efficiently classically, assuming strongly held conjectures in complexity theory. A feature dubbed quantum speedup. However, the effect of noise on these proposals is not well understood in general, and in certain cases it is known that simple noise can destroy the quantum speedup.
  Here we develop a fault-tolerant version of one family of these sampling problems, which we show can be implemented using quantum circuits of constant depth. We present two constructions, each taking $poly(n)$ physical qubits, some of which are prepared in noisy magic states. The first of our constructions is a constant depth quantum circuit composed of single and two-qubit nearest neighbour Clifford gates in four dimensions. This circuit has one layer of interaction with a classical computer before final measurements. Our second construction is a constant depth quantum circuit with single and two-qubit nearest neighbour Clifford gates in three dimensions, but with two layers of interaction with a classical computer before the final measurements.
  For each of these constructions, we show that there is no classical algorithm which can sample according to its output distribution in $poly(n)$ time, assuming two standard complexity theoretic conjectures hold. The noise model we assume is the so-called local stochastic quantum noise. Along the way, we introduce various new concepts such as constant depth magic state distillation (MSD), and constant depth output routing, which arise naturally in measurement based quantum computation (MBQC), but have no constant-depth analogue in the circuit model.",quant-ph,Quantum Physics
Atom-interferometric test of the equivalence principle at the $10^{-12}$ level,"Does gravity influence local measurements? We use a dual-species atom interferometer with $2\,\text{s}$ of free-fall time to measure the relative acceleration between $^{85}$Rb and $^{87}$Rb wave packets in the Earth's gravitational field. Systematic errors arising from kinematic differences between the isotopes are suppressed by calibrating the angles and frequencies of the interferometry beams. We find an Etvs parameter of $= [1.6\; \pm\; 1.8\; \text{(stat)}\; \pm \; 3.4 \; \text{(sys)}] \times 10^{-12}$, consistent with zero violation of the equivalence principle. With a resolution of up to $1.4 \times 10^{-11} \, g$ per shot, we demonstrate a sensitivity to $$ of $5.4 \times 10^{-11}\,/\sqrt{\text{Hz}}$.",quant-ph,Quantum Physics
A liquid nitrogen cooled superconducting transition edge sensor with ultra-high responsivity and GHz operation speeds,"Photodetectors based on nano-structured superconducting thin films are currently some of the most sensitive quantum sensors and are key enabling technologies in such broad areas as quantum information, quantum computation and radio-astronomy. However, their broader use is held back by the low operation temperatures which require expensive cryostats. Here, we demonstrate a nitrogen cooled superconducting transition edge sensor, which shows orders of magnitude improved performance characteristics of any superconducting detector operated above 77K, with a responsivity of 9.61x10^4 V/W, noise equivalent power of 15.9 fW/Hz-1/2 and operation speeds up to GHz frequencies. It is based on van der Waals heterostructures of the high temperature superconductor Bi2Sr2CaCu2O8, which are shaped into nano-wires with ultra-small form factor. To highlight the versatility of the detector we demonstrate its fabrication and operation on a telecom grade SiN waveguide chip. Our detector significantly relaxes the demands of practical applications of superconducting detectors and displays its huge potential for photonics based quantum applications.",quant-ph,Quantum Physics
$\mathcal{PT}$-Symmetric Generalized Extended Momentum Operator,"We develop further the concept of generalized extended momentum operator (GEMO), which has been introduced very recently in \citep{M.H2}, and propose the so called $\mathcal{PT}$-symmetric GEMO. In analogy with GEMO, the $\mathcal{PT}$-symmetric GEMO also satisfies the extended uncertainty principle (EUP) relation. Moreover, the corresponding Hamiltonian that is constructed upon the $\mathcal{PT}$-symmetric GEMO, with a real or $\mathcal{PT}$-symmetric potential, remains non-Hermitian but $\mathcal{PT}$-symmetric and consequently its energy and momentum eigenvalues are real. We apply our formalism to a quasi-free quantum particle and the exact solutions for the energy spectrum are presented.",quant-ph,Quantum Physics
The Bitter Truth About Quantum Algorithms in the NISQ Era,"Implementing a quantum algorithm on a NISQ device has several challenges that arise from the fact that such devices are noisy and have limited quantum resources. Thus, various factors contributing to the depth and width as well as to the noise of an implementation of an algorithm must be understood in order to assess whether an implementation will execute successfully on a given NISQ device. In this contribution, we discuss these factors and their impact on algorithm implementations. Especially, we will cover state preparation, oracle expansion, connectivity, circuit rewriting, and readout: these factors are very often ignored when presenting an algorithm but they are crucial when implementing such an algorithm on near-term quantum computers. Our contribution will help developers in charge of realizing algorithms on such machines in (i) achieving an executable implementation, and (ii) assessing the success of their implementation on a given machine.",quant-ph,Quantum Physics
A Monte Carlo Approach to the Worldline Formalism in Curved Space,"We present a numerical method to evaluate worldline (WL) path integrals defined on a curved Euclidean space, sampled with Monte Carlo (MC) techniques. In particular, we adopt an algorithm known as YLOOPS with a slight modification due to the introduction of a quadratic term which has the function of stabilizing and speeding up the convergence. Our method, as the perturbative counterparts, treats the non-trivial measure and deviation of the kinetic term from flat, as interaction terms. Moreover, the numerical discretization adopted in the present WLMC is realized with respect to the proper time of the associated bosonic point-particle, hence such procedure may be seen as an analogue of the time-slicing (TS) discretization already introduced to construct quantum path integrals in curved space. As a result, a TS counter-term is taken into account during the computation. The method is tested against existing analytic calculations of the heat kernel for a free bosonic point-particle in a D-dimensional maximally symmetric space.",quant-ph,Quantum Physics
Scrambling versus relaxation in Fermi and non-Fermi liquids,"We compute the Lyapunov exponent characterizing quantum scrambling in a family of generalized Sachdev-Ye-Kitaev models, which can be tuned between different low temperature states from Fermi liquids, through non-Fermi liquids to fast scramblers. The analytic calculation, controlled by a small coupling constant and large $N$, allows us to clarify the relations between the quasi-particle relaxation rate $1/$ and the Lyapunov exponent $_L$ characterizing scrambling. In the Fermi liquid states we find that the quasi-particle relaxation rate dictates the Lyapunov exponent. In non-Fermi liquids, where $1/\gg T$, we find that $_L$ is always $T$-linear with a prefactor that is independent of the coupling constant in the limit of weak coupling. Instead it is determined by a scaling exponent that characterizes the relaxation rate. $_L$ approaches the general upper bound $2T$ at the transition to a fast scrambling state. Finally in a marginal Fermi liquid state the exponent is linear in temperature with a prefactor that vanishes as a non analytic function $\sim g \ln (1/g)$ of the coupling constant $g$.",quant-ph,Quantum Physics
Anomaly Detection with Tensor Networks,"Originating from condensed matter physics, tensor networks are compact representations of high-dimensional tensors. In this paper, the prowess of tensor networks is demonstrated on the particular task of one-class anomaly detection. We exploit the memory and computational efficiency of tensor networks to learn a linear transformation over a space with dimension exponential in the number of original features. The linearity of our model enables us to ensure a tight fit around training instances by penalizing the model's global tendency to a predict normality via its Frobenius norm---a task that is infeasible for most deep learning models. Our method outperforms deep and classical algorithms on tabular datasets and produces competitive results on image datasets, despite not exploiting the locality of images.",quant-ph,Quantum Physics
Energy levels in a single-electron quantum dot with hydrostatic pressure,In this article we present a study of the effects of hydrostatic pressure on the energy levels of a quantum dot with an electron. A quantum dot is modeled using an infinite potential well and a two-dimensional harmonic oscillator and solved through the formalism of second quantization. A scheme for the implementation of a quantum NOT gate controlled with hydrostatic pressure is proposed.,quant-ph,Quantum Physics
A Preliminary Study for a Quantum-like Robot Perception Model,"Formalisms based on quantum theory have been used in Cognitive Science for decades due to their descriptive features. A quantum-like (QL) approach provides descriptive features such as state superposition and probabilistic interference behavior. Moreover, quantum systems dynamics have been found isomorphic to cognitive or biological systems dynamics. The objective of this paper is to study the feasibility of a QL perception model for a robot with limited sensing capabilities. We introduce a case study, we highlight its limitations, and we investigate and analyze actual robot behaviors through simulations, while actual implementations based on quantum devices encounter errors for unbalanced situations. In order to investigate QL models for robot behavior, and to study the advantages leveraged by QL approaches for robot knowledge representation and processing, we argue that it is preferable to proceed with simulation-oriented techniques rather than actual realizations on quantum backends.",quant-ph,Quantum Physics
Quantum Metamaterials: Applications in quantum information science,"Metamaterials are artificially engineered periodic structures with exceptional optical properties that are not found in conventional materials. However, this definition of metamaterials can be extended if we introduce a quantum degree of freedom by adding some quantum elements (e.g quantum dots, cold atoms, Josephson junctions, molecules). Quantum metamaterials can then be defined as artificially engineered nanostructures made up of quantum elements. Furthermore, they exhibit controllable quantum states, maintain quantum coherence for times much higher than the transversal time of the electromagnetic signal. Metamaterials have been used to realised invisibility cloaking, super-resolution, energy harvesting, and sensing. Most of these applications are performed in the classical regime. Of recent, metamaterials have gradually found their way into the quantum regime, particularly to quantum sensing and quantum information processing. The use of quantum metamaterials for quantum information processing is still new and rapidly growing. In quantum information processing, quantum metamaterials have enabled the control and manipulation of quantum states, single photon generation, creating quantum entanglement, quantum states switching, quantum search algorithm, quantum state engineering tasks, and many more. In this work, we briefly review the theory, fabrication and applications of quantum metamaterials to quantum information processing.",quant-ph,Quantum Physics
Quantum transport in the flux rhombic lattice,We analyse stationary current of the bosonic particles in the flux rhombic lattice connecting two particle reservoirs. For vanishing inter-particle interactions the current is shown to monotonically decrease as the flux is increased and become strictly zero for the Peierls phase equal to $$. Non-zero interactions modify this dependence and for moderate interaction strength the current is found to be independent of the flux value.,quant-ph,Quantum Physics
Two-axis two-spin squeezed states,"The states generated by the two-spin generalization of the two-axis countertwisting Hamiltonian are examined. We analyze the behavior at both short and long timescales, by calculating various quantities such as squeezing, spin expectation values, probability distributions, entanglement, Wigner functions, and Bell correlations. In the limit of large spin ensembles and short interaction times, the state can be described by a two-mode squeezed vacuum state; for qubits, Bell state entanglement is produced. We find that the Hamiltonian approximately produces two types of spin-EPR states, and the time evolution produces aperiodic oscillations between them. In a similar way to the basis invariance of Bell states and two-mode squeezed vacuum states, the Fock state correlations of spin-EPR states are basis invariant. We find that it is possible to violate a Bell inequality with such states, although the violation diminishes with increasing ensemble size. Effective methods to detect entanglement are also proposed, and formulas for the optimal times to enhance various properties are calculated.",quant-ph,Quantum Physics
Approach to realizing nonadiabatic geometric gates with prescribed evolution paths,"Nonadiabatic geometric phases are only dependent on the evolution path of a quantum system but independent of the evolution details, and therefore quantum computation based on nonadiabatic geometric phases is robust against control errors. To realize nonadiabatic geometric quantum computation, it is necessary to ensure that the quantum system undergoes a cyclic evolution and the dynamical phases are removed from the total phases. To satisfy these conditions, the evolution paths in previous schemes are usually restricted to some special forms, e.g, orange-slice-shaped loops, which make the paths unnecessarily long in general. In this paper, we put forward an approach to the realization of nonadiabatic geometric quantum computation by which a universal set of nonadiabatic geometric gates can be realized with any desired evolution paths. Our approach makes it possible to realize geometric quantum computation with an economical evolution time so the influence of environment noises on the quantum gates can be minimized further.",quant-ph,Quantum Physics
Oblique Ion Two-Stream Instability in the Foot Region of a Collisionless Shock,"Electrostatic behavior of a collisionless plasma in the foot region of high Mach number perpendicular shocks is investigated through the two-dimensional linear analysis and electrostatic particle-in-cell (PIC) simulation. The simulations are double periodic and taken as a proxy for the situation in the foot. The linear analysis for relatively cold unmagnetized plasmas with a reflected proton beam shows that obliquely propagating Buneman instability is strongly excited. We also found that when the electron temperature is much higher than the proton temperature, the most unstable mode is the highly obliquely propagating ion two-stream instability excited through the resonance between ion plasma oscillations of the background protons and of the beam protons, rather than the ion acoustic instability that is dominant for parallel propagation. To investigate nonlinear behavior of the ion two-stream instability, we have made PIC simulations for the shock foot region in which the initial state satisfies the Buneman instability condition. In the first phase, electrostatic waves grow two-dimensionally by the Buneman instability to heat electrons. In the second phase, highly oblique ion two-stream instability grows to heat mainly ions. This result is in contrast to previous studies based on one-dimensional simulations, for which ion acoustic instability further heats electrons. The present result implies that overheating problem of electrons for shocks in supernova remnants is resolved by considering ion two-stream instability propagating highly obliquely to the shock normal and that multi-dimensional analysis is crucial to understand the particle heating and acceleration processes in shocks.",astro-ph,Astrophysics
High Angular Resolution Observations at 7-mm of the Core of the Quadrupolar HH 111/121 Outflow,"We present sensitive, high angular resolution ($0\rlap.{''}05$) VLA continuum observations made at 7 mm of the core of the HH 111/121 quadrupolar outflow. We estimate that at this wavelength the continuum emission is dominated by dust, although a significant free-free contribution ($\sim$30%) is still present. The observed structure is formed by two overlapping, elongated sources approximately perpendicular to each other as viewed from Earth. We interpret this structure as either tracing two circumstellar disks that exist around each of the protostars of the close binary source at the core of this quadrupolar outflow or a disk and a jet perpendicular to it. Both interpretations have advantages and disadvantages, and future high angular resolution spectroscopic millimeter observations are required to favor one of them in a more conclusive way.",astro-ph,Astrophysics
A Molecular Line Survey of the Highly Evolved Carbon Star CIT 6,"We present a spectral line survey of the C-rich envelope CIT 6 in the 2mm and 1.3mm bands carried out with the Arizona Radio Observatory (ARO) 12m telescope and the Heinrich Hertz Submillimeter Telescope (SMT). The observations cover the frequency ranges of 131--160 GHz, 219--244 GHz, and 252--268 GHz with typical sensitivity limit of T_R<10 mK. A total of 74 individual emission features are detected, of which 69 are identified to arise from 21 molecular species and isotopologues, with 5 faint lines remaining unidentified. Two new molecules (C4H and CH3CN) and seven new isotopologues (C17O, 29SiC2, 29SiO, 30SiO, 13CS, C33S, and CS) are detected in this object for the first time. The column densities, excitation temperatures, and fractional abundances of the detected molecules are determined using rotation diagram analysis. Comparison of the spectra of CIT 6 to that of IRC+10216 suggests that the spectral properties of CIT 6 are generally consistent with those of IRC+10216. For most of the molecular species, the intensity ratios of the lines detected in the two objects are in good agreement with each other. Nevertheless, there is evidence suggesting enhanced emission from CN and HC3N and depleted emission from HCN, SiS, and C4H in CIT 6. Based on their far-IR spectra, we find that CIT 6 probably has a lower dust-to-molecular gas ratio than IRC+10216. To investigate the chemical evolution of evolved stars, we compare the molecular abundances in the AGB envelopes CIT 6 and IRC+10216 and those in the bright proto-planetary nebula CRL 618. The implication on the circumstellar chemistry is discussed.",astro-ph,Astrophysics
Most Population III Supernovae are Duds,"One Population III dud supernova produces enough oxygen to enable ten million solar masses of primordial gas to bind into M dwarfs. This is possible because radiation from other Population III stars implodes the mixture of oxygen ejecta and primordial gas into a globular cluster. Model atmosphere calculations for oxygen dwarfs show that water blocks most of the infrared flux. The flux is redistributed into the visible to produce an unfamiliar, distinctive energy distribution. One million dud supernovae in a large protogalaxy are sufficient to produce the ""dark matter"" halo.",astro-ph,Astrophysics
The Dark-Matter Fraction in the Elliptical Galaxy Lensing the Quasar PG1115+080,"We determine the most likely dark-matter fraction in the elliptical galaxy quadruply lensing the quasar PG1115+080 based on analyses of the X-ray fluxes of the individual images in 2000 and 2008. Between the two epochs, the A2 image of PG1115+080 brightened relative to the other images by a factor of six in X-rays. We argue that the A2 image had been highly demagnified in 2000 by stellar microlensing in the intervening galaxy and has recently crossed a caustic, thereby creating a new pair of micro-images and brightening in the process. Over the same period, the A2 image has brightened by a factor of only 1.2 in the optical. The most likely ratio of smooth material (dark matter) to clumpy material (stars) in the lensing galaxy to explain the observations is ~90% of the matter in a smooth dark-matter component and ~10% in stars.",astro-ph,Astrophysics
Chromospheric magnetic field and density structure measurements using hard X-rays in a flaring coronal loop,"A novel method of using hard X-rays as a diagnostic for chromospheric density and magnetic structures is developed to infer sub-arcsecond vertical variation of magnetic flux tube size and neutral gas density.Using Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) X-ray data and the newly developed X-ray visibilities forward fitting technique we find the FWHM and centroid positions of hard X-ray sources with sub-arcsecond resolution ($\sim 0.2""$) for a solar limb flare. We show that the height variations of the chromospheric density and the magnetic flux densities can be found with unprecedented vertical resolution of $\sim$ 150 km by mapping 18-250 keV X-ray emission of energetic electrons propagating in the loop at chromospheric heights of 400-1500 km. Our observations suggest that the density of the neutral gas is in good agreement with hydrostatic models with a scale height of around $140\pm 30$ km. FWHM sizes of the X-ray sources decrease with energy suggesting the expansion (fanning out) of magnetic flux tube in the chromosphere with height. The magnetic scale height $B(z)(dB/dz)^{-1}$ is found to be of the order of 300 km and strong horizontal magnetic field is associated with noticeable flux tube expansion at a height of $\sim$ 900 km.",astro-ph,Astrophysics
Time Dependent Monochromatic Scattering of Radiation in One Dimensional Media: Analytical and Numerical Solutions,"In order to choose a numerical method for solving the time dependent equations of radiative transport, we obtain an exact solution for the time dependent radiation field in a one dimensional infinite medium with monochromatic, isotropic scattering for sources with an arbitrary spatial distribution and an arbitrary time variation of their power. The Lax-Wendroff method seems to be the most suitable. Because it is assumed that radiation delay is caused by the finite speed of light, the following difficulty arises when the numerical method is used: the region of variation of the variables (dimensionless coordinate and time t) is triangular (the inequality < t). This difficulty is overcome by expanding the unknown functions in series in terms of small values of the time and coordinate. By comparing the numerical and exact solutions for a point source with a given time dependence for its power and with pure scattering, the steps in the variables required to obtain a desired accuracy are estimated. This numerical method can be used to calculate the intensity and polarization of the radiation from sources in the early universe during epochs close to the recombination epoch.",astro-ph,Astrophysics
High-resolution X-ray spectroscopy of Theta Car,"Context : The peculiar hot star Theta Car in the open cluster IC2602 is a blue straggler as well as a single-line binary of short period (2.2d). Aims : Its high-energy properties are not well known, though X-rays can provide useful constraints on the energetic processes at work in binaries as well as in peculiar, single objects. Methods : We present the analysis of a 50ks exposure taken with the XMM-Newton observatory. It provides medium as well as high-resolution spectroscopy. Results : Our high-resolution spectroscopy analysis reveals a very soft spectrum with multiple temperature components (1--6MK) and an X-ray flux slightly below the `canonical' value (log[L_X(0.1-10.)/L_{BOL}] ~ -7). The X-ray lines appear surprisingly narrow and unshifted, reminiscent of those of beta Cru and tau Sco. Their relative intensities confirm the anomalous abundances detected in the optical domain (C strongly depleted, N strongly enriched, O slightly depleted). In addition, the X-ray data favor a slight depletion in neon and iron, but they are less conclusive for the magnesium abundance (solar-like?). While no significant changes occur during the XMM-Newton observation, variability in the X-ray domain is detected on the long-term range. The formation radius of the X-ray emission is loosely constrained to <5 R_sol, which allows for a range of models (wind-shock, corona, magnetic confinement,...) though not all of them can be reconciled with the softness of the spectrum and the narrowness of the lines.",astro-ph,Astrophysics
Gamma-Ray Studies of Blazars: Synchro-Compton Analysis of Flat Spectrum Radio Quasars,"We extend a method for modeling synchrotron and synchrotron self-Compton radiations in blazar jets to include external Compton processes. The basic model assumption is that the blazar radio through soft X-ray flux is nonthermal synchrotron radiation emitted by isotropically-distributed electrons in the randomly directed magnetic field of outflowing relativistic blazar jet plasma. Thus the electron distribution is given by the synchrotron spectrum, depending only on the Doppler factor $_{\rm D}$ and mean magnetic field $B$, given that the comoving emission region size scale $R_b^\prime \lesssim c \dD t_v/(1+z)$, where $t_v$ is variability time and $z$ is source redshift. Generalizing the approach of Georganopoulos, Kirk, and Mastichiadis (2001) to arbitrary anisotropic target radiation fields, we use the electron spectrum implied by the synchrotron component to derive accurate Compton-scattered $$-ray spectra throughout the Thomson and Klein-Nishina regimes for external Compton scattering processes. We derive and calculate accurate $$-ray spectra produced by relativistic electrons that Compton-scatter (i) a point source of radiation located radially behind the jet, (ii) photons from a thermal Shakura-Sunyaev accretion disk and (iii) target photons from the central source scattered by a spherically-symmetric shell of broad line region (BLR) gas. Calculations of broadband spectral energy distributions from the radio through $$-ray regimes are presented, which include self-consistent $$ absorption on the same radiation fields that provide target photons for Compton scattering. Application of this baseline flat spectrum radio/$$-ray quasar model is considered in view of data from $$-ray telescopes and contemporaneous multi-wavelength campaigns.",astro-ph,Astrophysics
Lighthouses with two lights: burst oscillations from the accretion-powered millisecond pulsars,"The key contribution of the discovery of nuclear-powered pulsations from the accretion-powered millisecond pulsars (AMPs) has been the establishment of burst oscillation frequency as a reliable proxy for stellar spin rate. This has doubled the sample of rapidly-rotating accreting neutron stars and revealed the unexpected absence of any stars rotating near the break-up limit. The resulting `braking problem' is now a major concern for theorists, particularly given the possible role of gravitational wave emission in limiting spin. This, however, is not the only area where burst oscillations from the AMPs are having an impact. Burst oscillation timing is developing into a promising technique for verifying the level of spin variability in the AMPs (a topic of considerable debate). These sources also provide unique input to our efforts to understand the still-elusive burst oscillation mechanism. This is because they are the only stars where we can reliably gauge the role of uneven fuel deposition and, of course, the magnetic field.",astro-ph,Astrophysics
The Variable Stars of the Draco Dwarf Spheroidal Galaxy - Revisited,"We present a CCD survey of variable stars in the Draco dwarf spheroidal galaxy. This survey, which has the largest areal coverage since the original variable star survey by Baade & Swope, includes photometry for 270 RR Lyrae stars, 9 anomalous Cepheids, 2 eclipsing binaries, and 12 slow, irregular red variables, as well as 30 background QSOs. Twenty-six probable double-mode RR Lyrae stars were identified. Observed parameters, including mean V and I magnitudes, V amplitudes, and periods, have been derived. Photometric metallicities of the ab-type RR Lyrae stars were calculated according to the method of Jurcsik & Kovacs, yielding a mean metallicity of <[Fe/H]> = -2.19 +/- 0.03. The well known Oosterhoff intermediate nature of the RR Lyrae stars in Draco is reconfirmed, although the double-mode RR Lyrae stars with one exception have properties similar to those found in Oosterhoff type II globular clusters. The period-luminosity relation of the anomalous Cepheids is rediscussed with the addition of the new Draco anomalous Cepheids.",astro-ph,Astrophysics
The Evolution of L and T Dwarfs in Color-Magnitude Diagrams,"We present new evolution sequences for very low mass stars, brown dwarfs and giant planets and use them to explore a variety of influences on the evolution of these objects. We compare our results with previous work and discuss the causes of the differences and argue for the importance of the surface boundary condition provided by atmosphere models including clouds.
  The L- to T-type ultracool dwarf transition can be accommodated within the Ackerman & Marley (2001) cloud model by varying the cloud sedimentation parameter. We develop a simple model for the evolution across the L/T transition. By combining the evolution calculation and our atmosphere models, we generate colors and magnitudes of synthetic populations of ultracool dwarfs in the field and in galactic clusters. We focus on near infrared color- magnitude diagrams (CMDs) and on the nature of the ``second parameter'' that is responsible for the scatter of colors along the Teff sequence. Variations in metallicity and cloud parameters, unresolved binaries and possibly a relatively young population all play a role in defining the spread of brown dwarfs along the cooling sequence. We find that the transition from cloudy L dwarfs to cloudless T dwarfs slows down the evolution and causes a pile up of substellar objects in the transition region, in contradiction with previous studies. We apply the same model to the Pleiades brown dwarf sequence. Taken at face value, the Pleiades data suggest that the L/T transition occurs at lower Teff for lower gravity objects. The simulated populations of brown dwarfs also reveal that the phase of deuterium burning produces a distinctive feature in CMDs that should be detectable in ~50-100 Myr old clusters.",astro-ph,Astrophysics
Fitting the Gamma-Ray Spectrum from Dark Matter with DMFIT: GLAST and the Galactic Center Region,"We study the potential of GLAST to unveil particle dark matter properties with gamma-ray observations of the Galactic center region. We present full GLAST simulations including all gamma-ray sources known to date in a region of 4 degrees around the Galactic center, in addition to the diffuse gamma-ray background and to the dark matter signal. We introduce DMFIT, a tool that allows one to fit gamma-ray emission from pair-annihilation of generic particle dark matter models and to extract information on the mass, normalization and annihilation branching ratios into Standard Model final states. We assess the impact and systematic effects of background modeling and theoretical priors on the reconstruction of dark matter particle properties. Our detailed simulations demonstrate that for some well motivated supersymmetric dark matter setups with one year of GLAST data it will be possible not only to significantly detect a dark matter signal over background, but also to estimate the dark matter mass and its dominant pair-annihilation mode.",astro-ph,Astrophysics
Ubiquitous Water Masers in Nearby Star-Forming Galaxies,"We report the detection of water maser emission from four nearby galaxies hosting ultradense HII (UDHII) regions, He 2-10, the Antennae galaxies (NGC 4038/4039), NGC 4214, and NGC 5253, with the Green Bank Telescope. Our detection rate is 100%, and all of these H2O ""kilomasers"" (L(H2O) < 10 L_sun) are located toward regions of known star formation as traced by UDHII regions and bright 24 micron emission. Some of the newly discovered H2O masers have luminosities 1-2 orders of magnitude less than previous extragalactic studies and the same order of magnitude as those typical of Galactic massive star-forming regions. The unusual success of this minisurvey suggests that H2O maser emission may be very common in starburst galaxies, and the paucity of detections to date is due to a lack of sufficient sensitivity. While the galaxy sample was selected by the presence of UDHII regions, and the UDHII regions lie within the telescope beam, in the absence of H2O spectral line maps the connection between H2O masers and UDHII regions has not yet been demonstrated.",astro-ph,Astrophysics
A discontinuity in the low-mass IMF - the case of high multiplicity,"The empirical binary properties of brown dwarfs (BDs) differ from those of normal stars suggesting BDs form a separate population. Recent work by Thies & Kroupa revealed a discontinuity of the initial mass function (IMF) in the very-low-mass star regime under the assumption of a low multiplicity of BDs of about 15 per cent. However, previous observations had suggested that the multiplicity of BDs may be significantly higher, up to 45 per cent. This contribution investigates the implication of a high BD multiplicity on the appearance of the IMF for the Orion Nebula Cluster, Taurus-Auriga, IC 348 and the Pleiades. We show that the discontinuity remains pronounced even if the observed MF appears to be continuous, even for a BD binary fraction as high as 60%. We find no evidence for a variation of the BD IMF with star-forming conditions. The BD IMF has a power-law index alpha = +0.3 and about two BDs form per 10 low-mass stars assuming equal-mass pairing of BDs.",astro-ph,Astrophysics
DLA kinematics and outflows from starburst galaxies,"We present results from a numerical study of the multiphase interstellar medium in sub-Lyman-break galaxy protogalactic clumps. Such clumps are abundant at z=3 and are thought to be a major contributor to damped Ly-alpha absorption. We model the formation of winds from these clumps and show that during star formation episodes they feature outflows with neutral gas velocity widths up to several hundred km/s. Such outflows are consistent with the observed high-velocity dispersion in DLAs. In our models thermal energy feedback from winds and supernovae results in efficient outflows only when cold (~ 300 K), dense (> 100 msun/pc^3) clouds are resolved at grid resolution of 12 pc. At lower 24 pc resolution the first signs of the multiphase medium are spotted; however, at this low resolution thermal injection of feedback energy cannot yet create hot expanding bubbles around star-forming regions -- instead feedback tends to erase high-density peaks and suppress star formation. At 12 pc resolution feedback compresses cold clouds, often without disrupting the ongoing star formation; at the same time a larger fraction of feedback energy is channeled into low-density bubbles and winds. These winds often entrain compact neutral clumps which produce multi-component metal absorption lines.",astro-ph,Astrophysics
Statistical Tools for Analyzing the Cosmic Ray Energy Spectrum,In this paper un-binned statistical tools for analyzing the cosmic ray energy spectrum are developed and illustrated with a simulated data set. The methods are designed to extract accurate and precise model parameter estimators in the presence of statistical and systematic energy errors. Two robust methods are used to test for the presence of flux suppression at the highest energies: the Tail-Power statistic and a likelihood ratio test. Both tests give evidence of flux suppression in the simulated data. The tools presented can be generalized for use on any astrophysical data set where the power-law assumption is relevant and can be used to aid observational design.,astro-ph,Astrophysics
Modelling the evolution and nucleosynthesis of carbon-enhanced metal-poor stars,"We present the results of binary population simulations of carbon-enhanced metal-poor (CEMP) stars. We show that nitrogen and fluorine are useful tracers of the origin of CEMP stars, and conclude that the observed paucity of very nitrogen-rich stars puts strong constraints on possible modifications of the initial mass function at low metallicity. The large number fraction of CEMP stars may instead require much more efficient dredge-up from low-metallicity asymptotic giant branch stars.",astro-ph,Astrophysics
Long term radio variability of AGN,"A large number of AGN have been monitored for nearly 30 years at 22, 37 and 87 GHz in Metshovi Radio Observatory. These data were combined with lower frequency 4.8, 8.0 and 14.5 GHz data from the University of Michigan Radio Astronomy Observatory, higher frequency data at 90 and 230 GHz from SEST, and supplementary higher frequency data from the literature to study the long-term variability of a large sample of AGN. Both the characteristics of individual flares from visual inspection and statistically-determined variability timescales as a function of frequency and optical class type were determined. Based on past behaviour, predictions of sources expected to exhibit large flares in 2008--2009 appropriate for study by GLAST and other instruments are made. The need for long-term data for properly understanding source behaviour is emphasised.",astro-ph,Astrophysics
J1420--0545: The radio galaxy larger than 3C236,"We report the discovery of the largest giant radio galaxy, J1420-0545: a FR type II radio source with an angular size of 17.4' identified with an optical galaxy at z=0.3067. Thus, the projected linear size of the radio structure is 4.69 Mpc (if we assume that H_{0}=71 km\s\Mpc, Omega_{m}=0.27, and Omega_=0.73). This makes it larger than 3C236, which is the largest double radio source known to date. New radio observations with the 100 m Effelsberg telescope and the Giant Metrewave Radio Telescope, as well as optical identification with a host galaxy and its optical spectroscopy with the William Herschel Telescope are reported. The spectrum of J1420-0545 is typical of elliptical galaxies in which continuum emission with the characteristic 4000A discontinuity and the H and K absorption lines are dominated by evolved stars. The dynamical age of the source, its jets' power, the energy density, and the equipartition magnetic field are calculated and compared with the corresponding parameters of other giant and normal-sized radio galaxies from a comparison sample. The source is characterized by the exceptionally low density of the surrounding IGM and an unexpectedly high expansion speed of the source along the jet axis. All of these may suggest a large inhomogeneity of the IGM.",astro-ph,Astrophysics
Difference in Narrow Emission Line Spectra of Seyfert 1 and 2 galaxies,"In the unification scheme of Seyfert galaxies, a dusty torus blocks the continuum source and broad line region in Seyfert 2 galaxies. However it is not clear whether or not and to what extent the torus affects the narrow line spectra. In this paper, we show that Seyfert 1 and Seyfert 2 galaxies have different distributions on the [OIII]/H$$ vs [NII]/H$$ diagram (BPT diagram) for narrow lines. Seyfert 2 galaxies display a clear left boundary on the BPT diagram and only 7.3% of them lie on the left. By contrast, Seyfert 1 galaxies do not show such a cutoff and 33.0% of them stand on the left side of the boundary. Among Seyfert 1 galaxies, the distribution varies with the extinction to broad lines. As the extinction increases, the distribution on BPT diagram moves to larger [NII]/H$$ value. We interpret this as an evidence for the obscuration of inner dense narrow line region by the dusty torus. We also demonstrate that the [OIII] and broad line luminosity correlation depends on the extinction of broad lines in the way that high extinction objects have lower uncorrected [OIII] luminosities, suggesting that [OIII] is partially obscured in these objects. Therefore, using [OIII] as an indicator for the nuclear luminosity will systematically under-estimate the nuclear luminosity of Seyfert 2 galaxies.",astro-ph,Astrophysics
A seismic approach to testing different formation channels of subdwarf B stars,"There are many unknowns in the formation of subdwarf B stars. Different formation channels are considered to be possible and to lead to a variety of helium-burning subdwarfs. All seismic models to date, however, assume that a subdwarf B star is a post-helium-flash-core surrounded by a thin inert layer of hydrogen. We examine an alternative formation channel, in which the subdwarf B star originates from a massive (>~2 Msun) red giant with a non-degenerate helium-core. Although these subdwarfs may evolve through the same region of the log g-Teff diagram as the canonical post-flash subdwarfs, their interior structure is rather different. We examine how this difference affects their pulsation modes and whether it can be observed.
  Using detailed stellar evolution calculations we construct subdwarf B models from both formation channels. The iron accumulation in the driving region due to diffusion, which causes the excitation of the modes, is approximated by a Gaussian function. The pulsation modes and frequencies are calculated with a non-adiabatic pulsation code. A detailed comparison of two subdwarf B models from different channels, but with the same log g and Teff, shows that their mode excitation is different. The excited frequencies are lower for the post-flash than for the post-non-degenerate subdwarf B star. This is mainly due to the differing chemical composition of the stellar envelope. A more general comparison between two grids of models shows that the excited frequencies of most post-non-degenerate subdwarfs cannot be well-matched with the frequencies of post-flash subdwarfs. In the rare event that an acceptable seismic match is found, additional information, such as mode identification and log g and Teff determinations, allows us to distinguish between the two formation channels.",astro-ph,Astrophysics
Monitoring Supergiant Fast X-ray Transients with Swift. III. Outbursts of the prototypical SFXTs IGR J17544-2619 and XTE J1739-302,"IGR J17544-2619 and XTE J1739-302 are considered the prototypical sources of the new class of High Mass X-ray Binaries, the Supergiant Fast X-ray Transients (SFXTs).These sources were observed during bright outbursts on 2008 March 31 and 2008 April 8, respectively, thanks to an on-going monitoring campaign we are performing with Swift, started in October 2007. Simultaneous observations with XRT and BAT allowed us to perform for the first time a broad band spectroscopy of their outbursts. The X-ray emission is well reproduced with absorbed cutoff powerlaws, similar to the typical spectral shape from accreting pulsars. IGR J17544-2619 shows a significantly harder spectrum during the bright flare (where a luminosity in excess of 1E36 erg/s is reached) than during the long-term low level flaring activity (1E33-1E34 erg/s), while XTE J1739-302 displayed the same spectral shape, within the uncertainties, and a higher column density during the flare than in the low level activity. The light curves of these two SFXTs during the bright flare look similar to those observed during recent flares from other two SFXTs, IGRJ11215-5952 and IGRJ16479-4514, reinforcing the connection among the members of this class of X-ray sources.",astro-ph,Astrophysics
The Photometric Variability of HH 30,"HH 30 is an edge-on disk around a young stellar object. Previous imaging with the Hubble Space Telescope has show morphological variability that is possibly related to the rotation of the star or the disk. We report the results of two terrestrial observing campaigns to monitor the integrated magnitude of HH 30. We use the Lomb-Scargle periodogram to look for periodic modulation with periods between 2 days and almost 90 days in these two data sets and in a third, previously published, data set. We develop a method to deal with short-term correlations in the data. Our results indicate that none of the data sets shows evidence for significant periodic photometric modulation.",astro-ph,Astrophysics
The star-formation history of K-selected galaxies,"We have studied the uJy radio properties of K-selected galaxies detected in the Ultra-Deep Survey portion of UKIDSS using 610- and 1,400-MHz images from the VLA and GMRT. These deep radio mosaics, combined with the largest and deepest K image currently available, allow high-S/N detections of many K-selected sub-populations, including sBzK and pBzK star-forming and passive galaxies. We find a strong correlation between the radio and K fluxes and a linear relationship between SFR and K luminosity. We find no evidence, from either radio spectral indices or a comparison with submm-derived SFRs, that the full sample is strongly contaminated by AGN. The sBzK and pBzK galaxies have similar levels of radio flux, SFR and specific SFR (SSFR) at z < 1.4, suggesting there is strong contamination of the pBzK sample by star-forming galaxies. At z > 1.4, pBzK galaxies become difficult to detect in the radio stack, though the implied SFRs are still much higher than expected for passively evolving galaxies. Their radio emission may come from low-luminosity AGN. EROs straddle the passive and star-forming regions of the BzK diagram and also straddle the two groups in terms of their radio properties. K-bright ERO samples are dominated by passive galaxies and faint ERO samples contain more star-forming galaxies. The star-formation history (SFH) from stacking all K sources in the UDS agrees well with that derived for other wavebands and other radio surveys, at least out to z ~ 2. The radio-derived SFH then appears to fall more steeply than that measured at other wavelengths. The SSFR for K-selected sources rises strongly with redshift at all stellar masses, and shows a weak dependence on stellar mass. High- and low-mass galaxies show a similar decline in SSFR since z ~ 2 (abridged).",astro-ph,Astrophysics
Direct calculation of the radiative efficiency of an accretion disk around a black hole,"Numerical simulation of magnetohydrodynamic (MHD) turbulence makes it possible to study accretion dynamics in detail. However, special effort is required to connect inflow dynamics (dependent largely on angular momentum transport) to radiation (dependent largely on thermodynamics and photon diffusion). To this end we extend the flux-conservative, general relativistic MHD code HARM from axisymmetry to full 3D. The use of an energy conserving algorithm allows the energy dissipated in the course of relativistic accretion to be captured as heat. The inclusion of a simple optically thin cooling function permits explicit control of the simulated disk's geometric thickness as well as a direct calculation of both the amplitude and location of the radiative cooling associated with the accretion stresses. Fully relativistic ray-tracing is used to compute the luminosity received by distant observers. For a disk with aspect ratio H/r ~ 0.1 accreting onto a black hole with spin parameter a/M = 0.9, we find that there is significant dissipation beyond that predicted by the classical Novikov-Thorne model. However, much of it occurs deep in the potential, where photon capture and gravitational redshifting can strongly limit the net photon energy escaping to infinity. In addition, with these parameters and this radiation model, significant thermal and magnetic energy remains with the gas and is accreted by the black hole. In our model, the net luminosi ty reaching infinity is 6% greater than the Novikov-Thorne prediction. If the accreted thermal energy were wholly radiated, the total luminosity of the accretion flow would be ~20% greater than the Novikov-Thorne value.",astro-ph,Astrophysics
Strong mass segregation around a massive black hole,"We show that the mass-segregation solution for the steady state distribution of stars around a massive black hole (MBH) has two branches: the known weak segregation solution (Bahcall & Wolf 1977), and a newly discovered strong segregation solution, presented here. The nature of the solution depends on the heavy-to-light stellar mass ratio M_H/M_L and on the unbound population number ratio N_H/N_L, through the relaxational coupling parameter =4 N_H M_H^2 /[N_L M_L^2(3+M_H/M_L)]. When the heavy stars are relatively common (>>1), they scatter frequently on each other. This efficient self-coupling leads to weak mass segregation, where the stars form n \propto r^{-_M} mass-dependent cusps near the MBH, with indices _H=7/4 for the heavy stars and 3/2<_L<7/4 for the light stars (i.e. \max(_H-_L)~=1/4). However, when the heavy stars are relatively rare (<<1), they scatter mostly on light stars, sink to the center by dynamical friction and settle into a much steeper cusp with 2~<_H<11/4, while the light stars form a 3/2<_L<7/4 cusp, resulting in strong segregation (i.e. \max(_H-_L)~=1). We show that the present-day mass function of evolved stellar populations (coeval or continuously star forming) with a universal initial mass function, separate into two distinct mass scales, ~1 Mo of main sequence and compact dwarfs, and ~10 Mo of stellar black holes (SBHs), and have <0.1. We conclude that it is likely that many relaxed galactic nuclei are strongly segregated. We review indications of strong segregation in observations of the Galactic Center and in results of numeric simulations, and briefly list some possible implications of a very high central concentration of SBHs around a MBH.",astro-ph,Astrophysics
The W40 Cloud Complex,"The W40 complex is a nearby site of recent massive star formation composed of a dense molecular cloud adjacent to an HII region that contains an embedded OB star cluster. The HII region is beginning to blister out and break free from its envelope of molecular gas, but our line of sight to the central stars is largely obscured by intervening dust. Several bright OB stars in W40 - visible at optical, infrared, or cm wavelengths - are providing the ionizing flux that heats the HII region. The known stellar component of W40 is dominated by a small number of partly or fully embedded OB stars which have been studied at various wavelengths, but the lower mass stellar population remains largely unexamined. Despite its modest optical appearance, at 600pc W40 is one of the nearest massive star forming regions, and with a UV flux of about 1/10th of the Orion Nebula Cluster, this neglected region deserves detailed investigation.",astro-ph,Astrophysics
Diffusion of cosmic-rays and the Gamma-ray Large Area Telescope: Phenomenology at the 1-100 GeV regime,"This paper analyzes astrophysical scenarios that may be detected at the upper end of the energy range of the Gamma Ray Large Area Space Telescope (GLAST), as a result of cosmic-ray (CR) diffusion in the interstellar medium (ISM). Hadronic processes are considered as the source of $$-ray photons from localized molecular enhancements nearby accelerators. Two particular cases are presented: a) the possibility of detecting spectral energy distributions (SEDs) with maxima above 1 GeV, which may be constrained by detection or non-detection at very-high energies (VHE) with observations by ground-based Cerenkov telescopes, and b) the possibility of detecting V-shaped, inverted spectra, due to confusion of a nearby (to the line of sight) arrangement of accelerator/target scenarios with different characteristic properties. We show that the finding of these signatures (in particular, a peak at the 1--100 GeV energy region) is indicative for an identification of the underlying mechanism producing the $$-rays that is realized by nature: which accelerator (age and relative position to the target cloud) and under which diffusion properties CR propagate.",astro-ph,Astrophysics
A general method of estimating stellar astrophysical parameters from photometry,"Applying photometric catalogs to the study of the population of the Galaxy is obscured by the impossibility to map directly photometric colors into astrophysical parameters. Most of all-sky catalogs like ASCC or 2MASS are based upon broad-band photometric systems, and the use of broad photometric bands complicates the determination of the astrophysical parameters for individual stars. This paper presents an algorithm for determining stellar astrophysical parameters (effective temperature, gravity and metallicity) from broad-band photometry even in the presence of interstellar reddening. This method suits the combination of narrow bands as well. We applied the method of interval-cluster analysis to finding stellar astrophysical parameters based on the newest Kurucz models calibrated with the use of a compiled catalog of stellar parameters. Our new method of determining astrophysical parameters allows all possible solutions to be located in the effective temperature-gravity-metallicity space for the star and selection of the most probable solution.",astro-ph,Astrophysics
Using radioactivities to improve the search for nearby radio-quiet neutron stars,"Neutron stars (NS) and black holes (BH) are sources of gravitational waves (GW) and the investigation of young isolated radio-quiet NS can in principle lead to constraints of the equation of state (EoS). The GW signal of merging NSs critically depends on the EoS. However, unlike radio pulsars young isolated radio-quiet neutron stars are hard to detect and only seven of them are known so far. Furthermore, for GW projects it is necessary to confine regions in the sky where and of which quantity sources of GW can be expected. We suggest strategies for the search for young isolated radio-quiet NSs. One of the strategies is to look for radioactivities which are formed during a supernova (SN) event and are detectable due to their decay. Radioactivities with half lives of ~1 Myr can indicate such an event while other remnants like nebulae only remain observable for a few kyrs. Here we give a brief overview of our strategies and discuss advantages and disadvantages",astro-ph,Astrophysics
The Stellar Populations of M33's Outer Regions IV: Inflow History and Chemical Evolution,"We have modelled the observed color-magnitude diagram (CMD) at one location in M33's outskirts under the framework of a simple chemical evolution scenario which adopts instantaneous and delayed recycling for the nucleosynthetic products of Type II and Ia supernovae. In this scenario, interstellar gas forms stars at a rate modulated by the Kennicutt-Schmidt relation and gas outflow occurs at a rate proportional to the star formation rate (SFR). With this approach, we put broad constraints on the role of gas flows during this region's evolution and compare its [alpha/Fe] vs. [Fe/H] relation with that of other Local Group systems. We find that models with gas inflow are significantly better than the closed box model at reproducing the observed distribution of stars in the CMD. The best models have a majority of gas inflow taking place in the last 7 Gyr, and relatively little in the last 3 Gyr. These models predict most stars in this region to have [alpha/Fe] ratios lower than the bulk of the Milky Way's halo. The predictions for the present-day SFR, gas mass, and oxygen abundance compare favorably to independent empirical estimates. Our results paint a picture in which M33's outer disc formed from the protracted inflow of gas over several Gyr with at least half of the total inflow occurring since z ~ 1.",astro-ph,Astrophysics
LSD and AMAZE: the mass-metallicity relation at z>3,"We present the first results on galaxy metallicity evolution at z>3 from two projects, LSD (Lyman-break galaxies Stellar populations and Dynamics) and AMAZE (Assessing the Mass Abundance redshift Evolution). These projects use deep near-infrared spectroscopic observations of a sample of ~40 LBGs to estimate the gas-phase metallicity from the emission lines. We derive the mass-metallicity relation at z$>$3 and compare it with the same relation at lower redshift. Strong evolution from z=0 and z=2 to z=3 is observed, and this finding puts strong constrains on the models of galaxy evolution. These preliminary results show that the effective oxygen yields does not increase with stellar mass, implying that the simple outflow model does not apply at z>3.",astro-ph,Astrophysics
Magnetic Reconnection by a Self-Retreating X-Line,"Particle-in-cell (PIC) simulations of collisionless magnetic reconnection are performed to study asymmetric reconnection in which an outflow is blocked by a hard wall while leaving sufficiently large room for the outflow of the opposite direction. This condition leads to a slow, roughly constant motion of the diffusion region away from the wall, the so-called `X-line retreat'. The typical retreat speed is ~0.1 times the Alfven speed. At the diffusion region, ion flow pattern shows strong asymmetry and the ion stagnation point and the X-line are not collocated. A surprise, however, is that the reconnection rate remains the same unaffected by the retreat motion.",astro-ph,Astrophysics
A Possible Icy Kuiper Belt around HD 181327,"We have obtained a Gemini South T-ReCS Qa-band (18.3 micron) image and a Spitzer MIPS SED-mode observation of HD181327, an F5/F6V member of the ~12 Myr old beta Pictoris moving group. We resolve the disk in thermal-emission for the first time and find that the northern arm of the disk is 1.4x brighter than the southern arm. In addition, we detect a broad peak in the combined Spitzer IRS and MIPS spectra at 60 - 75 micron that may be produced by emission from crystalline water ice. We model the IRS and MIPS data using a size distribution of amorphous olivine and water ice grains (dn/da proportional to a^{-2.25} with a_{min} consistent with the minimum blow out size and a_{max} = 20 micron) located at a distance of 86.3 AU from the central star, as observed in previously published scattered-light images. Since the photo-desorption lifetime for the icy particles is ~1400 yr, significantly less than the estimated ~12 Myr age of the system, we hypothesize that we have detected debris that may be steadily replenished by collisions among icy Kuiper belt object-like parent bodies in a newly forming planetary system.",astro-ph,Astrophysics
Sound Waves Excitation by Jet-Inflated Bubbles in Clusters of Galaxies,"We show that repeated sound waves in the intracluster medium (ICM) can be excited by a single inflation episode of an opposite bubble pair. To reproduce this behavior in numerical simulations the bubbles should be inflated by jets, rather than being injected artificially. The multiple sound waves are excited by the motion of the bubble-ICM boundary that is caused by vortices inside the inflated bubbles and the backflow (`cocoon') of the ICM around the bubble. These sound waves form a structure that can account for the ripples observed in the Perseus cooling flow cluster. We inflate the bubbles using slow massive jets, with either a wide opening angle or that are precessing. The jets are slow in the sense that they are highly sub-relativistic, $v_j \sim 0.01c-0.1c$, and they are massive in the sense that the pair of bubbles carry back to the ICM a large fraction of the cooling mass, i.e., $\sim 1-50 M_\odot \yr^{-1}$. We use a two-dimensional axisymmetric (referred to as 2.5D) hydrodynamical numerical code (VH-1).",astro-ph,Astrophysics
The impact of dust on the scaling properties of galaxy clusters,"We investigate the effect of dust on the scaling properties of galaxy clusters based on hydrodynamic N-body simulations of structure formation. We have simulated five dust models plus a radiative cooling and adiabatic models using the same initial conditions for all runs. The numerical implementation of dust was based on the analytical computations of Montier and Giard (2004). We set up dust simulations to cover different combinations of dust parameters that put in evidence the effects of size and abundance of dust grains. Comparing our radiative plus dust cooling runs to a purely radiative cooling simulation we find that dust has an impact on cluster scaling relations. It mainly affects the normalisation of the scalings (and their evolution), whereas it introduces no significant differences on their slopes. The strength of the effect depends critically on the dust abundance and grain size parameters as well as on the cluster scaling. Indeed, cooling due to dust is effective at the cluster regime and has a stronger effect on the ""baryon driven"" statistical properties of clusters such as $L_{\rm X}-M$, $Y- M$, $S-M$ scaling relations. Major differences, relative to the radiative cooling model, are as high as 25% for the $L_{\rm X}-M$ normalisation, and about 10% for the $Y-M$ and $S-M$ normalisations at redshift zero. On the other hand, we find that dust has almost no impact on the ""dark matter driven"" $T_{\rm mw}-M$ scaling relation. The effects are found to be dependent in equal parts on both dust abundances and grain sizes distributions for the scalings investigated in this paper. Higher dust abundances and smaller grain sizes cause larger departures from the radiative cooling (i.e. with no dust) model.",astro-ph,Astrophysics
"The Nature of a Cosmic-Ray Accelerator, CTB37 B, Observed with Suzaku and Chandra","We report on Suzaku and Chandra observations of the young supernova remnant CTB37B, from which TeV gamma-rays were detected by the H.E.S.S. Cherenkov telescope. The 80 ks Suzaku observation provided us with a clear image of diffuse emission and high-quality spectra. The spectra revealed that the diffuse emission is comprised of thermal and non-thermal components. The thermal component can be represented by an NEI model with a temperature, a pre-shock electron density and an age of 0.9(0.7-1.1) keV, 0.4(0.3-0.5) cm^{-3} and 650(350-3150) yr, respectively. This suggests that the explosion of CTB37B occurred in a low-density space. A non-thermal power-law component was found from the southern region of CTB37B. Its photon index of ~1.5 and a high roll-off energy (>15 keV) indicate efficient cosmic-ray acceleration. A comparison of this X-ray spectrum with the TeV gamma-ray spectrum leads us to conclude that the TeV gamma-ray emission seems to be powered by either multi-zone Inverse Compton scattering or the decay of neutral pions. The point source resolved by Chandra near the shell is probably associated with CTB37B, because of the common hydrogen column density with the diffuse thermal emission. Spectral and temporal characteristics suggest that this source is a new anomalous X-ray pulsar.",astro-ph,Astrophysics
Recent Structural Evolution of Early-Type Galaxies: Size Growth from z=1 to z=0,"Strong size and internal density evolution of early-type galaxies between z~2 and the present has been reported by several authors. Here we analyze samples of nearby and distant (z~1) galaxies with dynamically measured masses in order to confirm the previous, model-dependent results and constrain the uncertainties that may play a role. Velocity dispersion measurements are taken from the literature for 50 morphologically selected 0.8<z<1.2 field and cluster early-type galaxies with typical masses 2e11 Msol. Sizes are determined with ACS imaging. We compare the distant sample with a large sample of nearby (0.04<z<0.08) early-type galaxies extracted from the SDSS for which we determine sizes, masses, and densities in a consistent manner, using simulations to quantify systematic differences between the size measurements of nearby and distant galaxies. We find a highly significant structural difference between the nearby and distant samples, regardless of sample selection effects. The implied evolution in size at fixed mass between z=1 and the present is a factor of 1.97(0.15). This is in qualitative agreement with semianalytic models; however, the observed evolution is much faster than the predicted evolution. Our results reinforce and are quantitatively consistent with previous, photometric studies that found size evolution of up to a factor of 5 since z~2. A combination of structural evolution of individual galaxies through the accretion of companions and the continuous formation of early-type galaxies through increasingly gas-poor mergers is one plausible explanation of the observations.",astro-ph,Astrophysics
Towards a realistic axion star,In this work we estimate the radius and the mass of a self-gravitating system made of axions. The quantum axion field satisfies the Klein-Gordon equation in a curved space-time and the metric components of this space-time are solutions to the Einstein equations with a source term given by the vacuum expectation value of the energy-momentum operator constructed from the axion field. As a first step towards an axion star we consider the up to the sixth term in the axion potential expansion. We found that axion stars would have masses of the order of asteroids and radius of the order of few centimeters.,astro-ph,Astrophysics
The relationship between gas content and star formation rate in spiral galaxies. Comparing the local field with the Virgo cluster,"Despite many studies of the star formation in spiral galaxies, a complete and coherent understanding of the physical processes that regulate the birth of stars has not yet been achieved, nor a unanimous consent was reached, despite the many attempts, on the effects of the environment on the star formation in galaxies member of rich clusters. We focus on the local and global Schmidt law and we investigate how cluster galaxies have their star formation activity perturbed. We collect multifrequency imaging for a sample of spiral galaxies, member of the Virgo cluster and of the local field; we compute the surface density profiles for the young and for the bulk of the stellar components, for the molecular and for the atomic gas. Our analysis shows that the bulk of the star formation correlates with the molecular gas, but the atomic gas is important or even crucial in supporting the star formation activity in the outer part of the disks. Moreover, we show that cluster members which suffer from a moderate HI removal have their molecular component and their SFR quenched, while highly perturbed galaxies show an additional truncation in their star forming disks. Our results are consistent with a model in which the atomic hydrogen is the fundamental fuel for the star formation, either directly or indirectly through the molecular phase; therefore galaxies whose HI reservoirs have been depleted suffer from starvation or even from truncation of their star formation activity.",astro-ph,Astrophysics
VW LMi: tightest quadruple system known. Light-time effect and possible secular changes of orbits,"Tightest known quadruple systems VW LMi consists of contact eclipsing binary with P_12 = 0.477551 days and detached binary with P_34 = 7.93063 days revolving in rather tight, 355.0-days orbit. This paper presents new photometric and spectroscopic observations yielding 69 times of minima and 36 disentangled radial velocities for the component stars. All available radial velocities and minima times are combined to better characterize the orbits and to derive absolute parameters of components. The total mass of the quadruple system was estimated at 4.56 M_sun. The detached, non-eclipsing binary with orbital period P = 7.93 days is found to show apsidal motion with U approximately 80 years. Precession period in this binary, caused by the gravitational perturbation of the contact binary, is estimated to be about 120 years. The wide mutual orbit and orbit of the non-eclipsing pair are found to be close to coplanarity, preventing any changes of the inclination angle of the non-eclipsing orbit and excluding occurrence of the second system of eclipses in future. Possibilities of astrometric solution and direct resolving of the wide, mutual orbit are discussed. Nearby star, HD95606, was found to form loose binary with quadruple system VW LMi.",astro-ph,Astrophysics
GHASP : An Halpha kinematic survey of 203 spiral and irregular galaxies - VII. Revisiting the analysis of Halpha data cubes for 97 galaxies,"The GHASP survey (Gassendi HAlpha survey of SPirals) consists of 3D Ha data cubes for 203 spiral and irregular galaxies, covering a large range in morphological types and absolute magnitudes, for kinematics analysis. It is the largest sample of Fabry-Perot data published up to now. In order to provide an homogenous sample, reduced and analyzed using the same procedure, we present in this paper the new reduction and analysis for a set of 97 galaxies already published in previous papers but now using the new data reduction procedure adopted for the whole sample. The GHASP survey is now achieved and the whole sample is reduced using adaptive binning techniques based on Voronoi tessellations. We have derived Ha data cubes from which are computed Ha maps, radial velocity fields as well as residual velocity fields, position-velocity diagrams, rotation curves and kinematical parameters for almost all galaxies. The rotation curves, the kinematical parameters and their uncertainties are computed homogeneously using the new method based on the power spectrum of the residual velocity field. This paper provides the kinematical parameters for the whole sample. For the first time, the integrated Ha profiles have been computed and are presented for the whole sample. The total Ha fluxes deduced from these profiles have been used in order to provide a flux calibration for the 203 GHASP galaxies. This paper confirms the conclusions already drawn from half the sample concerning (i) the increased accuracy of position angles measurements using kinematical data, (ii) the difficulty to have robust determinations of both morphological and kinematical inclinations in particular for low inclination galaxies and (iii) the very good agreement between the Tully-Fisher relationship derived from our data and previous determinations.",astro-ph,Astrophysics
Subaru and Keck Observations of the Peculiar Type Ia Supernova 2006gz at Late Phases,"Recently, a few peculiar Type Ia supernovae (SNe) that show exceptionally large peak luminosity have been discovered. Their luminosity requires more than 1 Msun of 56Ni ejected during the explosion, suggesting that they might have originated from super-Chandrasekhar mass white dwarfs. However, the nature of these objects is not yet well understood. In particular, no data have been taken at late phases, about one year after the explosion. We report on Subaru and Keck optical spectroscopic and photometric observations of the SN Ia 2006gz, which had been classified as being one of these ""overluminous"" SNe Ia. The late-time behavior is distinctly different from that of normal SNe Ia, reinforcing the argument that SN 2006gz belongs to a different subclass than normal SNe Ia. However, the peculiar features found at late times are not readily connected to a large amount of 56Ni; the SN is faint, and it lacks [Fe II] and [Fe III] emission. If the bulk of the radioactive energy escapes the SN ejecta as visual light, as is the case in normal SNe Ia, the mass of 56Ni does not exceed ~ 0.3 Msun. We discuss several possibilities to remedy the problem. With the limited observations, however, we are unable to conclusively identify which process is responsible. An interesting possibility is that the bulk of the emission might be shifted to longer wavelengths, unlike the case in other SNe Ia, which might be related to dense C-rich regions as indicated by the early-phase data. Alternatively, it might be the case that SN 2006gz, though peculiar, was actually not substantially overluminous at early times.",astro-ph,Astrophysics
MHD mode conversion in a stratified atmosphere,"Mode conversion in the region where the sound and Alfven speeds are equal is a complex process, which has been studied both analytically and numerically, and has been seen in observations. In order to further the understanding of this process we set up a simple, one-dimensional model, and examine wave propagation through this system using a combination of analytical and numerical techniques. Simulations are carried out in a gravitationally stratified atmosphere with a uniform, vertical magnetic field for both isothermal and non-isothermal cases. For the non-isothermal case a temperature profile is chosen to mimic the steep temperature gradient encountered at the transition region. In all simulations, a slow wave is driven on the upper boundary, thus propagating down from low-beta to high-beta plasma across the mode-conversion region. In addition, a detailed analytical study is carried out where we predict the amplitude and phase of the transmitted and converted components of the incident wave as it passes through the mode-conversion region. A comparison of these analytical predictions with the numerical results shows good agreement, giving us confidence in both techniques. This knowledge may be used to help determine wave types observed and give insight into which modes may be involved in coronal heating.",astro-ph,Astrophysics
A photometric redshift of $z=1.8^{+0.4}_{-0.3}$ for the \agile GRB 080514B,"Aims: The AGILE gamma-ray burst GRB 080514B is the first burst with detected emission above 30 MeV and an optical afterglow. However, no spectroscopic redshift for this burst is known.
  Methods: We compiled ground-based photometric optical/NIR and millimeter data from several observatories, including the multi-channel imager GROND, as well as ultraviolet \swift UVOT and X-ray XRT observations. The spectral energy distribution of the optical/NIR afterglow shows a sharp drop in the \swift UVOT UV filters that can be utilized for the estimation of a redshift.
  Results: Fitting the SED from the \swift UVOT $uvw2$ band to the $H$ band, we estimate a photometric redshift of $z=1.8^{+0.4}_{-0.3}$, consistent with the pseudo redshift reported by Pelangeon & Atteia (2008) based on the gamma-ray data.
  Conclusions: The afterglow properties of GRB 080514B do not differ from those exhibited by the global sample of long bursts, supporting the view that afterglow properties are basically independent of prompt emission properties.",astro-ph,Astrophysics
Gamma Rays from Ultra-High Energy Cosmic Rays in Cygnus A,"Ultra-high energy cosmic rays (UHECRs) accelerated in the jets of active galactic nuclei can accumulate in high magnetic field, ~100 kpc-scale regions surrounding powerful radio galaxies. Photohadronic processes involving UHECRs and photons of the extragalactic background light make ultra-relativistic electrons and positrons that initiate electromagnetic cascades, leading to the production of a gamma-ray synchrotron halo. We calculate the halo emission in the case of Cygnus A and show that it should be detectable with the Fermi Gamma ray Space Telescope and possibly detectable with ground-based gamma-ray telescopes if radio galaxies are the sources of UHECRs.",astro-ph,Astrophysics
Use of Astronomical Literature - A Report on Usage Patterns,"In this paper we present a number of metrics for usage of the SAO/NASA Astrophysics Data System (ADS). Since the ADS is used by the entire astronomical community, these are indicative of how the astronomical literature is used. We will show how the use of the ADS has changed both quantitatively and qualitatively. We will also show that different types of users access the system in different ways. Finally, we show how use of the ADS has evolved over the years in various regions of the world.
  The ADS is funded by NASA Grant NNG06GG68G.",astro-ph,Astrophysics
"The Combined NVSS-FIRST Galaxies (CoNFIG) Sample - I. Sample Definition, Classification and Evolution","The CoNFIG (Combined NVSS-FIRST Galaxies) sample is a new sample of 274 bright radio sources at 1.4 GHz. It was defined by selecting all sources with S_1.4GHz > 1.3 Jy from the NRAO-VLA Sky Survey (NVSS) in the North field of the Faint Images of the Radio Sky at Twenty-cm (FIRST) survey. New radio observations obtained with the VLA for 31 of the sources are presented. The sample has complete FRI/FRII morphology identification; optical identifications and redshifts are available for 80% and 89% of the sample respectively, yielding a mean redshift of ~0.71. One of the goals of this survey is to get better definitions of luminosity distributions and source counts of FRI/FRII sources separately, in order to determine the evolution of the luminosity function for each type of source. We present a preliminary analysis, showing that these data are an important step towards examining various evolutionary schemes for these objects and to confirm or correct the dual population unified scheme for radio AGN. Improving our understanding of radio galaxy evolution will give better insight into the role of AGN feedback in galaxy formation.",astro-ph,Astrophysics
Adiabatic fluctuations from cosmic strings in a contracting universe,"We show that adiabatic, super-Hubble, and almost scale invariant density fluctuations are produced by cosmic strings in a contracting universe. An essential point is that isocurvature perturbations produced by topological defects such as cosmic strings on super-Hubble scales lead to a source term which seeds the growth of curvature fluctuations on these scales. Once the symmetry has been restored at high temperatures, the isocurvature seeds disappear, and the fluctuations evolve as adiabatic ones in the expanding phase. Thus, cosmic strings may be resurrected as a mechanism for generating the primordial density fluctuations observed today.",astro-ph,Astrophysics
High-Resolution Spectroscopy of Long-Periodic Eclipsing Binary Epsilon Aurigae,"The results of spectroscopic observations of long-periodic eclipsing binary Epsilon Aur are reported. The observations were carried out during 2 nights in 2007 at 2-meter telescope located at the peak Terskol, Northern Caucasus (Russia). Here we present series of Epsilon Aur spectra together with EW measurements of the most prominent absorption lines.",astro-ph,Astrophysics
Testing the Dark-Energy-Dominated Cosmology by the Solar-System Experiments,"According to the recent astronomical data, the most part of energy in the Universe is in the 'dark' form, which is effectively described by Lambda-term in Einstein equations. All arguments in favor of the dark energy were obtained so far from the observational data related to very large (intergalactic) scales. Is it possible to find a manifestation of the dark energy at much less scales (e.g. inside the Solar system)?",astro-ph,Astrophysics
A red supergiant nebula at 25 micron: arcsecond scale mass-loss asymmetries of mu Cep,"We present diffraction limited (0.6"") 24.5micron Subaru/COMICS images of the red supergiant mu Cep. We report the detection of a circumstellar nebula, that was not detected at shorter wavelengths. It extends to a radius of at least 6"" in the thermal infrared. On these angular scales, the nebula is roughly spherical, in contrast, it displays a pronounced asymmetric morphology closer in. We simultaneously model the azimuthally averaged intensity profile of the nebula and the observed spectral energy distribution using spherical dust radiative transfer models. The models indicate a constant mass-loss process over the past 1000 years, for mass-loss rates a few times 10^(-7) Msun/yr. This work supports the idea that at least part of the asymmetries in shells of evolved massive stars and supernovae may be due to the mass-loss process in the red supergiant phase.",astro-ph,Astrophysics
On the origin of radio emission in radio quiet quasars,"The radio emission in radio loud quasars originates in a jet carrying relativistic electrons. In radio quiet quasars (RQQs) the relative radio emission is ~10^3 times weaker, and its origin is not established yet. We show here that there is a strong correlation between the radio luminosity (L_R) and X-ray luminosity (L_X) with L_R~10^-5L_X, for the radio quiet Palomar-Green (PG) quasar sample. The sample is optically selected, with nearly complete radio and X-ray detections, and thus this correlation cannot be due to direct selection biases. The PG quasars lie on an extension of a similar correlation noted by Panessa et al., for a small sample of nearby low luminosity type 1 AGN. A remarkably similar correlation, known as the Guedel-Benz relation, where L_R/L_X~10^-5, holds for coronally active stars. The Guedel-Benz relation, together with correlated stellar X-ray and radio variability, implies that the coronae are magnetically heated. We therefore raise the possibility that AGN coronae are also magnetically heated, and that the radio emission in RQQ also originates in coronal activity. If correct, then RQQ should generally display compact flat cores at a few GHz due to synchrotron self-absorption, while at a few hundred GHz we should be able to see directly the X-ray emitting corona, and relatively rapid and large amplitude variability, correlated with the X-ray variability, is likely to be seen. We also discuss possible evidence that the radio and X-ray emission in ultra luminous X-ray sources and Galactic black holes may be of coronal origin as well.",astro-ph,Astrophysics
Fresnel interferometric arrays for space-based imaging: testbed results,"This paper presents the results of a Fresnel Interferometric Array testbed. This new concept of imager involves diffraction focussing by a thin foil, in which many thousands of punched subapertures form a pattern related to a Fresnel zone plate. This kind of array is intended for use in space, as a way to realizing lightweight large apertures for high angular resolution and high dynamic range observations. The chromaticity due to diffraction focussing is corrected by a small diffractive achromatizer placed close to the focal plane of the array.
  The laboratory test results presented here are obtained with an 8 centimeter side orthogonal array, yielding a 23 meter focal length at 600 nm wavelength. The primary array and the focal optics have been designed and assembled in our lab. This system forms an achromatic image. Test targets of various shapes, sizes, dynamic ranges and intensities have been imaged. We present the first images, the achieved dynamic range, and the angular resolution.",astro-ph,Astrophysics
Proper Motions and Brightness Variations of Nonthermal X-ray Filaments in the Cassiopeia A Supernova Remnant,"We present Chandra ACIS X-ray observations of the Galactic supernova remnant Cassiopeia A taken in December 2007. Combining these data with previous archival Chandra observations taken in 2000, 2002, and 2004, we estimate the remnant's forward shock velocity at various points around the outermost shell to range between 4200 and 5200 +/- 500 km/s. Using these results together with previous analyses of Cas A's X-ray emission, we present a model for the evolution of Cas A and find that it's expansion is well fit by a rho_ej ~ r^{-(7-9)} ejecta profile running into a circumstellar wind. We further find that while the position of the reverse shock in this model is consistent with that measured in the X-rays, in order to match the forward shock velocity and radius we had to assume that ~ 30% of the explosion energy has gone into accelerating cosmic rays at the forward shock. The new X-ray images also show that brightness variations can occur for some forward shock filaments like that seen for several nonthermal filaments seen projected in the interior of the remnant. Spectral fits to exterior forward shock filaments and interior nonthermal filaments show that they exhibit similar spectra. This together with similar flux variations suggests that interior nonthermal filaments might be simply forward shock filaments seen in projection and not located at the reverse shock as has been recently proposed.",astro-ph,Astrophysics
Neutrino Astrophysics,"I review the current status of neutrino astrophysics, including solar neutrinos; atmospheric neutrinos; neutrino mass and oscillations; supernova neutrinos; neutrino nucleosynthesis (Big Bang nucleosynthesis, the neutrino process, the r-process); neutrino cooling and red giants; and high energy neutrino astronomy.",astro-ph,Astrophysics
The properties of penumbral microjets inclination,We investigate the dependence of penumbral microjets inclination on the position within penumbra. The high cadence observations taken on 10 November 2006 with the Hinode satellite through the \ion{Ca}{ii} H and G--band filters were analysed to determine the inclination of penumbral microjets. The results were then compared with the inclination of the magnetic field determined through the inversion of the spectropolarimetric observations of the same region. The penumbral microjet inclination is increasing towards the outer edge of the penumbra. The results suggest that the penumbral microjet follows the opening magnetic field lines of a vertical flux tube that creates the sunspot.,astro-ph,Astrophysics
Rapidly spinning massive black holes in active galactic nuclei: evidence from the black hole mass function,"The comparison of the black hole mass function (BHMF) of active galactic nuclei (AGN) relics with the measured mass function of the massive black holes in galaxies provides strong evidence for the growth of massive black holes being dominated by mass accretion. We derive the Eddington ratio distributions as functions of black hole mass and redshift from a large AGN sample with measured Eddington ratios given by Kollmeier et al. We find that, even at the low mass end, most black holes are accreting at Eddington ratio ~0.2, which implies that the objects accreting at extremely high rates should be rare or such phases are very short. Using the derived Eddington ratios, we explore the cosmological evolution of massive black holes with an AGN bolometric luminosity function (LF). It is found that the resulted BHMF of AGN relics is unable to match the measured local BHMF of galaxies for any value of (constant) radiative efficiency. Motivated by Volonteri, Sikora & Lasota's study on the spin evolution of massive black holes, we assume the radiative efficiency to be dependent of black hole mass, i.e., it is low for M<10^8 solar masses and it increases with black hole mass for black holes with >10^8 solar masses. We find that the BHMF of AGN relics can roughly reproduce the local BHMF of galaxies if the radiative efficiency ~0.08 for the black holes with <10^8 solar masses and it increases to ~0.18 for black holes with >10^9 solar masses, which implies that most massive black holes (>10^9 solar masses) are spinning very rapidly.",astro-ph,Astrophysics
Injection of Short-Lived Radionuclides into the Early Solar System from a Faint Supernova with Mixing-Fallback,"Several short-lived radionuclides (SLRs) were present in the early solar system, some of which should have formed just prior to or soon after the solar system formation. Stellar nucleosynthesis has been proposed as the mechanism for production of SLRs in the solar system, but no appropriate stellar source has been found to explain the abundances of all solar system SLRs.
  In this study, we propose a faint supernova with mixing and fallback as a stellar source of SLRs with mean lives of <5 Myr (26Al, 41Ca, 53Mn, and 60Fe) in the solar system. In such a supernova, the inner region of the exploding star experiences mixing, a small fraction of mixed materials is ejected, and the rest undergoes fallback onto the core. The modeled SLR abundances agree well with their solar system abundances if mixing-fallback occurs within the C/O-burning layer. In some cases, the initial solar system abundances of the SLRs can be reproduced within a factor of 2. The dilution factor of supernova ejecta to the solar system materials is ~10E-4 and the time interval between the supernova explosion and the formation of oldest solid materials in the solar system is ~1 Myr. If the dilution occurred due to spherically symmetric expansion, a faint supernova should have occurred nearby the solar system forming region in a star cluster.",astro-ph,Astrophysics
The effect of stellar winds on the formation of a protocluster,"We present SPH simulations of protoclusters including the effects of winds from massive stars. Using a particle-injection method, we investigate the effect of structure close to the wind sources and the short-timescale influence of winds on protoclusters. Structures such as disks and gaseous filaments have a strong collimating effect. By a different technique of injecting momentum from point sources, we compare the large-scale, long-term effects of isotropic and intrinsically-collimated winds and find them to be similar. Both types of wind dramatically slow the global star formation process, but the timescale on which they expel significant mass from the cluster is rather long (approaching 10 freefall times). Clusters may then experience rapid star formation early in their lifetimes, before switching to a mode where gas is gradually expelled, while star formation proceeds much more slowly. This complicates conclusions regarding slow star formation derived from measuring the star-formation efficiency per freefall time. Estimates of the efficacy of winds in dispersing clusters derived simply from the total wind momentum flux may not be very reliable. This is due to material being expelled from deep within stellar potential wells, often to velocities well in excess of the cluster escape velocity, and also to the loss of momentum flux through holes in the gas distribution. Winds have little effect on the accretion--driven stellar IMF except at the very high-mass end, where they serve to prevent some of the most massive objects accreting. We also find that the morphology of the gas, the rapid motions of the wind sources and the action of accretion flows prevent the formation of bubble-like structures. This may make it difficult to discern the influence of winds on very young clusters.",astro-ph,Astrophysics
Circles-in-the-sky searches and observable cosmic topology in the inflationary limit,"While the topology of the Universe is at present not specified by any known fundamental theory, it may in principle be determined through observations. In particular, a non-trivial topology will generate pairs of matching circles of temperature fluctuations in maps of the cosmic microwave background, the so-called circles-in-the-sky. A general search for such pairs of circles would be extremely costly and would therefore need to be confined to restricted parameter ranges. To draw quantitative conclusions from the negative results of such partial searches for the existence of circles we need a concrete theoretical framework. Here we provide such a framework by obtaining constraints on the angular parameters of these circles as a function of cosmological density parameters and the observer's position. As an example of the application of our results, we consider the recent search restricted to pairs of nearly back-to-back circles with negative results. We show that assuming the Universe to be very nearly flat, with its total matter-energy density satisfying the bounds $ 0 <|_0 - 1| \lesssim 10^{-5}$, compatible with the predictions of typical inflationary models, this search, if confirmed, could in principle be sufficient to exclude a detectable non-trivial cosmic topology for most observers. We further relate explicitly the fraction of observers for which this result holds to the cosmological density parameters.",astro-ph,Astrophysics
Mergers and Mass Accretion Rates in Galaxy Assembly: The Millennium Simulation Compared to Observations of z~2 Galaxies,"Recent observations of UV-/optically selected, massive star forming galaxies at z~2 indicate that the baryonic mass assembly and star formation history is dominated by continuous rapid accretion of gas and internal secular evolution, rather than by major mergers. We use the Millennium Simulation to build new halo merger trees, and extract halo merger fractions and mass accretion rates. We find that even for halos not undergoing major mergers the mass accretion rates are plausibly sufficient to account for the high star formation rates observed in z~2 disks. On the other hand, the fraction of major mergers in the Millennium Simulation is sufficient to account for the number counts of submillimeter galaxies (SMGs), in support of observational evidence that these are major mergers. When following the fate of these two populations in the Millennium Simulation to z=0, we find that subsequent mergers are not frequent enough to convert all z~2 turbulent disks into elliptical galaxies at z=0. Similarly, mergers cannot transform the compact SMGs/red sequence galaxies at z~2 into observed massive cluster ellipticals at z=0. We argue therefore, that secular and internal evolution must play an important role in the evolution of a significant fraction of z~2 UV-/optically and submillimeter selected galaxy populations.",astro-ph,Astrophysics
Contradiction between strong lensing statistics and a feedback solution to the cusp/core problem,"Standard cosmology has many successes on large scales, but faces some fundamental difficulties on small, galactic scales. One such difficulty is the cusp/core problem. High resolution observations of the rotation curves for dark matter dominated low surface brightness (LSB) galaxies imply that galactic dark matter halos have a density profile with a flat central core, whereas N-body structure formation simulations predict a divergent (cuspy) density profile at the center. It has been proposed that this problem can be resolved by stellar feedback driving turbulent gas motion that erases the initial cusp. However, strong gravitational lensing prefers a cuspy density profile for galactic halos. In this paper, we use the most recent high resolution observations of the rotation curves of LSB galaxies to fit the core size as a function of halo mass, and compare the resultant lensing probability to the observational results for the well defined combined sample of the Cosmic Lens All-Sky Survey (CLASS) and Jodrell Bank/Very Large Array Astrometric Survey (JVAS). The lensing probabilities based on such density profiles are too low to match the observed lensing in CLASS/JVAS. High baryon densities in the galaxies that dominate the lensing statistics can reconcile this discrepancy, but only if they steepen the mass profile rather than making it more shallow. This places contradictory demands upon the effects of baryons on the central mass profiles of galaxies.",astro-ph,Astrophysics
The Current Understanding on the UV Upturn,"The unexpected high bump in the UV part of the spectrum found in nearby giant elliptical galaxies, a.k.a. the UV upturn, has been a subject of debate. A remarkable progress has been made lately from the observational side, mainly involving space telescopes. The GALEX UV telescope has been obtaining thousands of giant ellipticals in the nearby universe, while HST is resolving local galaxies into stars and star clusters. An important clue has also been found regarding the origin of hot HB stars, and perhaps of sdB stars. That is, extreme amounts of helium are suspected to be the origin of the extended HB and even to the UV upturn phenomenon. A flurry of studies are pursuing the physics behind it. All this makes me optimistic that the origin of the UV upturn will be revealed in the next few years. I review some of the most notable progress and remaining issues.",astro-ph,Astrophysics
GRIPS - Gamma-Ray Burst Investigation via Polarimetry and Spectroscopy,"The primary scientific goal of the GRIPS mission is to revolutionize our understanding of the early universe using gamma-ray bursts. We propose a new generation gamma-ray observatory capable of unprecedented spectroscopy over a wide range of gamma-ray energies (200 keV--50 MeV) and of polarimetry (200--1000 keV). Secondary goals achievable by this mission include direct measurements of supernova interiors through gamma-rays from radioactive decays, nuclear astrophysics with massive stars and novae, and studies of particle acceleration near compact stars, interstellar shocks, and clusters of galaxies.",astro-ph,Astrophysics
Distribution and kinematics of the HCN(3-2) emission down to the innermost region in the envelope of the O-rich star W Hya,"We report high angular resolution observations of the HCN (3-2) line emission in the circumstellar envelope of the O-rich star W Hya with the Submillimeter Array. The proximity of this star allows us to image its molecular envelope with a spatial resolution of just ~40 AU, corresponding to about 10 times the stellar diameter. We resolve the HCN (3-2) emission and find that it is centrally peaked and has a roughly spherically symmetrical distribution. This shows that HCN is formed in the innermost region of the envelope (within ~10 stellar radii), which is consistent with predictions from pulsation-driven shock chemistry models, and rules out the scenario in which HCN forms through photochemical reactions in the outer envelope. Our model suggests that the envelope decreases steeply in temperature and increases smoothly in velocity with radius, inconsistent with the standard model for mass-loss driven by radiative pressure on dust grains. We detect a velocity gradient of ~5 km/s in the NW--SE direction over the central 40 AU. This velocity gradient is reminescent of that seen in OH maser lines, and could be caused by the rotation of the envelope or by a weak bipolar outflow.",astro-ph,Astrophysics
"Dark energy constraints and correlations with systematics from CFHTLS weak lensing, SNLS supernovae Ia and WMAP5","We combine measurements of weak gravitational lensing from the CFHTLS-Wide survey, supernovae Ia from CFHT SNLS and CMB anisotropies from WMAP5 to obtain joint constraints on cosmological parameters, in particular, the dark energy equation of state parameter w. We assess the influence of systematics in the data on the results and look for possible correlations with cosmological parameters.
  We implement an MCMC algorithm to sample the parameter space of a flat CDM model with a dark-energy component of constant w. Systematics in the data are parametrised and included in the analysis. We determine the influence of photometric calibration of SNIa data on cosmological results by calculating the response of the distance modulus to photometric zero-point variations. The weak lensing data set is tested for anomalous field-to-field variations and a systematic shape measurement bias for high-z galaxies.
  Ignoring photometric uncertainties for SNLS biases cosmological parameters by at most 20% of the statistical errors, using supernovae only; the parameter uncertainties are underestimated by 10%. The weak lensing field-to-field variance pointings is 5%-15% higher than that predicted from N-body simulations. We find no bias of the lensing signal at high redshift, within the framework of a simple model. Assuming a systematic underestimation of the lensing signal at high redshift, the normalisation sigma_8 increases by up to 8%. Combining all three probes we obtain -0.10<1+w<0.06 at 68% confidence (-0.18<1+w<0.12 at 95%), including systematic errors. Systematics in the data increase the error bars by up to 35%; the best-fit values change by less than 0.15sigma. [Abridged]",astro-ph,Astrophysics
Target star catalog for Darwin: Nearby Stellar sample for a search for terrestrial planets,"In order to evaluate and develop mission concepts for a search for Terrestrial Exoplanets, we have prepared a list of potential target systems. In this paper we present and discuss the criteria for selecting potential target stars suitable for the search for Earth like planets, with a special emphasis on the aspects of the habitable zone for these stellar systems. Planets found within these zones would be potentially able to host complex life forms. We derive a final target star sample of potential target stars, the Darwin All Sky Star Catalog (DASSC). The DASSC contains a sample of 2303 identified objects of which 284 are F, 464 G, 883 K, 615 M type stars and 57 stars without B-V index. Of these objects 949 objects are flagged in the DASSC as multiple systems, resulting in 1229 single main sequence stars of which 107 are F, 235 are G, 536 are K, and 351 are M type. We derive configuration dependent subcatalogs from the DASSC for two technical designs, the initial baseline design and the advanced Emma design as well as a catalog using an inner working angle cut off. We discuss the selection criteria, derived parameters and completeness of sample for different classes of stars.",astro-ph,Astrophysics
Time Variability of Quasars: the Structure Function Variance,"Significant progress in the description of quasar variability has been recently made by employing SDSS and POSS data. Common to most studies is a fundamental assumption that photometric observations at two epochs for a large number of quasars will reveal the same statistical properties as well-sampled light curves for individual objects. We critically test this assumption using light curves for a sample of $\sim$2,600 spectroscopically confirmed quasars observed about 50 times on average over 8 years by the SDSS stripe 82 survey. We find that the dependence of the mean structure function computed for individual quasars on luminosity, rest-frame wavelength and time is qualitatively and quantitatively similar to the behavior of the structure function derived from two-epoch observations of a much larger sample. We also reproduce the result that the variability properties of radio and X-ray selected subsamples are different. However, the scatter of the variability structure function for fixed values of luminosity, rest-frame wavelength and time is similar to the scatter induced by the variance of these quantities in the analyzed sample. Hence, our results suggest that, although the statistical properties of quasar variability inferred using two-epoch data capture some underlying physics, there is significant additional information that can be extracted from well-sampled light curves for individual objects.",astro-ph,Astrophysics
Causal dissipative hydrodynamics obtained from the nonextensive/dissipative correspondence,"We derive the constitutive equations of causal relativistic dissipative hydrodynamics ($d$-hydrodynamics) from perfect nonextensive hydrodynamics ($q$-hydrodynamics) using the nonextensive/dissipative correspondence (NexDC) proposed by us recently. The $q$-hydrodynamics can be thus regarded as a possible model for the $d$-hydrodynamics facilitating its application to high energy multiparticle production processes. As an example we have shown that applying the NexDC to the perfect 1+1 $q$-hydrodynamics, one obtains a proper time evolution of the bulk pressure and the Reynolds number.",astro-ph,Astrophysics
Mergers of luminous early-type galaxies in the local universe and gravitational wave background,"Supermassive black hole (SMBH) coalescence in galaxy mergers is believed to be one of the primary sources of very low frequency gravitational waves (GWs). Significant contribution of the GWs comes from mergers of massive galaxies with redshifts z<2. Very few previous studies gave the merger rate of massive galaxies. % We selected a large sample (1209) of close pairs of galaxies with projected separations 7<r_p<50 kpc from 87,889 luminous early-type galaxies (M_r<-21.5) from the Sloan Digital Sky Survey Data Release 6. These pairs constitute a complete volume-limited sample in the local universe (z<0.12). Using our newly developed technique, 249 mergers have been identified by searching for interaction features. From them, we found that the merger fraction of luminous early-type galaxies is 0.8%, and the merger rate in the local universe is % R_g=(1.0+/-0.4)*10^{-5} Mpc^{-3} Gyr^{-1}} % with an uncertainty mainly depending on the merging timescale. % We estimated the masses of SMBHs in the centers of merging galaxies based on their luminosities. We found that the chirp mass distribution of the SMBH binaries follows a power law with an index of -3.0+/-0.5 in the range 5*10^8--5*10^{9} M_{\odot}. % Using the SMBH population in the mergers and assuming that the SMBHs can be efficiently driven into the GW regime, we investigated the stochastic GW background in the frequency range 10^{-9}--10^{-7} Hz. We obtained the spectrum of the GW background of h_c(f)=10^{-15}(f/yr^{-1})^{-2/3}, which is one magnitude higher than that obtained by Jaffe & Backer in 2003, but consistent with those calculated from galaxy-formation models.",astro-ph,Astrophysics
The effect of convection on pulsational stability,"A review on the current state of mode physics in classical pulsators is presented. Two, currently in use, time-dependent convection models are compared and their applications on mode stability are discussed with particular emphasis on the location of the Delta Scuti instability strip.",astro-ph,Astrophysics
Granulation in K-type Dwarf Stars. I. Spectroscopic observations,"Very high resolution (R~160,000-210,000), high signal-to-noise ratio (S/N>300) spectra of nine bright K-dwarfs were obtained with the 2dcoude spectrograph on the 2.7m Telescope at McDonald Observatory to determine wavelength shifts and asymmetries of Fe I lines. The observed shapes and positions of Fe I lines reveal asymmetries and wavelength shifts that indicate the presence of granulation. In particular, line bisectors show characteristic C-shapes while line core wavelengths are blueshifted by an amount that increases with decreasing equivalent width (EW). On average, Fe I line bisectors have a span that ranges from nearly 0 for the weakest lines (residual core flux > 0.7) to about 75 m/s for the strongest lines (residual core flux ~ 0.3) while wavelength shifts range from about -150 m/s in the weakest (EW ~ 10 mA) lines to 0 in the strongest (EW > 100 mA) features. A more detailed inspection of the bisectors and wavelength shifts reveals star-to-star differences that are likely associated with differences in stellar parameters, projected rotational velocity, and stellar activity. For the inactive, slow projected rotational velocity stars, we detect, unequivocally, a plateau in the line-shifts at large EW values (EW > 100 mA), a behavior that had been identified before only in the solar spectrum. The detection of this plateau allows us to determine the zero point of the convective blueshifts, which is useful to determine absolute radial velocities. Thus, we are able to measure such velocities with a mean uncertainty of about 60 m/s.",astro-ph,Astrophysics
Magnetars and fossil-field model of origin of magnetic field,"The evolution and genesis of Anomalous X-ray Pulsars and Soft Gamma ray Repeaters are investigated. The new arguments in favor of magnetar model are found. It is shown, that these objects are formed from more massive stars and responsible for their high magnetic fields is fossil-field model.",astro-ph,Astrophysics
Super-Orbital Variability in Hard X-rays,"We present the results of a study with the \emph{Swift} Burst Alert Telescope in the 14 -- 195 keV range of the long-term variability of 5 low mass X-ray binaries with reported or suspected super-orbital periods -- 4U 1636-536, 4U 1820-303, 4U 1916-053, Cyg X-2 and Sco X-1. No significant persistent periodic modulation was detected around the previously reported periods in the 4U 1916-053, Cyg X-2 or Sco X-1 light curves. The $\sim$170 d period of 4U 1820-303 was detected up to 24 keV, consistent with variable accretion due to the previously proposed triple system model. The $\sim$46 d period in 4U 1636-536 was detected up to 100 keV, with the modulation in the low and high energy bands found to be phase shifted by $\sim180^\circ$ with respect to each other. This phase shift, when taken together with the near-coincident onset of the $\sim$46 d modulation and the low/hard X-ray state, leads us to speculate that the modulation could herald transient jet formation.",astro-ph,Astrophysics
The stellar association around Gamma Velorum and its relationship with Vela OB2,"We present the results of a photometric BVI survey of 0.9 square degrees around the Wolf-Rayet binary gamma^2 Vel and its early-type companion gamma^1 Vel. Several hundred PMS stars are identified and the youth of a subset of these is confirmed by the presence of lithium, H-alpha emission and X-ray activity. We show that the PMS stars are kinematically coherent and spatially concentrated around gamma Vel. The PMS stars have similar proper motions to gamma Vel, to main-sequence stars around gammaVel and to early-type stars of the wider Vela OB2 association of which gamma^2 Vel is the brightest member. The ratio of main-sequence stars to low-mass (0.1-0.6 Msun) PMS stars is consistent with a Kroupa mass function. Main-sequence fitting to stars around gamma Vel gives a distance modulus of 7.76+/-0.07 mag, consistent with a similarly-determined distance for Vela OB2 and with interferometric distances to gamma^2 Vel. High-mass stellar models indicate an age of 3-4 Myr for gamma^2 Vel, but the low-mass PMS stars have ages of ~10 Myr according to low-mass evolutionary models and 5-10 Myr by empirically placing them in an age sequence with other clusters based on colour-magnitude diagrams and lithium depletion. We conclude that the low-mass PMS stars form a genuine association with gamma Vel and that this is a subcluster within the larger Vela OB2 association. We speculate that gamma^2 Vel formed after the low-mass stars, expelling gas, terminating star formation and unbinding the association. The velocity dispersion of the PMS stars is too low for this star forming event to have produced all the stars in Vela OB2. Instead, star formation must have started at several sites within a molecular cloud, either sequentially or, simultaneously after some triggering event [abridged].",astro-ph,Astrophysics
The Atomic to Molecular Transition in Galaxies. II: HI and H_2 Column Densities,"Gas in galactic disks is collected by gravitational instabilities into giant atomic-molecular complexes, but only the inner, molecular parts of these structures are able to collapse to form stars. Determining what controls the ratio of atomic to molecular hydrogen in complexes is therefore a significant problem in star formation and galactic evolution. In this paper we use the model of H_2 formation, dissociation, and shielding developed in the previous paper in this series to make theoretical predictions for atomic to molecular ratios as a function of galactic properties. We find that the molecular fraction in a galaxy is determined primarily by its column density and secondarily by its metallicity, and is to good approximation independent of the strength of the interstellar radiation field. We show that the column of atomic hydrogen required to shield a molecular region against dissociation is ~10 Msun pc^-2 at solar metallicity. We compare our model to data from recent surveys of the Milky Way and of nearby galaxies, and show that the both the primary dependence of molecular fraction on column density and the secondary dependence on metallicity that we predict are in good agreement with observed galaxy properties.",astro-ph,Astrophysics
Atmospheric Escape from Hot Jupiters,"Photoionization heating from UV radiation incident on the atmospheres of hot Jupiters may drive planetary mass loss. We construct a model of escape that includes realistic heating and cooling, ionization balance, tidal gravity, and pressure confinement by the host star wind. We show that mass loss takes the form of a hydrodynamic (""Parker"") wind, emitted from the planet's dayside during lulls in the stellar wind. When dayside winds are suppressed by the confining action of the stellar wind, nightside winds might pick up if there is sufficient horizontal transport of heat. A hot Jupiter loses mass at maximum rates of ~2 x 10^12 g/s during its host star's pre-main-sequence phase and ~2 x10^10 g/s during the star's main sequence lifetime, for total maximum losses of ~0.06% and ~0.6% of the planet's mass, respectively. For UV fluxes F_UV < 10^4 erg/cm^2/s, the mass loss rate is approximately energy-limited and is proportional to F_UV^0.9. For larger UV fluxes, such as those typical of T Tauri stars, radiative losses and plasma recombination force the mass loss rate to increase more slowly as F_UV^0.6. Dayside winds are quenched during the T Tauri phase because of confinement by overwhelming stellar wind pressure. We conclude that while UV radiation can indeed drive winds from hot Jupiters, such winds cannot significantly alter planetary masses during any evolutionary stage. They can, however, produce observable signatures. Candidates for explaining why the Lyman-alpha photons of HD 209458 are absorbed at Doppler-shifted velocities of +/- 100 km/s include charge-exchange in the shock between the planetary and stellar winds.",astro-ph,Astrophysics
The minimum stellar metallicity observable in the Galaxy,"The first stars fundamentally transformed the early Universe through their production of energetic radiation and the first heavy chemical elements. The impact on cosmic evolution sensitively depends on their initial mass function (IMF), which can be empirically constrained through detailed studies of ancient, metal-poor halo stars in our Galaxy. We compare the lowest magnesium and iron abundances measured in Galactic halo stars with theoretical predictions for the minimum stellar enrichment provided by Population III stars under the assumption of a top-heavy IMF. To demonstrate that abundances measured in metal-poor stars reflect the chemical conditions at their formation, and that they can thus be used to derive constraints on the primordial IMF, we carry out a detailed kinematic analysis of a large sample of metal-poor stars drawn from the SDSS survey. We assess whether interstellar accretion has altered their surface abundances. We find that accretion is generally negligible, even at the extremely low levels where the primordial IMF can be tested. We conclude that the majority of the first stars were very massive, but had likely masses below ~140 M.",astro-ph,Astrophysics
"Quark-Novae, cosmic reionization, and early r-process element production","We examine the case for Quark-Novae (QNe) as possible sources for the reionization and early metal enrichment of the universe. Quark-Novae are predicted to arise from the explosive collapse (and conversion) of sufficiently massive neutron stars into quark stars. A Quark-Nova (QN) can occur over a range of time scales following the supernova event. For QNe that arise days to weeks after the supernovae, we show that dual-shock that arises as the QN ejecta encounter the supernova ejecta can produce enough photons to reionize hydrogen in most of the Inter-Galactic medium (IGM) by z ~ 6. Such events can explain the large optical depth tau_e ~ 0.1 as measured by WMAP, if the clumping factor, C, of the material being ionized is smaller than 10. We suggest a way in which a normal initial mass function (IMF) for the oldest stars can be reconciled with a large optical depth as well as the mean metallicity of the early IGM post reionization. We find that QN also make a contribution to r-process element abundances for atomic numbers A > 130. We predict that the main cosmological signatures of Quark-Novae are the gamma-ray bursts that announce their birth. These will be clustered at redshifts in the range z ~ 7-8 in our model.",astro-ph,Astrophysics
Optical photometry and spectroscopy of the type Ibn supernova SN 2006jc until the onset of dust formation,"We present optical UBVRI photometric and spectroscopic data of the type Ibn supernova SN 2006jc, until the onset of the dust forming phase. The optical spectrum shows a blue continuum and is dominated by the presence of moderately narrow (velocity ~2500 km/s) He I emission lines superimposed over a relatively weak supernova spectrum. The helium lines are produced in a pre-existing He rich circumstellar shell. The observed helium line fluxes indicate the circumstellar shell is dense, with a density of ~10^9 - 10^{10} cm^{-3}. The helium mass in this shell is estimated to be <~0.07 Msun. The optical light curves show a clear signature of dust formation, indicated by a sharp decrease in the magnitudes around day 50, accompanied by a reddening of the colours. The evolution of the optical light curves during the early phase and that of the uvoir bolometric light curve at all phases is reasonably similar to normal Ib/c supernovae.",astro-ph,Astrophysics
Transient jets in the symbiotic prototype Z Andromedae,"We present development of the collimated bipolar jets from the symbiotic prototype Z And that appeared and disappeared during its 2006 outburst. In 2006 July Z And reached its historical maximum at U ~ 8.0. During this period, rapid photometric variations with Dm ~ 0.06 mag on the timescale of hours developed. Simultaneously, high-velocity satellite components appeared on both sides of the H-alpha and H-beta emission line profiles. They were launched asymmetrically with the red/blue velocity ratio of 1.2 - 1.3. From about mid-August they became symmetric. Their spectral properties indicated ejection of bipolar jets collimated within an average opening angle of 6.1 degrees. We estimated average outflow rate via jets to dM(jet)/dt ~ 2xE10-6(R(jet)/1AU)**(1/2) M(Sun)/year, during their August - September maximum, which corresponds to the emitting mass in jets, M(jet, emitting) ~ 6xE-10(Rjet)/1AU)^{3/2} M(Sun). During their lifetime, the jets released the total mass of M(jet, total) approx 7.4x1E-7 M(Sun). Evolution in the rapid photometric variability and asymmetric ejection of jets around the optical maximum can be explained by a disruption of the inner parts of the disk caused by radiation-induced warping of the disk.",astro-ph,Astrophysics
The Spectral Shape and Photon Fraction as Signatures of the GZK-Cutoff,"With the prospect of measuring the fraction of arriving secondary photons, produced through photo-pion energy loss interactions of ultra high energy cosmic ray (UHECR) protons with the microwave background during propagation, we investigate how information about the local UHECR source distribution can be inferred from the primary (proton) to secondary (photon) ratio. As an aid to achieve this, we develop an analytic description for both particle populations as a function of propagation time. Through a consideration of the shape of the GZK cut-off and the corresponding photon fraction curve, we investigate the different results expected for both different maximum proton energies injected by the sources, as well as a change in the local source distribution following a perturbative deformation away from a homogeneous description. At the end of the paper, consideration is made as to how these results are modified through extra-galactic magnetic field effects on proton propagation. The paper aims to demonstrate how the shape of the cosmic ray flux in the cut-off region, along with the photon fraction, are useful indicators of the cutoff origin as well as the local UHECR source distribution.",astro-ph,Astrophysics
Searching for substellar companions of young isolated neutron stars,"Context: Only two planetary systems around old ms-pulsars are currently known. Young radio pulsars and radio-quiet neutron stars cannot be covered by the usually-applied radio pulse timing technique. However, finding substellar companions around these neutron stars would be of great interest -- not only because of the companion's possible exotic formation but also due to the potential access to neutron star physics.
  Aims: We investigate the closest young neutron stars to search for substellar companions around them.
  Methods: Young, thus warm substellar companions are visible in the Near Infrared while the neutron star itself is much fainter. Four young neutron stars are moving fast enough to enable a common proper motion search for substellar companions within few years.
  Results. For Geminga, RX J0720.4-3125, RX J1856.6-3754, and PSR J1932+1059 we did not find any co-moving companion down to 12, 15, 11, 42 Jupiter masses for assumed ages of 1, 1, 1, 3.1 Myrs and distances of 250, 361, 167, 361 pc, respectively. Near Infrared limits are presented for these four as well as five other neutron stars for which we currently have only observations at one epoch.
  Conclusions: We conclude that young isolated neutron stars rarely have brown dwarf companions.",astro-ph,Astrophysics
The Lacerta OB1 Association,"Lac OB1 is a nearby OB association in its final stage of star formation. While the member stars suggest an expansion time scale of tens of Myr, the latest star formation episode, as manifested by the existence of massive and pre-main sequence stars, took place no more than a few Myr ago. The remnant molecular clouds in the region provide evidence of starbirth triggered by massive stars.",astro-ph,Astrophysics
AGN population in the deepest hard X-ray extragalactic survey,"We present the results of the analysis of the AGN population in the deepest extragalactic hard X-ray survey. The survey is based on INTEGRAL observation of the 3C 273/Coma cluster region, and covers 2500 deg2 with a 20-60 keV flux limit 1.5 times lower than other surveys at similar energies, resolving about 2.5% of the cosmic hard X-ray background. Using this survey, we can constrain in an unbiased way the distribution of hydrogen column absorption up to Nh=10^25 cm-2. We put an upper limit of 24% to the fraction of Compton-thick objects. Compared to models of the AGN population selected in the 2-10 keV band, the Log N-Log S diagram is generally in good agreement, but the Nh distribution is significantly different, with significantly less unabsorbed sources (Nh<10^22 cm-2) at a given flux limit compared to the models. We also study the local hard X-ray luminosity function (LF), which is compatible with what is found in other recent hard X-ray surveys. The extrapolation of the 2-10 keV LF is lower than the hard X-ray LF. The discrepancy is resolved if AGN spectra typically present reflection humps with reflection fraction R~1. Finally, we use the population properties of this survey to show that a future ultra-deep INTEGRAL extragalactic survey can result in a quite large AGN sample with enough objects at redshifts larger than z=0.05 so that we can detect evolution in the hard X-ray LF.",astro-ph,Astrophysics
Systematic thermal reduction of neutronization in core-collapse supernovae,"We investigate to what extent the temperature dependence of the nuclear symmetry energy can affect the neutronization of the stellar core prior to neutrino trapping during gravitational collapse. To this end, we implement a one-zone simulation to follow the collapse until beta equilibrium is reached and the lepton fraction remains constant. Since the strength of electron capture on the neutron-rich nuclei associated to the supernova scenario is still an open issue, we keep it as a free parameter. We find that the temperature dependence of the symmetry energy consistently yields a small reduction of deleptonization, which corresponds to a systematic effect on the shock wave energetics: the gain in dissociation energy of the shock has a small yet non-negligible value of about 0.4 foe (1 foe = 10^51 erg) and this result is almost independent from the strength of nuclear electron capture. The presence of such a systematic effect and its robustness under changes of the parameters of the one-zone model are significative enough to justify further investigations with detailed numerical simulations of supernova explosions.",astro-ph,Astrophysics
The WEBT Campaign on the Intermediate BL Lac Object 3C66A in 2007-2008,"Prompted by a high optical state in September 2007, the Whole Earth Blazar Telescope (WEBT) consortium organized an intensive optical, near-IR (JHK) and radio observing campaign on the intermediate BL Lac object 3C 66A throughout the fall and winter of 2007 -- 2008. The source remained in a high optical state throughout the observing period and exhibited several bright flares on time scales of ~ 10 days. This included an exceptional outburst around September 15 - 20, 2007, reaching a peak brightness at R ~ 13.4. Our campaign revealed microvariability with flux changes up to |dR/dt| ~ 0.02 mag/hr. Our observations do not reveal evidence for systematic spectral variability or spectral lags. We infer a value of the magnetic field in the emission region of B ~ 19 e_B^{2/7} _h^{-6/7} D_1^{13/7} G. From the lack of systematic spectral variability, we can derive an upper limit on the Doppler factor, D <= 28 _h^{-1/8} e_B^{3/16}. This is in agreement with superluminal motion measurements of _{app} \le 27 and argues against models with very high Lorentz factors of > 50, required for a one-zone SSC interpretation of some high-frequency-peaked BL Lac objects detected at TeV gamma-ray energies.",astro-ph,Astrophysics
Magnetic field amplification by SN-driven interstellar turbulence,"Within the interstellar medium, supernovae are thought to be the prevailing agents in driving turbulence. Until recently, their effects on magnetic field amplification in disk galaxies remained uncertain. Analytical models based on the uncorrelated-ensemble approach predicted that any created field would be expelled from the disk before it could be amplified significantly. By means of direct simulations of supernova-driven turbulence, we demonstrate that this is not the case. Accounting for galactic differential rotation and vertical stratification, we find an exponential amplification of the mean field on timescales of several hundred million years. We especially highlight the importance of rotation in the generation of helicity by showing that a similar mechanism based on Cartesian shear does not lead to a sustained amplification of the mean magnetic field.",astro-ph,Astrophysics
INTEGRAL and XMM-Newton observations of AX J1845.0-0433,"AX J1845.0-0433 is a transient high-mass X-ray binary discovered by ASCA. The source displays bright and short flares observed recently with INTEGRAL. The transient behaviour and the bright and short flares are studied in order to understand the accretion mechanisms and the nature of the source. Public INTEGRAL data and a pointed XMM-Newton observation are used to study in details the flaring and quiescent phases.
  AX J1845.0-0433 is a persistent X-ray binary with a O9.5I supergiant companion emitting at a low 0.2-100 keV luminosity of ~1e35 erg/s with seldom flares reaching luminosities of 1e36 erg/s. The most-accurate X-ray position is R.A. (2000) =18h45m01s.4 and Dec. = -04deg33'57''.7 (uncertainty 2''). Variability factors of 50 are observed on time scale as short as hundreds of seconds. The broad-band high-energy spectrum is typical of wind-fed accreting pulsars with an intrinsic absorption of NH=(2.6+/-0.2)e22 cm-2, a hard continuum of Gamma=(0.7-0.9)+/-0.1 and a high-energy cutoff at Ecut=16+/-3 keV. An excess at low energies is also observed fitted with a black body with a temperature of kT=0.18+/-0.05 keV. Optically-thin and highly-ionised iron (Fe XVIII-XIX) located near the supergiant star is detected during the quiescence phase. The spectral shape of the X-ray continuum is constant. The flare characteristics in contrast to the persistent quiescent emission suggest that clumps of mass M~1e22 g are formed within the stellar wind of the supergiant companion.",astro-ph,Astrophysics
Probing the complex environments of GRB host galaxies and intervening systems: high resolution spectroscopy of GRB050922C,"The aim of this paper is to investigate the environment of gamma ray bursts (GRBs) and the interstellar matter of their host galaxies. We use to this purpose high resolution spectroscopic observations of the afterglow of GRB050922C, obtained with UVES/VLT 3.5 hours after the GRB event. We found that, as for most high resolution spectra of GRBs, the spectrum of the afterglow of GRB050922C is complex. At least seven components contribute to the main absorption system at z=2.1992. The detection of lines of neutral elements like MgI and the detection of fine-structure levels of the ions FeII, SiII and CII allows us to separate components in the GRB ISM along the line of sight. Moreover, in addition to the main system, we have analyzed the five intervening systems between z = 2.077 and z = 1.5664 identified along the GRB line of sight. GRB afterglow spectra are very complex, but full of information. This can be used to disentangle the contribution of the different parts of the GRB host galaxy and to study their properties. Our metallicity estimates agree with the scenario of GRBs exploding in low metallicity galaxies",astro-ph,Astrophysics
On the opacity change required to compensate for the revised solar composition,"Recent revisions of the determination of the solar composition have resulted in solar models in marked disagreement with helioseismic inferences. The effect of the composition change on the model is largely caused by the change in the opacity. Thus we wish to determine an intrinsic opacity change that would compensate for the revision of the composition. By comparing models computed with the old and revised composition we determine the required opacity change. Models are computed with the opacity thus modified and used as reference in helioseismic inversions to determine the difference between the solar and model sound speed. An opacity increase varying from around 30 per cent near the base of the convection zone to a few percent in the solar core results in a sound-speed profile, with the revised composition, which is essentially indistinguishable from the original solar model. As a function of the logarithm of temperature this is well represented by a simple cubic fit. The physical realism of such a change remains debatable, however.",astro-ph,Astrophysics
Gamma ray emission and stochastic particle acceleration in galaxy clusters,"FERMI (formely GLAST) will shortly provide crucial information on relativistic particles in galaxy clusters. We discuss non-thermal emission in the context of general calculations in which relativistic particles (protons and secondary electrons due to proton-proton collisions) interact with MHD turbulence generated in the cluster volume during cluster mergers. Diffuse cluster-scale radio emission (Radio Halos) and hard X-rays are produced during massive mergers while gamma ray emission, at some level, is expected to be common in galaxy clusters.",astro-ph,Astrophysics
High Accuracy Imaging Polarimetry with NICMOS,"The ability of NICMOS to perform high accuracy polarimetry is currently hampered by an uncalibrated residual instrumental polarization at a level of 1.2-1.5%. To better quantify and characterize this residual we obtained observations of three polarimetric standard stars at three separate space-craft roll angles. Combined with archival data, these observations were used to characterize the residual instrumental polarization to enable NICMOS to reach its full polarimetric potential. Using these data, we calculate values of the parallel transmission coefficients that reproduce the ground-based results for the polarimetric standards. The uncertainties associated with the parallel transmission coefficients, a result of the photometric repeatability of the observations, dominate the accuracy of p and theta. However, the new coefficients now enable imaging polarimetry of targets with p~1.0% at an accuracy of +/-0.6% and +/-15 degrees.",astro-ph,Astrophysics
Gravitational Wave Recoil Oscillations of Black Holes: Implications for Unified Models of Active Galactic Nuclei,"We consider the consequences of gravitational wave recoil for unified models of active galactic nuclei (AGNs). Spatial oscillations of supermassive black holes (SMBHs) around the cores of galaxies following gravitational wave (GW) recoil imply that the SMBHs spend a significant fraction of time off-nucleus, at scales beyond that of the molecular obscuring torus. Assuming reasonable distributions of recoil velocities, we compute the off-core timescale of (intrinsically type-2) quasars. We find that roughly one-half of major mergers result in a SMBH being displaced beyond the torus for a time of 30 Myr or more, comparable to quasar activity timescales. Since major mergers are most strongly affected by GW recoil, our results imply a deficiency of type 2 quasars in comparison to Seyfert 2 galaxies. Other consequences of the recoil oscillations for the observable properties of AGNs are also discussed.",astro-ph,Astrophysics
Constraining Spinning Dust Parameters with the WMAP Five-Year Data,"We characterize spinning dust emission in the warm ionized medium by comparing templates of Galactic dust and Halpha with the 5-year maps from the Wilkinson Microwave Anisotropy Probe. The Halpha-correlated microwave emission deviates from the thermal bremsstrahlung (free-free) spectrum expected for ionized gas, exhibiting an additional broad bump peaked at ~40 GHz which provides ~20% of the peak intensity. We confirm that the bump is consistent with a modified Draine & Lazarian (1998) spinning dust model, though the peak frequency of the emission is somewhat lower than the 50 GHz previously claimed. This frequency shift results from systematic errors in the large-scale modes of the 3-year WMAP data which have been corrected in the 5-year data release. We show that the bump is not the result of errors in the Halpha template by analyzing regions of high free-free intensity, where the WMAP K-band map may be used as the free-free template. We rule out a pure free-free spectrum for the Halpha-correlated emission at high confidence: ~27sigma for the nearly full-sky fit, even after marginalizing over the CMB cross-correlation bias. We also extend the previous analysis by searching the parameter space of the Draine & Lazarian model but letting the amplitude float. The best fit for reasonable values of the characteristic electric dipole moment and density requires an amplitude factor of ~0.3. This suggests that small PAHs in the warm ionized medium are depleted by a factor of ~3.",astro-ph,Astrophysics
Measuring the Sources of the Intergalactic Ionizing Flux,"We use a wide-field (0.9 square degree) X-ray sample with optical and GALEX ultraviolet observations to measure the contribution of Active Galactic Nuclei (AGNs) to the ionizing flux as a function of redshift. Our analysis shows that the AGN contribution to the metagalactic ionizing background peaks around z=2. The measured values of the ionizing background from the AGNs are lower than previous estimates and confirm that ionization from AGNs is insufficient to maintain the observed ionization of the intergalactic medium (IGM) at z>3. We show that only sources with broad lines in their optical spectra have detectable ionizing flux and that the ionizing flux seen in an AGN is not correlated with its X-ray color. We also use the GALEX observations of the GOODS-N region to place a 2-sigma upper limit of 0.008 on the average ionization fraction fnu(700 A)/fnu(1500 A) for 626 UV selected galaxies in the redshift range z=0.9-1.4. We then use this limit to estimate an upper bound to the galaxy contribution in the redshift range z=0-5. If the z~1.15 ionization fraction is appropriate for higher redshift galaxies, then contributions from the galaxy population are also too low to account for the IGM ionization at the highest redshifts (z>4).",astro-ph,Astrophysics
X-Ray and high energy flares from late internal shocks of gamma-ray bursts,"We study afterglow flares of gamma-ray bursts (GRBs) in the framework of the late internal shock (LIS) model based on a careful description for the dynamics of a pair of shocks generated by a collision between two homogeneous shells,. First, by confronting the model with some fundamental observational features of X-ray flares, we find some constraints on the properties of the pre-collision shells that are directly determined by the central engine of GRBs. Second, high energy emission associated with X-ray flares, which arises from synchrotron self-Compton (SSC) emission of LISs, is investigated in a wide parameter space. The predicted flux of high energy flares may reach as high as $\sim 10^{-8}\rm erg cm^{-2}s^{-1}$, which is likely to be detectable with the Large Area Telescope (LAT) aboard \textit{the Fermi Space Telescope}",astro-ph,Astrophysics
The seismology programme of CoRoT,"We introduce the main lines and specificities of the CoRoT Seismology Core Programme. The development and consolidation of this programme has been made in the framework of the CoRoT Seismology Working Group. With a few illustrative examples, we show how CoRoT data will help to address various problems associated with present open questions of stellar structure and evolution.",astro-ph,Astrophysics
Lopsided Spiral Galaxies,"The light distribution in the disks of many galaxies is non-axisymmetric or `lopsided' with a spatial extent much larger along one half of a galaxy than the other, as in M101. Recent near-IR observations show that lopsidedness is common. The stellar disks in nearly 30 % of galaxies have significant lopsidedness, greater than 10 % measured as the Fourier amplitude of the m=1 component normalized to the average value. This asymmetry is traced particularly well by the atomic hydrogen gas distribution lying in the outer parts. The lopsidedness also occurs in the nuclear regions, where the nucleus is offset with respect to the outer isophotes. The galaxies in a group environment show higher lopsidedness. The origin of lopsidedness could be due to the disk response to a tidally distorted halo, or via gas accretion. The lopsidedness has a large impact on the dynamics of the galaxy, its evolution, the star formation in it, and on the growth of the central black hole and on the nuclear fueling, merging of binary black holes etc. The disk lopsidedness can be used as a diagnostic to study the halo asymmetry. This is an emerging area in galactic structure and dynamics. In this review, the observations to measure the lopsided distribution, as well as the theoretical progress made so far to understand its origin and properties, and the related open problems will be discussed. (abridged).",astro-ph,Astrophysics
Cosmological scenario of stop NLSP with gravitino LSP and the cosmic lithium problem,"The discrepancy on Li^7 and Li^6 abundances between the observational data and the standard Big Bang Nucleosynthesis theory prediction has been a nagging problem in astrophysics and cosmology, given the highly attractive and succesful Big Bang paradigm. One possible solution of this lithium problem is through hadronic decays of a massive metastable particle which alter the primordial element abundances. We explore this possibility using gravitino dark matter framework in which the next lightest supersymmetric particle (NLSP) is typically long-lived. We found that stop NLSP can provide an attractive solution to the lithium problem.",astro-ph,Astrophysics
Input from opacity data in computation of pulsation instability,"Several types of variable stars are found along the HR diagram whose pulsations are driven by the $$-mechanism. Given their nature, the precise ($T_{\rm eff}-L$) domain where these pulsators are located is highly dependent on the value of opacity and on its variation inside the star. We analyze the sensitivity of opacity driven pulsators of spectral-type A and B ($$ Scuti, $$ Cephei and SPB stars) to the opacity tables (OP/OPAL) and to the chemical composition of the stellar matter. We also briefly discuss the effect of opacity on pulsators whose oscillations are not driven by the $$-mechanism, such as $$ Doradus and solar-like stars.",astro-ph,Astrophysics
Particle trajectories and acceleration during 3D fan reconnection,"Context. The primary energy release in solar flares is almost certainly due to magnetic reconnection, making this a strong candidate as a mechanism for particle acceleration. While particle acceleration in 2D geometries has been widely studied, investigations in 3D are a recent development. Two main classes of reconnection regimes at a 3D magnetic null point have been identified: fan and spine reconnection Aims. Here we investigate particle trajectories and acceleration during reconnection at a 3D null point, using a test particle numerical code, and compare the efficiency of the fan and spine regimes in generating an energetic particle population. Methods. We calculated the time evolution of the energy spectra. We discuss the geometry of particle escape from the two configurations and characterise the trapped and escaped populations. Results. We find that fan reconnection is less efficent than spine reconnection in providing seed particles to the region of strong electric field where acceleration is possible. The establishment of a steady-state spectrum requires approximately double the time in fan reconnection. The steady-state energy spectrum at intermediate energies (protons 1 keV to 0.1 MeV) is comparable in the fan and spine regimes. While in spine reconnection particle escape takes place in two symmetric jets along the spine, in fan reconnection no jets are produced and particles escape in the fan plane, in a ribbon-like structure.",astro-ph,Astrophysics
Galaxy evolution from strong lensing statistics: the differential evolution of the velocity dispersion function in concord with the LambdaCDM paradigm,"We study galaxy evolution from z=1 to z=0 as a function of velocity dispersion sigma for galaxies with sigma > 95 km/s based on the measured and Monte Carlo realised local velocity dispersion functions (VDFs) of galaxies and the revised statistical properties of 30 strongly-lensed sources. We assume that the total (luminous plus dark) mass profile of a galaxy is isothermal in the optical region for 0 < z < 1 as suggested by mass modelling of lensing galaxies. For the evolutionary behaviours of the VDFs we find that: (1) the number density of massive (mostly early-type) galaxies with sigma > 200 km/s evolves differentially in the way that the number density evolution is greater at a higher velocity dispersion; (2) the number density of intermediate and low mass early-type galaxies (95 km/s < sigma < 200 km/s) is nearly constant; (3) the late-type VDF transformed from the Monte Carlo realised circular velocity function is consistent with no evolution in its shape or integrated number density consistent with galaxy survey results. These evolutionary behaviours of the VDFs are strikingly similar to those of the dark halo mass function (DMF) from N-body simulations and the stellar mass function (SMF) predicted by recent semi-analytic models of galaxy formation under the current LambdaCDM hierarchical structure formation paradigm. Interestingly, the VDF evolutions appear to be qualitatively different from ``stellar mass-downsizing'' evolutions obtained by many galaxy surveys. The coevolution of the DMF, the VDF and the SMF is investigated in quantitative detail in a following paper. We consider several possible systematic errors for the lensing analysis and find that they are not likely to alter the conclusions.(abridged)",astro-ph,Astrophysics
Effects of Rotation on Standing Accretion Shock Instability in Nonlinear Phase for Core-Collapse Supernovae,"We studied the effects of rotation on standing accretion shock instability (SASI) by performing three-dimensional hydrodynamics simulations. Taking into account a realistic equation of state and neutrino heating/cooling, we prepared a spherically symmmetric and steady accretion flow through a standing shock wave onto a proto-neutron star (PNS). When the SASI entered the nonlinear phase, we imposed uniform rotation on the flow advecting from the outer boundary of the iron core, whose specific angular momentum was assumed to agree with recent stellar evolution models. Using spherical harmonics in space and Fourier decompositions in time, we performed mode analysis of the nonspherical deformed shock wave to observe rotational effects on the SASI in the nonlinear phase. We found that rotation imposed on the axisymmetric SASI did not make any spiral modes and hardly affected sloshing modes, except for steady l=2, m=0 modes. In contrast, rotation imposed on the non-axisymmetric flow increased the amplitude of spiral modes so that some spiral flows accreting on the PNS were more clearly formed inside the shock wave than without rotation. The amplitudes of spiral modes increased significantly with rotation in the progressive direction.",astro-ph,Astrophysics
The total mass of the early-type galaxy NGC 4649 (M60),In this paper we analyze the problem of the total mass and the total mass-to-light ratio of the early-type galaxy NGC 4649 (M60). We have used two independent techniques: the X-ray methodology which is based on the temperature of the X-ray halo of NGC 4649 and the tracer mass estimator (TME) which uses globular clusters (GCs) observed in this galaxy. We calculated the mass in Newtonian and MOdified Newtonian Dynamics (MOND) approaches and found that interior to 3 effective radii (Re) there is no need for large amounts of dark matter. Beyond 3Re dark matter starts to play important dynamical role. We also discuss possible reasons for the discrepancy between the estimates of the total mass based on X-rays and TME in the outer regions of NGC 4649.,astro-ph,Astrophysics
The Impact of Uncertainties in Reaction Q-values on Nucleosynthesis in Type I X-Ray Bursts,"Nucleosynthesis in Type I X-ray bursts may involve up to several thousand nuclear processes. The majority of these processes have only been determined theoretically due to the lack of sufficient experimental information. Accurate reaction Q-values are essential for reliable theoretical estimates of reaction-rates. Those reactions with small Q-values (< 1 MeV) are of particular interest in these environments as they may represent waiting points for a continuous abundance flow towards heavier-mass nuclei. To explore the nature of these waiting points, we have performed a comprehensive series of post-processing calculations which examine the sensitivity of nucleosynthesis in Type I X-ray bursts to uncertainties in reaction Q-values. We discuss and list the relatively few critical masses for which measurements could better constrain the results of our studies. In particular, we stress the importance of measuring the mass of 65As to obtain an experimental Q-value for the 64Ge(p,gamma)65As reaction.",astro-ph,Astrophysics
"Giant cavities, cooling and metallicity substructure in Abell 2204","We present results from deep Chandra and XMM-Newton observations of the relaxed X-ray luminous galaxy cluster Abell 2204. We detect metallicity inhomogeneities in the intracluster medium on a variety of distance scales, from a ~12 kpc enhancement containing a few times 10^7 Msun of iron in the centre, to a region at 400 kpc radius with an excess of a few times 10^9 Msun. Subtracting an average surface brightness profile from the X-ray image yields two surface brightness depressions to the north and south of the cluster. Their morphology is similar to the cavities observed in cluster cores, but they have radii of 240 kpc and 160 kpc and have a total enthalpy of 2x10^62 erg. If they are fossil radio bubbles, their buoyancy timescales imply a total mechanical heating power of 5x10^46 erg/s, the largest such bubble heating power known. More likely, they result from the accumulation of many past bubbles. Energetically this is more feasible, as the enthalpy of these regions could combat X-ray cooling in this cluster to 500 kpc radius for around 2 Gyr. The core of the cluster also contains five to seven ~4 kpc radius surface brightness depressions that are not associated with the observed radio emission. If they are bubbles generated by the nucleus, they are too small to balance cooling in the core by an order of magnitude. However if the radio axis is close to the line of sight, projection effects may mask more normal bubbles. Using RGS spectra we detect a FeXVII line. Spectral fitting reveals temperatures down to ~0.7 keV; the cluster therefore shows a range in X-ray temperature of at least a factor of 15. The quantity of low temperature gas is consistent with a mass deposition rate of 65 Msun/yr.",astro-ph,Astrophysics
Cometary masses derived from non-gravitational forces,"We compute masses and densities for ten periodic comets with known sizes: 1P/Halley, 2P/Encke, 6P/d'Arrest, 9P/Tempel 1, 10P/Tempel 2, 19P/Borrelly, 22P/Kopff, 46P/Wirtanen, 67P/Churyumov-Gerasimenko and 81P/Wild 2. The method follows the one developed by Rickman and colleagues (Rickman 1986, 1989; Rickman et al. 1987), which is based on the gas production curve and on the change in the orbital period due to the non-gravitational force. The gas production curve is inferred from the visual lightcurve. We found that the computed masses cover more than three orders of magnitude: ~(0.3 - 400)*10^12 kg. The computed densities are in all cases very low (<= 0.8 g cm^-3), with an average value of 0.4 g cm^-3, in agreement with previous results and models of the cometary nucleus depicting it as a very porous object. The computed comet densities turn out to be the lowest among the different populations of solar system minor bodies, in particular as compared to those of near-Earth asteroids (NEAs). We conclude that the model applied in this work, in spite of its simplicity (as compared to more sophisticated thermophysical models applied to very few comets), is useful for a statistical approach to the mean density of the cometary nuclei. However, we cannot assess from this simple model if there is a real dispersion among the bulk densities of comets that could tell us about differences in physical structure (porosity) and/or chemical composition.",astro-ph,Astrophysics
Bulge-Disk Decompositions and Structural Bimodality of Ursa Major Cluster Spiral Galaxies,"We present bulge and disk (B/D) decompositions of existing K'-band surface brightness profiles for 65 Ursa Major cluster spiral galaxies. This improves upon the disk-only fits of Tully et al. (1996). The 1996 disk fits were used by Tully & Verheijen (1997) for their discovery of the bimodality of structural parameters in the UMa cluster galaxies. It is shown that our new 1D B/D decompositions yield disk structural parameters that differ only slightly from the basic fits of Tully et al. and evidence for structural bimodality of UMa galaxies is maintained. Our B/D software for the decomposition of 1D surface brightness profiles of galaxies uses a non-linear minimization scheme to recover the best fitting Sersic bulge and exponential disk while accounting for the possible presence of a compact nucleus and spiral arms and for the effects of seeing and disk truncations. In agreement with Tully & Verheijen, we find that the distribution of near-infrared disk central surface brightnesses is bimodal with an F-test confidence of 80%. There is also strong evidence for a local minimum in the luminosity function at M_K' ~ -22. A connection between the brightness bimodality and a dynamical bimodality, based on new HI line widths, is identified. The B/D parameters are presented in an Appendix.",astro-ph,Astrophysics
LISA detection of massive black hole binaries: imprint of seed populations and of exterme recoils,"All the physical processes involved in the formation, merging, and accretion history of massive black holes along the hierarchical build--up of cosmic structures are likely to leave an imprint on the gravitational waves detectable by future space--borne missions, such as LISA. We report here the results of recent studies, carried out by means of dedicated simulations of black hole build--up, aiming at understanding the impact on LISA observations of two ingredients that are crucial in every massive black hole formation scenario, namely: (i) the nature and abundance of the first black hole seeds and (ii) the large gravitational recoils following the merger of highly spinning black holes. We predict LISA detection rates spanning two order of magnitude, in the range 3-300 events per year, depending on the detail of the assumed massive black hole seed model. On the other hand, large recoil velocities do not dramatically compromise the efficiency of LISA observations. The number of detections may drop substantially (by ~60%), in scenarios characterized byabundant light seeds, but if seeds are already massive and/or relatively rare, the detection rate is basically unaffected.",astro-ph,Astrophysics
"A spectroscopic survey of dwarf galaxies in the Coma Cluster: Stellar populations, environment and downsizing","We investigate the stellar populations in a sample of 89 faint (M*+2 to M*+4) red galaxies in the Coma cluster, using high S/N spectroscopy from the MMT. Our sample is drawn from two 1-degree fields, one centred on the cluster core and the other located a degree to the south west of the cluster centre. For a comparison sample we use published high-S/N data for red-sequence galaxies in the Shapley Supercluster. We use state-of-the-art stellar population models to infer the SSP-equivalent age and metallicity (Fe/H) for each galaxy, as well as the abundances of the light elements Mg, Ca, C and N. The ages of the Coma dwarfs span a wide range from <2 Gyr to >10 Gyr, with a strong environmental dependence. The oldest galaxies are found only in the core, while most of the galaxies in the outer south-west field have ages ~3 Gyr. The galaxies have a metallicity range -1.0 < [Fe/H] < 0.0, and follow the same age-metallicity-mass plane as high-mass galaxies, but with increased intrinsic scatter. The Mg/Fe abundance ratios are on average slightly super-solar, and span a range -0.1 < [Mg/Fe] < +0.4. The highest Mg enhancements are found only in the cluster core, while solar ratios predominate in the outskirts. Depending on the assumed star-formation history (quenched versus burst-dominated), the number of dwarf galaxies on the red sequence in the Coma core has doubled since z~0.4-0.7. These estimates bracket the red-sequence growth timescales found by direct studies of distant clusters. In the south-west field, the red sequence was established only at z~0.1-0.2. Our observations confirm previous indications of very recently quenched star formation in this part of the cluster. Our results support the picture in which much of the cluster passive dwarf population was generated by environment-driven transformation of infalling late-type galaxies.",astro-ph,Astrophysics
"Galaxy Interactions, Star Formation History, and Bulgeless Galaxies","Hierarchical Lambda CDM models provide a successful paradigm for the growth of dark matter on large scales, but they face important challenges in predicting how the baryonic components of galaxies evolve. I present constraints on two aspects of this evolution: (1) The interaction history of galaxies over the last 7 Gyr and the impact of interactions on their star formation properties, based on Jogee et al. (2008a,b); (2) Constraints on the origin of bulges in hierarchical models and the challenge posed in accounting for galaxies with low bulge-to-total ratios, based on Weinzirl, Jogee, Khochar, Burkert, and Kormendy (2008, hereafter WJKBK08)",astro-ph,Astrophysics
New Primordial-Magnetic-Field Limit from The Latest LIGO S5 data,"Since the energy momentum tensor of a magnetic field always contains a spin-2 component in its anisotropic stress, stochastic primordial magnetic field (PMF) in the early universe must generate stochastic gravitational wave (GW) background. This process will greatly affect the relic gravitational wave (RGW), which is one of major scientific goals of the laser interferometer GW detections. Recently, the fifth science (S5) run of laser interferometer gravitational-wave observatory (LIGO) gave a latest upper limit $_{GW}<6.9\times10^{-6}$ on the RGW background. Utilizing this upper limit, we derive new PMF Limits: for a scale of galactic cluster $=1$ Mpc, the amplitude of PMF, that produced by the electroweak phase transition (EPT), has to be weaker than $B_ \leq 4\times 10^{-7}$ Gauss; for a scale of supercluster $=100$ Mpc, the amplitude of PMF has to be weaker than $B_ \leq 9\times 10^{-11}$ Gauss. In this manner, GW observation has potential to make interesting contributions to the study of primordial magnetic field.",astro-ph,Astrophysics
Suzaku observations of metallicity distribution in the intracluster medium of the NGC 5044 group,"The metallicity distribution in the intracluster medium of the NGC 5044 group was studied up to 0.3 r_180 using the XIS instrument on board the Suzaku satellite. Abundances of O, Mg, Si, S, and Fe were measured with high accuracy. The region within a radius of 0.05 r_180 from the center shows approximately solar abundances of Mg, Si, S, and Fe, while the O/Fe ratio is about 0.5--0.6 in solar units. In the outer region, the Fe abundance gradually drops to 0.3 solar. Radial abundance profiles of Mg, Si and S are similar to that of Fe, while that of O seems to be flatter. At r>0.05 r_180, the mass density profile of O differs from that of Fe, showing a shoulder-like structure that traces the luminosity density profile of galaxies. The mass-to-light ratios for O and Fe in NGC 5044 are one of the largest among groups of galaxies, but they are still smaller than those in rich clusters. These abundance features probably reflect the metal enrichment history of this relaxed group hosting a giant elliptical galaxy in the center.",astro-ph,Astrophysics
Explosive Nucleosynthesis of Weak r-Process Elements in Extremely Metal-Poor Core-Collapse Supernovae,"There have been attempts to fit the abundance patterns of extremely metal-poor stars with supernova nucleosynthesis models for the lighter elements than Zn. On the other hand, observations have revealed that the presence of EMP stars with peculiarly high ratio of ""weak r-process elements"" Sr, Y and Zr. Although several possible processes were suggested for the origin of these elements, the complete solution for reproducing those ratios is not found yet. In order to reproduce the abundance patterns of such stars, we investigate a model with neutron rich matter ejection from the inner region of the conventional mass-cut. We find that explosive nucleosynthesis in a high energy supernova (or ""hypernova"") can reproduce the high abundances of Sr, Y and Zr but that the enhancements of Sr, Y and Zr are not achieved by nucleosynthesis in a normal supernova. Our results imply that, if these elements are ejected from a normal supernova, nucleosynthesis in higher entropy flow than that of the supernova shock is required.",astro-ph,Astrophysics
The Spectral Energy Distributions of Red 2MASS AGN,"We present infrared (IR) to X-ray spectral energy distributions (SEDs) for 44 red AGN selected from the 2MASS survey on the basis of their red J-K$_S$ color (>2 mag) and later observed by Chandra. In comparison with optically-, radio-, and X-ray selected AGN, their median SEDs are red in the optical and near-IR with little/no blue bump. It thus seems that near-IR color selection isolates the reddest subset of AGN that can be classified optically. The shape of the SEDs is generally consistent with modest absorption by gas (in the X-ray) and dust (in the optical-IR). The levels of obscuration, estimated from X-rays, far-IR and our detailed optical/near-IR color modeling are all consistent implying N_H < few*10^{22} cm^{-2}. We present SED models that show how the AGN optical/near-IR colors change due to differing amounts of reddening, AGN to host galaxy ratio, redshift and scattered light emission and apply them to the sources in the sample. We find that the 2MASS AGN optical color, B-R, and to a lesser extent the near-IR color, J-K$_S$, are strongly affected by reddening, host galaxy emission, redshift, and in few, highly polarized objects, also by scattered AGN light. The obscuration/inclination of the AGN allows us to see weaker emission components which are generally swamped by the AGN.",astro-ph,Astrophysics
The observation of Extensive Air Showers from Space,We summarise some basic issues relevant to the optimisation and design of space-based experiments for the observation of the Extensive Air Showers produced by Ultra-High Energy Cosmic Particles interacting with the atmosphere. A number of basic relations is derived and discussed with a twofold goal: defining requirements for the experimental apparatus and estimating the exptected performance.,astro-ph,Astrophysics
PCA of the spectral energy distribution and emission line properties of red 2MASS AGN,"We analyze the spectral energy distributions (SEDs) and emission line properties of the red (J-K$_S$ > 2) 2MASS AGN observed by Chandra using principle component analysis. The sample includes 44 low redshift AGN with low or moderate obscuration (N_H < 10^{23} cm^{-2}) as indicated by X-rays and SED modeling. The obscuration of the AGN allows us to see weaker emission components (host galaxy emission, AGN scattered light) which are usually outshone by the AGN. The first four eigenvectors explain 70% of the variance in the sample. The dominant cause of variance in the sample (eigenvector 1) is the L/Ledd ratio strengthened by intrinsic absorption. Eigenvector 2 is related to host galaxy (relative to the observed AGN) emission and eigenvectors 3 and 4 distinguish between two sources of obscuration: host galaxy absorption and circumnuclear absorption. Although our analysis is consistent with unification schemes where inclination dependent obscuration is important in determining the AGN SEDs, the L/Ledd ratio is the most important factor, followed by host galaxy emission.",astro-ph,Astrophysics
Solar surface emerging flux regions: a comparative study of radiative MHD modeling and Hinode SOT observations,"We present results from numerical modeling of emerging flux regions on the solar surface. The modeling was carried out by means of 3D radiative MHD simulations of the rise of buoyant magnetic flux tubes through the convection zone and into the photosphere. Due to the strong stratification of the convection zone, the rise results in a lateral expansion of the tube into a magnetic sheet, which acts as a reservoir for small-scale flux emergence events at the scale of granulation. The interaction of the convective downflows and the rising magnetic flux undulates it to form serpentine field lines emerging into the photosphere. Observational characteristics including the pattern of emerging flux regions, the cancellation of surface flux and associated high speed downflows, the convective collapse of photospheric flux tubes, the appearance of anomalous darkenings, the formation of bright points and the possible existence of transient kilogauss horizontal fields are discussed in the context of new observations from the Hinode Solar Optical Telescope. Implications for the local helioseismology of emerging flux regions are also discussed.",astro-ph,Astrophysics
LISA as a dark energy probe,"Recently it was shown that the inclusion of higher signal harmonics in the inspiral signals of binary supermassive black holes (SMBH) leads to dramatic improvements in parameter estimation with the Laser Interferometer Space Antenna (LISA). In particular, the angular resolution becomes good enough to identify the host galaxy or galaxy cluster, in which case the redshift can be determined by electromagnetic means. The gravitational wave signal also provides the luminosity distance with high accuracy, and the relationship between this and the redshift depends sensitively on the cosmological parameters, such as the equation-of-state parameter $w=p_{\rm DE}/_{\rm DE}$ of dark energy. With a single binary SMBH event at $z < 1$ having appropriate masses and orientation, one would be able to constrain $w$ to within a few percent. We show that, if the measured sky location is folded into the error analysis, the uncertainty on $w$ goes down by an additional factor of 2-3, leaving weak lensing as the only limiting factor in using LISA as a dark energy probe.",astro-ph,Astrophysics
Accurate universal models for the mass accretion histories and concentrations of dark matter halos,"A large amount of observations have constrained cosmological parameters and the initial density fluctuation spectrum to a very high accuracy. However, cosmological parameters change with time and the power index of the power spectrum varies with mass scale dramatically in the so-called concordance Lambda CDM cosmology. Thus, any successful model for its structural evolution should work well simultaneously for various cosmological models and different power spectra. We use a large set of high-resolution N-body simulations of a variety of structure formation models (scale-free, standard CDM, open CDM, and Lambda CDM) to study the mass accretion histories (MAHs), the mass and redshift dependence of concentrations and the concentration evolution histories of dark matter halos. We find that there is significant disagreement between the much-used empirical models in the literature and our simulations. According to two simple but tight correlations we find from the simulation results, we develop new empirical models for both the MAHs and the concentration evolution histories of dark matter halos, and the latter can also be used to predict the mass and redshift dependence of halo concentrations. These models are accurate and universal: the same set of model parameters works well for different cosmological models and for halos of different masses at different redshifts and the model predictions are highly accurate even when the histories are traced to very high redshift. These models are also simple and easy to implement. A web calculator and a user-friendly code to make the relevant calculations are available from http://www.shao.ac.cn/dhzhao/mandc.html . We explain why Lambda CDM halos on nearly all mass scales show two distinct phases in their evolution histories.",astro-ph,Astrophysics
LISA parameter estimation using numerical merger waveforms,"Recent advances in numerical relativity provide a detailed description of the waveforms of coalescing massive black hole binaries (MBHBs), expected to be the strongest detectable LISA sources. We present a preliminary study of LISA's sensitivity to MBHB parameters using a hybrid numerical/analytic waveform for equal-mass, non-spinning holes. The Synthetic LISA software package is used to simulate the instrument response and the Fisher information matrix method is used to estimate errors in the parameters. Initial results indicate that inclusion of the merger signal can significantly improve the precision of some parameter estimates. For example, the median parameter errors for an ensemble of systems with total redshifted mass of one million Solar masses at a redshift of one were found to decrease by a factor of slightly more than two for signals with merger as compared to signals truncated at the Schwarzchild ISCO.",astro-ph,Astrophysics
Gamma Ray Astronomy and the Origin of Galactic Cosmic Rays,"Diffusive shock acceleration operating at expanding supernova remnant shells is by far the most popular model for the origin of galactic cosmic rays. Despite the general consensus received by this model, an unambiguous and conclusive proof of the supernova remnant hypothesis is still missing. In this context, the recent developments in gamma ray astronomy provide us with precious insights into the problem of the origin of galactic cosmic rays, since production of gamma rays is expected both during the acceleration of cosmic rays at supernova remnant shocks and during their subsequent propagation in the interstellar medium. In particular, the recent detection of a number of supernova remnants at TeV energies nicely fits with the model, but it still does not constitute a conclusive proof of it, mainly due to the difficulty of disentangling the hadronic and leptonic contributions to the observed gamma ray emission. In this paper, the most relevant cosmic-ray-related results of gamma ray astronomy are briefly summarized, and the foreseeable contribution of future gamma ray observations to the final solution of the problem of cosmic ray origin is discussed.",astro-ph,Astrophysics
"Unified Rotation Curve of the Galaxy -- Decomposition into de Vaucouleurs Bulge, Disk, Dark Halo, and the 9-kpc Rotation Dip --","We present a unified rotation curve of the Galaxy re-constructed from the existing data by re-calculating the distances and velocities for a set of galactic constants R_0=8 kpc and V_0=200 km/s. We decompose it into a bulge with de Vaucouleurs-law profile of half-mass scale radius 0.5 kpc and mass 1.8 x 10^{10}M_{sun}, an exponential disk of scale radius 3.5 kpc of 6.5 x 10^{10}M_{sun}, and an isothermal dark halo of terminal velocity 200 km/s. The r^{1/4}-law fit was obtained for the first time for the Milky Way's rotation curve. After fitting by these fundamental structures, two local minima, or the dips, of rotation velocity are prominent at radii 3 and 9 kpc. The 3-kpc dip is consistent with the observed bar. It is alternatively explained by a massive ring with the density maximum at radius 4 kpc. The 9-kpc dip is clearly exhibited as the most peculiar feature in the galactic rotation curve. We explain it by a massive ring of amplitude as large as 0.3 to 0.4 times the disk density with the density peak at radius 11 kpc. This great ring may be related to the Perseus arm, while no peculiar feature of HI-gas is associated.",astro-ph,Astrophysics
INTEGRAL monitoring of unusually long X-ray bursts,"X-ray bursts are thermonuclear explosions on the surface of accreting neutron stars in low mass X-ray binaries. As most of the known X-ray bursters are frequently observed by INTEGRAL, an international collaboration have been taking advantage of its instrumentation to specifically monitor the occurrence of exceptional burst events lasting more than ~10 minutes. Half of the so-called intermediate long bursts registered so far have been observed by INTEGRAL. The goal is to derive a comprehensive picture of the relationship between the nuclear ignition processes and the accretion states of the system leading up to such long bursts. Depending on the composition of the accreted material, these bursts may be explained by either the unstable burning of a large pile of mixed hydrogen and helium, or the ignition of a thick pure helium layer. Intermediate long bursts are particularly expected to occur at very low accretion rates and make possible to study the transition from a hydrogen-rich bursting regime to a pure helium regime.",astro-ph,Astrophysics
The peculiar molecular envelope around the post-AGB star IRAS 08544--4431,"Circumbinary disks have been hypothesized to exist around a number of binary post-AGB stars. Although most of the circumbinary disks have been inferred through the near IR excess, a few of them are strong emitters of molecular emission. Here we present high angular resolution observations of the emission of $^{12}$CO and its isotopomer $^{13}$CO J=2--1 line from the circumstellar envelope around the binary post-AGB star IRAS 08544$-$4431, which is one of the most prominent members of this class of objects. We find that the envelope is resolved in our observations and two separate components can be identified: (a) a central extended and strong component with very narrow linewidth between 2 - 6 \kms; (b) a weak bipolar outflow with expansion velocity up to 8 \kms. The central compact component possesses low and variable $^{12}$CO/$^{13}$CO J=2--1 line ratio, indicating optically thick emission of the main isotope. We estimate a molecular gas mass of 0.0047 M$_\odot$ for this component based on the optically thinner $^{13}$CO J=2--1 line. We discuss the relation of the molecular envelope and the circumbinary disk inferred from near IR excess and compare with other known cases where the distribution of molecular gas has been imaged at high angular resolution.",astro-ph,Astrophysics
VLT/FORS1 spectrophotometry of the first planetary nebula discovered in the Phoenix dwarf galaxy,"Context: A planetary nebula (PN) candidate was discovered during FORS imaging of the Local Group dwarf galaxy Phoenix. Aims: Use this PN to complement abundances from red-giant stars. Methods: FORS spectroscopy was used to confirm the PN classification. Empirical methods and photoionization modeling were used to derive elemental abundances from the emission line fluxes and to characterize the central star. Results: For the elements deemed most reliable for measuring the metallicity of the interstellar medium (ISM) from which the PN formed, [O/H] ~ -0.46 and [Ar/H] ~ -1.03. [O/H] has lower measurement errors but greater uncertainties due to the unresolved issue of oxygen enrichment in the PN precursor star. Conclusions: Earlier than 2 Gyr ago (the lower limit of the derived age for the central star) the ISM had Z = 0.002--0.008, a range slightly more metal-rich than the one provided by stars. Comparing our PN-to-stellar values to surveys for other dwarf Local Group galaxies, Phoenix appears as an outlier.",astro-ph,Astrophysics
Massive star formation in Wolf-Rayet galaxies: I. Optical and NIR photometric results,"(Abridged) We have performed a comprehensive multiwavelength analysis of a sample of 20 starburst galaxies that show the presence of a substantial population of massive stars. The main aims are the study of the massive star formation and stellar populations in these galaxies, and the role that interactions with or between dwarf galaxies and/or low surface companion objects have in triggering the bursts. We completed new deep optical and \NIR\ broad-band images, as well as the new continuum-subtracted H$$ maps, of our sample of Wolf-Rayet galaxies. We analyze the morphology of each system and its surroundings and quantify the photometric properties of all important objects. All data were corrected for both extinction and nebular emission using our spectroscopic data. The age of the most recent star-formation burst is estimated and compared with the age of the underlying older low-luminosity population. The \Ha-based star-formation rate, number of O7V equivalent stars, mass of ionized gas, and mass of the ionizing star cluster are also derived. We found interaction features in many (15 up to 20) of the analyzed objects, which were extremely evident in the majority. We checked that the correction for nebular emission to the broad-band filter fluxes is important in compact objects and/or with intense nebular emission to obtain realistic colors and compare with the predictions of evolutionary synthesis models. The estimate of the age of the most recent star-formation burst is derived consistently. With respect to the results found in individual objects, we remark the strong \Ha\ emission found in IRAS 08208+2816, UM 420, and SBS 0948+532, the detection of a double-nucleus in SBS 0926+606A, a possible galactic wind in Tol 9, and one (two?) nearby dwarf star-forming galaxies surrounding Tol 1457-437.",astro-ph,Astrophysics
Multi-Satellite Observations of Cygnus X-1 to Study the Focused Wind and Absorption Dips,"High-mass X-ray binary systems are powered by the stellar wind of their donor stars. The X-ray state of Cygnus X-1 is correlated with the properties of the wind which defines the environment of mass accretion. Chandra-HETGS observations close to orbital phase 0 allow for an analysis of the photoionzed stellar wind at high resolution, but because of the strong variability due to soft X-ray absorption dips, simultaneous multi-satellite observations are required to track and understand the continuum, too. Besides an earlier joint Chandra and RXTE observation, we present first results from a recent campaign which represents the best broad-band spectrum of Cyg X-1 ever achieved: On 2008 April 18/19 we observed this source with XMM-Newton, Chandra, Suzaku, RXTE, INTEGRAL, Swift, and AGILE in X- and gamma-rays, as well as with VLA in the radio. After superior conjunction of the black hole, we detect soft X-ray absorption dips likely due to clumps in the focused wind covering >95 % of the X-ray source, with column densities likely to be of several 10^23 cm^-2, which also affect photon energies above 20 keV via Compton scattering.",astro-ph,Astrophysics
The Science Impact of Astronomy PhD Granting Departments in the United States,"The scientific impact of the research of 36 astronomy PhD granting departments is measured and ranked here. Because of the complex nature of Universities, this study looks at the Universities in two ways; first analyzing the impact of the published work over a 10 year period of the Department which grants the PhD and; second, looking at the impact of the published work as a whole including Laboratories, Centers, and Facilities. The Universities considered in the study are drawn from the 1992 NRC study on Programs of Research, Doctorate in Astrophysics and Astronomy with three Universities added. Johns Hopkins, Michigan State, and Northwestern all host substantial astronomical research within their Departments of Physics and Astronomy and so are included here. The first method of measuring impact concentrates on tenured and tenured track faculty, with the top quartile being 1. Caltech, 2. UC Santa Cruz, 3. Princeton, 4. Harvard, 5. U Colorado, Boulder, 6. SUNY, Stony Brook, 7. Johns Hopkins, 8. Penn State, and 9. U Michigan, Ann Arbor. The second method additionally includes ""soft money"" scientists in research and adjunct faculty positions, with the top quartile being 1. UC Santa Cruz, 2. Princeton, 3. Johns Hopkins 4. Penn State, 5. SUNY Stony Brook, 6. U Michigan, Ann Arbor, 7. New Mexico State, 8. UMass, Amherst, and 9. U Virginia. Both methods reveal important aspects of Universities, representing both the depth and the breadth of the science available at the University. Finally, a comparison is made of the total articles published in the 10 year period, both from the departments alone and from the larger universities. Three Universities have both impact index in the top quartile, and have more than 1000 publications in a decade; UC at Santa Cruz, Princeton, and Johns Hopkins.",astro-ph,Astrophysics
The Environmental Influence on the Evolution of Local Galaxies,"The results of an Halpha photometric survey of 30 dwarf galaxies of various morphologies in the Centaurus A and Sculptor groups are presented. Of these 30, emission was detected in 13: eight are of late-type, two are early-type and three are of mixed-morphology. The typical flux detection limit of 2e-16 erg s-1 cm-2, translates into a Star Formation Rate (SFR) detection limit of 4e-6 M_sol yr-1 . In the light of these results, the morphology-density relation is reexamined: It is shown that, despite a number of unaccounted parameters, there are significant correlations between the factors determining the morphological type of a galaxy and its environment. Dwarf galaxies in high density regions have lower current SFR and lower neutral gas content than their low density counterparts, confirming earlier results from the Local Group and other denser environments. The effect of environment is also seen in the timescale formed from the ratio of blue luminosity to current SFR - dwarfs in higher density environments have larger values, indicating relatively higher past average SFR. The influence of environment extends very far and no dwarfs from our sample can be identified as 'field' objects.",astro-ph,Astrophysics
The role of luminous substructure in the gravitational lens system MG 2016+112,"MG 2016+112 is a quadruply imaged lens system with two complete images A and B and a pair of merging partial images in region C as seen in the radio. The merging images are found to violate the expected mirror symmetry. This indicates an astrometric anomaly which could only be of gravitational origin and could arise due to substructure in the environment or line-of-sight of the lens galaxy. We present new high resolution multi-frequency VLBI observations at 1.7, 5 and 8.4 GHz. Three new components are detected in the new VLBI imaging of both the lensed images A and B. The expected opposite parity of the lensed images A and B was confirmed due to the detection of non-collinear components. Furthermore, the observed properties of the newly detected components are inconsistent with the predictions of previous mass models. We present new scenarios for the background quasar which are consistent with the new observations. We also investigate the role of the satellite galaxy situated at the same redshift as the main lensing galaxy. Our new mass models demonstrate quantitatively that the satellite galaxy is the primary cause of the astrometric anomaly found in region C. The detected satellite is consistent with the abundance of subhaloes expected in the halo from cold dark matter (CDM) simulations. However, the fraction of the total halo mass in the satellite as computed from lens modeling is found to be higher than that predicted by CDM simulations.",astro-ph,Astrophysics
Quasinormal resonances of near-extremal Kerr-Newman black holes,"We study analytically the fundamental resonances of near-extremal, slowly rotating Kerr-Newman black holes. We find a simple analytic expression for these black-hole quasinormal frequencies in terms of the black-hole physical parameters: omega=m Omega-2i pi T(l+1+n), where T and Omega are the temperature and angular velocity of the black hole. The mode parameters l and m are the spheroidal harmonic index and the azimuthal harmonic index of a co-rotating mode, respectively. This analytical formula is valid in the regime Im omega << Re omega <<1/M, where M is the black-hole mass.",astro-ph,Astrophysics
Solar system tests do not rule out 1/R gravity,"We argue that Solar system tests do not rule out 1/R gravity at least due to the reason addressed in Phys. Rev. D 74 (2006) 121501 [astro-ph/0610483] (ref. [1]) and subsequent published papers.
  Ref. [1] has not only modified the Einstein-Hilbert action but also has changed the boundary conditions since they altered the equations of motion. In Einstein-Hilbert action equations are second order, so the fall off of the fields suffices to single out a unique solution. In 1/R gravity the equations are fourth order, so we should impose additional boundary conditions. Perhaps the boundary condition we must impose is that the abrupt change in the second derivative of the metric near the surface of the Sun remains intact by adding `1/R' corrections to the equations of motion. The solution of 1/R gravity with this boundary condition remains consistent with the solar system tests.
  Ref. [1] assumes that as soon as they perturbatively modified the equations then the Ricci scalar becomes smooth on the surface of the Sun. This assumption is simply wrong because the boundary conditions and equations of motions are two different entities.",astro-ph,Astrophysics
Collision Dynamics and Rung Formation of Non-Abelian Vortices,"We investigate the collision dynamics of two non-Abelian vortices and find that, unlike Abelian vortices, they neither reconnect themselves nor pass through each other, but create a rung between them in a topologically stable manner. Our predictions are verified using the model of the cyclic phase of a spin-2 spinor Bose-Einstein condensate.",astro-ph,Astrophysics
High resolution in z-direction: The simulation of disc-bulge-halo galaxies using the particle-mesh code SUPERBOX,"SUPERBOX is known as a very efficient particle-mesh code with highly-resolving sub-grids. Nevertheless, the height of a typical galactic disc is small compared to the size of the whole system. Consequently, the numerical resolution in z-direction, i.e. vertically with respect to the plane of the disc, remains poor. Here, we present a new version of SUPERBOX that allows for a considerably higher resolution along z. The improved code is applied to investigate disc heating by the infall of a galaxy satellite. We describe the improvement and communicate our results. As an important application we discuss the disruption of a dwarf galaxy within a disc-bulge-halo galaxy that consists of some 10^6 particles.",astro-ph,Astrophysics
INTEGRAL observes the 2007 outburst of the Be transient SAX J2103.5+4545,"We performed a detailed study of the 2007 outburst of the 352s pulsar SAXJ2103.5+4545, a Be/X-ray transient observed by INTEGRAL, to study its spectral and temporal properties during the evolution of the outburst. SAXJ2103.5+4545 was observed with IBIS/ISGRI from 25 to 27 April 2007 and from 6 to 8 May 2007. The 20-100keV spectrum is well described by a bremsstrahlung model with a temperature kT = 24keV. The pulse profiles are variable with time and energy. A pulse period derivative of pdot = -3.4E-7 s/s has been observed during the outburst. Instead, a spin-down of pdot = 5.5E-9 s/s is observed between the 2007 outburst reported here and the previous one occurred in December 2004. This is the largest spin-down measured for SAXJ2103.5+4545 since its discovery. We estimate a neutron star magnetic field in the range (1.6-3)E13 G using the Ghosh & Lamb torque model.",astro-ph,Astrophysics
Supergiant Fast X-ray Transients: interpretation of archival INTEGRAL data,"INTEGRAL monitoring of the Galactic Plane in the last 5 years revealed a new subclass of High Mass X-ray Binaries (HMXBs), the Supergiant Fast X-ray Transients (SFXTs). They display flares lasting from minutes to hours, with peak luminosity of 1E36-1E37 erg/s and a frequent long term flaring activity reaching an X-ray luminosity of 1E33-1E34 erg/s, as recently detected by the Swift satellite. The quiescent level is around 1E32 erg/s. We performed a systematic re-analysis of archival INTEGRAL data of four SFXTs: IGRJ16479-4514, XTEJ1739-302, IGRJ17544-2619, IGRJ18410-0535. This led to the discovery of previously unnoticed outbursts from IGRJ16479-4514 and IGRJ17544-2619. We discuss these results in the framework of the different structure of the supergiant wind proposed to explain the outburst from this new class of sources.",astro-ph,Astrophysics
Jet-BLR connection in the radio galaxy 3C 390.3,"Variations of the optical continuum emission in the radio galaxy 3C 390.3 are compared to the properties of radio emission from the compact, sub-parsec-scale jet in this object. We showed that very long-term variations of optical continuum emission (>10 years) is correlated with the radio emission from the base of the jet located above the disk, while the optical long-term variations (1-2 years) follows the radio flares from the stationary component in the jet with time delay of about 1 year. This stationary feature is most likely to be a standing shock formed in the continuous relativistic flow seen at a distance of ~0.4 parsecs from the base of the jet. To account for the correlations observed we propose a model of the nuclear region of 3C 390.3 in which the beamed continuum emission from the jet and counterjet ionizes material in a subrelativistic outflow surrounding the jet. This results in the formation of two conical regions with double-peaked broad emission lines (in addition to the conventional broad line region around the central nucleus) at a distance ~0.6 parsecs from the central engine.",astro-ph,Astrophysics
ULTRACAM observations of two accreting white dwarf pulsators,"In this paper we present high time-resolution observations of GW Librae and SDSS J161033.64-010223.3 -- two cataclysmic variables which have shown periodic variations attributed to non-radial pulsations of the white dwarf. We observed both these systems in their quiescent states and detect the strong pulsations modes reported by previous authors. The identification of further periodicities in GW Lib is limited by the accretion-driven flickering of the source, but in the case of SDSS 1610 we identify several additional low-amplitude periodicities. In the case of SDSS 1610, there is evidence to suggest that the two primary signals have a different colour dependence, suggesting that they may be different spherical harmonic modes. We additionally observed GW Lib during several epochs following its 2007 dwarf nova outburst: the first time a dwarf nova containing a pulsating white dwarf has been observed in such a state. We do not observe any periodicities, suggesting that the heating of the white dwarf had either switched-off the pulsations entirely, or reduced their relative amplitude in flux to the point where they are undetectable. Further observations eleven months after the outburst still do not show the pulsation modes previously observed, but do show the emergence of two new periodic signals. In addition to the WD pulsations, our observations of GW Lib in quiescence show a larger-amplitude modulation in luminosity with a period of approximately 2.1 hours. This has been previously observed, and its origin is unclear: it is unrelated to the orbital period. We find this modulation to vary over the course of our observations in phase and/or period. Our data support the conclusion that this is an accretion-related phenomenon which originates in the accretion disc.",astro-ph,Astrophysics
Convective hydrocodes for radial stellar pulsation. Physical and numerical formulation,"In this paper we describe our convective hydrocodes for radial stellar pulsation. We adopt the Kuhfuss (1986) model of convection, reformulated for the use in stellar pulsation hydrocodes. Physical as well as numerical assumptions of the code are described in detail. Described tests show, that our models are numerically robust and reproduce basic observational constraints.
  We discuss the effects of different treatment of some quantities in other pulsation hydrocodes. Our most important finding concerns the treatment of the turbulent source function in convectively stable regions. In our code we allow for negative values of source function in convectively stable zones, which reflects negative buoyancy. However, some authors restrict the source term to non-negative values. We show that this assumption leads to very high turbulent energies in convectively stable regions. The effect looks like overshooting, but it is not, because turbulence is generated by pulsations. Also, turbulent elements do not carry kinetic nor thermal energy, into convectively stable layers. The range of this artificial overshooting (as we shall call it) is as large as 6 local pressure scale heights, leading to unphysical internal damping through the eddy-viscous forces, in deep, convectively stable parts of the star.",astro-ph,Astrophysics
Determining the WIMP mass from a single direct detection experiment,"The energy spectrum of nuclear recoils in Weakly Interacting Massive Particle (WIMP) direct detection experiments depends on the underlying WIMP mass (strongly for light WIMPs, weakly for heavy WIMPs). We discuss how the accuracy with which the WIMP mass could be determined by a single direct detection experiment depends on the detector configuration and the WIMP properties. In particular we examine the effects of varying the underlying WIMP mass, the detector target nucleus, exposure, energy threshold and maximum energy, the local velocity distribution and the background event rate and spectrum.",astro-ph,Astrophysics
Photometric redshifts as a tool to study the Coma cluster galaxy populations,"We investigate the Coma cluster galaxy luminosity function (GLF) at faint magnitudes, in particular in the u* band by applying photometric redshift techniques applied to deep u*, B, V, R, I images covering a region of ~1deg2 (R 24). Global and local GLFs in the B, V, R and I bands obtained with photometric redshift selection are consistent with our previous results based on a statistical background subtraction.
  In the area covered only by the u* image, the GLF was also derived after applying a statistical background subtraction. The GLF in the u* band shows an increase of the faint end slope towards the outer regions of the cluster (from alpha~1 in the cluster center to alpha~2 in the cluster periphery). This could be explained assuming a short burst of star formation in these galaxies when entering the cluster.
  The analysis of the multicolor type spatial distribution reveals that late type galaxies are distributed in clumps in the cluster outskirts, where X-ray substructures are also detected and where the GLF in the u* band is steeper.",astro-ph,Astrophysics
The dense ring in the Coalsack: the merging of two subsonic flows,"A recent high angular resolution extinction map toward the most opaque molecular globule, Globule 2, in the Coalsack Nebula revealed that it contains a strong central ring of dust column density. This ring represents a region of high density and pressure that is likely a transient and possibly turbulent structure. Dynamical models suggest that the ring has formed as a result of a sudden increase in external pressure which is driving a compression wave into the Globule. Here we combine the extinction measurements with a detailed study of the C18O (1-0) molecular line profiles toward Globule 2 in order to investigate the overall kinematics and, in doing so, test this dynamical model. We find that the ring corresponds to an enhancement in the C18O non-thermal velocity dispersion and non-thermal pressure. We observe a velocity gradient across the Globule that appears to trace two distinct systematic subsonic velocity flows that happen to converge within the ring. We suggest, therefore, that the ring has formed as two subsonic flows of turbulent gas merge within the Globule. The fact that the outer layers of the Globule appear stable against collapse yet there is no centrally condensed core, suggests that the Globule may be evolving from the outside in and has yet to stabilize, confirming its youth.",astro-ph,Astrophysics
Magnetic fields and chemical peculiarities of the very young intermediate-mass binary system HD 72106,"The recently discovered magnetic Herbig Ae and Be stars may provide qualitatively new information about the formation and evolution of magnetic Ap and Bp stars. We have performed a detailed investigation of one particularly interesting binary system with a Herbig Ae secondary and a late B-type primary possessing a strong, globally ordered magnetic field. Twenty high-resolution Stokes V spectra of the system were obtained with the ESPaDOnS instrument mounted on the CFHT. In these observations we see clear evidence for a magnetic field in the primary, but no evidence for a magnetic field in the secondary. A detailed abundance analysis was performed for both stars, revealing strong chemical peculiarities in the primary and normal chemical abundances in the secondary. The primary is strongly overabundant in Si, Cr, and other iron-peak elements, as well as Nd, and underabundant in He. The primary therefore appears to be a very young Bp star. In this context, line profile variations of the primary suggest non-uniform lateral distributions of surface abundances. Interpreting the 0.63995 +/- 0.00009 day variation period of the Stokes I and V profiles as the rotational period of the star, we have modeled the magnetic field geometry and the surface abundance distributions of Si, Ti, Cr and Fe using Magnetic Doppler Imaging. We derive a dipolar geometry of the surface magnetic field, with a polar strength of 1230 G and an obliquity of 57 degrees. The distributions Ti, Cr and Fe are all qualitatively similar, with an elongated patch of enhanced abundance situated near the positive magnetic pole. The Si distribution is somewhat different, and its relationship to the magnetic field geometry less clear.",astro-ph,Astrophysics
Cometary Astropause of Mira Revealed in the Far-Infrared,"Evolved mass-losing stars such as Mira enrich the interstellar medium (ISM) significantly by their dust-rich molecular wind. When these stars move fast enough relative to the ISM, the interaction between the wind and ISM generates the structure known as the astropause (a stellar analog of the heliopause), which is a cometary stellar wind cavity bounded by the contact discontinuity surface between the wind and ISM. Far-infrared observations of Mira spatially resolve the structure of its astropause for the first time, distinguishing the contact surface between Mira's wind and the ISM and the termination shock due to Mira's wind colliding with the ISM. The physical size of the astropause and the estimated speed of the termination shock suggest the age of the astropause to be about 40,000 yr, confirming a theoretical prediction of the shock re-establishment time after Mira has entered the Local Bubble.",astro-ph,Astrophysics
A Multi-Epoch HST Study of the Herbig-Haro Flow from XZ Tauri,"We present nine epochs of Hubble Space Telescope optical imaging of the bipolar outflow from the pre-main sequence binary XZ Tauri. Our data monitors the system from 1995-2005 and includes emission line images of the flow. The northern lobe appears to be a succession of bubbles, the outermost of which expanded ballistically from 1995-1999 but in 2000 began to deform and decelerate along its forward edge. It reached an extent of 6"" from the binary in 2005. A larger and fainter southern counterbubble was detected for the first time in deep ACS images from 2004. Traces of shocked emission are seen as far as 20"" south of the binary. The bubble emission nebulosity has a low excitation overall, as traced by the [S II]/H-alpha line ratio, requiring a nearly comoving surrounding medium that has been accelerated by previous ejections or stellar winds.
  Within the broad bubbles there are compact emission knots whose alignments and proper motions indicate that collimated jets are ejected from each binary component. The jet from the southern component, XZ Tau A, is aligned with the outflow axis of the bubbles and has tangential knot velocities of 70-200 km/s. Knots in the northern flow are seen to slow and brighten as they approach the forward edge of the outermost bubble. The knots in the jet from the other star, XZ Tau B, have lower velocities of ~100 km/s.",astro-ph,Astrophysics
Photometric redshift and classification for the XMM-COSMOS sources,"We present photometric redshifts and spectral energy distribution (SED) classifications for a sample of 1542 optically identified sources detected with XMM in the COSMOS field. Our template fitting classifies 46 sources as stars and 464 as non-active galaxies, while the remaining 1032 require templates with an AGN contribution. High accuracy in the derived photometric redshifts was accomplished as the result of 1) photometry in up to 30 bands with high significance detections, 2) a new set of SED templates including 18 hybrids covering the far-UV to mid-infrared, which have been constructed by the combination of AGN and non-active galaxies templates, and 3) multi-epoch observations that have been used to correct for variability (most important for type 1 AGN). The reliability of the photometric redshifts is evaluated using the sub-sample of 442 sources with measured spectroscopic redshifts. We achieved an accuracy of $_{z/(1+z_{spec})} = 0.014$ for i$_{AB}^*<$22.5 ($_{z/(1+z_{spec})} \sim0.015$ for i$_{AB}^*<$24.5). The high accuracies were accomplished for both type 2 (where the SED is often dominated by the host galaxy) and type 1 AGN and QSOs out to $z=4.5$. The number of outliers is a large improvement over previous photometric redshift estimates for X-ray selected sources (4.0% and 4.8% outliers for i$_{AB}^*<$22.5 and i$_{AB}^*<$24.5, respectively). We show that the intermediate band photometry is vital to achieving accurate photometric redshifts for AGN, whereas the broad SED coverage provided by mid infrared (Spitzer/IRAC) bands is important to reduce the number of outliers for normal galaxies.",astro-ph,Astrophysics
Acceleration of Galactic Supershells by Lyman Alpha Radiation,"Scattering of Lyman Alpha (hereafter Lya) photons by neutral hydrogen gas in a single outflowing 'supershell' around star forming regions often explains the shape and offset of the observed Lya emission line from galaxies. We compute the radiation pressure that is exerted by this scattered Lya radiation on the outflowing material. We show that for reasonable physical parameters, Lya radiation pressure alone can accelerate supershells to velocities in the range v_sh=200-400 km/s. These supershells possibly escape from the gravitational potential well of their host galaxies and contribute to the enrichment of the intergalactic medium. We compute the physical properties of expanding supershells that are likely to be present in a sample of known high-redshift (z=2.7-5.0) galaxies, under the assumption that they are driven predominantly by Lya radiation pressure. We predict ranges of radii r_sh=0.1-10 kpc, ages t_sh=1-100 Myr, and energies E_sh=1e53-1e55 ergs, which are in reasonable agreement with the properties of local galactic supershells. Furthermore, we find that the radius, r_sh, of a Lya-driven supershell of constant mass depends uniquely on the intrinsic Lya luminosity of the galaxy, L_alpha, the HI column density of the supershell, N_HI, and the shell speed, v_sh, through the scaling relation r_sh ~ L_alpha/(N_HI v_sh^2). We derive mass outflow rates in supershells that reach ~10-100% of the star formation rates of their host galaxies.",astro-ph,Astrophysics
Near-Infrared spectral diagnostics for unresolved stellar population galaxies,"We want to develop spectral diagnostics of stellar populations in the near-infrared (NIR), for unresolved stellar populations. We created a semi-empirical population model and we compare the model output with the observed spectra of a sample of elliptical and bulge-dominated galaxies that have reliable Lick-indices from literature to test if the correlation between Mg2 and CO 1.62 micron remains valid in galaxies and to calibrate it as an abundance indicator. We find that (i) there are no significant correlations between any NIR feature and the optical Mg2; (ii) the CaI, NaI and CO trace the alpha-enhancement; and (iii) the NIR absorption features are not influenced by the galaxy's age.",astro-ph,Astrophysics
Observing Strategies for the NICI Campaign to Directly Image Extrasolar Planets,"We discuss observing strategy for the Near Infrared Coronagraphic Imager (NICI) on the 8-m Gemini South telescope. NICI combines a number of techniques to attenuate starlight and suppress superspeckles: 1) coronagraphic imaging, 2) dual channel imaging for Spectral Differential Imaging (SDI) and 3) operation in a fixed Cassegrain rotator mode for Angular Differential Imaging (ADI). NICI will be used both in service mode and for a dedicated 50 night planet search campaign. While all of these techniques have been used individually in large planet-finding surveys, this is the first time ADI and SDI will be used with a coronagraph in a large survey. Thus, novel observing strategies are necessary to conduct a viable planet search campaign.",astro-ph,Astrophysics
Luminous Blue Variable Stars In The Two Extremely Metal-Deficient Blue Compact Dwarf Galaxies DDO 68 and PHL 293B,"We present photometric and spectroscopic observations of two luminous blue variable (LBV) stars in two extremely metal-deficient blue compact dwarf (BCD) galaxies, DDO 68 with 12+logO/H = 7.15 and PHL 293B with 12+logO/H = 7.72. These two BCDs are the lowest-metallicity galaxies where LBV stars have been detected, allowing to study the LBV phenomenon in the extremely low metallicity regime, and shedding light of the evolution of the first generation of massive stars born from primordial gas. We find that the strong outburst of the LBV star in DDO 68 occurred sometime between February 2007 and January 2008. We have compared the properties of the broad line emission in low-metallicity LBVs with those in higher metallicity LBVs. We find that, for the LBV star in DDO 68, broad emission with a P Cygni profile is seen in both H and He I emission lines. On the other hand, for the LBV star in PHL 293B, P Cygni profiles are detected only in H lines. For both LBVs, no heavy element emission line such as Fe II was detected. The Halpha luminosities of LBV stars in both galaxies are comparable to the one obtained for the LBV star in NGC 2363 (Mrk 71) which has a higher metallicity 12+logO/H = 7.89. On the other hand, the terminal velocities of the stellar winds in both low-metallicity LBVs are high, ~800 km/s, a factor of ~4 higher than the terminal velocities of the winds in high-metallicity LBVs. This suggests that stellar winds at low metallicity are driven by a different mechanism than the one operating in high-metallicity winds.",astro-ph,Astrophysics
"The Spitzer View of Low-Metallicity Star Formation: II. Mrk 996, a Blue Compact Dwarf Galaxy with an Extremely Dense Nucleus","(abridged) We present new Spitzer, UKIRT and MMT observations of the blue compact dwarf galaxy (BCD) Mrk 996, with an oxygen abundance of 12+log(O/H)=8.0. This galaxy has the peculiarity of possessing an extraordinarily dense nuclear star-forming region, with a central density of ~10^6 cm^{-3}. The nuclear region of Mrk 996 is characterized by several unusual properties: a very red color J-K = 1.8, broad and narrow emission-line components, and ionizing radiation as hard as 54.9 eV, as implied by the presence of the OIV 25.89 micron line. The nucleus is located within an exponential disk with colors consistent with a single stellar population of age >1 Gyr. The infrared morphology of Mrk 996 changes with wavelength. The IRS spectrum shows strong narrow Polycyclic Aromatic Hydrocarbon (PAH) emission, with narrow line widths and equivalent widths that are high for the metallicity of Mrk 996. Gaseous nebular fine-structure lines are also seen. A CLOUDY model requires that they originate in two distinct HII regions: a very dense HII region of radius ~580 pc with densities declining from ~10^6 at the center to a few hundreds cm^{-3} at the outer radius, where most of the optical lines arise; and a HII region with a density of ~300 cm^{-3} that is hidden in the optical but seen in the MIR. We suggest that the infrared lines arise mainly in the optically obscured HII region while they are strongly suppressed by collisional deexcitation in the optically visible one. The hard ionizing radiation needed to account for the OIV 25.89 micron line is most likely due to fast radiative shocks propagating in an interstellar medium. A hidden population of Wolf-Rayet stars of type WNE-w or a hidden AGN as sources of hard ionizing radiation are less likely possibilities.",astro-ph,Astrophysics
Dynamics and constraints of the Unified Dark Matter flat cosmologies,"We study the dynamics of the scalar field FLRW flat cosmological models within the framework of the Unified Dark Matter (UDM) scenario. In this model we find that the main cosmological functions such as the scale factor of the Universe, the scalar field, the Hubble flow and the equation of state parameter are defined in terms of hyperbolic functions. These analytical solutions can accommodate an accelerated expansion, equivalent to either the dark energy or the standard $$ models. Performing a joint likelihood analysis of the recent supernovae type Ia data and the Baryonic Acoustic Oscillations traced by the SDSS galaxies, we place tight constraints on the main cosmological parameters of the UDM cosmological scenario. Finally, we compare the UDM scenario with various dark energy models namely $$ cosmology, parametric dark energy model and variable Chaplygin gas. We find that the UDM scalar field model provides a large and small scale dynamics which are in fair agreement with the predictions by the above dark energy models although there are some differences especially at high redshifts.",astro-ph,Astrophysics
A search for iron emission lines in the Chandra X-ray spectra of neutron star low-mass X-ray binaries,"While iron emission lines are well studied in black hole systems, both in X-ray binaries and Active Galactic Nuclei, there has been less of a focus on these lines in neutron star low-mass X-ray binaries (LMXBs). However, recent observations with Suzaku and XMM-Newton have revealed broad asymmetric iron line profiles in 4 neutron star LMXBs, confirming an inner disk origin for these lines in neutron star systems. Here, we present a search for iron lines in 6 neutron star LMXBs. For each object we have simultaneous Chandra and RXTE observations at 2 separate epochs, allowing for both a high resolution spectrum, as well as broadband spectral coverage. Out of the six objects in the survey, we only find significant iron lines in two of the objects, GX 17+2 and GX 349+2. However, we cannot rule out that there are weak, broad lines present in the other sources. The equivalent width of the line in GX 17+2 is consistent between the 2 epochs, while in GX 349+2 the line equivalent width increases by a factor of ~3 between epochs as the source flux decreases by a factor of 1.3. This suggests that the disk is highly ionized, and the line is dominated by recombination emission. We find that there appears to be no specific locations in the long-term hardness-intensity diagrams where iron emission lines are formed, though more sources and further observations are required.",astro-ph,Astrophysics
Metallicity in the Galactic Center: The Quintuplet cluster,"We present a measurement of metallicity in the Galactic center Quintuplet Cluster made using quantitative spectral analysis of two Luminous Blue Variables (LBVs). The analysis employs line-blanketed NLTE wind/atmosphere models fit to high-resolution near-infrared spectra containing lines of H, HeI, SiII, MgII, and FeII. We are able to break the H/He ratio vs. mass-loss rate degeneracy found in other LBVs and to obtain robust estimates of the He content of both objects. Our results indicate solar iron abundance and roughly twice solar abundance in the alpha-elements. These results are discussed within the framework of recent measurements of oxygen and carbon composition in the nearby Arches Cluster and iron abundances in red giants and supergiants within the central 30 pc of the Galaxy. The relatively large enrichment of alpha-elements with respect to iron is consistent with a history of more nucleosynthesis in high mass stars than the Galactic disk.",astro-ph,Astrophysics
Hidden gauginos of an unbroken U(1): Cosmological constraints and phenomenological prospects,"We study supersymmetric scenarios where the dark matter is the gaugino of an unbroken hidden U(1) which interacts with the visible world only via a small kinetic mixing with the hypercharge. Strong constraints on the parameter space can be derived from avoiding overclosure of the Universe and from requiring successful Big Bang Nucleosynthesis and structure formation. We find that for typical values of the mixing parameter, scenarios with neutralino NLSP are excluded, while scenarios with slepton NLSP are allowed when the mixing parameter lies in the range chi~O(10^(-13) - 10^(-10)). We also show that if the gravitino is the LSP and the hidden U(1) gaugino the NLSP, the bounds on the reheating temperature from long lived charged MSSM relics can be considerably relaxed and we comment on the signatures of these scenarios at future colliders. Finally, we discuss the case of an anomalously small mixing, chi<<10^(-16), where the neutralino becomes a decaying dark matter candidate, and derive constraints from gamma ray experiments.",astro-ph,Astrophysics
Evidence for a Photoevaporated Circumbinary Disk in Orion,"We have found a photoevaporated disk in the Orion Nebula that includes a wide binary. HST/ACS observations of the proplyd 124-132 show two point-like sources separated by 0"".15, or about 60 AU at the distance of Orion. The two sources have nearly identical I and z magnitudes. We analyze the brightest component, Source N, comparing the observed magnitudes with those predicted using a 1 Myr Baraffe/NEXTGEN isochrone with different accretion luminosities and extinctions. We find that a low mass (\simeq 0.04 M_\odot) brown dwarf ~1 Myr old with mass accretion rate \log\dot{M}\simeq -10.3, typical for objects of this mass, and about 2 magnitudes of visual extinction provides the best fit to the data. This is the first observation of a circumbinary disk undergoing photoevaporation and, if confirmed by spectroscopic observations, the first direct detection of a wide substellar pair still accreting and enshrouded in its circumbinary disk.",astro-ph,Astrophysics
Some Cosmological Implications of Hidden Sectors,"We discuss some cosmological implications of extensions of the Standard Model with hidden sector scalars coupled to the Higgs boson. We put special emphasis on the conformal case, in which the electroweak symmetry is broken radiatively with a Higgs mass above the experimental limit. Our refined analysis of the electroweak phase transition in this kind of models strengthens the prediction of a strongly first-order phase transition as required by electroweak baryogenesis. We further study gravitational wave production and the possibility of low-scale inflation as well as a viable dark matter candidate.",astro-ph,Astrophysics
Spectral hardness evolution characteristics of tracking Gamma-ray Burst pulses,"Employing a sample presented by Kaneko et al. (2006) and Kocevski et al. (2003), we select 42 individual tracking pulses (here we defined tracking as the cases in which the hardness follows the same pattern as the flux or count rate time profile) within 36 Gamma-ray Bursts (GRBs) containing 527 time-resolved spectra and investigate the spectral hardness, $E_{peak}$ (where $E_{peak}$ is the maximum of the $F_$ spectrum), evolutionary characteristics. The evolution of these pulses follow soft-to-hard-to-soft (the phase of soft-to-hard and hard-to-soft are denoted by rise phase and decay phase, respectively) with time. It is found that the overall characteristics of $E_{peak}$ of our selected sample are: 1) the $E_{peak}$ evolution in the rise phase always start on the high state (the values of $E_{peak}$ are always higher than 50 keV); 2) the spectra of rise phase clearly start at higher energy (the median of $E_{peak}$ are about 300 keV), whereas the spectra of decay phase end at much lower energy (the median of $E_{peak}$ are about 200 keV); 3) the spectra of rise phase are harder than that of the decay phase and the duration of rise phase are much shorter than that of decay phase as well. In other words, for a complete pulse the initial $E_{peak}$ is higher than the final $E_{peak}$ and the duration of initial phase (rise phase) are much shorter than the final phase (decay phase). This results are in good agreement with the predictions of Lu et al. (2007) and current popular view on the production of GRBs. We argue that the spectral evolution of tracking pulses may be relate to both of kinematic and dynamic process even if we currently can not provide further evidences to distinguish which one is dominant. Moreover, our statistical results give some witnesses to constrain the current GRB model.",astro-ph,Astrophysics
Recent Topics on Very High Energy Gamma-ray Astronomy,"With the advent of imaging atmospheric Cherenkov telescopes in late 1980's, ground-based observation of TeV gamma-rays came into reality after struggling trials by pioneers for twenty years, and the number of gamma-ray sources detected at TeV energies has increased to be over seventy now. In this review, recent findings from ground-based very-high-energy gamma-ray observations are summarized (as of 2008 March), and up-to-date problems in this research field are presented.",astro-ph,Astrophysics
On Cosmic Rays Sources,"Study of astrophysical objects with strong dipolar magnetic fields show that the spectrum of the accelerated charged particles leaving the sources has a power law form with exponent -2.5, where the exponent is calculated on purely geometrical bases and is independent on the particle species.",astro-ph,Astrophysics
A detailed analysis of structure growth in $f(R)$ theories of gravity,"We investigate the connection between dark energy and fourth order gravity by analyzing the behavior of scalar perturbations around a Friedmann-Robertson-Walker background. The evolution equations for scalar perturbation are derived using the covariant and gauge invariant approach and applied to two widely studied $f(R)$ gravity models. The structure of the general fourth order perturbation equations and the analysis of scalar perturbations lead to the discovery of a characteristic signature of fourth order gravity in the matter power spectrum, the details of which have not seen before in other works in this area. This could provide a crucial test for fourth order gravity on cosmological scales.",astro-ph,Astrophysics
A measurement of large-scale peculiar velocities of clusters of galaxies: technical details,"This paper presents detailed analysis of large-scale peculiar motions derived from a sample of ~ 700 X-ray clusters and cosmic microwave background (CMB) data obtained with WMAP. We use the kinematic Sunyaev-Zeldovich (KSZ) effect combining it into a cumulative statistic which preserves the bulk motion component with the noise integrated down. Such statistic is the dipole of CMB temperature fluctuations evaluated over the pixels of the cluster catalog (Kashlinsky & Atrio-Barandela 2000). To remove the cosmological CMB fluctuations the maps are Wiener-filtered in each of the eight WMAP channels (Q, V, W) which have negligible foreground component. Our findings are as follows: The thermal SZ (TSZ) component of the clusters is described well by the Navarro-Frenk-White profile expected if the hot gas traces the dark matter in the cluster potential wells. Such gas has X-ray temperature decreasing rapidly towards the cluster outskirts, which we demonstrate results in the decrease of the TSZ component as the aperture is increased to encompass the cluster outskirts. We then detect a statistically significant dipole in the CMB pixels at cluster positions. Arising exclusively at the cluster pixels this dipole cannot originate from the foreground or instrument noise emissions and must be produced by the CMB photons which interacted with the hot intracluster gas via the SZ effect. The dipole remains as the monopole component, due to the TSZ effect, vanishes within the small statistical noise out to the maximal aperture where we still detect the TSZ component. We demonstrate with simulations that the mask and cross-talk effects are small for our catalog and contribute negligibly to the measurements. The measured dipole thus arises from the KSZ effect produced by the coherent large scale bulk flow motion.",astro-ph,Astrophysics
What we (would like to) know about the neutrino mass,"We present updated values for the mass-mixing parameters relevant to neutrino oscillations, with particular attention to emerging hints in favor of theta_13>0. We also discuss the status of absolute neutrino mass observables, and a possible approach to constrain theoretical uncertainties in neutrinoless double beta decay. Desiderata for all these issues are also briefly mentioned.",astro-ph,Astrophysics
A sharp look at the gravitationally lensed quasar SDSS J0806+2006 with Laser Guide Star Adaptive Optics,"We present the first VLT near-IR observations of a gravitationally lensed quasar, using adaptive optics and laser guide star. These observations can be considered as a test bench for future systematic observations of lensed quasars with adaptive optics, even when bright natural guide stars are not available in the nearby field. With only 14 minutes of observing time, we derived very accurate astrometry of the quasar images and of the lensing galaxy, with 0.05 \arcsec spatial resolution, comparable to the Hubble Space Telescope (HST). In combination with deep VLT optical spectra of the quasar images, we use our adaptive optics images to constrain simple models for the mass distribution of the lensing galaxy. The latter is almost circular and does not need any strong external shear to fit the data. The time delay predicted for SDSS0806+2006, assuming a singular isothermal ellipsoid model and the concordance cosmology, is Delta t \simeq 50 days. Our optical spectra indicate a flux ratio between the quasar images of A/B=1.3 in the continuum and A/B=2.2 in both the MgII and in the CIII] broad emission lines. This suggests that microlensing affects the continuum emission. However, the constant ratio between the two emission lines indicates that the broad emission line region is not microlensed. Finally, we see no evidence of reddening by dust in the lensing galaxy.",astro-ph,Astrophysics
Young Stars in the Camelopardalis Dust and Molecular Clouds. IV. Spectral Observations of the Suspected YSOs,"In the first three papers of this series, about 200 objects in Camelopardalis and the nearby areas of Cassiopeia, Perseus and Auriga were suspected of being pre-main-sequence stars in different stages of evolution. To confirm the evolutionary status of the 15 brightest objects, their far-red range (600--950 nm) spectra were obtained. Almost all these objects are young stars with emissions in H alpha, OI, CaII and P9 lines. The equivalent widths of emission lines and approximate spectral classes of the objects are determined.",astro-ph,Astrophysics
APOGEE: The Apache Point Observatory Galactic Evolution Experiment,"APOGEE is a large-scale, NIR, high-resolution (R~20,000) spectroscopic survey of Galactic stars. It is one of the four experiments in SDSS-III. Because APOGEE will observe in the H band, it will be the first survey to pierce through Galactic dust and provide a vast, uniform database of chemical abundances and radial velocities for stars across all Galactic populations (bulge, disk, and halo). The survey will be conducted with a dedicated, 300-fiber, cryogenic, spectrograph that is being built at the University of Virginia, coupled to the ARC 2.5m telescope at Apache Point Observatory. APOGEE will use a significant fraction of the SDSS-III bright time during a three-year period to observe, at high signal-to-noise ratio (S/N>100), about 100,000 giant stars selected directly from 2MASS down to a typical flux limit of H<13. The main scientific objectives of APOGEE are: (1) measuring unbiased metallicity distributions and abundance patterns for the different Galactic stellar populations, (2) studying the processes of star formation, feedback, and chemical mixing in the Milky Way, (3) surveying the dynamics of the bulge and disk, placing constraints on the nature and influence of the Galactic bar and spiral arms, and (4) using extensive chemodynamical data, particularly in the inner Galaxy, to unravel its formation and evolution.",astro-ph,Astrophysics
"Model-independent implications of the e+, e-, anti-proton cosmic ray spectra on properties of Dark Matter","Taking into account spins, we classify all two-body non-relativistic Dark Matter annihilation channels to the allowed polarization states of Standard Model particles, computing the energy spectra of the stable final-state particles relevant for indirect DM detection. We study the DM masses, annihilation channels and cross sections that can reproduce the PAMELA indications of an e+ excess consistently with the PAMELA p-bar data and the ATIC/PPB-BETS e++e- data. From the PAMELA data alone, two solutions emerge: (i) either the DM particles that annihilate into W,Z,h must be heavier than about 10 TeV or (ii) the DM must annihilate only into leptons. Thus in both cases a DM particle compatible with the PAMELA excess seems to have quite unexpected properties. The solution (ii) implies a peak in the e++e- energy spectrum, which, indeed, seems to appear in the ATIC/PPB-BETS data around 700 GeV. If upcoming data from ATIC-4 and GLAST confirm this feature, this would point to a O(1) TeV DM annihilating only into leptons. Otherwise the solution (i) would be favored. We comment on the implications of these results for DM models, direct DM detection and colliders as well as on the possibility of an astrophysical origin of the excess.",astro-ph,Astrophysics
Effects of dark matter annihilation on the first stars,"We study the evolution of the first stars in the universe (Population III) from the early pre-Main Sequence until the end of helium burning in the presence of WIMP dark matter annihilation inside the stellar structure. The two different mechanisms that can provide this energy source are the contemporary contraction of baryons and dark matter, and the capture of WIMPs by scattering off the gas with subsequent accumulation inside the star. We find that the first mechanism can generate an equilibrium phase, previously known as a ""dark star"", which is transient and present in the very early stages of pre-MS evolution. The mechanism of scattering and capture acts later, and can support the star virtually forever, depending on environmental characteristic of the dark matter halo and on the specific WIMP model.",astro-ph,Astrophysics
Enlightening the structure and dynamics of Abell 1942,"We present a dynamical analysis of the galaxy cluster Abell 1942 based on a set of 128 velocities obtained at the European Southern Observatory. Data on individual galaxies are presented and the accuracy of the determined velocities is discussed as well as some properties of the cluster. We have also made use of publicly available Chandra X-ray data. We obtained an improved mean redshift value z = 0.22513 \pm 0.0008 and velocity dispersion sigma = 908^{+147}_{-139} km/s. Our analysis indicates that inside a radius of ~1.5 h_{70}^{-1} Mpc (~7 arcmin) the cluster is well relaxed, without any remarkable feature and the X-ray emission traces fairly well the galaxy distribution. Two possible optical substructures are seen at ~5 arcmin from the centre towards the Northwest and the Southwest direction, but are not confirmed by the velocity field. These clumps are however, kinematically bound to the main structure of Abell 1942. X-ray spectroscopic analysis of Chandra data resulted in a temperature kT = 5.5 \pm 0.5 keV and metal abundance Z = 0.33 \pm 0.15 Z_odot. The velocity dispersion corresponding to this temperature using the T_X-sigma scaling relation is in good agreement with the measured galaxies velocities. Our photometric redshift analysis suggests that the weak lensing signal observed at the south of the cluster and previously attributed to a ""dark clump"", is produced by background sources, possibly distributed as a filamentary structure.",astro-ph,Astrophysics
Detection and extraction of signals from the epoch of reionization using higher order one-point statistics,"Detecting redshifted 21cm emission from neutral hydrogen in the early Universe promises to give direct constraints on the epoch of reionization (EoR). It will, though, be very challenging to extract the cosmological signal (CS) from foregrounds and noise which are orders of magnitude larger. Fortunately, the signal has some characteristics which differentiate it from the foregrounds and noise, and we suggest that using the correct statistics may tease out signatures of reionization. We generate mock datacubes simulating the output of the Low Frequency Array (LOFAR) EoR experiment. These cubes combine realistic models for Galactic and extragalactic foregrounds and the noise with three different simulations of the CS. We fit out the foregrounds, which are smooth in the frequency direction, to produce residual images in each frequency band. We denoise these images and study the skewness of the one-point distribution in the images as a function of frequency. We find that, under sufficiently optimistic assumptions, we can recover the main features of the redshift evolution of the skewness in the 21cm signal. We argue that some of these features - such as a dip at the onset of reionization, followed by a rise towards its later stages - may be generic, and give us a promising route to a statistical detection of reionization.",astro-ph,Astrophysics
Ergodic Properties of Fractional Brownian-Langevin Motion,"We investigate the time average mean square displacement $\overline{^2}(x(t))=\int_0^{t-}[x(t^\prime+)-x(t^\prime)]^2 dt^\prime/(t-)$ for fractional Brownian and Langevin motion. Unlike the previously investigated continuous time random walk model $\overline{^2}$ converges to the ensemble average $<x^2 > \sim t^{2 H}$ in the long measurement time limit. The convergence to ergodic behavior is however slow, and surprisingly the Hurst exponent $H=3/4$ marks the critical point of the speed of convergence. When $H<3/4$, the ergodicity breaking parameter ${EB} = {Var} (\overline{^2}) / < \overline{^2} >^2\sim k(H) \cdot\cdot t^{-1}$, when $H=3/4$, ${EB} \sim (9/16)(\ln t) \cdot\cdot t^{-1}$, and when $3/4<H <1, {EB} \sim k(H)^{4-4H} t^{4H-4}$. In the ballistic limit $H \to 1$ ergodicity is broken and ${EB} \sim 2$. The critical point $H=3/4$ is marked by the divergence of the coefficient $k(H)$. Fractional Brownian motion as a model for recent experiments of sub-diffusion of mRNA in the cell is briefly discussed and comparison with the continuous time random walk model is made.",astro-ph,Astrophysics
Constraining the Spin-Independent WIMP-Nucleon Coupling from Direct Dark Matter Detection Data,"Weakly Interacting Massive Particles (WIMPs) are one of the leading candidates for Dark Matter. For understanding the properties of WIMPs and identifying them among new particles produced at colliders (hopefully in the near future), determinations of their mass and their couplings on nucleons from direct Dark Matter detection experiments are essential. Based on our method for determining the WIMP mass model-independently from experimental data, we present a way to also estimate the spin-independent (SI) WIMP-nucleon coupling by using measured recoil energies directly. This method isindependent of the as yet unknown velocity distribution of halo WIMPs. In spite of the uncertainty of the local WIMP density (of a factor of ~ 2), at least an upper limit on the SI WIMP-nucleon coupling could be given, once two (or more) experiments with different target nuclei obtain positive signals. In a background-free environment, for a WIMP mass of 100 GeV its SI coupling on nucleons could in principle be estimated with a statistical error of only ~ 15% with just 50 events from each experiment.",astro-ph,Astrophysics
High-frequency VLBI observations of SgrA* during a multi-frequency campaign in May 2007,"In May 2007 the compact radio source Sgr A* was observed in a global multi-frequency monitoring campaign, from radio to X-ray bands. Here we present and discuss first and preliminary results from polarization sensitive VLBA observations, which took place during May 14-25, 2007. Here, Sgr A* was observed in dual polarization on 10 consecutive days at 22, 43, and 86 GHz. We describe the VLBI experiments, our data analysis, monitoring program and show preliminary images obtained at the various frequencies. We discuss the data with special regard also to the short term variability.",astro-ph,Astrophysics
Self-consistent physical parameters for MC clusters from CMD modelling: application to SMC clusters observed with the SOAR telescope,"The Magellanic Clouds (MCs) present a rich system of stellar clusters that can be used to probe the dynamical and chemical evolution of these neighboring and interacting irregular galaxies. In particular, these stellar clusters (SCs) present combinations of age and metallicity that are not found for this class of objects in the Milky Way, being therefore very useful templates to test and to calibrate integrated light simple stellar population (SSP) models applied to unresolved distance galaxies. On its turn, the age and metallicity for a cluster can be determined spatially resolving its stars, by means of analysis of its colour-magnitude diagrams (CMDs). In this work we present our method to determine self-consistent physical parameters (age, metallicity, distance modulus and reddening) for a stellar cluster, from CMDs modelling of relatively unstudied SCs in the Small Magellanic Cloud (SMC) imaged in the BVI filters with the 4.1 m SOAR telescope. Our preliminary results confirm our expectations that come from a previous integrated spectra and colour analysis: at least one of them (Lindsay 2) is an intermediate-age stellar cluster with ~ 2.6 Gyr and [Fe/H] ~ -1.3, being therefore a new interesting witness regarding the reactivation of the star formation in the MCs in the last 4 Gyr.",astro-ph,Astrophysics
Searching for modified growth patterns with tomographic surveys,"In alternative theories of gravity, designed to produce cosmic acceleration at the current epoch, the growth of large scale structure can be modified. We study the potential of upcoming and future tomographic surveys such as DES and LSST, with the aid of CMB and supernovae data, to detect departures from the growth of cosmic structure expected within General Relativity. We employ parametric forms to quantify the potential time- and scale-dependent variation of the effective gravitational constant, and the differences between the two Newtonian potentials. We then apply the Fisher matrix technique to forecast the errors on the modified growth parameters from galaxy clustering, weak lensing, CMB, and their cross-correlations across multiple photometric redshift bins. We find that even with conservative assumptions about the data, DES will produce non-trivial constraints on modified growth, and that LSST will do significantly better.",astro-ph,Astrophysics
Atomic Fluorescence and Prospects for Observing Magnetic Geometry Using Atomic Magnetic Realignment,"Yan and Lazarian have proposed a new technique through which the magnetic field geometry in the diffuse interstellar medium, or in circumstellar matter, could be determined from the linear polarization of interstellar absorption or fluorescence emission lines from ions pumped by an anisotropic illuminating flux. New long-slit spectroscopic observations of the reflection nebula NGC2023, obtained with the Southern African Large Telescope Robert Stobie Spectrograph (RSS), have detected a number of atomic fluorescence lines of OI, NI, SiII, and FeII for the first time in a neutral medium. A model which predicts these lines and others illustrates which lines would be appropriate targets for an RSS spectropolarimetric investigation of this new diagnostic.",astro-ph,Astrophysics
Effect of Night Laboratories on Learning Objectives for a Non-Major Astronomy Class,"We tested the effectiveness on learning of hands-on, night-time laboratories that challenged student misconceptions in a non-major introductory astronomy class at Rensselaer Polytechnic Institute. We present a new assessment examination used to assess learning in this study. We were able to increase learning, at the 8.0 sigma level, on one of the moon phase objectives that was addressed in a cloudy night activity. There is weak evidence of some improvement on a broader range of learning objectives. We show evidence that the overall achievement levels of the four sections of the class is correlated with the amount of clear whether the sections had for observing, even though the learning objectives were addressed primarily in activities that did not require clear skies. This last result should be confirmed with future studies. We describe our first attempt to cycle the students through different activity stations in an attempt to handle 18 students at a time in the laboratories, and lessons learned from this.",astro-ph,Astrophysics
Molecular Hydrogen in the FUSE Translucent Lines of Sight: The Full Sample,"We report total abundances and related parameters for the full sample of the FUSE survey of molecular hydrogen in 38 translucent lines of sight. New results are presented for the ""second half"" of the survey involving 15 lines of sight to supplement data for the first 23 lines of sight already published. We assess the correlations between molecular hydrogen and various extinction parameters in the full sample, which covers a broader range of conditions than the initial sample. In particular, we are now able to confirm that many, but not all, lines of sight with shallow far-UV extinction curves and large values of the total-to-selective extinction ratio, $R_V$ = $A_V$ / $E(B-V)$ -- characteristic of larger than average dust grains -- are associated with particularly low hydrogen molecular fractions ($f_{\rm H2}$). In the lines of sight with large $R_V$, there is in fact a wide range in molecular fractions, despite the expectation that the larger grains should lead to less H$_2$ formation. However, we see specific evidence that the molecular fractions in this sub-sample are inversely related to the estimated strength of the UV radiation field and thus the latter factor is more important in this regime. We have provided an update to previous values of the gas-to-dust ratio, $N$(H$_{\rm tot}$)/$E(B-V)$, based on direct measurements of $N$(H$_2$) and $N$(H I). Although our value is nearly identical to that found with Copernicus data, it extends the relationship by a factor of 2 in reddening. Finally, as the new lines of sight generally show low to moderate molecular fractions, we still find little evidence for single monolithic ""translucent clouds"" with $f_{\rm H2}$ $\sim$ 1.",astro-ph,Astrophysics
Dynamics of Black Hole Pairs I: Periodic Tables,"Although the orbits of comparable mass, spinning black holes seem to defy simple decoding, we find a means to decipher all such orbits. The dynamics is complicated by extreme perihelion precession compounded by spin-induced precession. We are able to quantitatively define and describe the fully three dimensional motion of comparable mass binaries with one black hole spinning and expose an underlying simplicity. To do so, we untangle the dynamics by capturing the motion in the orbital plane. Our results are twofold: (1) We derive highly simplified equations of motion in a non-orthogonal orbital basis, and (2) we define a complete taxonomy for fully three-dimensional orbits. More than just a naming system, the taxonomy provides unambiguous and quantitative descriptions of the orbits, including a determination of the zoom-whirliness of any given orbit. Through a correspondence with the rationals, we are able to show that zoom-whirl behavior is prevalent in comparable mass binaries in the strong-field regime. A first significant conclusion that can be drawn from this analysis is that all generic orbits in the final stages of inspiral under gravitational radiation losses are characterized by precessing clovers with few leaves and that no orbit will behave like the tightly precessing ellipse of Mercury. The gravitational waveform produced by these low-leaf clovers will reflect the natural harmonics of the orbital basis -- harmonics that, importantly, depend only on radius. The significance for gravitational wave astronomy will depend on the number of windings the pair executes in the strong-field regime and could be more conspicuous for intermediate mass pairs than for stellar mass pairs.",astro-ph,Astrophysics
On aberration in gravitational lensing,"It is known that a relative translational motion between the deflector and the observer affects gravitational lensing. In this paper, a lens equation is obtained to describe such effects on actual lensing observables. Results can be easily interpreted in terms of aberration of light-rays. Both radial and transverse motions with relativistic velocities are considered. The lens equation is derived by first considering geodesic motion of photons in the rest-frame Schwarzschild spacetime of the lens, and, then, light-ray detection in the moving observer's frame. Due to the transverse motion images are displaced and distorted in the observer's celestial sphere, whereas the radial velocity along the line of sight causes an effective re-scaling of the lens mass. The Einstein ring is distorted to an ellipse whereas the caustics in the source plane are still point-like. Either for null transverse motion or up to linear order in velocities, the critical curve is still a circle with its radius corrected by a factor (1+z_d) with respect to the static case, z_d being the relativistic Doppler shift of the deflector. From the observational point of view, the orbital motion of the Earth can cause potentially observable corrections of the order of the microarcsec in lensing towards the super-massive black hole at the Galactic center. On a cosmological scale, tangential peculiar velocities of cluster of galaxies bring about a typical flexion in images of background galaxies in the weak lensing regime but future measurements seem to be too much challenging.",astro-ph,Astrophysics
The effects of primordial non-Gaussianity on the cosmological reionization,"We investigate the effects of non-Gaussianity in the primordial density field on the reionization history. We rely on a semi-analytic method to describe the processes acting on the intergalactic medium (IGM), relating the distribution of the ionizing sources to that of dark matter haloes. Extending previous work in the literature, we consider models in which the primordial non-Gaussianity is measured by the dimensionless non-linearity parameter f_NL, using the constraints recently obtained from cosmic microwave background data. We predict the ionized fraction and the optical depth at different cosmological epochs assuming two different kinds of non-Gaussianity, characterized by a scale-independent and a scale-dependent f_NL and comparing the results to those for the standard Gaussian scenario. We find that a positive f_NL enhances the formation of high-mass haloes at early epochs, when reionization begins, and, as a consequence, the IGM ionized fraction can grow by a factor up to 5 with respect to the corresponding Gaussian model. The increase of the filling factor has a small impact on the reionization optical depth and is of order ~ 10 per cent if a scale-dependent non-Gaussianity is assumed. Our predictions for non-Gaussian models are in agreement with the latest WMAP results within the error bars, but a higher precision is required to constrain the scale dependence of non-Gaussianity.",astro-ph,Astrophysics
UHECRs from the Radio Lobes of AGNs,"We report a stochastic mechanism of particle acceleration from first principles in an environment having properties like those of Radio Lobes in AGNs. We show that energies $\sim 10^{20}$ eV are reached in $\sim 10^6$ years for protons. Our results reopen the question regarding the nature of the high-energy cutoff in the observed spectrum: whether it is due solely to propagation effects, or whether it is also affected by the maximum energy permitted by the acceleration process itself.",astro-ph,Astrophysics
Centimeter-wave continuum radiation from the rho Ophiuchi molecular cloud,"The rho Oph molecular cloud is undergoing intermediate-mass star formation. UV radiation from its hottest young stars heats and dissociates exposed layers, but does not ionize hydrogen. Only faint radiation from the Rayleigh-Jeans tail of ~10-100K dust is expected at wavelengths longwards of 3mm. Yet Cosmic Background Imager (CBI) observations reveal that the rho Oph W photo-dissociation region (PDR) is surprisingly bright at centimetre wavelengths. We searched for interpretations consistent with the WMAP radio spectrum, new ISO-LWS parallel mode images and archival Spitzer data. Dust-related emission mechanisms at 1 cm, as proposed by Draine & Lazarian, are a possibility. But a magnetic enhancement of the grain opacity at 1cm is inconsistent with the morphology of the dust column maps Nd and the lack of detected polarization. Spinning dust, or electric-dipole radiation from spinning very small grains (VSGs), comfortably explains the radio spectrum, although not the conspicuous absence from the CBI data of the infrared circumstellar nebulae around the B-type stars S1 and SR~3. Allowing for VSG depletion can marginally reconcile spinning dust with the data. As an alternative interpretation we consider the continuum from residual charges in rho Oph W, where most of carbon should be photoionised by the close binary HD147889 (B2IV, B3IV). Electron densities of ~100 cm^{-3}, or H-nucleus densities n_H > 1E6 cm^{-3}, are required to interpret rho Oph W as the CII Stromgren sphere of HD147889. However the observed steep and positive low-frequency spectral index would then require optically thick emission from an hitherto unobserved ensemble of dense clumps or sheets with a filling factor ~1E-4 and n_H ~ 1E7 cm^{-3}.",astro-ph,Astrophysics
A cosmic-ray precursor model for a Balmer-dominated shock in Tycho's supernova remnant,"We present a time-dependent cosmic-ray modified shock model for which the calculated H-alpha emissivity profile agrees well with the H-alpha flux increase ahead of the Balmer-dominated shock at knot g in Tycho's supernova remnant, observed by Lee et al (2007). The backreaction of the cosmic ray component on the thermal component is treated in the two-fluid approximation, and we include thermal particle injection and energy transfer due to the acoustic instability in the precursor. The transient state of our model that describes the current state of the shock at knot g, occurs during the evolution from a thermal gas dominated shock to a smooth cosmic-ray dominated shock. Assuming a distance of 2.3 kpc to Tycho's remnant we obtain values for the cosmic ray diffusion coefficient, the injection parameter, and the time scale for the energy transfer of 10^{24} cm^{2} s^{-1}, 4.2x10^{-3}, and 426 y, respectively. We have also studied the parameter space for fast (300 km s^{-1} - 3000 km s^{-1}), time-asymptotically steady shocks and have identified a branch of solutions, for which the temperature in the cosmic ray precursor typically reaches 2-6x10^{4} K and the bulk acceleration of the flow through the precursor is less than 10 km s^{-1}. These solutions fall into the low cosmic ray acceleration efficiency regime and are relatively insensitive to shock parameters. This low cosmic ray acceleration efficiency branch of solutions may provide a natural explanation for the line broadening of the H-alpha narrow component observed in non-radiative shocks in many supernova remnants.",astro-ph,Astrophysics
High redshift quasars in the COSMOS survey: the space density of z>3 X-ray selected QSOs,"We present a new measurement of the space density of high redshift (3.0<z<4.5), X-ray selected QSOs obtained by exploiting the deep and uniform multiwavelength coverage of the COSMOS survey. We have assembled a statistically large (40 objects), X-ray selected (F_{0.5-2 keV} >10^{-15} cgs), homogeneous sample of z>3 QSOs for which spectroscopic (22) or photometric (18) redshifts are available. We present the optical (color-color diagrams) and X-ray properties, the number counts and space densities of the z>3 X-ray selected quasars population and compare our findings with previous works and model predictions. We find that the optical properties of X-ray selected quasars are not significantly different from those of optically selected samples. There is evidence for substantial X-ray absorption (logN_H>23 cm^{-2}) in about 20% of the sources in the sample. The comoving space density of luminous (L_X >10^{44} erg s^-1) QSOs declines exponentially (by an e--folding per unit redshift) in the z=3.0-4.5 range, with a behavior similar to that observed for optically bright unobscured QSOs selected in large area optical surveys. Prospects for future, large and deep X-ray surveys are also discussed.",astro-ph,Astrophysics
Extragalactic Constraints on the Initial Mass Function,"The local stellar mass density is observed to be significantly lower than the value obtained from integrating the cosmic star formation history (SFH), assuming that all the stars formed with a Salpeter initial mass function (IMF). Even other favoured IMFs, more successful in reconciling the observed $z=0$ stellar mass density with that inferred from the SFH, have difficulties in reproducing the stellar mass density observed at higher redshift. In this study we investigate to what extent this discrepancy can be alleviated for any universal power-law IMF. We find that an IMF with a high-mass slope shallower (2.15) than the Salpeter slope (2.35) reconciles the observed stellar mass density with the cosmic star formation history, but only at low redshifts. At higher redshifts $z>0.5$ we find that observed stellar mass densities are systematically lower than predicted from the cosmic star formation history, for any universal power-law IMF.",astro-ph,Astrophysics
Inducing the cosmological constant from five-dimensional Weyl space,"We investigate the possibility of inducing the cosmological constant from extra dimensions by embedding our four-dimensional Riemannian space-time into a five-dimensional Weyl integrable space. Following approach of the induced matter theory we show that when we go down from five to four dimensions, the Weyl field may contribute both to the induced energy-tensor as well as to the cosmological constant, or more generally, it may generate a time-dependent cosmological parameter. As an application, we construct a simple cosmological model which has some interesting properties.",astro-ph,Astrophysics
SCP06F6: A carbon-rich extragalactic transient at redshift z~0.14?,"We show that the spectrum of the unusual transient SCP06F6 is consistent with emission from a cool, optically thick and carbon-rich atmosphere if the transient is located at a redshift of z~0.14. The implied extragalactic nature of the transient rules out novae, shell flashes, and V838 Mon-like events as cause of the observed brightening. The distance to SCP06F6 implies a peak magnitude of M_I ~- 18, in the regime of supernovae. While the morphology of the light curve of SCP06F6 around the peak in brightness resembles the slowly evolving Type IIn supernovae SN1994Y and SN2006gy its spectroscopic appearence differs from all previous observed supernovae. We further report the detection of an X-ray source co-incident with SCP06F6 in a target of opportunity XMM-Newton observation made during the declining phase of the transient. The X-ray luminosity of L_X ~- (5+-1)e42 erg/s is two orders of magnitude higher than observed to date from supernovae. If related to a supernova event, SCP06F6 may define a new class. An alternative, though less likely, scenario is the tidal disruption of a carbon-rich star.",astro-ph,Astrophysics
MultiNest: an efficient and robust Bayesian inference tool for cosmology and particle physics,"We present further development and the first public release of our multimodal nested sampling algorithm, called MultiNest. This Bayesian inference tool calculates the evidence, with an associated error estimate, and produces posterior samples from distributions that may contain multiple modes and pronounced (curving) degeneracies in high dimensions. The developments presented here lead to further substantial improvements in sampling efficiency and robustness, as compared to the original algorithm presented in Feroz & Hobson (2008), which itself significantly outperformed existing MCMC techniques in a wide range of astrophysical inference problems. The accuracy and economy of the MultiNest algorithm is demonstrated by application to two toy problems and to a cosmological inference problem focussing on the extension of the vanilla $$CDM model to include spatial curvature and a varying equation of state for dark energy. The MultiNest software, which is fully parallelized using MPI and includes an interface to CosmoMC, is available at http://www.mrao.cam.ac.uk/software/multinest/. It will also be released as part of the SuperBayeS package, for the analysis of supersymmetric theories of particle physics, at http://www.superbayes.org",astro-ph,Astrophysics
Spinning down newborn neutron stars: nonlinear development of the r-mode instability,"We model the nonlinear saturation of the r-mode instability via three-mode couplings and the effects of the instability on the spin evolution of young neutron stars. We include one mode triplet consisting of the r-mode and two near resonant inertial modes that couple to it. We find that the spectrum of evolutions is more diverse than previously thought. The evolution of the star is dynamic and initially dominated by fast neutrino cooling. Nonlinear effects become important when the r-mode amplitude grows above its first parametric instability threshold. The balance between neutrino cooling and viscous heating plays an important role in the evolution. Depending on the initial r-mode amplitude, and on the strength of the viscosity and of the cooling this balance can occur at different temperatures. If thermal equilibrium occurs on the r-mode stability curve, where gravitational driving equals viscous damping, the evolution may be adequately described by a one-mode model. Otherwise, nonlinear effects are important and lead to various more complicated scenarios. Once thermal balance occurs, the star spins-down oscillating between thermal equilibrium states until the instability is no longer active. For lower viscosity we observe runaway behavior in which the r-mode amplitude passes several parametric instability thresholds. In this case more modes need to be included to model the evolution accurately. In the most optimistic case, we find that gravitational radiation from the r-mode instability in a very young, fast spinning neutron star within about 1 Mpc of Earth may be detectable by advanced LIGO for years, and perhaps decades, after formation. Details regarding the amplitude and duration of the emission depend on the internal dissipation of the modes of the star, which would be probed by such detections.",astro-ph,Astrophysics
Chemistry in Disks. II. -- Poor molecular content of the AB Aur disk,"We study the molecular content and chemistry of a circumstellar disk surrounding the Herbig Ae star AB Aur at (sub-)millimeter wavelengths. Our aim is to reconstruct the chemical history and composition of the AB Aur disk and to compare it with disks around low-mass, cooler T Tauri stars. We observe the AB Aur disk with the IRAM Plateau de Bure Interferometer in the C- and D- configurations in rotational lines of CS, HCN, C2H, CH3OH, HCO+, and CO isotopes. Using an iterative minimization technique, observed columns densities and abundances are derived. These values are further compared with results of an advanced chemical model that is based on a steady-state flared disk structure with a vertical temperature gradient, and gas-grain chemical network with surface reactions. We firmly detect HCO+ in the 1--0 transition, tentatively detect HCN, and do not detect CS, C2H, and CH3OH. The observed HCO+ and 13CO column densities as well as the upper limits to the column densities of HCN, CS, C2H, and CH3OH are in good agreement with modeling results and those from previous studies. The AB Aur disk possesses more CO, but is less abundant in other molecular species compared to the DM Tau disk. This is primarily caused by intense UV irradiation from the central Herbig A0 star, which results in a hotter disk where CO freeze out does not occur and thus surface formation of complex CO-bearing molecules might be inhibited.",astro-ph,Astrophysics
Suzaku and XMM-Newton Observations of Diffuse X-ray Emission from the Eastern Tip Region of the Carina Nebula,"The eastern tip region of the Carina Nebula was observed with the Suzaku XIS for 77 ks to conduct a high-precision spectral study of extended X-ray emission. XMM-Newton EPIC data of this region were also utilized to detect point sources. The XIS detected strong extended X-ray emission from the entire field-of-view with a 0.2--5 keV flux of $0.7\sim4\times10^{-14}$ erg s$^{-1}$ arcmin$^{-2}$. The emission has a blob-like structure that coincides with an ionized gas filament observed in mid-infrared images. Contributions of astrophysical backgrounds and the detected point sources were insignificant. Thus the emission is diffuse in nature. The X-ray spectrum of the diffuse emission was represented by a two-temperature plasma model with temperatures of 0.3 and 0.6 keV and an absorption column density of 2$\times10^{21}$ cm$^{-1}$. The X-ray emission showed normal nitrogen-to-oxygen abundance ratios and a high iron-to-oxygen abundance ratio. The spectrally deduced parameters, such as temperatures and column densities, are common to the diffuse X-ray emission near $$ Car. Thus, the diffuse X-ray emission in these two fields may have the same origin. The spectral fitting results are discussed to constrain the origin in the context of stellar winds and supernovae.",astro-ph,Astrophysics
The resonant structure of Jupiter's trojan asteroids-II. What happens for different configurations of the planetary system,"In a previous paper, we have found that the resonance structure of the present Jupiter Trojan swarms could be split up into four different families of resonances. Here, in a first step, we generalize these families in order to describe the resonances occurring in Trojan swarms embedded in a generic planetary system. The location of these families changes under a modification of the fundamental frequencies of the planets and we show how the resonant structure would evolve during a planetary migration. We present a general method, based on the knowledge of the fundamental frequencies of the planets and on those that can be reached by the Trojans, which makes it possible to predict and localize the main events arising in the swarms during migration. In particular, we show how the size and stability of the Trojan swarms are affected by the modification of the frequencies of the planets. Finally, we use this method to study the global dynamics of the Jovian Trojan swarms when Saturn migrates outwards. Besides the two resonances found by Morbidelli et al (2005) which could have led to the capture of the current population just after the crossing of the 2:1 orbital resonance, we also point out several sequences of chaotic events that can influence the Trojan population.",astro-ph,Astrophysics
O-like Stars in the Direction of the North America and Pelican Nebulae,"In the area covering the complex of the North America and Pelican nebulae we identified 13 faint stars with J-H and H-Ks color indices which simulate heavily reddened O-type stars. One of these stars is CP05-4 classified as O5 V by Comeron and Pasquali (2005). Combining magnitudes of these stars in the passbands I, J, H, Ks and [8.3] we were able to suspect that two of them are carbon stars and five are late M-type AGB stars. Interstellar extinction in the direction of these stars was estimated from the background red clump giants in the J-H vs. H-Ks diagram and from star counts in the Ks passband. Four or five stars are found to have a considerable probability of being O-type stars, contributing to the ionization of North America and Pelican. If they really are O-type stars, their interstellar extinction A(V) should be from 16 to 35 mag. Two of them seem to be responsible for bright E and J radio rims discovered by Matthews and Goss (1980).",astro-ph,Astrophysics
Large-scale structure formation in cosmology with classical and tachyonic scalar fields,The evolution of scalar perturbations is studied for 2-component (non-relativistic matter and dark energy) cosmological models at the linear and non-linear stages. The dark energy is assumed to be the scalar field with either classical or tachyonic Lagrangian and constant equation-of-state parameter w. The fields and potentials were reconstructed for the set of cosmological parameters derived from observations. The comparison of the calculated within these models and experimental large-scale structure characteristics is made. It is shown that for w=const such analysis can't remove the existing degeneracy of the dark energy models.,astro-ph,Astrophysics
Vsop2/Astro-G Project,"We introduce a new space VLBI project, the Second VLBI Space Observatory Program (VSOP2), following the success of the VLBI Space Observatory Program (VSOP1). VSOP2 has 10 times higher angular resolution, up to about 40 micro arcseconds, 10 times higher frequency up to 43 GHz, and 10 times higher sensitivity compared to VSOP1. Then VSOP2 should become a most powerful tool to observe innermost regions of AGN and astronomical masers. ASTRO-G is a spacecraft for VSOP2 project constructing in ISAS/JAXA since July 2007. ASTRO-G will be launched by JAXA H-IIA rocket in fiscal year 2012. ASTRO-G and ground-based facilities are combined as VSOP2. To achieve the good observation performances, we must realize new technologies. They are large precision antenna, fast-position switching capability, new LNAs, and ultra wide-band down link, etc.. VSOP2 is a huge observation system involving ASTRO-G, ground radio telescopes, tracking stations, and correlators, one institute can not prepare a whole system of VSOP2. Then we must need close international collaboration to get sufficient quality of resultant maps and to give a sufficient quantity of observation time for astronomical community. We formed a new international council to provide guidance on scientific aspects related of VSOP2, currently called the VSOP2 International Science Council (VISC2).",astro-ph,Astrophysics
Predicting RNA Secondary Structures with Arbitrary Pseudoknots by Maximizing the Number of Stacking Pairs,"The paper investigates the computational problem of predicting RNA secondary structures. The general belief is that allowing pseudoknots makes the problem hard. Existing polynomial-time algorithms are heuristic algorithms with no performance guarantee and can only handle limited types of pseudoknots. In this paper we initiate the study of predicting RNA secondary structures with a maximum number of stacking pairs while allowing arbitrary pseudoknots. We obtain two approximation algorithms with worst-case approximation ratios of 1/2 and 1/3 for planar and general secondary structures,respectively. For an RNA sequence of $n$ bases, the approximation algorithm for planar secondary structures runs in $O(n^3)$ time while that for the general case runs in linear time. Furthermore, we prove that allowing pseudoknots makes it NP-hard to maximize the number of stacking pairs in a planar secondary structure. This result is in contrast with the recent NP-hard results on psuedoknots which are based on optimizing some general and complicated energy functions.",q-bio,Quantitative Biology
VLSI layouts and DNA physical mappings,We show that an important problem ($k$-ICG) in computational biology is equivalent to a colored version of a well-known graph layout problem ($k$-CVS).,q-bio,Quantitative Biology
Efficient pooling designs for library screening,"We describe efficient methods for screening clone libraries, based on pooling schemes which we call ``random $k$-sets designs''. In these designs, the pools in which any clone occurs are equally likely to be any possible selection of $k$ from the $v$ pools. The values of $k$ and $v$ can be chosen to optimize desirable properties. Random $k$-sets designs have substantial advantages over alternative pooling schemes: they are efficient, flexible, easy to specify, require fewer pools, and have error-correcting and error-detecting capabilities. In addition, screening can often be achieved in only one pass, thus facilitating automation. For design comparison, we assume a binomial distribution for the number of ``positive'' clones, with parameters $n$, the number of clones, and $c$, the coverage. We propose the expected number of {\em resolved positive} clones---clones which are definitely positive based upon the pool assays---as a criterion for the efficiency of a pooling design. We determine the value of $k$ which is optimal, with respect to this criterion, as a function of $v$, $n$ and $c$. We also describe superior $k$-sets designs called $k$-sets packing designs. As an illustration, we discuss a robotically implemented design for a 2.5-fold-coverage, human chromosome 16 YAC library of $n=1,298$ clones. We also estimate the probability each clone is positive, given the pool-assay data and a model for experimental errors.",q-bio,Quantitative Biology
On Reidys and Stadler's metrics for RNA secondary structures,We compute explicitly several abstract metrics for RNA secondary structures defined by Reidys and Stadler.,q-bio,Quantitative Biology
Analysis of Three-Dimensional Protein Images,"A fundamental goal of research in molecular biology is to understand protein structure. Protein crystallography is currently the most successful method for determining the three-dimensional (3D) conformation of a protein, yet it remains labor intensive and relies on an expert's ability to derive and evaluate a protein scene model. In this paper, the problem of protein structure determination is formulated as an exercise in scene analysis. A computational methodology is presented in which a 3D image of a protein is segmented into a graph of critical points. Bayesian and certainty factor approaches are described and used to analyze critical point graphs and identify meaningful substructures, such as alpha-helices and beta-sheets. Results of applying the methodologies to protein images at low and medium resolution are reported. The research is related to approaches to representation, segmentation and classification in vision, as well as to top-down approaches to protein structure prediction.",q-bio,Quantitative Biology
Automated tuning of bifurcations via feedback,"The present paper studies a feedback regulation problem, which may be interpreted as an adaptive control problem, but has not yet been studied in the control literature. The problem, which arises in at least two different biological applications (Hopf bifurcations in the auditory system, and neural integrators used to maintain persistent neural activity), is that of tuning a parameter so as to bring it to a value at which a bifurcation takes place.",q-bio,Quantitative Biology
On the Critical Capacity of the Hopfield Model,We estimate the critical capacity of the zero-temperature Hopfield model by using a novel and rigorous method. The probability of having a stable fixed point is one when $\le 0.113$ for a large number of neurons. This result is an advance on all rigorous results in the literature and the relationship between the capacity $$ and retrieval errors obtained here for small $$ coincides with replica calculation results.,q-bio,Quantitative Biology
Control of Spatially Heterogeneous and Time-Varying Cellular Reaction Networks: A New Summation Law,"A hallmark of a plethora of intracellular signaling pathways is the spatial separation of activation and deactivation processes that potentially results in precipitous gradients of activated proteins. The classical Metabolic Control Analysis (MCA), which quantifies the influence of an individual process on a system variable as the control coefficient, cannot be applied to spatially separated protein networks. The present paper unravels the principles that govern the control over the fluxes and intermediate concentrations in spatially heterogeneous reaction networks. Our main results are two types of the control summation theorems. The first type is a non-trivial generalization of the classical theorems to systems with spatially and temporally varying concentrations. In this generalization, the process of diffusion, which enters as the result of spatial concentration gradients, plays a role similar to other processes such as chemical reactions and membrane transport. The second summation theorem is completely novel. It states that the control by the membrane transport, the diffusion control coefficient multiplied by two, and a newly introduced control coefficient associated with changes in the spatial size of a system (e.g., cell), all add up to one and zero for the control over flux and concentration. Using a simple example of a kinase/phosphatase system in a spherical cell, we speculate that unless active mechanisms of intracellular transport are involved, the threshold cell size is limited by the diffusion control, when it is beginning to exceed the spatial control coefficient significantly.",q-bio,Quantitative Biology
Cortical Potential Distributions and Cognitive Information Processing,The use of cortical field potentials rather than the details of spike trains as the basis for cognitive information processing is proposed. This results in a space of cognitive elements with natural metrics. Sets of spike trains may also be considered to be points in a multidimensional metric space. The closeness of sets of spike trains in such a space implies the closeness of points in the resulting function space of potential distributions.,q-bio,Quantitative Biology
Determination of Functional Network Structure from Local Parameter Dependence Data,"In many applications, such as those arising from the field of cellular networks, it is often desired to determine the interaction (graph) structure of a set of differential equations, using as data measured sensitivities. This note proposes an approach to this problem.",q-bio,Quantitative Biology
The Dynamics of a Vertically Transmitted Disease,"An SIRS epidemiological model for a vertically transmitted disease is discussed. We give a complete global analysis in terms of three explicit threshold parameters which respectively govern the existence and stability of an endemic proportion equilibrium, the increase of the total population and the growth of the infective population. This paper gereralize the results of Busenberg and van den Driessche.",q-bio,Quantitative Biology
In search of an evolutionary coding style,"In the near future, all the human genes will be identified. But understanding the functions coded in the genes is a much harder problem. For example, by using block entropy, one has that the DNA code is closer to a random code then written text, which in turn is less ordered then an ordinary computer code; see \cite{schmitt}.
  Instead of saying that the DNA is badly written, using our programming standards, we might say that it is written in a different style -- an evolutionary style.
  We will suggest a way to search for such a style in a quantified manner by using an artificial life program, and by giving a definition of general codes and a definition of style for such codes.",q-bio,Quantitative Biology
Ecological model of extinctions,"We present numerical results based on a simplified ecological system in evolution, showing features of extinction similar to that claimed for the biosystem on Earth. In the model each species consists of a population in interaction with the others, that reproduces and evolves in time. Each species is simultaneously a predator and a prey in a food chain. Mutations that change the interactions are supposed to occur randomly at a low rate. Extinctions of populations result naturally from the predator-prey dynamics. The model is not pinned in a fitness variable, and natural selection arises from the dynamics.",q-bio,Quantitative Biology
Self-organized Criticality in Living Systems,"We suggest that ensembles of self-replicating entities such as biological systems naturally evolve into a self-organized critical state in which fluctuations, as well as waiting-times between phase transitions are distributed according to a 1/f power law. We demonstrate these concepts by analyzing a population of self-replicating strings (segments of computer-code) subject to mutation and survival of the fittest.",q-bio,Quantitative Biology
On Modelling Life,"We present a theoretical as well as experimental investigation of a population of self-replicating segments of code subject to random mutation and survival of the fittest. Under the assumption that such a system constitutes a minimal system with characteristics of life, we obtain a number of statements on the evolution of complexity and the trade-off between entropy and information.",q-bio,Quantitative Biology
Stochastic S-I-S-O-E Epidemic Model,An stochastic SIS epidemic model in an open environment is presented.,q-bio,Quantitative Biology
Evolutionary Dynamics and Optimization: Neutral Networks as Model-Landscapes for RNA Secondary-Structure Folding-Landscapes,"We view the folding of RNA-sequences as a map that assigns a pattern of base pairings to each sequence, known as secondary structure. These preimages can be constructed as random graphs (i.e. the neutral networks associated to the structure $s$). By interpreting the secondary structure as biological information we can formulate the so called Error Threshold of Shapes as an extension of Eigen's et al. concept of an error threshold in the single peak landscape. Analogue to the approach of Derrida & Peliti for a of the population on the neutral network. On the one hand this model of a single shape landscape allows the derivation of analytical results, on the other hand the concept gives rise to study various scenarios by means of simulations, e.g. the interaction of two different networks. It turns out that the intersection of two sets of compatible sequences (with respect to the pair of secondary structures) plays a key role in the search for ''fitter'' secondary structures.",q-bio,Quantitative Biology
Cancer Detection via Determination of Fractal Cell Dimension,We utilize the fractal dimension of the perimeter surface of cell sections as a new observable to characterize cells of different types. We propose that it is possible to distinguish cancerous from healthy cells with the aid of this new approach. As a first application we show that it is possible to perform this distinction between patients with hairy-cell lymphocytic leukemia and those with normal blood lymphocytes.,q-bio,Quantitative Biology
A model for evolution and extinction,"We present a model for evolution and extinction in large ecosystems. The model incorporates the effects of interactions between species and the influences of abiotic environmental factors. We study the properties of the model by approximate analytic solution and also by numerical simulation, and use it to make predictions about the distribution of extinctions and species lifetimes that we would expect to see in real ecosystems. It should be possible to test these predictions against the fossil record. The model indicates that a possible mechanism for mass extinction is the coincidence of a large coevolutionary avalanche in the ecosystem with a severe environmental disturbance.",q-bio,Quantitative Biology
Smoothing representation of fitness landscapes - the genotype-phenotype map of evolution,We investigate an simple evolutionary game of sequences and demonstrate on this example the structure of fitness landscapes in discrete problems. We show the smoothing action of the genotype-phenotype mapping which still makes it feasible for evolution to work. Further we propose the density of sequence states as a classifying measure of fitness landscapes.,q-bio,Quantitative Biology
Biased Random-Walk Learning: A Neurobiological Correlate to Trial-and-Error,"Neural network models offer a theoretical testbed for the study of learning at the cellular level. The only experimentally verified learning rule, Hebb's rule, is extremely limited in its ability to train networks to perform complex tasks. An identified cellular mechanism responsible for Hebbian-type long-term potentiation, the NMDA receptor, is highly versatile. Its function and efficacy are modulated by a wide variety of compounds and conditions and are likely to be directed by non-local phenomena. Furthermore, it has been demonstrated that NMDA receptors are not essential for some types of learning. We have shown that another neural network learning rule, the chemotaxis algorithm, is theoretically much more powerful than Hebb's rule and is consistent with experimental data. A biased random-walk in synaptic weight space is a learning rule immanent in nervous activity and may account for some types of learning -- notably the acquisition of skilled movement.",q-bio,Quantitative Biology
A quantitative measurement of spatial order in ventricular fibrillation,"As an objective measurement of spatial order in ventricular fibrillation (VF), spatial correlation functions and their characteristic lengths were estimated from epicardial electrograms of pigs in VF. The correlation length of the VF in pigs was found to be approximately 4-10 mm, varying as fibrillation progressed. The degree of correlation decreased in the first 4 seconds after fibrillation then increased over the next minute. The correlation length is much smaller than the scale of the heart, suggesting that many independent regions of activity exist on the epicardium at any one time. On the other hand, the correlation length is 4 to 10 times the interelectrode spacing, indicating that some coherence is present. These results imply that the heart behaves during VF as a high-dimensional, but not random, system involving many spatial degrees of freedom, which may explain the lack of convergence of fractal dimension estimates reported in the literature. Changes in the correlation length also suggest that VF reorganizes slightly in the first minute after an initial breakdown in structure.",q-bio,Quantitative Biology
Self-organization of hierarchical structures in nonlocally coupled replicator models,"We study a simple replicator model with non-symmetric and nonlocal interactions. Hierarchical structures with prey-predator relations are self-organized from a homogeneous state, induced by the dynamical instability of nonlinear interactions.",q-bio,Quantitative Biology
A model for mutation in bacterial populations,"We describe the evolution of $E.coli$ populations through a Bak-Sneppen type model which incorporates random mutations. We show that, for a value of the mutation level which coincides with the one estimated from experiments, this model reproduces the measures of mean fitness relative to that of a common ancestor, performed for over 10,000 bacterial generations.",q-bio,Quantitative Biology
Adaptive learning and coloniality in birds,"We introduce here three complementary models to analyze the role of predation pressure in avian coloniality. Different explanations have been proposed for the existence of colonial breeding behavior in birds, but field studies offer no conclusive results. We first propose a learning model in which the decision of birds are taken according to the collective performance. The properties of the system are then studied according to a model in which birds choose according to their individual experience, and the agreement of the introduction of spatial structure with field data are then shown.",q-bio,Quantitative Biology
"Reply to ``Comments on Kullback-Leibler and renormalized entropies: Applications to electroencephalograms of epilepsy patients""","Kopitzki et al (preceeding comment) claim that the relationship between Renormalized and Kullback-Leibler entropies has already been given in their previous papers. Moreover, they argue that the first can give more useful information for e.g. localizing the seizure-generating area in epilepsy patients.
  In our reply we stress that if the relationship between both entropies would have been known by them, they should have noticed that the condition on the effective temperature is unnecessary. Indeed, this condition led them to choose different reference segments for different channels, even if this was physiologically unplausible. Therefore, we still argue that it is very unlikely that renormalized entropy will give more information than the conventional Kullback-Leibler entropy.",q-bio,Quantitative Biology
Regularization of Synchronized Chaotic Bursts,The onset of regular bursts in a group of irregularly bursting neurons with different individual properties is one of the most interesting dynamical properties found in neurobiological systems. In this paper we show how synchronization among chaotically bursting cells can lead to the onset of regular bursting. In order to clearly present the mechanism behind such regularization we model the individual dynamics of each cell with a simple two-dimensional map that produces chaotic bursting behavior similar to biological neurons.,q-bio,Quantitative Biology
Noise Effects on the Complex Patterns of Abnormal Heartbeats,"Patients at high risk for sudden death often exhibit complex heart rhythms in which abnormal heartbeats are interspersed with normal heartbeats. We analyze such a complex rhythm in a single patient over a 12-hour period and show that the rhythm can be described by a theoretical model consisting of two interacting oscillators with stochastic elements. By varying the magnitude of the noise, we show that for an intermediate level of noise, the model gives best agreement with key statistical features of the dynamics.",q-bio,Quantitative Biology
Multiple mechanisms of spiral wave breakup in a model of cardiac electrical activity,"It has become widely accepted that the most dangerous cardiac arrhythmias are due to re- entrant waves, i.e., electrical wave(s) that re-circulate repeatedly throughout the tissue at a higher frequency than the waves produced by the heart's natural pacemaker (sinoatrial node). However, the complicated structure of cardiac tissue, as well as the complex ionic currents in the cell, has made it extremely difficult to pinpoint the detailed mechanisms of these life-threatening reentrant arrhythmias. A simplified ionic model of the cardiac action potential (AP), which can be fitted to a wide variety of experimentally and numerically obtained mesoscopic characteristics of cardiac tissue such as AP shape and restitution of AP duration and conduction velocity, is used to explain many different mechanisms of spiral wave breakup which in principle can occur in cardiac tissue. Some, but not all, of these mechanisms have been observed before using other models; therefore, the purpose of this paper is to demonstrate them using just one framework model and to explain the different parameter regimes or physiological properties necessary for each mechanism (such as high or low excitability, corresponding to normal or ischemic tissue, spiral tip trajectory types, and tissue structures such as rotational anisotropy and periodic boundary conditions). Each mechanism is compared with data from other ionic models or experiments to illustrate that they are not model-specific phenomena. The fact that many different breakup mechanisms exist has important implications for antiarrhythmic drug design and for comparisons of fibrillation experiments using different species, electromechanical uncoupling drugs, and initiation protocols.",q-bio,Quantitative Biology
Morphological Instability and Dynamics of Fronts in Bacterial Growth Models with Nonlinear Diffusion,"It has been argued that there is biological and modeling evidence that a non-linear diffusion coefficient of the type D(b) = D_0 b^{k} underlies the formation of a number of growth patterns of bacterial colonies. We study a reaction-diffusion system with a non-linear diffusion coefficient introduced by Ben-Jacob et al. Due to the fact that the bacterial diffusion coefficient vanishes when the bacterial density b -> 0, the standard linear stability analysis for fronts cannot be used. We introduce an extension of the stability analysis which can be applied to such singular fronts, map out the region of stability in the D-k-plane and derive an interfacial approximation in some limits. Our linear stability analysis and sharp interface formulation will also be applicable to other examples of interface formation due to nonlinear diffusion, like in porous media or in the problem of vortex motion in superconductors.",q-bio,Quantitative Biology
Zipf's Law in Gene Expression,"Using data from gene expression databases on various organisms and tissues, including yeast, nematodes, human normal and cancer tissues, and embryonic stem cells, we found that the abundances of expressed genes exhibit a power-law distribution with an exponent close to -1, i.e., they obey Zipf's law. Furthermore, by simulations of a simple model with an intra-cellular reaction network, we found that Zipf's law of chemical abundance is a universal feature of cells where such a network optimizes the efficiency and faithfulness of self-reproduction. These findings provide novel insights into the nature of the organization of reaction dynamics in living cells.",q-bio,Quantitative Biology
Mass Extinctions vs. Uniformitarianism in Biological Evolution,"It is usually believed that Darwin's theory leads to a smooth gradual evolution, so that mass extinctions must be caused by external shocks. However, it has recently been argued that mass extinctions arise from the intrinsic dynamics of Darwinian evolution. Species become extinct when swept by intermittent avalanches propagating through the global ecology. These ideas are made concrete through studies of simple mathematical models of coevolving species. The models exhibit self-organized criticality and describe some general features of the extinction pattern in the fossil record.",q-bio,Quantitative Biology
Helicoidal model for DNA opening,"We present a new dynamical model of DNA. This model has two degrees of freedom per base-pair: one radial variable related to the opening of the hydrogen bonds and an angular one related to the twisting of each base-pair responsible for the helicoidal structure of the molecule. The small amplitude dynamics of the model is studied analytically : we derive small amplitude envelope solutions made of a breather in the radial variables combined with a kink in the angular variables, showing the role of the topological constraints associated to the helicoidal geometry. We check the stability of the solutions by numerical integration of the motion equations.",q-bio,Quantitative Biology
Topology of Central Pattern Generators: Selection by Chaotic Neurons,"Central Pattern Generators (CPGs) in invertebrates are comprised of networks of neurons in which every neuron has reciprocal connections to other members of the CPG. This is a ``closed'' network topology. An ``open'' topology, where one or more neurons receives input but does not send output to other member neurons, is not found in these CPGs. In this paper we investigate a possible reason for this topological structure using the ability to perform a biological functional task as a measure of the efficacy of the network. When the CPG is composed of model neurons which exhibit regular membrane voltage oscillations, open topologies are essentially as able to maximize this functionality as closed topologies. When we replace these models by neurons which exhibit chaotic membrane voltage oscillations, the functional criterion selects closed topologies when the demands of the task are increased, and these are the topologies observed in known CPG networks. As isolated neurons from invertebrate CPGs are known in some cases to undergo chaotic oscillations (Abarbanel et al 1996, Hayashi Ishuzuka 1992) this provides a biological basis for understanding the class of closed network topologies we observe.",q-bio,Quantitative Biology
Extremal Coupled Map Lattices,"We propose a model for co-evolving ecosystems that takes into account two levels of description of an organism, for instance genotype and phenotype. Performance at the macroscopic level forces mutations at the microscopic level. These, in turn, affect the dynamics of the macroscopic variables. In some regions of parameter space, the system self-organises into a state with localised activity and power law distributions.",q-bio,Quantitative Biology
Phase Transition in Random Networks with Multiple States,"The critical boundaries separating ordered from chaotic behavior in randomly wired S-state networks are calculated. These networks are a natural generalization of random Boolean nets and are proposed as on extended approach to genetic regulatory systems, sets of cells in different states or collectives of agents engaged into a set of S possible tasks. A order parameter for the transition is computed and analysed. The relevance of these networks to biology, their relationships with standard cellular automata and possible extensions are outlined.",q-bio,Quantitative Biology
Large deviations for the Fleming-Viot process with neutral mutation and selection,"Large deviation principles are established for the Fleming-Viot processes with neutral mutation and selection, and the corresponding equilibrium measures as the sampling rate goes to 0. All results are first proved for the finite allele model, and then generalized, through the projective limit technique, to the infinite allele model. Explicit expressions are obtained for the rate functions.",q-bio,Quantitative Biology
Mass-extinction: Evolution and the effects of external influences on unfit species,"We present a new model for extinction in which species evolve in bursts or `avalanches', during which they become on average more susceptible to environmental stresses such as harsh climates and so are more easily rendered extinct. Results of simulations and analytic calculations using our model show a power-law distribution of extinction sizes which is in reasonable agreement with fossil data. e also see a number of features qualitatively similar to those seen in the fossil record. For example, we see frequent smaller extinctions in the wake of a large mass extinction, which arise because there is reduced competition for resources in the aftermath of a large extinction event, so that species which would not normally be able to compete can get a foothold, but only until the next cold winter or bad attack of the flu comes along to wipe them out.",q-bio,Quantitative Biology
Error threshold in the evolution of diploid organisms,"The effects of error propagation in the reproduction of diploid organisms are studied within the populational genetics framework of the quasispecies model. The dependence of the error threshold on the dominance parameter is fully investigated. In particular, it is shown that dominance can protect the wild-type alleles from the error catastrophe. The analysis is restricted to a diploid analogue of the single-peaked landscape.",q-bio,Quantitative Biology
Transition between immune and disease states in a cellular automaton model of clonal immune response,"In this paper we extend the Celada-Seiden (CS) model of the humoral immune response to include infectious virus and cytotoxic T lymphocytes (cellular response). The response of the system to virus involves a competition between the ability of the virus to kill the host cells and the host's ability to eliminate the virus. We find two basins of attraction in the dynamics of this system, one is identified with disease and the other with the immune state. There is also an oscillating state that exists on the border of these two stable states. Fluctuations in the population of virus or antibody can end the oscillation and drive the system into one of the stable states. The introduction of mechanisms of cross-regulation between the two responses can bias the system towards one of them. We also study a mean field model, based on coupled maps, to investigate virus-like infections. This simple model reproduces the attractors for average populations observed in the cellular automaton. All the dynamical behavior connected to spatial extension is lost, as is the oscillating feature. Thus the mean field approximation introduced with coupled maps destroys oscillations.",q-bio,Quantitative Biology
A condition for the genotype-phenotype mapping: Causality,"The appropriate choice of the genotype-phenotype mapping in combination with the mutation operator is important for a successful evolutionary search process. We suggest a measure to quantify the quality of this combination by addressing the question whether the relation among distances is carried over from one space to the other. Search processes which do not destroy the neighbourhood structure are termed strongly causal. We apply the proposed measure to parameter and structure optimisation problems in order to assess the combination (mapping, mutation operator) and at the same time to be able to propose improved settings.",q-bio,Quantitative Biology
Sentient Networks,"In this paper we consider the question whether a distributed network of sensors and data processors can form ""perceptions"" based on the sensory data. Because sensory data can have exponentially many explanations, the use of a central data processor to analyze the outputs from a large ensemble of sensors will in general introduce unacceptable latencies for responding to dangerous situations. A better idea is to use a distributed ""Helmholtz machine"" architecture in which the collective state of the network as a whole provides an explanation for the sensory data.",q-bio,Quantitative Biology
Coherence and clustering in ensembles of neural networks,"Large ensembles of globally coupled chaotic neural networks undergo a transition to complete synchronization for high coupling intensities. The onset of this fully coherent behavior is preceded by a regime where clusters of networks with identical activity are spontaneously formed. In these regimes of coherent collective evolution the dynamics of each neural network is still chaotic. These results may be relevant to the study of systems where interaction between elements is able to give rise to coherent complex behavior, such as in cortex activity or in insect societies.",q-bio,Quantitative Biology
Learning from Mistakes,"A simple model of self-organised learning with no classical (Hebbian) reinforcement is presented. Synaptic connections involved in mistakes are depressed. The model operates at a highly adaptive, probably critical, state reached by extremal dynamics similar to that of recent evolution models. Thus, one might think of the mechanism as synaptic Darwinism.",q-bio,Quantitative Biology
Speciation as Pattern Formation by Competition in a Smooth Fitness Landscape,"We investigate the problem of speciation and coexistence in simple ecosystems when the competition among individuals is included in the Eigen model for quasi-species. By suggesting an analogy between the competition among strains and the diffusion of a chemical inhibitor in a reaction-diffusion system, the speciation phenomenon is considered the analogous of chemical pattern formation in genetic space. In the limit of vanishing mutation rate we obtain analytically the conditions for speciation. Using different forms of the competition interaction we show that the speciation is absent for the genetic equivalent of a normal diffusing inhibitor, and is present for shorter-range interactions. The comparison with numerical simulations is very good.",q-bio,Quantitative Biology
Cellular Automaton Model for Immunology of Tumor Growth,"The stochastic discrete space-time model of an immune response on tumor spreading in a two-dimensional square lattice has been developed. The immunity-tumor interactions are described at the cellular level and then transferred into the setting of cellular automata (CA). The multistate CA model for system, in which all statesoflattice sites, composing of both immune and tumor cells populations, are the functions of the states of the 12 nearest neighbors. The CA model incorporates the essential featuresof the immunity-tumor system. Three regimes of neoplastic evolution including metastatic tumor growth and screen effect by inactive immune cells surrounding a tumor have been predicted.",q-bio,Quantitative Biology
Distributed Self-regulation Induced by Negative Feedbacks in Ecological and Economic Systems,We consider an ecological system governed by Lotka-Volterra dynamics and an example of an economic system as a mesomarket with perfect competition. We propose a mechanism for cooperative self-regulation that enables the system under consideration to respond properly to changes in the environment. This mechanism is based on (1) active individual behavior of the system elements at each hierarchical level and (2) self-processing of information caused by the hierarchical organization. It is shown how the proposed mechanism suppresses nonlocal interaction of elements belonging to a particular level as mediated by higher levels.,q-bio,Quantitative Biology
Large-scale evolution and extinction in a hierarchically structured environment,"A class of models for large-scale evolution and mass extinctions is presented. These models incorporate environmental changes on all scales, from influences on a single species to global effects. This is a step towards a unified picture of mass extinctions, which enables one to study coevolutionary effects and external abiotic influences with the same means. The generic features of such models are studied in a simple version, in which all environmental changes are generated at random and without feedback from other parts of the system.",q-bio,Quantitative Biology
Rapid parapatric speciation on holey adaptive landscapes,"A classical view of speciation is that reproductive isolation arises as a by-product of genetic divergence. Here, individual-based simulations are used to evaluate whether the mechanisms implied by this view may result in rapid speciation if the only source of genetic divergence are mutation and random genetic drift. Distinctive features of the simulations are the consideration of the complete process of speciation (from initiation until completion), and of a large number of loci, which was only one order of magnitude smaller than that of bacteria. It is demonstrated that rapid speciation on the time scale of hundreds of generations is plausible without the need for extreme founder events, complete geographic isolation, the existence of distinct adaptive peaks or selection for local adaptation. The plausibility of speciation is enhanced by population subdivision. Simultaneous emergence of more than two new species from a subdivided population is highly probable. Numerical examples relevant to the theory of centrifugal speciation and to the conjectures about the fate of ``ring species'' and ``sexual continuums'' are presented.",q-bio,Quantitative Biology
Two boundary model for freezing front propagation in biological tissue,"The response of the living tissue to the effects of strong heating or cooling can cause the blood flow rate to vary by an order of magnitude. A mathematical model for the freezing of living tissue is formulated which takes into account the nonlocal temperature dependence of the blood flow rate when the temperature distribution in the tissue is substantially nonuniform, as in cryosurgery.",q-bio,Quantitative Biology
Decline in extinction rates and scale invariance in the fossil record,"We show that the decline in the extinction rate during the Phanerozoic can be accurately parameterized by a logarithmic fit to the cumulative total extinction. This implies that extinction intensity is falling off approximately as the reciprocal of time. We demonstrate that this observation alone is sufficient to explain the existence of the proposed power-law forms in the distribution of the sizes of extinction events and in the power spectrum of Phanerozoic extinction, results which previously have been explained by appealing to self-organized critical theories of evolutionary dynamics.",q-bio,Quantitative Biology
Synchronous Behavior of Two Coupled Electronic Neurons,We report on experimental studies of synchronization phenomena in a pair of analog electronic neurons (ENs). The ENs were designed to reproduce the observed membrane voltage oscillations of isolated biological neurons from the stomatogastric ganglion of the California spiny lobster Panulirus interruptus. The ENs are simple analog circuits which integrate four dimensional differential equations representing fast and slow subcellular mechanisms that produce the characteristic regular/chaotic spiking-bursting behavior of these cells. In this paper we study their dynamical behavior as we couple them in the same configurations as we have done for their counterpart biological neurons. The interconnections we use for these neural oscillators are both direct electrical connections and excitatory and inhibitory chemical connections: each realized by analog circuitry and suggested by biological examples. We provide here quantitative evidence that the ENs and the biological neurons behave similarly when coupled in the same manner. They each display well defined bifurcations in their mutual synchronization and regularization. We report briefly on an experiment on coupled biological neurons and four dimensional ENs which provides further ground for testing the validity of our numerical and electronic models of individual neural behavior. Our experiments as a whole present interesting new examples of regularization and synchronization in coupled nonlinear oscillators.,q-bio,Quantitative Biology
Temporal correlations versus noise in the correlation matrix formalism: an example of the brain auditory response,"We adopt the concept of the correlation matrix to study correlations among sequences of time-extended events occuring repeatedly at consecutive time-intervals. As an application we analyse the magnetoencephalography recordings obtained from human auditory cortex in epoch mode during delivery of sound stimuli to the left or right ear. We look into statistical properties and the eigenvalue spectrum of the correlation matrix C calculated for signals corresponding to different trials and originating from the same or opposite hemispheres. The spectrum of C largely agrees with the universal properties of the Gaussian orthogonal ensemble of random matrices, with deviations characterised by eigenvectors with high eigenvalues. The properties of these eigenvectors and eigenvalues provide an elegant and powerful way of quantifying the degree of the underlying collectivity during well defined latency intervals with respect to stimulus onset. We also extend this analysis to study the time-lagged interhemispheric correlations, as a computationally less demanding alternative to other methods such as mutual information.",q-bio,Quantitative Biology
A Cellular Automata Model for Citrus Variagated Chlorosis,"A cellular automata model is proposed to analyze the progress of Citrus Variegated Chlorosis epidemics in So Paulo oranges plantation. In this model epidemiological and environmental features, such as motility of sharpshooter vectors which perform Lvy flights, hydric and nutritional level of plant stress and seasonal climatic effects, are included. The observed epidemics data were quantitatively reproduced by the proposed model varying the parameters controlling vectors motility, plant stress and initial population of diseased plants.",q-bio,Quantitative Biology
Theory of Robustness of Irreversible Differentiation in a Stem Cell System: Chaos hypothesis,"Based on extensive study of a dynamical systems model of the development of a cell society, a novel theory for stem cell differentiation and its regulation is proposed as the ``chaos hypothesis''. Two fundamental features of stem cell systems - stochastic differentiation of stem cells and the robustness of a system due to regulation of this differentiation - are found to be general properties of a system of interacting cells exhibiting chaotic intra-cellular reaction dynamics and cell division, whose presence does not depend on the detail of the model. It is found that stem cells differentiate into other cell types stochastically due to a dynamical instability caused by cell-cell interactions, in a manner described by the Isologous Diversification theory. This developmental process is shown to be stable not only with respect to molecular fluctuations but also with respect to removal of cells. With this developmental process, the irreversible loss of multipotency accompanying the change from a stem cell to a differentiated cell is shown to be characterized by a decrease in the chemical diversity in the cell and of the complexity of the cellular dynamics. The relationship between the division speed and this loss of multipotency is also discussed. Using our model, some predictions that can be tested experimentally are made for a stem cell system.",q-bio,Quantitative Biology
Evolution of genetic code through isologous diversification of cellular states,"Evolution of genetic code is studied as the change in the choice of enzymes that are used to synthesize amino acids from the genetic information of nucleic acids. We propose the following theory: the differentiation of physiological states of a cell allows for the different choice of enzymes, and this choice is later fixed genetically through evolution. To demonstrate this theory, a dynamical systems model consisting of the concentrations of metabolites, enzymes, amino acyl tRNA synthetase, and tRNA-amino acid complex in a cell is introduced and numerically studied. It is shown that the biochemical states of cells are differentiated by cell-cell interaction, and each differentiated type takes to use different synthetase. Through the mutation of genes, this difference in the genetic code is amplified and stabilized. Relevance of this theory to the evolution of non-universal genetic code in mitochondria is suggested.
  The present theory for the evolution of genetic code is based on our recent theory of isologous symbiotic speciation, which is briefly reviewed. According to the theory, phenotypes of organisms are first differentiated into distinct types through the interaction and developmental dynamics, even though they have identical genotypes, and later with the mutation in genotype, the genotype also differentiates into discrete types, while maintaining the `symbiotic' relationship between the types. Relevance of the theory to natural as well as artificial evolution is discussed.",q-bio,Quantitative Biology
Magnitude and Sign Correlations in Heartbeat Fluctuations,"We propose an approach for analyzing signals with long-range correlations by decomposing the signal increment series into magnitude and sign series and analyzing their scaling properties. We show that signals with identical long-range correlations can exhibit different time organization for the magnitude and sign. We find that the magnitude series relates to the nonlinear properties of the original time series, while the sign series relates to the linear properties. We apply our approach to the heartbeat interval series and find that the magnitude series is long-range correlated, while the sign series is anticorrelated and that both magnitude and sign series may have clinical applications.",q-bio,Quantitative Biology
Waiting time to (and duration of) parapatric speciation,"Using a weak migration and weak mutation approximation, I study the average waiting time to and the average duration of parapatric speciation. The description of reproductive isolation used is based on the classical Dobzhansky model and its recently proposed multilocus generalizations. The dynamics of parapatric speciation is modeled as a biased random walk with absorption performed by the average genetic distance between the residents and immigrants. If a small number of genetic changes is sufficient for complete reproductive isolation, mutation and random genetic drift alone can cause speciation on the time scale of 10-1000 times the inverse of the mutation rate. Even relatively weak selection for local adaptation can dramatically decrease the waiting time to speciation. The duration of parapatric speciation is shorter by orders of magnitude than the waiting time to speciation. For a wide range of parameter values, the duration of speciation is order one over the mutation rate. In general, parapatric speciation is expected to be triggered by changes in the environment.",q-bio,Quantitative Biology
Obtaining single stimulus evoked potentials with Wavelet Denoising,"We present a method for the analysis of electroencephalograms (EEG).
  In particular, small signals due to stimulation, so called evoked potentials, have to be detected in the background EEG. This is achieved by using a denoising implementation based on the wavelet decomposition. One recording of visual evoked potentials, and recordings of auditory evoked potentials from 4 subjects corresponding to different age groups are analyzed. We find higher variability in older individuals. Moreover, since the evoked potentials are identified at the single stimulus level (without need of ensemble averaging), this will allow the calculation of better resolved averages. Since the method is parameter free (i.e. it does not need to be adapted to the particular characteristics of each recording), implementations in clinical settings are imaginable.",q-bio,Quantitative Biology
Phenotypical Behavior and Evolutionary Slavery,"A new evolutionary solution to Prisoner Dilemma situations is proposed in this paper. A specific genetic code may have different phenotypes, meaning different strategies for different individuals carrying that gene. This means that, under the right parameters, it is a good evolutionary solution to create two types of phenotypes with different strategies, here called as leaders and servants. In this solution, servants always cooperate with the leaders and leaders never do with the servants. Inside the same class, the usual strategies apply. Possible applications of this solution are discussed.",q-bio,Quantitative Biology
The Influence of Predator-Prey Population Dynamics on the Long-term Evolution of Food Web Structure,"We develop a set of equations to describe the population dynamics of many interacting species in food webs. Predator-prey interactions are non-linear, and are based on ratio-dependent functional responses. The equations account for competition for resources between members of the same species, and between members of different species. Predators divide their total hunting/foraging effort between the available prey species according to an evolutionarily stable strategy (ESS). The ESS foraging behaviour does not correspond to the predictions of optimal foraging theory. We use the population dynamics equations in simulations of the Webworld model of evolving ecosystems. New species are added to an existing food web due to speciation events, whilst species become extinct due to coevolution and competition. We study the dynamics of species-diversity in Webworld on a macro-evolutionary timescale. Coevolutionary interactions are strong enough to cause continuous overturn of species, in contrast to our previous Webworld simulations with simpler population dynamics. Although there are significant fluctuations in species diversity because of speciation and extinction, very large scale extinction avalanches appear to be absent from the dynamics, and we find no evidence for self-organised criticality.",q-bio,Quantitative Biology
Analysis of immune network dynamical system model with small number of degrees of freedom,"We numerically study a dynamical system model of an idiotypic immune network with a small number of degrees of freedom. The model was originally introduced by Varela et.al., and describes antibodies interacting in a body in order to prepare for the invasion of external antigens.
  The main purpose of this paper is to investigate the direction of change in the network system when antigens invade it. We investigate three models, original model, a modified model and a modified model with a threshold of concentration over which each antibody can recognize other antibodies. In all these models, both chaotic and periodic states exist. In particular, we find peculiar states organized in the network, the clustering state.",q-bio,Quantitative Biology
Stabilization/destabilization of cell membranes by multivalent ions: Implications for membrane fusion and division,"We propose a mechanism for the stabilization/destabilization of cell membranes by multivalent ions with an emphasis on its implications for the division and fusion of cells. We find that multivalent cations preferentially adsorbed onto a membrane dramatically changes the membrane stability. They not only reduce the surface charge density of the membrane but also induce a repulsive barrier to pore growth. While both of these effects lead to enhanced membrane stability against vesiculation and pore growth, the repulsive barrier arises from correlated fluctuations of the adsorbed cations and favors closure of a pore. Finally, the addition of a small amount of multivalent anions can reverse the membrane stabilization, providing an effective way to regulate membrane stability.",q-bio,Quantitative Biology
"Comment on ""Elasticity Model of a Supercoiled DNA Molecule""",We perform simulations to numerically study the writhe distribution of a stiff polymer. We compare with analytic results of Bouchiat and Mezard (PRL 80 1556- (1998); cond-mat/9706050).,q-bio,Quantitative Biology
Analytical solution of a model for complex food webs,"We investigate numerically and analytically a recently proposed model for food webs [Nature {\bf 404}, 180 (2000)] in the limit of large web sizes and sparse interaction matrices. We obtain analytical expressions for several quantities with ecological interest, in particular the probability distributions for the number of prey and the number of predators. We find that these distributions have fast-decaying exponential and Gaussian tails, respectively. We also find that our analytical expressions are robust to changes in the details of the model.",q-bio,Quantitative Biology
Dynamic modeling of gene expression data,We describe the time evolution of gene expression levels by using a time translational matrix to predict future expression levels of genes based on their expression levels at some initial time. We deduce the time translational matrix for previously published DNA microarray gene expression data sets by modeling them within a linear framework using the characteristic modes obtained by singular value decomposition. The resulting time translation matrix provides a measure of the relationships among the modes and governs their time evolution. We show that a truncated matrix linking just a few modes is a good approximation of the full time translation matrix. This finding suggests that the number of essential connections among the genes is small.,q-bio,Quantitative Biology
Computer Simulations for Biological Ageing and Sexual Reproduction,"The sexual version of the Penna model of biological ageing, simulated since 1996, is compared here with alternative forms of reproduction as well as with models not involving ageing. In particular we want to check how sexual forms of life could have evolved and won over earlier asexual forms hundreds of million years ago. This computer model is based on the mutation-accumulation theory of ageing, using bits-strings to represent the genome. Its population dynamics is studied by Monte Carlo methods.",q-bio,Quantitative Biology
Small world patterns in food webs,"The analysis of some species-rich, well-defined food webs shows that they display the so called Small World behavior shared by a number of disparate complex systems. The three systems analysed (Ythan estuary web, Silwood web and the Little Rock lake web) have different levels of taxonomic resolution, but all of them involve high clustering and short path lengths between species. Additionally, the distribution of connections with fat-tail power law behavior. These features suggest that communities might be self-organized in such a way that high homeostasis to perturbations (with short transient times to recovery) would be at work. The consequences for ecological theory are outlined.",q-bio,Quantitative Biology
Concerted motion of protons in hydrogen bonds of DNA-type molecules,"We study the dynamical behaviour of the proton transfer in the hydrogen bonds in the base-pairs of the double helices of the DNA type. Under the assumption that the elastic and the tunnelling degrees of freedom may be coupled, we derive a non-linear and non-local Schrodinger equation (SNLNL) that describes the concerted motion of the proton tunnelling. Rough estimates of the solutions to the SNLNL show an intimate interplay between the concerted tunnelling of protons and the symmetry of double helix.",q-bio,Quantitative Biology
Analytical description of finite size effects for RNA secondary structures,"The ensemble of RNA secondary structures of uniform sequences is studied analytically. We calculate the partition function for very long sequences and discuss how the cross-over length, beyond which asymptotic scaling laws apply, depends on thermodynamic parameters. For realistic choices of parameters this length can be much longer than natural RNA molecules. This has to be taken into account when applying asymptotic theory to interpret experiments or numerical results.",q-bio,Quantitative Biology
Topological Properties of Citation and Metabolic Networks,"Topological properties of ""scale-free"" networks are investigated by determining their spectral dimensions $d_S$, which reflect a diffusion process in the corresponding graphs. Data bases for citation networks and metabolic networks together with simulation results from the growing network model \cite{barab} are probed. For completeness and comparisons lattice, random, small-world models are also investigated. We find that $d_S$ is around 3 for citation and metabolic networks, which is significantly different from the growing network model, for which $d_S$ is approximately 7.5. This signals a substantial difference in network topology despite the observed similarities in vertex order distributions. In addition, the diffusion analysis indicates that whereas the citation networks are tree-like in structure, the metabolic networks contain many loops.",q-bio,Quantitative Biology
Replica-exchange multicanonical algorithm and multicanonical replica-exchange method for simulating systems with rough energy landscape,"We propose two efficient algorithms for configurational sampling of systems with rough energy landscape. The first one is a new method for the determination of the multicanonical weight factor.
  In this method a short replica-exchange simulation is performed and the multicanonical weight factor is obtained by the multiple-histogram reweighting techniques. The second one is a further extension of the first in which a replica-exchange multicanonical simulation is performed with a small number of replicas. These new algorithms are particularly useful for studying the protein folding problem.",q-bio,Quantitative Biology
Application of two-parameter dynamical replica theory to retrieval dynamics of associative memory with non-monotonic neurons,"The two-parameter dynamical replica theory (2-DRT) is applied to investigate retrieval properties of non-monotonic associative memory, a model which lacks thermodynamic potential functions. 2-DRT reproduces dynamical properties of the model quite well, including the capacity and basin of attraction. Superretrieval state is also discussed in the framework of 2-DRT. The local stability condition of the superretrieval state is given, which provides a better estimate of the region in which superretrieval is observed experimentally than the self-consistent signal-to-noise analysis (SCSNA) does.",q-bio,Quantitative Biology
Dynamic concentration of motors in microtubule arrays,We present experimental and theoretical studies of the dynamics of molecular motors in microtubule arrays and asters. By solving a convection-diffusion equation we find that the density profile of motors in a two-dimensional aster is characterized by continuously varying exponents. Simulations are used to verify the assumptions of the continuum model. We observe the concentration profiles of kinesin moving in quasi two-dimensional artificial asters by fluorescent microscopy and compare with our theoretical results.,q-bio,Quantitative Biology
Excluded Volume in Protein Sidechain Packing,"To examine the relationship between sidechain geometry and sidechain packing, we use an all-atom Monte Carlo simulation to sample the large space of sidechain conformations. We study three models of excluded volume and use umbrella sampling to effectively explore the entire space. We find that while excluded volume constraints reduce the size of conformational space by many orders of magnitude, the number of allowed conformations is still large. An average repacked conformation has 20% of its chi angles in a non-native state. Interestingly, well-packed conformations, with up to 50% non-native chi's exist. Entropy is distributed non-uniformly over positions, and we partially explain the observed distribution using rotamer probabilities derived from the pdb database. In spite of our finding that 65% of the native rotamers and 85% of chi 1 angles can be correctly predicted on the basis of excluded volume only, 95% of positions can accomodate more than 1 rotamer in simulation. We estimate that in order to quench the sidechain entropy observed in the presence of excluded volume interactions, other interactions (hydrophobic, polar, electrostatic) must provide an additional stabilization of at least 0.6 kT per residue in order to single out the native state.",q-bio,Quantitative Biology
Diffusing-wave spectroscopy of nonergodic media,"We introduce an elegant method which allows the application of diffusing-wave spectroscopy (DWS) to nonergodic, solid-like samples. The method is based on the idea that light transmitted through a sandwich of two turbid cells can be considered ergodic even though only the second cell is ergodic. If absorption and/or leakage of light take place at the interface between the cells, we establish a so-called ""multiplication rule"", which relates the intensity autocorrelation function of light transmitted through the double-cell sandwich to the autocorrelation functions of individual cells by a simple multiplication. To test the proposed method, we perform a series of DWS experiments using colloidal gels as model nonergodic media. Our experimental data are consistent with the theoretical predictions, allowing quantitative characterization of nonergodic media and demonstrating the validity of the proposed technique.",q-bio,Quantitative Biology
Dynamical scaling of the DNA unzipping transition,"We report studies of the equilibrium and the dynamics of a general set of lattice models which capture the essence of the force-induced or mechanical DNA unzipping transition. Besides yielding the whole equilibrium phase diagram in the force vs temperature plane, which reveals the presence of an interesting re-entrant unzipping transition for low T, these models enable us to characterize the dynamics of the process starting from a non-equilibrium initial condition. The thermal melting of the DNA strands displays a model dependent time evolution. On the contrary, our results suggest that the dynamical mechanism for the unzipping by force is very robust and the scaling behaviour does not depend on the details of the description we adopt.",q-bio,Quantitative Biology
Modeling two-state cooperativity in protein folding,"A protein model with the pairwise interaction energies varying as local environment changes, i.e., including some kinds of collective effect between the contacts, is proposed. Lattice Monte Carlo simulations on the thermodynamical characteristics and free energy profile show a well-defined two-state behavior and cooperativity of folding for such a model. As a comparison, related simulations for the usual G model, where the interaction energies are independent of the local conformations, are also made. Our results indicate that the evolution of interactions during the folding process plays an important role in the two-state cooperativity in protein folding.",q-bio,Quantitative Biology
A Multicanonical Molecular Dynamics Study on a Simple Bead-Spring Model for Protein Folding,"We have performed a multicanonical molecular dynamics simulation on a simple model protein.We have studied a model protein composed of charged, hydrophobic, and neutral spherical bead monomers.Since the hydrophobic interaction is considered to significantly affect protein folding, we particularly focus on the competition between effects of the Coulomb interaction and the hydrophobic interaction. We found that the transition which occurs upon decreasing the temperature is markedly affected by the change in both parameters and forms of the hydrophobic potential function, and the transition changes from first order to second order, when the Coulomb interaction becomes weaker.",q-bio,Quantitative Biology
Nondiffusive heat transfer in muscle tissue. Preliminary results,"We present preliminary experimental data that enable us to suggest that heat transfer in cellular tissue under local strong heating is a more complex phenomenon than a simple heat diffusion. Namely, we demonstrate that under local strong heating of a muscle tissue heat transfer in it exhibits substantial anisotropy unexplained in the context of the standard diffusion model. The observed temperature dynamics is also characterized by nonlinear behavior as well as by a certain repeat reversibility. The latter means that the time variations in the temperature of a cellular tissue undergoing repeated acts of heating go in the same way at least approximately. We explain the observed anomalous properties of heat transfer by suggesting the flow of the interstitial liquid to appear due to nonuniform heating which, in turn, affects the heat transfer. A possible mechanism responsible for this effect is discussed.",q-bio,Quantitative Biology
A Study of Sequence Distribution of a Painted Globule as a Model for Proteins with Good Folding Properties,"In this paper we present a method to study the folding structure of a simple model consisting of two kinds of monomers, hydrophobic and hydrophilic. This method has three main steps: an efficient simulation method to bring an open sequence of homopolymer to a folded state, the application of a painting method called (regular hull) to the folded globule and the refolding process of the obtained copolymer sequence. This study allows us to suggest a theoretical function of disorder distribution for copolymer sequences that give rise to a compacted and well micro-phase separated globule.",q-bio,Quantitative Biology
A Bit-String Model for Biological Aging,We present a simple model for biological aging. We studied it through computer simulations and we have found this model to reflect some features of real populations.,q-bio,Quantitative Biology
Efficient Monte Carlo Simulation of Biological Aging,"A bit-string model of biological life-histories is parallelized, with hundreds of millions of individuals. It gives the desired drastic decay of survival probabilities with increasing age for 32 age intervals.",q-bio,Quantitative Biology
Protein Folding and Spin Glass,We explicitly show the connection between the protein folding problem and spin glass transition. This is then used to identify appropriate quantities that are required to describe the transition. A possible way of observing the spin glass transition is proposed.,q-bio,Quantitative Biology
Elasticity of Semiflexible Biopolymer Networks,"We develop a model for gels and entangled solutions of semiflexible biopolymers such as F-actin. Such networks play a crucial structural role in the cytoskeleton of cells. We show that the rheologic properties of these networks can result from nonclassical rubber elasticity. This model can explain a number of elastic properties of such networks {\em in vitro}, including the concentration dependence of the storage modulus and yield strain.",q-bio,Quantitative Biology
Kinetic non-optimality and vibrational stability of proteins,"Scaling of folding times in Go models of proteins and of decoy structures with the Lennard-Jones potentials in the native contacts reveal %robust power law trends when studied under optimal folding conditions. The power law exponent depends on the type of native geometry. Its value indicates lack of kinetic optimality in the model proteins. In proteins, mechanical and thermodynamic stabilities are correlated.",q-bio,Quantitative Biology
Errors drive the evolution of biological signalling to costly codes,"Reduction of costs in biological signalling seems an evolutionary advantage, but recent experiments have shown signalling codes shifted to signals of high cost with a underutilisation of low cost signals. Here I show that errors in the efficient translation of biological states into signals shift codes to higher costs, effectively performing a quality control. The statistical structure of signal usage is predicted to be of a generalised Boltzmann form that penalises signals that are costly and sensitive to errors. This predicted distribution of signal usage against signal cost has two main features: an exponential tail required for cost efficiency and an underutilisation of the low cost signals required to protect the signalling quality from the errors. These predictions are shown to correspond quantitatively to the experiments in which gathering signal statistics is feasible as in visual cortex neurons.",q-bio,Quantitative Biology
Unzipping dsDNA with a force: scaling theory,"A double stranded DNA molecule under the stress of a pulling force acting on the strand terminals exhibits a partially denatured structure or can be completely unzipped depending the magnitude of the pulling force. A scaling argument for relationships amongst basic length scales is presented that takes into account the heterogeneity of the sequence. The result agrees with our numerical simulation data, which provides a critical test of the power laws in the unzipping transition region.",q-bio,Quantitative Biology
Folding dynamics of the helical structures in a minimal model,"The folding of a polypeptide is an example of the cooperative effects of the amino-acid residues. Of recent interest is how a secondary structure, such as a helix, spontaneously forms during the collapse of a peptide from an initial denatured state. The Monte Carlo implementation of a recent helix-forming model enables us to study the entire folding process dynamically.
 As shown by the computer simulations, the foldability and helical propagation are both strongly correlated to the nucleation properties of the sequence.",q-bio,Quantitative Biology
Structural transitions in biomolecules - a numerical comparison of two approaches for the study of phase transitions in small systems,We compare two recently proposed methods for the characterization of phase transitions in small systems. The usefulness of these techniques is evaluated for the case of structural transition in alanine-based peptides.,q-bio,Quantitative Biology
Anomalous properties of heat diffusion in living tissue caused by branching artery network. Qualitative description,"We analyze the effect of blood flow through large arteries of peripheral circulation on heat transfer in living tissue. Blood flow in such arteries gives rise to fast heat propagation over large scales, which is described in terms of heat superdiffusion. The corresponding bioheat heat equation is derived. In particular, we show that under local strong heating of a small tissue domain the temperature distribution inside the surrounding tissue is affected substantially by heat superdiffusion.",q-bio,Quantitative Biology
An optimal Q-state neural network using mutual information,"Starting from the mutual information we present a method in order to find a hamiltonian for a fully connected neural network model with an arbitrary, finite number of neuron states, Q. For small initial correlations between the neurons and the patterns it leads to optimal retrieval performance. For binary neurons, Q=2, and biased patterns we recover the Hopfield model. For three-state neurons, Q=3, we find back the recently introduced Blume-Emery-Griffiths network hamiltonian. We derive its phase diagram and compare it with those of related three-state models. We find that the retrieval region is the largest.",q-bio,Quantitative Biology
Sucrose Solutions as Prospective Medium to Study the Vesicle Structure: SAXS and SANS study,The possibility to use sucrose solutions as medium for X-ray and neutron small-angle scattering experiments has been explored for dimyristoylphosphatidylcholine (DMPC) vesicles and mixed DMPC/C_(12)E_(8) aggregates. The influence of sucrose concentration on phospholipid vesicles size and polydispersity has been investigated by complimentary X-ray and neutron scattering. Sucrose solutions decreased vesicle size and polydispersity and increased a contrast between phospholipid membrane and bulk solvent sufficiently for X-rays. 40% sucrose in H2O increased X-ray contrast by up to 10 times compared to pure H2O. The range of sucrose concentration 30%-40% created the best experimental conditions for the X-ray small-angle experiment with phospholipid vesicles.,q-bio,Quantitative Biology
Theoretical modeling of prion disease incubation,"We present a theory for the laboratory and epidemiological data for incubation times in infectious prion diseases. The central feature of our model is that slow growth of misfolded protein-aggregates from small initial seeds controls the `latent' or `lag' phase, whereas aggregate-fissioning and subsequent spreading leads to an exponential growth or doubling phase. Such a general framework can account for many features of prion diseases including the striking reproducibility of incubation times when high doses are inoculated into lab animals. Broad incubation time distributions arise for low infectious dose, while our calculated distributions narrow to sharply defined onset times with increased dose. We apply our distributions to epidemiological vCJD data and extract estimates of incubation times.",q-bio,Quantitative Biology
Neural Networks with Finite Width Action Potentials,The paper was done as an assigned Princeton university project.  It is being withdrawn since it needs to be changed and updated substantially.,q-bio,Quantitative Biology
Phase field approach for modeling intracellular dynamics,"We introduce a phase field approach for diffusion inside and outside a closed cell with damping and with source terms at the interface. The method is compared to exact solutions (where possible) and the more traditional finite element method. It is shown to be very accurate, easy to implement and computationally inexpensive. We apply our method to a recently introduced model for chemotaxis by Rappel et al. [Biophys. J. 83, 1361 (2002)].",q-bio,Quantitative Biology
Simple Models of Plant Learning and Memory,Plants are capable of intelligent responses to complex environmental signals. Learning and memory play fundamental roles in such responses. Two simple models of plant memory are proposed based on the calcium-signalling system. The memory states correspond to steady state distributions of calcium ions.,q-bio,Quantitative Biology
Characterizing Width Uniformity by Wave Propagation,"This work describes a novel image analysis approach to characterize the uniformity of objects in agglomerates by using the propagation of normal wavefronts. The problem of width uniformity is discussed and its importance for the characterization of composite structures normally found in physics and biology highlighted. The methodology involves identifying each cluster (i.e. connected component) of interest, which can correspond to objects or voids, and estimating the respective medial axes by using a recently proposed wavefront propagation approach, which is briefly reviewed. The distance values along such axes are identified and their mean and standard deviation values obtained. As illustrated with respect to synthetic and real objects (in vitro cultures of neuronal cells), the combined use of these two features provide a powerful description of the uniformity of the separation between the objects, presenting potential for several applications in material sciences and biology.",q-bio,Quantitative Biology
Particle Dispersion on Rapidly Folding Random Hetero-Polymers,"We investigate the dynamics of a particle moving randomly along a disordered hetero-polymer subjected to rapid conformational changes which induce superdiffusive motion in chemical coordinates. We study the antagonistic interplay between the enhanced diffusion and the quenched disorder. The dispersion speed exhibits universal behavior independent of the folding statistics. On the other hand it is strongly affected by the structure of the disordered potential. The results may serve as a reference point for a number of translocation phenomena observed in biological cells, such as protein dynamics on DNA strands.",q-bio,Quantitative Biology
A chemically driven fluctuating ratchet model for actomyosin interaction,"With reference to the experimental observations by T. Yanagida and his co-workers on actomyosin interaction, a Brownian motor of fluctuating ratchet kind is designed with the aim to describe the interaction between a Myosin II head and a neighboring actin filament. Our motor combines the dynamics of the myosin head with a chemical external system related to the ATP cycle, whose role is to provide the energy supply necessary to bias the motion. Analytical expressions for the duration of the ATP cycle, for the Gibbs free energy and for the net displacement of the myosin head are obtained. Finally, by exploiting a method due to Sekimoto (1997, J. Phys. Soc. Jpn., 66, 1234), a formula is worked out for the amount of energy consumed during the ATP cycle.",q-bio,Quantitative Biology
Folding Mechanism of Small Proteins,"Extensive Monte Carlo folding simulations for four proteins of various structural classes are carried out, using a single atomistic potential. In all cases, collapse occurs at a very early stage, and proteins fold into their native-like conformations at appropriate temperatures. The results demonstrate that the folding mechanism is controlled not only by thermodynamic factors but also by kinetic factors: The way a protein folds into its native structure, is also determined by the convergence point of early folding trajectories, which cannot be obtained by the free energy surface.",q-bio,Quantitative Biology
Correspondence between time-evolution dynamics of a tumor and an attractively interacting Bose-Einstein Condensate with feeding and dissipation,"The morphology and time-evolution of tumors are expected to depend heavily on the detailed balance of the overall physics of the cell assembly (e.g., the kinetic pressure, the cell-cell interaction, and the external trapping by the tissue) and the biological processes of mitosis, necrosis, etc. Here, for the first time, we include such a detailed balance in a theoretical model by exploiting the {\it ab initio} mathematical framework of atomic Bose-Einstein Condensation (BEC) with feeding and dissipation, to study tumor evolution dynamics. We show that, with a characteristic length scaling, the Gross-Pitaevskii (GP) equation, which describes the many-body atomic BEC characteristics, indeed explains the detailed features of a prevascular tumor culture data. The agreement suggests the prevascular that the prevascular carcinoma may be a natural analog to BEC and predicts an intercellular wave connecting the cells.",q-bio,Quantitative Biology
DNA Spools under Tension,"DNA-spools, structures in which DNA is wrapped and helically coiled onto itself or onto a protein core are ubiquitous in nature. We develop a general theory describing the non-equilibrium behavior of DNA-spools under linear tension. Two puzzling and seemingly unrelated recent experimental findings, the sudden quantized unwrapping of nucleosomes and that of DNA toroidal condensates under tension are theoretically explained and shown to be of the same origin. The study provides new insights into nucleosome and chromatin fiber stability and dynamics.",q-bio,Quantitative Biology
Theory of Melting and the Optical Properties of Gold/DNA Nanocomposites,"We describe a simple model for the melting and optical properties of a DNA/gold nanoparticle aggregate. The optical properties at fixed wavelength change dramatically at the melting transition, which is found to be higher and narrower in temperature for larger particles, and much sharper than that of an isolated DNA link. All these features are in agreement with available experiments. The aggregate is modeled as a cluster of gold nanoparticles on a periodic lattice connected by DNA bonds, and the extinction coefficient is computed using the discrete dipole approximation. Melting takes place as an increasing number of these bonds break with increasing temperature. The melting temperature corresponds approximately to the bond percolation threshold.",q-bio,Quantitative Biology
Dielectric behavior of oblate spheroidal particles: Application to erythrocytes suspensions,"We have investigated the effect of particle shape on the eletrorotation (ER) spectrum of living cells suspensions. In particular, we consider coated oblate spheroidal particles and present a theoretical study of ER based on the spectral representation theory. Analytic expressions for the characteristic frequency as well as the dispersion strength can be obtained, thus simplifying the fitting of experimental data on oblate spheroidal cells that abound in the literature. From the theoretical analysis, we find that the cell shape, coating as well as material parameters can change the ER spectrum. We demonstrate good agreement between our theoretical predictions and experimental data on human erthrocytes suspensions.",q-bio,Quantitative Biology
Force and kinetic barriers in unzipping of DNA,"A theory of the unzipping of double-stranded (ds) DNA is presented, and is compared to recent micromanipulation experiments. It is shown that the interactions which stabilize the double helix and the elastic rigidity of single strands (ss) simply determine the sequence dependent =12 pN force threshold for DNA strand separation. Using a semi-microscopic model of the binding between nucleotide strands, we show that the greater rigidity of the strands when formed into dsDNA, relative to that of isolated strands, gives rise to a potential barrier to unzipping. The effects of this barrier are derived analytically. The force to keep the extremities of the molecule at a fixed distance, the kinetic rates for strand unpairing at fixed applied force, and the rupture force as a function of loading rate are calculated. The dependence of the kinetics and of the rupture force on molecule length is also analyzed.",q-bio,Quantitative Biology
Semiclassical Neural Network,We have constructed a simple semiclassical model of neural network where neurons have quantum links with one another in a chosen way and affect one another in a fashion analogous to action potentials. We have examined the role of stochasticity introduced by the quantum potential and compare the system with the classical system of an integrate-and-fire model by Hopfield. Average periodicity and short term retentivity of input memory are noted.,q-bio,Quantitative Biology
Neural Networks with c-NOT Gated Nodes,"We try to design a quantum neural network with qubits instead of classical neurons with deterministic states, and also with quantum operators replacing teh classical action potentials. With our choice of gates interconnecting teh neural lattice, it appears that the state of the system behaves in ways reflecting both the strengths of coupling between neurons as well as initial conditions. We find that depending whether there is a threshold for emission from excited to ground state, the system shows either aperiodic oscillations or coherent ones with periodicity depending on the strength of coupling.",q-bio,Quantitative Biology
Time delay as a key to Apoptosis Induction in the p53 Network,"A feedback mechanism that involves the proteins p53 and mdm2, induces cell death as a controled response to severe DNA damage. A minimal model for this mechanism demonstrates that the respone may be dynamic and connected with the time needed to translate the mdm2 protein. The response takes place if the dissociation constant k between p53 and mdm2 varies from its normal value. Although it is widely believed that it is an increase in k that triggers the response, we show that the experimental behaviour is better described by a decrease in the dissociation constant. The response is quite robust upon changes in the parameters of the system, as required by any control mechanism, except for few weak points, which could be connected with the onset of cancer.",q-bio,Quantitative Biology
Atomic Scale Fractal Dimensionality in Proteins,"The soft condensed matter of biological organisms exhibits atomic motions whose properties depend strongly on temperature and hydration conditions. Due to the superposition of rapidly fluctuating alternative motions at both very low temperatures (quantum effects) and very high temperatures (classical Brownian motion regime), the dimension of an atomic ``path'' is in reality different from unity. In the intermediate temperature regime and under environmental conditions which sustain active biological functions, the fractal dimension of the sets upon which atoms reside is an open question. Measured values of the fractal dimension of the sets on which the Hydrogen atoms reside within the Azurin protein macromolecule are reported. The distribution of proton positions was measured employing thermal neutron elastic scattering from Azurin protein targets. As the temperature was raised from low to intermediate values, a previously known and biologically relevant dynamical transition was verified for the Azurin protein only under hydrated conditions. The measured fractal dimension of the geometrical sets on which protons reside in the biologically relevant temperature regime is given by $D=0.65 \pm 0.1$. The relationship between fractal dimensionality and biological function is qualitatively discussed.",q-bio,Quantitative Biology
"Dreams, endocannabinoids and itinerant dynamics in neural networks: re elaborating Crick-Mitchison unlearning hypothesis","In this work we reevaluate and elaborate Crick-Mitchison's proposal that REM-sleep corresponds to a self-organized process for unlearning attractors in neural networks. This reformulation is made at the face of recent findings concerning the intense activation of the amygdalar complex during REM-sleep, the role of endocannabinoids in synaptic weakening and neural network models with itinerant associative dynamics. We distinguish between a neurological REM-sleep function and a related evolutionary/behavioral dreaming function. At the neurological level, we propose that REM-sleep regulates excessive plasticity and weakens over stable brain activation patterns, specially in the amygdala, hippocampus and motor systems. At the behavioral level, we propose that dream narrative evolved as exploratory behavior made in a virtual environment promoting ""emotional (un)learning"", that is, habituation of emotional responses, anxiety and fear. We make several experimental predictions at variance with those of Memory Consolidation Hipothesis. We also predict that the ""replay"" of cells ensembles is done at an increasing faster pace along REM-sleep.",q-bio,Quantitative Biology
The role of body rotation in bacterial flagellar bundling,"In bacterial chemotaxis, E. coli cells drift up chemical gradients by a series of runs and tumbles. Runs are periods of directed swimming, and tumbles are abrupt changes in swimming direction. Near the beginning of each run, the rotating helical flagellar filaments which propel the cell form a bundle. Using resistive-force theory, we show that the counter-rotation of the cell body necessary for torque balance is sufficient to wrap the filaments into a bundle, even in the absence of the swirling flows produced by each individual filament.",q-bio,Quantitative Biology
A steepest descent calculation of RNA pseudoknots,"We enumerate possible topologies of pseudoknots in single-stranded RNA molecules. We use a steepest-descent approximation in the large N matrix field theory, and a Feynman diagram formalism to describe the resulting pseudoknot structure.",q-bio,Quantitative Biology
Slow nucleic acid unzipping kinetics from sequence-defined barriers,"Recent experiments on unzipping of RNA helix-loop structures by force have shown that about 40-base molecules can undergo kinetic transitions between two well-defined `open' and `closed' states, on a timescale = 1 sec [Liphardt et al., Science 297, 733-737 (2001)]. Using a simple dynamical model, we show that these phenomena result from the slow kinetics of crossing large free energy barriers which separate the open and closed conformations. The dependence of barriers on sequence along the helix, and on the size of the loop(s) is analyzed. Some DNAs and RNAs sequences that could show dynamics on different time scales, or three(or more)-state unzipping, are proposed.",q-bio,Quantitative Biology
Statistical mechanics of RNA folding: importance of alphabet size,"We construct a minimalist model of RNA secondary-structure formation and use it to study the mapping from sequence to structure. There are strong, qualitative differences between two-letter and four or six-letter alphabets. With only two kinds of bases, there are many alternate folding configurations, yielding thermodynamically stable ground-states only for a small set of structures of high designability, i.e., total number of associated sequences. In contrast, sequences made from four bases, as found in nature, or six bases have far fewer competing folding configurations, resulting in a much greater average stability of the ground state.",q-bio,Quantitative Biology
A simple equation to calculate the diameters of biological vesicles,"The remarkable preference of biomembranes, to constitute vesicles of certain discrete sizes is explained by using the following properties of phospholipids that are either well understood or at least documented.
 A. By hexagonal close-packing their fatty acyl chains form a triangular lattice.
 Their molecules:
 B. Prefer to form linear arrays that occasionally make angles of 120 degrees.
 C. Form relatively large hexagons. Based on these properties a model for monolayers is proposed and a simple equation derived for the calculation of diameters of vesicles. The diameters of vesicles of neurotransmitters and hormones determined by electron microscopy were compared with those obtained with the equation. Statistical analysis of this comparison revealed the model to give very significant results (p=.0002).",q-bio,Quantitative Biology
Tempo and mode in quasispecies evolution,"Evolutionary dynamics in an uncorrelated rugged fitness landscape is studied in the framework of Eigen's molecular quasispecies model. We consider the case of strong selection, which is analogous to the zero temperature limit in the equivalent problem of directed polymers in random media. In this limit the population is always localized at a single temporary master sequence $^\ast(t)$, and we study the statistical properties of the evolutionary trajectory which $^\ast(t)$ traces out in sequence space. Numerical results for binary sequences of length N=10 and exponential and uniform fitness distributions are presented. Evolution proceeds by intermittent jumps between local fitness maxima, where high lying maxima are visited more frequently by the trajectories. The probability distribution for the total time $T$ required to reach the global maximum shows a $T^{-2}$-tail, which is argued to be universal and to derive from near-degenerate fitness maxima. The total number of jumps along any given trajectory is always small, much smaller than predicted by the statistics of records for random long-ranged evolutionary jumps.",q-bio,Quantitative Biology
Scale Invariance in the Nonstationarity of Physiological Signals,"We introduce a segmentation algorithm to probe temporal organization of heterogeneities in human heartbeat interval time series. We find that the lengths of segments with different local values of heart rates follow a power-law distribution. This scale-invariant structure is not a simple consequence of the long-range correlations present in the data. We also find that the differences in mean heart rates between consecutive segments display a common functional form, but with different parameters for healthy individuals and for patients with heart failure. This finding may provide information into the way heart rate variability is reduced in cardiac disease.",q-bio,Quantitative Biology
Statistical and Dynamical Study of Disease Propagation in a Small World Network,We study numerically statistical properties and dynamical disease propagation using a percolation model on a one dimensional small world network. The parameters chosen correspond to a realistic network of school age children. We found that percolation threshold decreases as a power law as the short cut fluctuations increase. We found also the number of infected sites grows exponentially with time and its rate depends logarithmically on the density of susceptibles. This behavior provides an interesting way to estimate the serology for a given population from the measurement of the disease growing rate during an epidemic phase. We have also examined the case in which the infection probability of nearest neighbors is different from that of short cuts. We found a double diffusion behavior with a slower diffusion between the characteristic times.,q-bio,Quantitative Biology
Memory beyond memory in heart beating: an efficient way to detect pathological conditions,"We study the long-range correlations of heartbeat fluctuations with the method of diffusion entropy. We show that this method of analysis yields a scaling parameter $$ that apparently conflicts with the direct evaluation of the distribution of times of sojourn in states with a given heartbeat frequency. The strength of the memory responsible for this discrepancy is given by a parameter $^{2}$, which is derived from real data. The distribution of patients in the ($$, $^{2}$)-plane yields a neat separation of the healthy from the congestive heart failure subjects.",q-bio,Quantitative Biology
Elementary Derivative Tasks and Neural Net Multiscale Analysis of Tasks,"Neural nets are known to be universal approximators. In particular, formal neurons implementing wavelets have been shown to build nets able to approximate any multidimensional task. Such very specialized formal neurons may be, however, difficult to obtain biologically and/or industrially. In this paper we relax the constraint of a strict ``Fourier analysis'' of tasks. Rather, we use a finite number of more realistic formal neurons implementing elementary tasks such as ``window'' or ``Mexican hat'' responses, with adjustable widths. This is shown to provide a reasonably efficient, practical and robust, multifrequency analysis. A training algorithm, optimizing the task with respect to the widths of the responses, reveals two distinct training modes. The first mode induces some of the formal neurons to become identical, hence promotes ``derivative tasks''. The other mode keeps the formal neurons distinct.",q-bio,Quantitative Biology
Extremely Dilute Modular Neuronal Networks: Neocortical Memory Retrieval Dynamics,"A model of the columnar functional organization of neocortical association areas is studied. The neuronal network is composed of many Hebbian autoassociators, or modules, each of which interacts with a relatively small number of the others. Every module encodes and stores a number of elementary percepts, or features. Memory items, or patterns, are peculiar combinations of features sparsely distributed over the multi-modular network. Any feature stored in any module can be involved in several of the stored patterns; feature-sharing is in fact source of local ambiguities and, consequently, a potential cause of erroneous memory retrieval activity spreading through the model network.
  The memory retrieval dynamics of the large multi-modular autoassociator is investigated by means of quantitative analysis and numerical simulations. An oscillatory retrieval process is found to be very efficient in overcoming feature-sharing drawbacks; it requires a mechanism that modulates the robustness of local attractors to noise, and neuronal activity sparseness such that quiescent and active modules are about equally noisy. Correlated activation of interconnected modules and extramodular neuronal contacts more effective than the intramodular ones seem to be general requirements in order to efficiently achieve satisfactory quality of memory retrieval. It is also shown that, even in ideal conditions, some spots of the network cannot be reached by retrieval activity spread. The locations of these activity isles depend on the pattern to retrieve and on the cue, while their extension only depends on architecture of the graph and statistics of the stored patterns. The existence of these isles determines an upper-bound to retrieval quality that does not depend on the specific retrieval dynamics adopted, nor on whether feature-sharing is permitted. The oscillatory retrieval process nearly saturates this bound.",q-bio,Quantitative Biology
Effect of defects on thermal denaturation of DNA Oligomers,The effect of defects on the melting profile of short heterogeneous DNA chains are calculated using the Peyrard-Bishop Hamiltonian. The on-site potential on a defect site is represented by a potential which has only the short-range repulsion and the flat part without well of the Morse potential. The stacking energy between the two neigbouring pairs involving a defect site is also modified. The results are found to be in good agreement with the experiments.,q-bio,Quantitative Biology
Complexation of DNA with positive spheres: phase diagram of charge inversion and reentrant condensation,"The phase diagram of a water solution of DNA and oppositely charged spherical macroions is studied. DNA winds around spheres to form beads-on-a-string complexes resembling the chromatin 10 nm fiber. At small enough concentration of spheres these ""artificial chromatin"" complexes are negative, while at large enough concentrations of spheres the charge of DNA is inverted by the adsorbed spheres. Charges of complexes stabilize their solutions. In the plane of concentrations of DNA and spheres the phases with positive and negative complexes are separated by another phase, which contains the condensate of neutral DNA-spheres complexes. Thus when the concentration of spheres grows, DNA-spheres complexes experience condensation and resolubilization (or reentrant condensation). Phenomenological theory of the phase diagram of reentrant condensation and charge inversion is suggested. Parameters of this theory are calculated by microscopic theory. It is shown that an important part of the effect of a monovalent salt on the phase diagram can be described by the nontrivial renormalization of the effective linear charge density of DNA wound around a sphere, due to the Onsager-Manning condensation. We argue that our phenomenological phase diagram or reentrant condensation is generic to a large class of strongly asymmetric electrolytes. Possible implication of these results for the natural chromatin are discussed.",q-bio,Quantitative Biology
Epidemic outbreaks in complex heterogeneous networks,"We present a detailed analytical and numerical study for the spreading of infections in complex population networks with acquired immunity. We show that the large connectivity fluctuations usually found in these networks strengthen considerably the incidence of epidemic outbreaks. Scale-free networks, which are characterized by diverging connectivity fluctuations, exhibit the lack of an epidemic threshold and always show a finite fraction of infected individuals. This particular weakness, observed also in models without immunity, defines a new epidemiological framework characterized by a highly heterogeneous response of the system to the introduction of infected individuals with different connectivity. The understanding of epidemics in complex networks might deliver new insights in the spread of information and diseases in biological and technological networks that often appear to be characterized by complex heterogeneous architectures.",q-bio,Quantitative Biology
Nonlinear Relaxation in Population Dynamics,We analyze the nonlinear relaxation of a complex ecosystem composed of many interacting species. The ecological system is described by generalized Lotka-Volterra equations with a multiplicative noise. The transient dynamics is studied in the framework of the mean field theory and with random interaction between the species. We focus on the statistical properties of the asymptotic behaviour of the time integral of the i-th population and on the distribution of the population and of the local field.,q-bio,Quantitative Biology
Mechanical oscillations at the cellular scale,"Active phenomena which involve force generation and motion play a key role in a number of phenomena in living cells such as cell motility, muscle contraction and the active transport of material and organelles. Here we discuss mechanical oscillations generated by active systems in cells. Examples are oscillatory regimes in muscles, the periodic beating of axonemal cilia and flagella and spontaneous oscillations of auditory hair cells which play a role in active amplification of weak sounds in hearing. As a prototype system for oscillation generation by proteins, we discuss a general mechanism by which many coupled active elements such as motor molecules can generate oscillations.",q-bio,Quantitative Biology
Tilt Texture Domains on a Membrane and Chirality induced Budding,"We study the equilibrium conformations of a lipid domain on a planar fluid membrane where the domain is decorated by a vector field representing the tilt of the stiff fatty acid chains of the lipid molecules, while the surrounding membrane is fluid and structureless. The inclusion of chirality in the bulk of the domain induces a novel budding of the membrane, which preempts the budding induced by a decrease in interfacial tension.",q-bio,Quantitative Biology
Adsorption of mono- and multivalent cat- and anions on DNA molecules,"Adsorption of monovalent and multivalent cat- and anions on a deoxyribose nucleic acid (DNA) molecule from a salt solution is investigated by computer simulation. The ions are modelled as charged hard spheres, the DNA molecule as a point charge pattern following the double-helical phosphate strands. The geometrical shape of the DNA molecules is modelled on different levels ranging from a simple cylindrical shape to structured models which include the major and minor grooves between the phosphate strands. The densities of the ions adsorbed on the phosphate strands, in the major and in the minor grooves are calculated. First, we find that the adsorption pattern on the DNA surface depends strongly on its geometrical shape: counterions adsorb preferentially along the phosphate strands for a cylindrical model shape, but in the minor groove for a geometrically structured model. Second, we find that an addition of monovalent salt ions results in an increase of the charge density in the minor groove while the total charge density of ions adsorbed in the major groove stays unchanged. The adsorbed ion densities are highly structured along the minor groove while they are almost smeared along the major groove. Furthermore, for a fixed amount of added salt, the major groove cationic charge is independent on the counterion valency. For increasing salt concentration the major groove is neutralized while the total charge adsorbed in the minor groove is constant. DNA overcharging is detected for multivalent salt. Simulations for a larger ion radii, which mimic the effect of the ion hydration, indicate an increased adsorbtion of cations in the major groove.",q-bio,Quantitative Biology
Lethality and centrality in protein networks,"In this paper we present the first mathematical analysis of the protein interaction network found in the yeast, S. cerevisiae. We show that, (a) the identified protein network display a characteristic scale-free topology that demonstrate striking similarity to the inherent organization of metabolic networks in particular, and to that of robust and error-tolerant networks in general. (b) the likelihood that deletion of an individual gene product will prove lethal for the yeast cell clearly correlates with the number of interactions the protein has, meaning that highly-connected proteins are more likely to prove essential than proteins with low number of links to other proteins. These results suggest that a scale-free architecture is a generic property of cellular networks attributable to universal self-organizing principles of robust and error-tolerant networks and that will likely to represent a generic topology for protein-protein interactions.",q-bio,Quantitative Biology
Melting and unzipping of DNA,"Experimental studies of the thermal denaturation of DNA yield a strong indication that the transition is first order. This transition has been theoretically studied since the early sixties, mostly within an approach in which the microscopic configurations of a DNA molecule are given by an alternating sequence of non-interacting bound segments and denaturated loops. Studies of these models neglect the repulsive, self-avoiding, interaction between different loops and segments and have invariably yielded continuous denaturation transitions. In this study we exploit recent results on scaling properties of polymer networks of arbitrary topology in order to take into account the excluded-volume interaction between denaturated loops and the rest of the chain. We thus obtain a first-order phase transition in d=2 dimensions and above, in agreement with experiments. We also consider within our approach the unzipping transition, which takes place when the two DNA strands are pulled apart by an external force acting on one end. We find that the unzipping transition is also first order. Although the denaturation and unzipping transitions are thermodynamically first order, they do exhibit critical fluctuations in some of their properties. For instance, the loop size distribution decays algebraically at the transition and the length of the denaturated end segment diverges as the transition is approached. We evaluate these critical properties within our approach.",q-bio,Quantitative Biology
Extended dynamical range as a collective property of excitable cells,"Receptor cells with electrically coupled axons can improve both their input sensitivity and dynamical range due to collective non-linear wave properties. This mechanism is illustrated by a network of axons modeled by excitable maps subjected to a Poison signal process with rate r. We find that, in a network of N cells, the amplification factor A (number of cells excited by a single signal event) decreases smoothly from A= O(N) to A=1 as r increases, preventing saturation in a self-organized way and leading to a Weber-Fechner law behavior. This self-limited amplification mechanism is generic for excitable media and could be implemented in other biological contexts and artificial sensor devices.",q-bio,Quantitative Biology
Proteinlike behavior of a spin system near the transition between ferromagnet and spin glass,"A simple spin system is studied as an analog for proteins. We investigate how the introduction of randomness and frustration into the system effects the designability and stability of ground state configurations. We observe that the spin system exhibits protein-like behavior in the vicinity of the transition between ferromagnet and spin glass.
 Our results illuminate some guiding principles in protein evolution.",q-bio,Quantitative Biology
"Higher-Order Neutral Networks, Polya Polynomials, and Fermi Cluster Diagrams","The problem of controlling higher-order interactions in neural networks is addressed with techniques commonly applied in the cluster analysis of quantum many-particle systems. For multi-neuron synaptic weights chosen according to a straightforward extension of the standard Hebbian learning rule, we show that higher-order contributions to the stimulus felt by a given neuron can be readily evaluated via Poly's combinatoric group-theoretical approach or equivalently by exploiting a precise formal analogy with fermion diagrammatics.",q-bio,Quantitative Biology
"Echinocyte Shapes: Bending, Stretching and Shear Determine Spicule Shape and Spacing","We study the shapes of human red blood cells using continuum mechanics. In particular, we model the crenated, echinocytic shapes and show how they may arise from a competition between the bending energy of the plasma membrane and the stretching/shear elastic energies of the membrane skeleton. In contrast to earlier work, we calculate spicule shapes exactly by solving the equations of continuum mechanics subject to appropriate boundary conditions. A simple scaling analysis of this competition reveals an elastic length which sets the length scale for the spicules and is, thus, related to the number of spicules experimentally observed on the fully developed echinocyte.",q-bio,Quantitative Biology
The spike-timing-dependent learning rule to encode spatiotemporal patterns in a network of spiking neurons,"We study associative memory neural networks based on the Hodgkin-Huxley type of spiking neurons. We introduce the spike-timing-dependent learning rule, in which the time window with the negative part as well as the positive part is used to describe the biologically plausible synaptic plasticity. The learning rule is applied to encode a number of periodical spatiotemporal patterns, which are successfully reproduced in the periodical firing pattern of spiking neurons in the process of memory retrieval. The global inhibition is incorporated into the model so as to induce the gamma oscillation. The occurrence of gamma oscillation turns out to give appropriate spike timings for memory retrieval of discrete type of spatiotemporal pattern. The theoretical analysis to elucidate the stationary properties of perfect retrieval state is conducted in the limit of an infinite number of neurons and shows the good agreement with the result of numerical simulations. The result of this analysis indicates that the presence of the negative and positive parts in the form of the time window contributes to reduce the size of crosstalk term, implying that the time window with the negative and positive parts is suitable to encode a number of spatiotemporal patterns. We draw some phase diagrams, in which we find various types of phase transitions with change of the intensity of global inhibition.",q-bio,Quantitative Biology
Monte Carlo simulation of the transmission of measles: Beyond the mass action principle,"We present a Monte Carlo simulation of the transmission of measles within a population sample during its growing and equilibrium states by introducing two different vaccination schedules of one and two doses. We study the effects of the contact rate per unit time $$ as well as the initial conditions on the persistence of the disease. We found a weak effect of the initial conditions while the disease persists when $$ lies in the range 1/L-10/L ($L$ being the latent period). Further comparison with existing data, prediction of future epidemics and other estimations of the vaccination efficiency are provided.
  Finally, we compare our approach to the models using the mass action principle in the first and another epidemic region and found the incidence independent of the number of susceptibles after the epidemic peak while it strongly fluctuates in its growing region. This method can be easily applied to other human, animals and vegetable diseases and includes more complicated parameters.",q-bio,Quantitative Biology
"Discrete charge patterns, Coulomb correlations and interactions in protein solutions","The effective Coulomb interaction between globular proteins is calculated as a function of monovalent salt concentration $c_s$, by explicit Molecular Dynamics simulations of pairs of model proteins in the presence of microscopic co and counterions. For discrete charge patterns of monovalent sites on the surface, the resulting osmotic virial coefficient $B_2$ is found to be a strikingly non-monotonic function of $c_s$. The non-monotonicity follows from a subtle Coulomb correlation effect which is completely missed by conventional non-linear Poisson-Boltzmann theory and explains various experimental findings.",q-bio,Quantitative Biology
Charge Transport in DNA Segments with fractal structures,By means of the concept of factorial moment the charge transfer rates in DNA segments with fractal structures are investigated. An analytical form for the electron transfer rate is obtained.,q-bio,Quantitative Biology
Entropy loss in long-distance DNA looping,The entropy loss due to the formation of one or multiple loops in circular and linear DNA chains is calculated from a scaling approach in the limit of long chain segments. The analytical results allow to obtain a fast estimate for the entropy loss for a given configuration. Numerical values obtained for some examples suggest that the entropy loss encountered in loop closure in typical genetic switches may become a relevant factor which has to be overcome by the released bond energy between the looping contact sites.,q-bio,Quantitative Biology
Force-induced unzipping of DNA with long-range correlated sequence,"We consider force-induced unzipping transition for a heterogeneous DNA model with a long-range correlated base-sequence. It is shown that as compared to the uncorrelated situation, long-range correlations smear the unzipping phase-transition, change its universality class and lead to non-self-averaging: the averaged behavior strongly differs from the typical ones. Several basic scenarios for this typical behavior are revealed and explained. The results can be relevant for explaining the biological purpose of long-range correlations in DNA.",q-bio,Quantitative Biology
Managing catastrophic changes in a collective,"We address the important practical issue of understanding, predicting and eventually controlling catastrophic endogenous changes in a collective. Such large internal changes arise as macroscopic manifestations of the microscopic dynamics, and their presence can be regarded as one of the defining features of an evolving complex system. We consider the specific case of a multi-agent system related to the El Farol bar model, and show explicitly how the information concerning such large macroscopic changes becomes encoded in the microscopic dynamics. Our findings suggest that these large endogenous changes can be avoided either by pre-design of the collective machinery itself, or in the post-design stage via continual monitoring and occasional `vaccinations'.",q-bio,Quantitative Biology
Stochastic Simulation of Gene Expression in a Single Cell,"In this paper, we consider two stochastic models of gene expression in prokaryotic cells. In the first model, sixteen biochemical reactions involved in transcription, translation and transcriptional regulation in the presence of inducer molecules are considered. The time evolution of the number of biomolecules of a particular type is determined using the stochastic simulation method based on the Gillespie Algorithm. The results obtained show that if the number of inducer molecules, N(I), is greater than or equal to the number of regulatory molecules, N(R), the average protein level is high in the steady state (state 2). The magnitude of the level is the same as long as N{I) greater than or equal to N(R). When N(I) is very very less than N(R), the average protein level is low, practically zero (state 1). As N(I) increases, the protein level continues to remain low. When N(I)becomes close to N(R), protein levels in the steady state are intermediate between high and low.In the presence of autocatalysis, a cell mostly exists in either state 1 or state 2 giving rise to a bimodal distribution in the protein levels in an ensemble of cells. This corresponds to the ""all or none'' phenomenon observed in experiments. In the second model, the inducer molecules are not considered explicitly. An exhaustive simulation over the parameter space of the model shows that there are three major patterns of gene expression, Type A, Type B and Type C. The effect of varying the cellular parameters on the patterns, in particular, the transition from one type of pattern to another, is studied. Type A and Type B patterns have been observed in experiments. Simple mathematical models of transcriptional regulation predict Type C pattern of gene expression in certain parameter regimes.",q-bio,Quantitative Biology
Hierarchical organization of modularity in metabolic networks,"Spatially or chemically isolated functional modules composed of several cellular components and carrying discrete functions are considered fundamental building blocks of cellular organization, but their presence in highly integrated biochemical networks lacks quantitative support. Here we show that the metabolic networks of 43 distinct organisms are organized into many small, highly connected topologic modules that combine in a hierarchical manner into larger, less cohesive units, their number and degree of clustering following a power law. Within Escherichia coli the uncovered hierarchical modularity closely overlaps with known metabolic functions. The identified network architecture may be generic to system-level cellular organization.",q-bio,Quantitative Biology
Phase Behavior of a Simple Model for Membrane Proteins,"We report a numerical simulation of the phase diagram of a simple model for membrane proteins constrained to move in a plane. In analogy with the corresponding three dimensional models, the liquid-gas transition becomes metastable as the range of attraction decreases. Spontaneous crystallization happens much more readily in the two dimensional models rather than in their three dimensional counterparts.",q-bio,Quantitative Biology
One way to Characterize the compact structures of lattice protein model,"On the study of protein folding, our understanding about the protein structures is limited. In this paper we find one way to characterize the compact structures of lattice protein model. A quantity called Partnum is given to each compact structure. The Partnum is compared with the concept Designability of protein structures emerged recently. It is shown that the highly designable structures have, on average, an atypical number of local degree of freedom. The statistical property of Partnum and its dependence on sequence length is also studied.",q-bio,Quantitative Biology
Microscopic Theory of Protein Folding Rates.II: Local Reaction Coordinates and Chain Dynamics,"The motion involved in barrier crossing for protein folding are investigated in terms of the chain dynamics of the polymer backbone, completing the microscopic description of protein folding presented in the previous paper. Local reaction coordinates are identified as collective growth modes of the unstable fluctuations about the saddle-points in the free energy surface. The description of the chain dynamics incorporates internal friction (independent of the solvent viscosity) arising from the elementary isomerizations of the backbone dihedral angles. We find that the folding rate depends linearly on the solvent friction for high viscosity, but saturates at low viscosity because of internal friction. For $$-repressor, the calculated folding rate prefactor, along with the free energy barrier from the variational theory, gives a folding rate that agrees well with the experimentally determined rate under highly stabilizing conditions, but the theory predicts too large a folding rate at the transition midpoint. This discrepancy obtained using a fairly complete quantitative theory inspires a new set of questions about chain dynamics, specifically detailed motions in individual contact formation.",q-bio,Quantitative Biology
Thermodynamic Capacity of a Protein,"We show that a protein can be trained to recognise multiple conformations, analogous to an associative memory, and provide capacity calculations based on energy fluctuations and information theory. Unlike the linear capacity of a Hopfield network, the number of conformations which can be remembered by a protein sequence depends on the size of the amino acid alphabet as ln A, independent of protein length. This admits the possibility of certain proteins, such as prions, evolving to fold to independent stable conformations, as well as novel possibilities for protein and heteropolymer design.",q-bio,Quantitative Biology
Kinetic Capacity of a Protein,"The ability of a protein to recognise multiple independent target conformations was demonstrated in [1]. Here we consider the recognition of correlated configurations, which we apply to funnel design for a single conformation. The maximum basin of attraction, as parametrised in our model, depends on the number of amino acid species as ln A, independent of protein length. We argue that the extent to which the protein energy landscape can be manipulated is fixed, effecting a trade-off between well breadth, well depth and well number. This clarifies the scope and limits of protein and heteropolymer function.",q-bio,Quantitative Biology
A Statistical Mechanical Approach to Combinatorial Chemistry,"An analogy between combinatorial chemistry and Monte Carlo computer simulation is pursued. Examples of how to design libraries for both materials discovery and protein molecular evolution are given. For materials discovery, the concept of library redesign, or the use previous experiments to guide the design of new experiments, is introduced. For molecular evolution, examples of how to use ``biased'' Monte Carlo to search the protein sequence space are given. Chemical information, whether intuition, theoretical calculations, or database statistics, can be naturally incorporated as an a priori bias in the Monte Carlo approach to library design in combinatorial chemistry. In this sense, combinatorial chemistry can be viewed as an extension of traditional chemical synthesis.",q-bio,Quantitative Biology
Amino acid classes and the protein folding problem,"We present and implement a distance-based clustering of amino acids within the framework of a statistically derived interaction matrix and show that the resulting groups faithfully reproduce, for well-designed sequences, thermodynamic stability in and kinetic accessibility to the native state. A simple interpretation of the groups is obtained by eigenanalysis of the interaction matrix.",q-bio,Quantitative Biology
Learning and generation of long-range correlated sequences,"We study the capability to learn and to generate long-range, power-law correlated sequences by a fully connected asymmetric network. The focus is set on the ability of neural networks to extract statistical features from a sequence. We demonstrate that the average power-law behavior is learnable, namely, the sequence generated by the trained network obeys the same statistical behavior. The interplay between a correlated weight matrix and the sequence generated by such a network is explored. A weight matrix with a power-law correlation function along the vertical direction, gives rise to a sequence with a similar statistical behavior.",q-bio,Quantitative Biology
The evolutionary advantage of diploid sex,"We modify the Penna Model for biological aging, which is based on the mutation-accumulation theory, in order to verify if there would be any evolutionary advantage of triploid over diploid organisms. We show that this is not the case, and that usual sex is always better than that involving three individuals.",q-bio,Quantitative Biology
DNA uptake into nuclei: Numerical and analytical results,"The dynamics of polymer translocation through a pore has been the subject of recent theoretical and experimental works. We have considered theoretical estimates and performed computer simulations to understand the mechanism of DNA uptake into the cell nucleus, a phenomenon experimentally investigated by attaching a small bead to the free end of the double helix and pulling this bead with the help of an optical trap. The experiments show that the uptake is monotonous and slows down when the remaining DNA segment becomes very short. Numerical and analytical studies of the entropic repulsion between the DNA filament and the membrane wall suggest a new interpretation of the experimental observations. Our results indicate that the repulsion monotonically decreases as the uptake progresses. Thus, the DNA is pulled in (i) either by a small force of unknown origin, and then the slowing down can be interpreted only statistically; (ii) or by a strong but slow ratchet mechanism, which would naturally explain the observed monotonicity, but then the slowing down requires additional explanations. Only further experiments can unambiguously distinguish between these two mechanisms.",q-bio,Quantitative Biology
Parametrical Neural Network,The storage capacity of the Hopfield model is about 15% of the network size. It can be increased significantly in the Potts-glass model of the associative memory only. In this model neurons can be in more than two different states. We show that even greater storage capacity can be achieved in the parametrical neural network (PNN) that is based on the parametrical four-wave mixing process that is well-known in nonlinear optics. We present a uniform formalism allowing us to describe both PNN and the Potts-glass associative memory. To estimate the storage capacity we use the Chebyshev-Chernov statistical technique.,q-bio,Quantitative Biology
Understanding the determinants of stability and folding of small globular proteins from their energetics,"The results of minimal model calculations suggest that the stability and the kinetic accessibility of the native state of small globular proteins are controlled by few ""hot"" sites. By mean of molecular dynamics simulations around the native conformation, which simulate the protein and the surrounding solvent at full--atom level, we generate an energetic map of the equilibrium state of the protein and simplify it with an Eigenvalue decomposition. The components of the Eigenvector associated with the lowest Eigenvalue indicate which are the ""hot"" sites responsible for the stability and for the fast folding of the protein. Comparison of these predictions with the results of mutatgenesis experiments, performed for five small proteins, provide an excellent agreement.",q-bio,Quantitative Biology
"Correlation between mutation pressure, selection pressure and occurrence of amino acids",We have found that the effective survival time of amino acids in organisms follows a power law with respect to frequency of their occurrence in genes. We have used mutation data matrix PAM1 PET91 to calculate selection pressure on each kind of amino acid. The results have been compared to MPM1 matrix (Mutation Probability Matrix) representing the pure mutational pressure in the Borrelia burgdorferi genome.The results are universal in the sense that the survival time of amino acids calculated from the higher order PAMk matrices (k>1) follows the same power law as in the case of PAM1 matrices.,q-bio,Quantitative Biology
Random Networks Growing Under a Diameter Constraint,"We study the growth of random networks under a constraint that the diameter, defined as the average shortest path length between all nodes, remains approximately constant. We show that if the graph maintains the form of its degree distribution then that distribution must be approximately scale-free with an exponent between 2 and 3. The diameter constraint can be interpreted as an environmental selection pressure that may help explain the scale-free nature of graphs for which data is available at different times in their growth. Two examples include graphs representing evolved biological pathways in cells and the topology of the Internet backbone. Our assumptions and explanation are found to be consistent with these data.",q-bio,Quantitative Biology
Origin of Scaling Behavior of Protein Packing Density: A Sequential Monte Carlo Study of Compact Long Chain Polymers,"Single domain proteins are thought to be tightly packed. The introduction of voids by mutations is often regarded as destabilizing. In this study we show that packing density for single domain proteins decreases with chain length. We find that the radius of gyration provides poor description of protein packing but the alpha contact number we introduce here characterize proteins well. We further demonstrate that protein-like scaling relationship between packing density and chain length is observed in off-lattice self-avoiding walks. A key problem in studying compact chain polymer is the attrition problem: It is difficult to generate independent samples of compact long self-avoiding walks. We develop an algorithm based on the framework of sequential Monte Carlo and succeed in generating populations of compact long chain off-lattice polymers up to length $N=2,000$. Results based on analysis of these chain polymers suggest that maintaining high packing density is only characteristic of short chain proteins. We found that the scaling behavior of packing density with chain length of proteins is a generic feature of random polymers satisfying loose constraint in compactness. We conclude that proteins are not optimized by evolution to eliminate packing voids.",q-bio,Quantitative Biology
On the formation of caveolae and similar membrane invaginations,"We study a physical model for the formation of bud-like invaginations on fluid membranes under tension, and apply this model to caveolae formation. We demonstrate that budding can be driven by membrane-bound inclusions (proteins) provided that they exert asymmetric forces on the membrane that give rise to bending moments. In particular, Caveolae formation may not necessarily require forces to be applied by the cytoskeleton. Our theoretical model is able to explain several features observed experimentally in caveolae, where proteins in the caveolin family are known to play a crucial role in the formation of caveolae buds. These include (i) the formation of caveolae buds with sizes in the 100nm range (ii) that a fairly large variation of bud shape is expected (iii) that certain N and C termini deletion mutants result in vesicles that are an order of magnitude larger. Finally, we discuss the possible origin of the morphological striations that are observed on the surfaces of the caveolae.",q-bio,Quantitative Biology
A constant extension ensembles model of double-stranded chain molecules,"Because the constant extension ensemble of single chain molecule is not always equivalent with constant force ensemble, a model of double-stranded conformations, as in RNA molecules and $$-sheets in proteins, with fixed extension constraint is built in this paper. Based on polymer-graph theory and the self-avoiding walks, sequence dependence and excluded-volume interactions are explicitly taken into account. Using the model, we investigate force-extension curves, contact distributions and force-temperature curves at given extensions. We find that, for the same homogeneous chains, the force-extension curves are almost consistent with the extension-force curves in the conjugated force ensembles. Especially, the consistence depends on chain lengths. But the curves of the two ensembles are completely different from each other if sequences are considered. In addition, contact distributions of homogeneous sequence show that the double-stranded regions in hairpin conformations tend to locate at two sides of the chain. We contribute the unexpected phenomena to the nonuniformity of excluded-volume interactions of the region and two tails with different lengths. This tendency will disappear if the interactions are canceled. Finally, in constant extension ensemble, the force-flipping transitions conjugated with re-entering phenomena in constant force ensemble are observed in hairpin conformations, while they do not present in secondary structure conformations.",q-bio,Quantitative Biology
Is there a universality of the helix-coil transition in protein models?,"The similarity in the thermodynamic properties of two completely different theoretical models for the helix-coil transition is examined critically. The first model is an all-atomic representation for a poly-alanine chain, while the second model is a minimal helix-forming model that contains no system specifics. Key characteristics of the helix-coil transition, in particular, the effective critical exponents of these two models agree with each other, within a finite-size scaling analysis.",q-bio,Quantitative Biology
Investigation of routes and funnels in protein folding by free energy functional methods,"We use a free energy functional theory to elucidate general properties of heterogeneously ordering, fast folding proteins, and we test our conclusions with lattice simulations. We find that both structural and energetic heterogeneity can lower the free energy barrier to folding. Correlating stronger contact energies with entropically likely contacts of a given native structure lowers the barrier, and anticorrelating the energies has the reverse effect. Designing in relatively mild energetic heterogeneity can eliminate the barrier completely at the transition temperature. Sequences with native energies tuned to fold uniformly, as well as sequences tuned to fold by a single or a few routes, are rare. Sequences with weak native energetic heterogeneity are more common; their folding kinetics is more strongly determined by properties of the native structure. Sequences with different distributions of stability throughout the protein may still be good folders to the same structure. A measure of folding route narrowness is introduced which correlates with rate, and which can give information about the intrinsic biases in ordering due to native topology. This theoretical framework allows us to systematically investigate the coupled effects of energy and topology in protein folding, and to interpret recent experiments which investigate these effects.",q-bio,Quantitative Biology
Odor recognition and segmentation by a model olfactory bulb and cortex,"We present a model of an olfactory system that performs odor segmentation. Based on the anatomy and physiology of natural olfactory systems, it consists of a pair of coupled modules, bulb and cortex. The bulb encodes the odor inputs as oscillating patterns. The cortex functions as an associative memory: When the input from the bulb matches a pattern stored in the connections between its units, the cortical units resonate in an oscillatory pattern characteristic of that odor. Further circuitry transforms this oscillatory signal to a slowly-varying feedback to the bulb. This feedback implements olfactory segmentation by suppressing the bulbar response to the pre-existing odor, thereby allowing subsequent odors to be singled out for recognition.",q-bio,Quantitative Biology
Why does a protein fold?,"With the help of lattice Monte Carlo modelling of heteropolymers, we show that the necessary condition for a protein to fold on short call is to proceed through partially folded intermediates. These elementary structures are formed at an early stage in the folding process and contain, at the local level, essentially all of the amino acids found in the folding core (transition state) of the protein, providing the local guidance for its formation. The sufficient condition for the protein to fold is that the designed sequence has an energy, in the native conformation, below $E_c$ (the lowest energy of the structurally dissimilar compact conformations) where it has not to compete with the bulk of misfolded conformations. Sequences with energy close to $E_c$ can display prion--like behaviour, folding to two structurally dissimilar conformations, one of them being the native.",q-bio,Quantitative Biology
Fluctuation-Facilitated Charge Migration along DNA,"We propose a model Hamiltonian for charge transfer along the DNA double helix with temperature driven fluctuations in the base pair positions acting as the rate limiting factor for charge transfer between neighboring base pairs. We compare the predictions of the model with the recent work of J.K. Barton and A.H. Zewail (Proc.Natl.Acad.Sci.USA, {\bf 96}, 6014 (1999)) on the unusual two-stage charge transfer of DNA.",q-bio,Quantitative Biology
Statistical Analysis of Genealogical Trees for Polygamic Species,"Repetitions within a given genealogical tree provides some information about the degree of consanguineity of a population. They can be analyzed with techniques usually employed in statistical physics when dealing with fixed point transformations. In particular we show that the tree features strongly depend on the fractions of males and females in the population, and also on the offspring probability distribution. We check different possibilities, some of them relevant to human groups, and compare them with simulations.",q-bio,Quantitative Biology
Learning by a nerual net in a noisy environment - The pseudo-inverse solution revisited,"A recurrent neural net is described that learns a set of patterns in the presence of noise. The learning rule is of Hebbian type, and, if noise would be absent during the learning process, the resulting final values of the weights would correspond to the pseudo-inverse solution of the fixed point equation in question. For a non-vanishing noise parameter, an explicit expression for the expectation value of the weights is obtained. This result turns out to be unequal to the pseudo-inverse solution. Furthermore, the stability properties of the system are discussed.",q-bio,Quantitative Biology
Helix Formation and Folding in an Artificial Peptide,"We study the relation between $$-helix formation and folding for a simple artificial peptide, Ala$_{10}$-Gly$_5$-Ala$_{10}$. Our data rely on multicanonical Monte Carlo simulations where the interactions among all atoms are taken into account. The free-energy landscape of the peptide is evaluated for various temperatures. Our data indicate that folding of this peptide is a two-step process: in a first step two $$-helices are formed which afterwards re-arrange themselves into a U-like structure.",q-bio,Quantitative Biology
Epidemic spreading in correlated complex networks,"We study a dynamical model of epidemic spreading on complex networks in which there are explicit correlations among the node's connectivities. For the case of Markovian complex networks, showing only correlations between pairs of nodes, we find an epidemic threshold inversely proportional to the largest eigenvalue of the connectivity matrix that gives the average number of links that from a node with connectivity $k$ go to nodes with connectivity $k'$. Numerical simulations on a correlated growing network model provide support for our conclusions.",q-bio,Quantitative Biology
"Topological complexity, contact order and protein folding rates","Monte Carlo simulations of protein folding show the emergence of a strong correlation between the relative contact order parameter, CO, and the folding time, t, of two-state folding proteins for longer chains with number of amino acids, N>=54, and higher contact order, CO > 0.17. The correlation is particularly strong for N=80 corresponding to slow and more complex folding kinetics. These results are qualitatively compatible with experimental data where a general trend towards increasing t with CO is indeed observed in a set of proteins with chain length ranging from 41 to 154 amino acids.",q-bio,Quantitative Biology
Exact Solution of the Munoz-Eaton Model for Protein Folding,"A transfer-matrix formalism is introduced to evaluate exactly the partition function of the Munoz-Eaton model, relating the folding kinetics of proteins of known structure to their thermodynamics and topology. This technique can be used for a generic protein, for any choice of the energy and entropy parameters, and in principle allows the model to be used as a first tool to characterize the dynamics of a protein of known native state and equilibrium population. Applications to a $$-hairpin and to protein CI-2, with comparisons to previous results, are also shown.",q-bio,Quantitative Biology
Backbone-induced semiconducting behavior in short DNA wires,"We propose a model Hamiltonian for describing charge transport through short homogeneous double stranded DNA molecules. We show that the hybridization of the overlapping pi orbitals in the base-pair stack coupled to the backbone is sufficient to predict the existence of a gap in the nonequilibrium current-voltage characteristics with a minimal number of parameters. Our results are in a good agreement with the recent finding of semiconducting behavior in short poly(G)-poly(C) DNA oligomers. In particular, our model provides a correct description of the molecular resonances which determine the quasi-linear part of the current out of the gap region.",q-bio,Quantitative Biology
"Entropic Barriers, Frustration and Order: Basic Ingredients in Protein Folding","We solve a model that takes into account entropic barriers, frustration, and the organization of a protein-like molecule. For a chain of size $M$, there is an effective folding transition to an ordered structure. Without frustration, this state is reached in a time that scales as $M^$, with $\simeq 3$. This scaling is limited by the amount of frustration which leads to the dynamical selectivity of proteins: foldable proteins are limited to $\sim 300$ monomers; and they are stable in {\it one} range of temperatures, independent of size and structure. These predictions explain generic properties of {\it in vivo} proteins.",q-bio,Quantitative Biology
Statistical Mechanics of Membrane Protein Conformation: A Homopolymer Model,"The conformation and the phase diagram of a membrane protein are investigated via grand canonical ensemble approach using a homopolymer model. We discuss the nature and pathway of $$-helix integration into the membrane that results depending upon membrane permeability and polymer adsorptivity. For a membrane with the permeability larger than a critical value, the integration becomes the second order transition that occurs at the same temperature as that of the adsorption transition. For a nonadsorbing membrane, the integration is of the first order due to the aggregation of $$-helices.",q-bio,Quantitative Biology
Master equation approach to protein folding,"The dynamics of two 12-monomer heteropolymers on the square lattice is studied exactly within the master equation approach. The time evolution of the occupancy of the native state is determined. At low temperatures, the median folding time follows the Arrhenius law and is governed by the longest relaxation time. For good folders, significant kinetic traps appear in the folding funnel whereas for bad folders, the traps also occur in non-native energy valleys.",q-bio,Quantitative Biology
Nonstationary Stochastic Resonance,"It is by now established that, remarkably, the addition of noise to a nonlinear system may sometimes facilitate, rather than hamper the detection of weak signals. This phenomenon, usually referred to as stochastic resonance, was originally associated with strictly periodic signals, but it was eventually shown to occur for stationary aperiodic signals as well. However, in several situations of practical interest, the signal can be markedly nonstationary. We demonstrate that the phenomenon of stochastic resonance extends to nonstationary signals as well, and thus could be relevant to a wider class of biological and electronic applications. Building on both nondynamic and aperiodic stochastic resonance, our scheme is based on a multilevel trigger mechanism, which could be realized as a parallel network of differentiated threshold sensors. We find that optimal detection is reached for a number of thresholds of order ten, and that little is gained by going much beyond that number. We raise the question of whether this is related to the fact that evolution has favored some fixed numbers of precisely this order of magnitude in certain aspects of sensory perception.",q-bio,Quantitative Biology
Generic Modeling of Chemotactic Based Self-Wiring of Neural Networks,"The proper functioning of the nervous system depends critically on the intricate network of synaptic connections that are generated during the system development. During the network formation, the growth cones migrate through the embryonic environment to their targets using chemical communication. A major obstacle in the elucidation of fundamental principles underlying this self-wiring is the complexity of the system being analyzed. Hence much effort is devoted to in-vitro experiments of simpler 2D model systems. In these experiments neurons are placed on Poly-L-Lysine (PLL) surfaces so it is easier to monitor their self-wiring. We developed a model to reproduce the salient features of the 2D systems, inspired by the study of bacterial colony's growth and the aggregation of amoebae. We represent the neurons (each composed of cell's soma, neurites and growth cones) by active elements that capture the generic features of the real neurons. The model also incorporates stationary units representing the cells' soma and communicating walkers representing the growth cones. The stationary units send neurites one at a time, and respond to chemical signaling. The walkers migrate in response to chemotaxis substances emitted by the soma and communicate with each other and with the soma by means of chemotactic ``feedback''. The interplay between the chemo-repulsive and chemo-attractive responses is determined by the dynamics of the walker's internal energy which is controlled by the soma. These features enable the neurons to perform the complex task of self-wiring.",q-bio,Quantitative Biology
"Folding, Design and Determination of Interaction Potentials Using Off-Lattice Dynamics of Model Heteropolymers","We present the results of a self-consistent, unified molecular dynamics study of simple model heteropolymers in the continuum with emphasis on folding, sequence design and the determination of the interaction parameters of the effective potential between the amino acids from the knowledge of the native states of the designed sequences.",q-bio,Quantitative Biology
Nonstationary Stochastic Resonance in a Single Neuron-Like System,"Stochastic resonance holds much promise for the detection of weak signals in the presence of relatively loud noise. Following the discovery of nondynamical and of aperiodic stochastic resonance, it was recently shown that the phenomenon can manifest itself even in the presence of nonstationary signals. This was found in a composite system of differentiated trigger mechanisms mounted in parallel, which suggests that it could be realized in some elementary neural networks or nonlinear electronic circuits. Here, we find that even an individual trigger system may be able to detect weak nonstationary signals using stochastic resonance. The very simple modification to the trigger mechanism that makes this possible is reminiscent of some aspects of actual neuron physics. Stochastic resonance may thus become relevant to more types of biological or electronic systems injected with an ever broader class of realistic signals.",q-bio,Quantitative Biology
Hydrophobicity and Unique Folding of Selected Polymers,"In suitable environments, proteins, nucleic acids and certain synthetic polymers fold into unique conformations. This work shows that it is possible to construct lattice models of foldable heteropolymers by expressing the energy only in terms of individual properties of monomers, such as the exposure to the solvent and the steric factor.",q-bio,Quantitative Biology
Determination of Interaction Potentials of Amino Acids from Native Protein Structures: Test on Simple Lattice Models,"We propose a novel method for the determination of the effective interaction potential between the amino acids of a protein. The strategy is based on the combination of a new optimization procedure and a geometrical argument, which also uncovers the shortcomings of any optimization procedure. The strategy can be applied on any data set of native structures such as those available from the Protein Data Bank (PDB). In this work, however, we explain and test our approach on simple lattice models, where the true interactions are known a priori. Excellent agreement is obtained between the extracted and the true potentials even for modest numbers of protein structures in the PDB. Comparisons with other methods are also discussed.",q-bio,Quantitative Biology
Sliding Columnar Phase of DNA-Lipid Complexes,"We introduce a simple model for DNA-cationic-lipid complexes in which galleries between planar bilayer lipid lamellae contain DNA 2D smectic lattices that couple orientationally and positionally to lattices in neighboring galleries. We identify a new equilibrium phase in which there are long-range orientational but not positional correlations between DNA lattices. We discuss properties of this new phase such as its X-ray structure factor S(r), which exhibits unusual exp(- const.ln^2 r) behavior as a function of in-plane separation r.",q-bio,Quantitative Biology
Hard Spheres in Vesicles: Curvature-Induced Forces and Particle-Induced Curvature,"We explore the interplay of membrane curvature and nonspecific binding due to excluded-volume effects among colloidal particles inside lipid bilayer vesicles. We trapped submicron spheres of two different sizes inside a pear-shaped, multilamellar vesicle and found the larger spheres to be pinned to the vesicle's surface and pushed in the direction of increasing curvature. A simple model predicts that hard spheres can induce shape changes in flexible vesicles. The results demonstrate an important relationship between the shape of a vesicle or pore and the arrangement of particles within it.",q-bio,Quantitative Biology
Fractal Analysis of Protein Potential Energy Landscapes,"The fractal properties of the total potential energy V as a function of time t are studied for a number of systems, including realistic models of proteins (PPT, BPTI and myoglobin). The fractal dimension of V(t), characterized by the exponent , is almost independent of temperature and increases with time, more slowly the larger the protein. Perhaps the most striking observation of this study is the apparent universality of the fractal dimension, which depends only weakly on the type of molecular system. We explain this behavior by assuming that fractality is caused by a self-generated dynamical noise, a consequence of intermode coupling due to anharmonicity. Global topological features of the potential energy landscape are found to have little effect on the observed fractal behavior.",q-bio,Quantitative Biology
Fractional Populations in Sex-linked Inheritance,We study the fractional populations in chromosome inherited diseases. The governing equations for the fractional populations are found and solved in the presence of mutation and selection. The physical fixed points obtained are used to discuss the cases of color blindness and hemophilia.,q-bio,Quantitative Biology
"Selection, Mutations and Codon Usage in Bacterial Model",We present a statistical model of bacterial evolution based on the coupling between codon usage and tRNA abundance. Such a model interprets this aspect of the evolutionary process as a balance between the codon homogenization effect due to mutation process and the improvement of the translation phase due to natural selection. We develop a thermodynamical description of the asymptotic state of the model. The analysis of naturally occurring sequences shows that the effect of natural selection on codon bias not only affects genes whose products are largely required at maximal growth rate conditions but also gene products that undergo rapid transient increases.,q-bio,Quantitative Biology
Electrostatics of Lipid Bilayer Bending,"The electrostatic contribution to spontaneous membrane curvature is calculated within Poisson-Boltzmann theory under a variety of assumptions and emphasizing parameters in the physiological range. Asymmetric surface charges, either fixed with respect to bilayer midplane area, or with respect to the lipid-water area both induce curvature but of opposite sign. Unequal screening layers on the two sides of a vesicle ({\it e.g.} multivalent cationic proteins on one side and monovalent salt on the other) also induces bending. For reasonable parameters, tubules formed by electrostatically induced bending can have radii in the 50-100nm range, often seen in many intracellular organelles. Thus membrane associated proteins may induce curvature and subsequent budding, without themselves being intrinsically curved. Furthermore, we derive the previously unexplored effects of respecting the strict conservation of charge within the interior of a vesicle. The electrostatic component to the bending modulus is small under most of our conditions, and is left as an experimental parameter. The large parameter space of conditions is surveyed in an array of graphs.",q-bio,Quantitative Biology
"Protein Folding Kinetics: Time Scales, Pathways, and Energy Landscapes in Terms of Sequence Dependent Properties","The folding kinetics of a number of sequences for off-lattice continuum model of proteins is studied using Langevin simulations at two values of the friction coefficient. We show that there is a remarkable correlation between folding times, $_{F}$, and $= (T_{} - T_{F})/T_{} $, where $T_{}$ and $T_{F}$ are the equilibrium collapse and folding transition temperatures, respectively. The microscopic dynamics reveals several scenarios for the refolding kinetics depending on the values of $$. Proteins with small $$ reach the native conformation via a nucleation collapse mechanism and their energy landscape is characterized by single dominant native basin of attraction. Proteins with large $$ get trapped in competing basins of attraction, in which they adopt misfolded structures. In this case only a small fraction of molecules $$ access the native state rapidly, the majority of them approach the native state by a three stage multipathway mechanism. The partition factor $$ is determined by $$: smaller the value of $$ larger is $$. The qualitative aspects of our results are found to be independent of the friction coefficient. Estimates for time scales for folding of small proteins via a nucleation collapse mechanism are presented.",q-bio,Quantitative Biology
The Origin of the Designability of Protein Structures,"We examined what determines the designability of 2-letter codes (H and P) lattice proteins from three points of view. First, whether the native structure is searched within all possible structures or within maximally compact structures. Second, whether the structure of the used lattice is bipartite or not. Third, the effect of the length of the chain, namely, the number of monomers on the chain. We found that the bipartiteness of the lattice structure is not a main factor which determines the designability. Our results suggest that highly designable structures will be found when the length of the chain is sufficiently long to make the hydrophobic core consisting of enough number of monomers.",q-bio,Quantitative Biology
Geometrically Reduced Number of Protein Ground State Candidates,"Geometrical properties of protein ground states are studied using an algebraic approach. It is shown that independent from inter-monomer interactions, the collection of ground state candidates for any folded protein is unexpectedly small: For the case of a two-parameter Hydrophobic-Polar lattice model for $L$-mers, the number of these candidates grows only as $L^2$. Moreover, the space of the interaction parameters of the model breaks up into well-defined domains, each corresponding to one ground state candidate, which are separated by sharp boundaries. In addition, by exact enumeration, we show there are some sequences which have one absolute unique native state. These absolute ground states have perfect stability against change of inter-monomer interaction potential.",q-bio,Quantitative Biology
Hydrodynamics of bacterial motion,In this paper we present a hydrodynamic approach to describe the motion of migrating bacteria as a special class of self-propelled systems. Analytical and numerical calculations has been performed to study the behavior of our model in the turbulent-like regime and to show that a phase transition occurs as a function of noise strength. Our results can explain previous experimental observations as well as results of numerical simulations.,q-bio,Quantitative Biology
The role of the energy gap in protein folding dynamics,"The dynamics of folding of proteins is studied by means of a phenomenological master equation. The energy distribution is taken as a truncated exponential for the misfolded states plus a native state sitting below the continuum. The influence of the gap on the folding dynamics is studied, for various models of the transition probabilities between the different states of the protein. We show that for certain models, the relaxation to the native state is accelerated by increasing the gap, whereas for others it is slowed down .",q-bio,Quantitative Biology
Realistic Models of Biological Motion,"The origin of biological motion can be traced back to the function of molecular motor proteins. Cytoplasmic dynein and kinesin transport organelles within our cells moving along a polymeric filament, the microtubule. The motion of the myosin molecules along the actin filaments is responsible for the contraction of our muscles. Recent experiments have been able to reveal some important features of the motion of individual motor proteins, and a new statistical physical description - often referred to as ``thermal ratchets'' - has been developed for the description of motion of these molecules. In this approach the motors are considered as Brownian particles moving along one-dimensional periodic structures due to the effect of nonequilibrium fluctuations. Assuming specific types of interaction between the particles the models can be made more realistic. We have been able to give analytic solutions for our model of kinesin with elastically coupled Brownian heads and for the motion of the myosin filament where the motors are connected through a rigid backbone. Our theoretical predictions are in a very good agreement with the various experimental results. In addition, we have considered the effects arising as a result of interaction among a large number of molecular motors, leading to a number of novel cooperative transport phenomena.",q-bio,Quantitative Biology
Spontaneous curvature-induced dynamical instability of Kirchhoff filaments: Application to DNA kink deformations,"The Kirchhoff elastic theory of thin filaments with spontaneous curvature is employed in the understanding of the onset of the kink transitions observed in short DNA rings. Dynamical analysis shows that when its actual curvature is less than some threshold value determined by the spontaneous curvature, a circular DNA will begin to buckle to other shapes. The observable and the dominant deformation modes are also determined by dynamical instability analysis, and the different effects of Zn$^{2+}$ and Mg$^{2+}$ ions on DNA configurational properties are qualitatively discussed.",q-bio,Quantitative Biology
Dynamical properties of a randomly diluted neural network with variable activity,"The subject of study is a neural network with binary neurons, randomly diluted synapses and variable pattern activity. We look at the system with parallel updating using a probabilistic approach to solve the one step dynamics with one condensed pattern. We derive restrictions on the storage capacity and the mutual information content occuring during the retrieval process. Special focus is on the constraints on the threshold for optimal performance. We also look at the effect of noisy updating, giving a dynamical version of the critical temperature, the corresponding threshold and an approximation for the time evolution for small temperatures. The description is applicable to the whole retrieval process in the limit of strong dilution. The analysis is carried out as exactly as possible and over the full parameter ranges, generalizing some former results.",q-bio,Quantitative Biology
Synchronous clusters in a noisy inhibitory neural network,"We study the stability and information encoding capacity of synchronized states in a neuronal network model that represents part of thalamic circuitry. Our model neurons have a Hodgkin-Huxley-type low threshold Calcium channel, display post inhibitory rebound, and are connected via GABAergic inhibitory synapses. We find that there is a threshold in synaptic strength, $_c$, below which there are no stable spiking network states. Above threshold the stable spiking state is a cluster state, where different groups of neurons fire consecutively, and each neuron fires with the same cluster each time. Weak noise destabilizes this state, but stronger noise drives the system into a different, self-organized, stochastically synchronized state. Neuronal firing is still organized in clusters, but individual neurons can hop from cluster to cluster. Noise can actually induce and sustain such a state below the threshold of synaptic strength. We do find a qualitative difference in the firing patterns between small ($\sim 10$ neurons) and large ($\sim 1000$ neurons) networks. We determine the information content of the spike trains in terms of two separate contributions: the spike time jitter around cluster firing times, and the hopping from cluster to cluster. We quantify the information loss due to temporally correlated interspike intervals. Recent experiments on the locust olfactory system and striatal neurons suggest that the nervous system may actually use these two channels to encode separate and unique information.",q-bio,Quantitative Biology
Protein folding using contact maps,"We present the development of the idea to use dynamics in the space of contact maps as a computational approach to the protein folding problem. We first introduce two important technical ingredients, the reconstruction of a three dimensional conformation from a contact map and the Monte Carlo dynamics in contact map space. We then discuss two approximations to the free energy of the contact maps and a method to derive energy parameters based on perceptron learning. Finally we present results, first for predictions based on threading and then for energy minimization of crambin and of a set of 6 immunoglobulins. The main result is that we proved that the two simple approximations we studied for the free energy are not suitable for protein folding. Perspectives are discussed in the last section.",q-bio,Quantitative Biology
An Analytical Approach to the Protein Designability Problem,"We present an analytical method for determining the designability of protein structures. We apply our method to the case of two-dimensional lattice structures, and give a systematic solution for the spectrum of any structure. Using this spectrum, the designability of a structure can be estimated. We outline a heirarchy of structures, from most to least designable, and show that this heirarchy depends on the potential that is used.",q-bio,Quantitative Biology
Simple models of proteins with repulsive non-native contacts,The Go model is extended to the case when the non-native contact energies may be either attractive or repulsive. The folding temperature is found to increase with the energy of non-native contacts. The repulsive non-native contact energies may lead to folding at T=0 for some two-dimensional sequences and to reduction in complexity of disconnectivity graphs for local energy minima.,q-bio,Quantitative Biology
Uncertainty on the Reproduction Ratio in the SIR Model,"The aim of this paper is to understand the extreme variability on the estimated reproduction ratio $R_0$ observed in practice. For expository purpose we consider a discrete time stochastic version of the Susceptible-Infected-Recovered (SIR) model, and introduce different approximate maximum likelihood (AML) estimators of $R_0$. We carefully discuss the properties of these estimators and illustrate by a Monte-Carlo study the width of confidence intervals on $R_0$.",econ.EM,Econometrics
Analysis of Randomized Experiments with Network Interference and Noncompliance,"Randomized experiments have become a standard tool in economics. In analyzing randomized experiments, the traditional approach has been based on the Stable Unit Treatment Value (SUTVA: \cite{rubin}) assumption which dictates that there is no interference between individuals. However, the SUTVA assumption fails to hold in many applications due to social interaction, general equilibrium, and/or externality effects. While much progress has been made in relaxing the SUTVA assumption, most of this literature has only considered a setting with perfect compliance to treatment assignment. In practice, however, noncompliance occurs frequently where the actual treatment receipt is different from the assignment to the treatment. In this paper, we study causal effects in randomized experiments with network interference and noncompliance. Spillovers are allowed to occur at both treatment choice stage and outcome realization stage. In particular, we explicitly model treatment choices of agents as a binary game of incomplete information where resulting equilibrium treatment choice probabilities affect outcomes of interest. Outcomes are further characterized by a random coefficient model to allow for general unobserved heterogeneity in the causal effects. After defining our causal parameters of interest, we propose a simple control function estimator and derive its asymptotic properties under large-network asymptotics. We apply our methods to the randomized subsidy program of \cite{dupas} where we find evidence of spillover effects on both short-run and long-run adoption of insecticide-treated bed nets. Finally, we illustrate the usefulness of our methods by analyzing the impact of counterfactual subsidy policies.",econ.EM,Econometrics
Lasso under Multi-way Clustering: Estimation and Post-selection Inference,"This paper studies high-dimensional regression models with lasso when data is sampled under multi-way clustering. First, we establish convergence rates for the lasso and post-lasso estimators. Second, we propose a novel inference method based on a post-double-selection procedure and show its asymptotic validity. Our procedure can be easily implemented with existing statistical packages. Simulation results demonstrate that the proposed procedure works well in finite sample. We illustrate the proposed method with a couple of empirical applications to development and growth economics.",econ.EM,Econometrics
Variational Bayesian Inference for Mixed Logit Models with Unobserved Inter- and Intra-Individual Heterogeneity,"Variational Bayes (VB), a method originating from machine learning, enables fast and scalable estimation of complex probabilistic models. Thus far, applications of VB in discrete choice analysis have been limited to mixed logit models with unobserved inter-individual taste heterogeneity. However, such a model formulation may be too restrictive in panel data settings, since tastes may vary both between individuals as well as across choice tasks encountered by the same individual. In this paper, we derive a VB method for posterior inference in mixed logit models with unobserved inter- and intra-individual heterogeneity. In a simulation study, we benchmark the performance of the proposed VB method against maximum simulated likelihood (MSL) and Markov chain Monte Carlo (MCMC) methods in terms of parameter recovery, predictive accuracy and computational efficiency. The simulation study shows that VB can be a fast, scalable and accurate alternative to MSL and MCMC estimation, especially in applications in which fast predictions are paramount. VB is observed to be between 2.8 and 17.7 times faster than the two competing methods, while affording comparable or superior accuracy. Besides, the simulation study demonstrates that a parallelised implementation of the MSL estimator with analytical gradients is a viable alternative to MCMC in terms of both estimation accuracy and computational efficiency, as the MSL estimator is observed to be between 0.9 and 2.1 times faster than MCMC.",econ.EM,Econometrics
Exact Testing of Many Moment Inequalities Against Multiple Violations,"This paper considers the problem of testing many moment inequalities, where the number of moment inequalities ($p$) is possibly larger than the sample size ($n$). Chernozhukov et al. (2019) proposed asymptotic tests for this problem using the maximum $t$ statistic. We observe that such tests can have low power if multiple inequalities are violated. As an alternative, we propose novel randomization tests based on a maximum non-negatively weighted combination of $t$ statistics. We provide a condition guaranteeing size control in large samples. Simulations show that the tests control size in small samples ($n = 30$, $p = 1000$), and often has substantially higher power against alternatives with multiple violations than tests based on the maximum $t$ statistic.",econ.EM,Econometrics
Fast Mesh Refinement in Pseudospectral Optimal Control,"Mesh refinement in pseudospectral (PS) optimal control is embarrassingly easy --- simply increase the order $N$ of the Lagrange interpolating polynomial and the mathematics of convergence automates the distribution of the grid points. Unfortunately, as $N$ increases, the condition number of the resulting linear algebra increases as $N^2$; hence, spectral efficiency and accuracy are lost in practice. In this paper, we advance Birkhoff interpolation concepts over an arbitrary grid to generate well-conditioned PS optimal control discretizations. We show that the condition number increases only as $\sqrt{N}$ in general, but is independent of $N$ for the special case of one of the boundary points being fixed. Hence, spectral accuracy and efficiency are maintained as $N$ increases. The effectiveness of the resulting fast mesh refinement strategy is demonstrated by using \underline{polynomials of over a thousandth order} to solve a low-thrust, long-duration orbit transfer problem.",econ.EM,Econometrics
Assessing Disparate Impacts of Personalized Interventions: Identifiability and Bounds,"Personalized interventions in social services, education, and healthcare leverage individual-level causal effect predictions in order to give the best treatment to each individual or to prioritize program interventions for the individuals most likely to benefit. While the sensitivity of these domains compels us to evaluate the fairness of such policies, we show that actually auditing their disparate impacts per standard observational metrics, such as true positive rates, is impossible since ground truths are unknown. Whether our data is experimental or observational, an individual's actual outcome under an intervention different than that received can never be known, only predicted based on features. We prove how we can nonetheless point-identify these quantities under the additional assumption of monotone treatment response, which may be reasonable in many applications. We further provide a sensitivity analysis for this assumption by means of sharp partial-identification bounds under violations of monotonicity of varying strengths. We show how to use our results to audit personalized interventions using partially-identified ROC and xROC curves and demonstrate this in a case study of a French job training dataset.",econ.EM,Econometrics
Parametric Modeling of Quantile Regression Coefficient Functions with Longitudinal Data,"In ordinary quantile regression, quantiles of different order are estimated one at a time. An alternative approach, which is referred to as quantile regression coefficients modeling (QRCM), is to model quantile regression coefficients as parametric functions of the order of the quantile. In this paper, we describe how the QRCM paradigm can be applied to longitudinal data. We introduce a two-level quantile function, in which two different quantile regression models are used to describe the (conditional) distribution of the within-subject response and that of the individual effects. We propose a novel type of penalized fixed-effects estimator, and discuss its advantages over standard methods based on $\ell_1$ and $\ell_2$ penalization. We provide model identifiability conditions, derive asymptotic properties, describe goodness-of-fit measures and model selection criteria, present simulation results, and discuss an application. The proposed method has been implemented in the R package qrcm.",econ.EM,Econometrics
"Breiman's ""Two Cultures"" Revisited and Reconciled","In a landmark paper published in 2001, Leo Breiman described the tense standoff between two cultures of data modeling: parametric statistical and algorithmic machine learning. The cultural division between these two statistical learning frameworks has been growing at a steady pace in recent years. What is the way forward? It has become blatantly obvious that this widening gap between ""the two cultures"" cannot be averted unless we find a way to blend them into a coherent whole. This article presents a solution by establishing a link between the two cultures. Through examples, we describe the challenges and potential gains of this new integrated statistical thinking.",econ.EM,Econometrics
Optimal selection of the number of control units in kNN algorithm to estimate average treatment effects,"We propose a simple approach to optimally select the number of control units in k nearest neighbors (kNN) algorithm focusing in minimizing the mean squared error for the average treatment effects. Our approach is non-parametric where confidence intervals for the treatment effects were calculated using asymptotic results with bias correction. Simulation exercises show that our approach gets relative small mean squared errors, and a balance between confidence intervals length and type I error. We analyzed the average treatment effects on treated (ATET) of participation in 401(k) plans on accumulated net financial assets confirming significant effects on amount and positive probability of net asset. Our optimal k selection produces significant narrower ATET confidence intervals compared with common practice of using k=1.",econ.EM,Econometrics
The Mode Treatment Effect,"Mean, median, and mode are three essential measures of the centrality of probability distributions. In program evaluation, the average treatment effect (mean) and the quantile treatment effect (median) have been intensively studied in the past decades. The mode treatment effect, however, has long been neglected in program evaluation. This paper fills the gap by discussing both the estimation and inference of the mode treatment effect. I propose both traditional kernel and machine learning methods to estimate the mode treatment effect. I also derive the asymptotic properties of the proposed estimators and find that both estimators follow the asymptotic normality but with the rate of convergence slower than the regular rate $\sqrt{N}$, which is different from the rates of the classical average and quantile treatment effect estimators.",econ.EM,Econometrics
Reserve Price Optimization for First Price Auctions,"The display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. In light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. In this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. Our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. We show that revenue in a first-price auction can be usefully decomposed into a \emph{demand} component and a \emph{bidding} component, and introduce techniques to reduce the variance of each component. We characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from Google ad exchange.",econ.EM,Econometrics
Estimating TVP-VAR models with time invariant long-run multipliers,"The main goal of this paper is to develop a methodology for estimating time varying parameter vector auto-regression (TVP-VAR) models with a timeinvariant long-run relationship between endogenous variables and changes in exogenous variables. We propose a Gibbs sampling scheme for estimation of model parameters as well as time-invariant long-run multiplier parameters. Further we demonstrate the applicability of the proposed method by analyzing examples of the Norwegian and Russian economies based on the data on real GDP, real exchange rate and real oil prices. Our results show that incorporating the time invariance constraint on the long-run multipliers in TVP-VAR model helps to significantly improve the forecasting performance.",econ.EM,Econometrics
Off-Policy Exploitability-Evaluation in Two-Player Zero-Sum Markov Games,"Off-policy evaluation (OPE) is the problem of evaluating new policies using historical data obtained from a different policy. In the recent OPE context, most studies have focused on single-player cases, and not on multi-player cases. In this study, we propose OPE estimators constructed by the doubly robust and double reinforcement learning estimators in two-player zero-sum Markov games. The proposed estimators project exploitability that is often used as a metric for determining how close a policy profile (i.e., a tuple of policies) is to a Nash equilibrium in two-player zero-sum games. We prove the exploitability estimation error bounds for the proposed estimators. We then propose the methods to find the best candidate policy profile by selecting the policy profile that minimizes the estimated exploitability from a given policy profile class. We prove the regret bounds of the policy profiles selected by our methods. Finally, we demonstrate the effectiveness and performance of the proposed estimators through experiments.",econ.EM,Econometrics
Efficient Covariate Balancing for the Local Average Treatment Effect,"This paper develops an empirical balancing approach for the estimation of treatment effects under two-sided noncompliance using a binary conditionally independent instrumental variable. The method weighs both treatment and outcome information with inverse probabilities to produce exact finite sample balance across instrument level groups. It is free of functional form assumptions on the outcome or the treatment selection step. By tailoring the loss function for the instrument propensity scores, the resulting treatment effect estimates exhibit both low bias and a reduced variance in finite samples compared to conventional inverse probability weighting methods. The estimator is automatically weight normalized and has similar bias properties compared to conventional two-stage least squares estimation under constant causal effects for the compliers. We provide conditions for asymptotic normality and semiparametric efficiency and demonstrate how to utilize additional information about the treatment selection step for bias reduction in finite samples. The method can be easily combined with regularization or other statistical learning approaches to deal with a high-dimensional number of observed confounding variables. Monte Carlo simulations suggest that the theoretical advantages translate well to finite samples. The method is illustrated in an empirical example.",econ.EM,Econometrics
Inflation Dynamics of Financial Shocks,"We study the effects of financial shocks on the United States economy by using a Bayesian structural vector autoregressive (SVAR) model that exploits the non-normalities in the data. We use this method to uniquely identify the model and employ inequality constraints to single out financial shocks. The results point to the existence of two distinct financial shocks that have opposing effects on inflation, which supports the idea that financial shocks are transmitted to the real economy through both demand and supply side channels.",econ.EM,Econometrics
"Nonparametric Identification of Production Function, Total Factor Productivity, and Markup from Revenue Data","Commonly used methods of production function and markup estimation assume that a firm's output quantity can be observed as data, but typical datasets contain only revenue, not output quantity. We examine the nonparametric identification of production function and markup from revenue data when a firm faces a general nonparametri demand function under imperfect competition. Under standard assumptions, we provide the constructive nonparametric identification of various firm-level objects: gross production function, total factor productivity, price markups over marginal costs, output prices, output quantities, a demand system, and a representative consumer's utility function.",econ.EM,Econometrics
A Practical Guide of Off-Policy Evaluation for Bandit Problems,"Off-policy evaluation (OPE) is the problem of estimating the value of a target policy from samples obtained via different policies. Recently, applying OPE methods for bandit problems has garnered attention. For the theoretical guarantees of an estimator of the policy value, the OPE methods require various conditions on the target policy and policy used for generating the samples. However, existing studies did not carefully discuss the practical situation where such conditions hold, and the gap between them remains. This paper aims to show new results for bridging the gap. Based on the properties of the evaluation policy, we categorize OPE situations. Then, among practical applications, we mainly discuss the best policy selection. For the situation, we propose a meta-algorithm based on existing OPE estimators. We investigate the proposed concepts using synthetic and open real-world datasets in experiments.",econ.EM,Econometrics
A Class of Time-Varying Vector Moving Average Models: Nonparametric Kernel Estimation and Application,"Multivariate dynamic time series models are widely encountered in practical studies, e.g., modelling policy transmission mechanism and measuring connectedness between economic agents. To better capture the dynamics, this paper proposes a wide class of multivariate dynamic models with time-varying coefficients, which have a general time-varying vector moving average (VMA) representation, and nest, for instance, time-varying vector autoregression (VAR), time-varying vector autoregression moving-average (VARMA), and so forth as special cases. The paper then develops a unified estimation method for the unknown quantities before an asymptotic theory for the proposed estimators is established. In the empirical study, we investigate the transmission mechanism of monetary policy using U.S. data, and uncover a fall in the volatilities of exogenous shocks. In addition, we find that (i) monetary policy shocks have less influence on inflation before and during the so-called Great Moderation, (ii) inflation is more anchored recently, and (iii) the long-run level of inflation is below, but quite close to the Federal Reserve's target of two percent after the beginning of the Great Moderation period.",econ.EM,Econometrics
"A step-by-step guide to design, implement, and analyze a discrete choice experiment","Discrete Choice Experiments (DCE) have been widely used in health economics, environmental valuation, and other disciplines. However, there is a lack of resources disclosing the whole procedure of carrying out a DCE. This document aims to assist anyone wishing to use the power of DCEs to understand people's behavior by providing a comprehensive guide to the procedure. This guide contains all the code needed to design, implement, and analyze a DCE using only free software.",econ.EM,Econometrics
On the Existence of Conditional Maximum Likelihood Estimates of the Binary Logit Model with Fixed Effects,"By exploiting McFadden (1974)'s results on conditional logit estimation, we show that there exists a one-to-one mapping between existence and uniqueness of conditional maximum likelihood estimates of the binary logit model with fixed effects and the configuration of data points. Our results extend those in Albert and Anderson (1984) for the cross-sectional case and can be used to build a simple algorithm that detects spurious estimates in finite samples. As an illustration, we exhibit an artificial dataset for which the STATA's command \texttt{clogit} returns spurious estimates.",econ.EM,Econometrics
Recent Developments on Factor Models and its Applications in Econometric Learning,"This paper makes a selective survey on the recent development of the factor model and its application on statistical learnings. We focus on the perspective of the low-rank structure of factor models, and particularly draws attentions to estimating the model from the low-rank recovery point of view. The survey mainly consists of three parts: the first part is a review on new factor estimations based on modern techniques on recovering low-rank structures of high-dimensional models. The second part discusses statistical inferences of several factor-augmented models and applications in econometric learning models. The final part summarizes new developments dealing with unbalanced panels from the matrix completion perspective.",econ.EM,Econometrics
Identification and Estimation of A Rational Inattention Discrete Choice Model with Bayesian Persuasion,"This paper studies the semi-parametric identification and estimation of a rational inattention model with Bayesian persuasion. The identification requires the observation of a cross-section of market-level outcomes. The empirical content of the model can be characterized by three moment conditions. A two-step estimation procedure is proposed to avoid computation complexity in the structural model. In the empirical application, I study the persuasion effect of Fox News in the 2000 presidential election. Welfare analysis shows that persuasion will not influence voters with high school education but will generate higher dispersion in the welfare of voters with a partial college education and decrease the dispersion in the welfare of voters with a bachelors degree.",econ.EM,Econometrics
Granger causality on horizontal sum of Boolean algebras,"The intention of this paper is to discuss the mathematical model of causality introduced by C.W.J. Granger in 1969. The Granger's model of causality has become well-known and often used in various econometric models describing causal systems, e.g., between commodity prices and exchange rates.
  Our paper presents a new mathematical model of causality between two measured objects. We have slightly modified the well-known Kolmogorovian probability model. In particular, we use the horizontal sum of set $$-algebras instead of their direct product.",econ.EM,Econometrics
Identifying the Discount Factor in Dynamic Discrete Choice Models,"Empirical research often cites observed choice responses to variation that shifts expected discounted future utilities, but not current utilities, as an intuitive source of information on time preferences. We study the identification of dynamic discrete choice models under such economically motivated exclusion restrictions on primitive utilities. We show that each exclusion restriction leads to an easily interpretable moment condition with the discount factor as the only unknown parameter. The identified set of discount factors that solves this condition is finite, but not necessarily a singleton. Consequently, in contrast to common intuition, an exclusion restriction does not in general give point identification. Finally, we show that exclusion restrictions have nontrivial empirical content: The implied moment conditions impose restrictions on choices that are absent from the unconstrained model.",econ.EM,Econometrics
On the Choice of Instruments in Mixed Frequency Specification Tests,"Time averaging has been the traditional approach to handle mixed sampling frequencies. However, it ignores information possibly embedded in high frequency. Mixed data sampling (MIDAS) regression models provide a concise way to utilize the additional information in high-frequency variables. In this paper, we propose a specification test to choose between time averaging and MIDAS models, based on a Durbin-Wu-Hausman test. In particular, a set of instrumental variables is proposed and theoretically validated when the frequency ratio is large. As a result, our method tends to be more powerful than existing methods, as reconfirmed through the simulations.",econ.EM,Econometrics
Uniform Inference in High-Dimensional Gaussian Graphical Models,"Graphical models have become a very popular tool for representing dependencies within a large set of variables and are key for representing causal structures. We provide results for uniform inference on high-dimensional graphical models with the number of target parameters $d$ being possible much larger than sample size. This is in particular important when certain features or structures of a causal model should be recovered. Our results highlight how in high-dimensional settings graphical models can be estimated and recovered with modern machine learning methods in complex data sets. To construct simultaneous confidence regions on many target parameters, sufficiently fast estimation rates of the nuisance functions are crucial. In this context, we establish uniform estimation rates and sparsity guarantees of the square-root estimator in a random design under approximate sparsity conditions that might be of independent interest for related problems in high-dimensions. We also demonstrate in a comprehensive simulation study that our procedure has good small sample properties.",econ.EM,Econometrics
Inference based on Kotlarski's Identity,"Kotlarski's identity has been widely used in applied economic research. However, how to conduct inference based on this popular identification approach has been an open question for two decades. This paper addresses this open problem by constructing a novel confidence band for the density function of a latent variable in repeated measurement error model. The confidence band builds on our finding that we can rewrite Kotlarski's identity as a system of linear moment restrictions. The confidence band controls the asymptotic size uniformly over a class of data generating processes, and it is consistent against all fixed alternatives. Simulation studies support our theoretical results.",econ.EM,Econometrics
Nonparametric Regression with Selectively Missing Covariates,"We consider the problem of regression with selectively observed covariates in a nonparametric framework. Our approach relies on instrumental variables that explain variation in the latent covariates but have no direct effect on selection. The regression function of interest is shown to be a weighted version of observed conditional expectation where the weighting function is a fraction of selection probabilities. Nonparametric identification of the fractional probability weight (FPW) function is achieved via a partial completeness assumption. We provide primitive functional form assumptions for partial completeness to hold. The identification result is constructive for the FPW series estimator. We derive the rate of convergence and also the pointwise asymptotic distribution. In both cases, the asymptotic performance of the FPW series estimator does not suffer from the inverse problem which derives from the nonparametric instrumental variable approach. In a Monte Carlo study, we analyze the finite sample properties of our estimator and we compare our approach to inverse probability weighting, which can be used alternatively for unconditional moment estimation. In the empirical application, we focus on two different applications. We estimate the association between income and health using linked data from the SHARE survey and administrative pension information and use pension entitlements as an instrument. In the second application we revisit the question how income affects the demand for housing based on data from the German Socio-Economic Panel Study (SOEP). In this application we use regional income information on the residential block level as an instrument. In both applications we show that income is selectively missing and we demonstrate that standard methods that do not account for the nonrandom selection process lead to significantly biased estimates for individuals with low income.",econ.EM,Econometrics
A Simple and Efficient Estimation of the Average Treatment Effect in the Presence of Unmeasured Confounders,"Wang and Tchetgen Tchetgen (2017) studied identification and estimation of the average treatment effect when some confounders are unmeasured. Under their identification condition, they showed that the semiparametric efficient influence function depends on five unknown functionals. They proposed to parameterize all functionals and estimate the average treatment effect from the efficient influence function by replacing the unknown functionals with estimated functionals. They established that their estimator is consistent when certain functionals are correctly specified and attains the semiparametric efficiency bound when all functionals are correctly specified. In applications, it is likely that those functionals could all be misspecified. Consequently their estimator could be inconsistent or consistent but not efficient. This paper presents an alternative estimator that does not require parameterization of any of the functionals. We establish that the proposed estimator is always consistent and always attains the semiparametric efficiency bound. A simple and intuitive estimator of the asymptotic variance is presented, and a small scale simulation study reveals that the proposed estimation outperforms the existing alternatives in finite samples.",econ.EM,Econometrics
Score Permutation Based Finite Sample Inference for Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) Models,"A standard model of (conditional) heteroscedasticity, i.e., the phenomenon that the variance of a process changes over time, is the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) model, which is especially important for economics and finance. GARCH models are typically estimated by the Quasi-Maximum Likelihood (QML) method, which works under mild statistical assumptions. Here, we suggest a finite sample approach, called ScoPe, to construct distribution-free confidence regions around the QML estimate, which have exact coverage probabilities, despite no additional assumptions about moments are made. ScoPe is inspired by the recently developed Sign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH case. ScoPe works by perturbing the score function using randomly permuted residuals. This produces alternative samples which lead to exact confidence regions. Experiments on simulated and stock market data are also presented, and ScoPe is compared with the asymptotic theory and bootstrap approaches.",econ.EM,Econometrics
"Factor models with many assets: strong factors, weak factors, and the two-pass procedure","This paper re-examines the problem of estimating risk premia in linear factor pricing models. Typically, the data used in the empirical literature are characterized by weakness of some pricing factors, strong cross-sectional dependence in the errors, and (moderately) high cross-sectional dimensionality. Using an asymptotic framework where the number of assets/portfolios grows with the time span of the data while the risk exposures of weak factors are local-to-zero, we show that the conventional two-pass estimation procedure delivers inconsistent estimates of the risk premia. We propose a new estimation procedure based on sample-splitting instrumental variables regression. The proposed estimator of risk premia is robust to weak included factors and to the presence of strong unaccounted cross-sectional error dependence. We derive the many-asset weak factor asymptotic distribution of the proposed estimator, show how to construct its standard errors, verify its performance in simulations, and revisit some empirical studies.",econ.EM,Econometrics
Ill-posed Estimation in High-Dimensional Models with Instrumental Variables,"This paper is concerned with inference about low-dimensional components of a high-dimensional parameter vector $^0$ which is identified through instrumental variables. We allow for eigenvalues of the expected outer product of included and excluded covariates, denoted by $M$, to shrink to zero as the sample size increases. We propose a novel estimator based on desparsification of an instrumental variable Lasso estimator, which is a regularized version of 2SLS with an additional correction term. This estimator converges to $^0$ at a rate depending on the mapping properties of $M$ captured by a sparse link condition. Linear combinations of our estimator of $^0$ are shown to be asymptotically normally distributed. Based on consistent covariance estimation, our method allows for constructing confidence intervals and statistical tests for single or low-dimensional components of $^0$. In Monte-Carlo simulations we analyze the finite sample behavior of our estimator.",econ.EM,Econometrics
Estimation and Inference for Policy Relevant Treatment Effects,"The policy relevant treatment effect (PRTE) measures the average effect of switching from a status-quo policy to a counterfactual policy. Estimation of the PRTE involves estimation of multiple preliminary parameters, including propensity scores, conditional expectation functions of the outcome and covariates given the propensity score, and marginal treatment effects. These preliminary estimators can affect the asymptotic distribution of the PRTE estimator in complicated and intractable manners. In this light, we propose an orthogonal score for double debiased estimation of the PRTE, whereby the asymptotic distribution of the PRTE estimator is obtained without any influence of preliminary parameter estimators as far as they satisfy mild requirements of convergence rates. To our knowledge, this paper is the first to develop limit distribution theories for inference about the PRTE.",econ.EM,Econometrics
Autoregressive Wild Bootstrap Inference for Nonparametric Trends,"In this paper we propose an autoregressive wild bootstrap method to construct confidence bands around a smooth deterministic trend. The bootstrap method is easy to implement and does not require any adjustments in the presence of missing data, which makes it particularly suitable for climatological applications. We establish the asymptotic validity of the bootstrap method for both pointwise and simultaneous confidence bands under general conditions, allowing for general patterns of missing data, serial dependence and heteroskedasticity. The finite sample properties of the method are studied in a simulation study. We use the method to study the evolution of trends in daily measurements of atmospheric ethane obtained from a weather station in the Swiss Alps, where the method can easily deal with the many missing observations due to adverse weather conditions.",econ.EM,Econometrics
State-Varying Factor Models of Large Dimensions,"This paper develops an inferential theory for state-varying factor models of large dimensions. Unlike constant factor models, loadings are general functions of some recurrent state process. We develop an estimator for the latent factors and state-varying loadings under a large cross-section and time dimension. Our estimator combines nonparametric methods with principal component analysis. We derive the rate of convergence and limiting normal distribution for the factors, loadings and common components. In addition, we develop a statistical test for a change in the factor structure in different states. We apply the estimator to U.S. Treasury yields and S&P500 stock returns. The systematic factor structure in treasury yields differs in times of booms and recessions as well as in periods of high market volatility. State-varying factors based on the VIX capture significantly more variation and pricing information in individual stocks than constant factor models.",econ.EM,Econometrics
Maastricht and Monetary Cooperation,"This paper describes the opportunities and also the difficulties of EMU with regard to international monetary cooperation. Even though the institutional and intellectual assistance to the coordination of monetary policy in the EU will probably be strengthened with the EMU, among the shortcomings of the Maastricht Treaty concerns the relationship between the founder members and those countries who wish to remain outside monetary union.",econ.EM,Econometrics
Stochastic model specification in Markov switching vector error correction models,"This paper proposes a hierarchical modeling approach to perform stochastic model specification in Markov switching vector error correction models. We assume that a common distribution gives rise to the regime-specific regression coefficients. The mean as well as the variances of this distribution are treated as fully stochastic and suitable shrinkage priors are used. These shrinkage priors enable to assess which coefficients differ across regimes in a flexible manner. In the case of similar coefficients, our model pushes the respective regions of the parameter space towards the common distribution. This allows for selecting a parsimonious model while still maintaining sufficient flexibility to control for sudden shifts in the parameters, if necessary. We apply our modeling approach to real-time Euro area data and assume transition probabilities between expansionary and recessionary regimes to be driven by the cointegration errors. The results suggest that the regime allocation is governed by a subset of short-run adjustment coefficients and regime-specific variance-covariance matrices. These findings are complemented by an out-of-sample forecast exercise, illustrating the advantages of the model for predicting Euro area inflation in real time.",econ.EM,Econometrics
Adaptive Bayesian Estimation of Mixed Discrete-Continuous Distributions under Smoothness and Sparsity,"We consider nonparametric estimation of a mixed discrete-continuous distribution under anisotropic smoothness conditions and possibly increasing number of support points for the discrete part of the distribution. For these settings, we derive lower bounds on the estimation rates in the total variation distance. Next, we consider a nonparametric mixture of normals model that uses continuous latent variables for the discrete part of the observations. We show that the posterior in this model contracts at rates that are equal to the derived lower bounds up to a log factor. Thus, Bayesian mixture of normals models can be used for optimal adaptive estimation of mixed discrete-continuous distributions.",econ.EM,Econometrics
Functional Sequential Treatment Allocation,"Consider a setting in which a policy maker assigns subjects to treatments, observing each outcome before the next subject arrives. Initially, it is unknown which treatment is best, but the sequential nature of the problem permits learning about the effectiveness of the treatments. While the multi-armed-bandit literature has shed much light on the situation when the policy maker compares the effectiveness of the treatments through their mean, much less is known about other targets. This is restrictive, because a cautious decision maker may prefer to target a robust location measure such as a quantile or a trimmed mean. Furthermore, socio-economic decision making often requires targeting purpose specific characteristics of the outcome distribution, such as its inherent degree of inequality, welfare or poverty. In the present paper we introduce and study sequential learning algorithms when the distributional characteristic of interest is a general functional of the outcome distribution. Minimax expected regret optimality results are obtained within the subclass of explore-then-commit policies, and for the unrestricted class of all policies.",econ.EM,Econometrics
Inference on Functionals under First Order Degeneracy,"This paper presents a unified second order asymptotic framework for conducting inference on parameters of the form $(_0)$, where $_0$ is unknown but can be estimated by $\hat_n$, and $$ is a known map that admits null first order derivative at $_0$. For a large number of examples in the literature, the second order Delta method reveals a nondegenerate weak limit for the plug-in estimator $(\hat_n)$. We show, however, that the `standard' bootstrap is consistent if and only if the second order derivative $_{_0}''=0$ under regularity conditions, i.e., the standard bootstrap is inconsistent if $_{_0}''\neq 0$, and provides degenerate limits unhelpful for inference otherwise. We thus identify a source of bootstrap failures distinct from that in Fang and Santos (2018) because the problem (of consistently bootstrapping a \textit{nondegenerate} limit) persists even if $$ is differentiable. We show that the correction procedure in Babu (1984) can be extended to our general setup. Alternatively, a modified bootstrap is proposed when the map is \textit{in addition} second order nondifferentiable. Both are shown to provide local size control under some conditions. As an illustration, we develop a test of common conditional heteroskedastic (CH) features, a setting with both degeneracy and nondifferentiability -- the latter is because the Jacobian matrix is degenerate at zero and we allow the existence of multiple common CH features.",econ.EM,Econometrics
Estimation of High-Dimensional Seemingly Unrelated Regression Models,"In this paper, we investigate seemingly unrelated regression (SUR) models that allow the number of equations (N) to be large, and to be comparable to the number of the observations in each equation (T). It is well known in the literature that the conventional SUR estimator, for example, the generalized least squares (GLS) estimator of Zellner (1962) does not perform well. As the main contribution of the paper, we propose a new feasible GLS estimator called the feasible graphical lasso (FGLasso) estimator. For a feasible implementation of the GLS estimator, we use the graphical lasso estimation of the precision matrix (the inverse of the covariance matrix of the equation system errors) assuming that the underlying unknown precision matrix is sparse. We derive asymptotic theories of the new estimator and investigate its finite sample properties via Monte-Carlo simulations.",econ.EM,Econometrics
Robust Principal Component Analysis with Non-Sparse Errors,"We show that when a high-dimensional data matrix is the sum of a low-rank matrix and a random error matrix with independent entries, the low-rank component can be consistently estimated by solving a convex minimization problem. We develop a new theoretical argument to establish consistency without assuming sparsity or the existence of any moments of the error matrix, so that fat-tailed continuous random errors such as Cauchy are allowed. The results are illustrated by simulations.",econ.EM,Econometrics
A Bootstrap Test for the Existence of Moments for GARCH Processes,"This paper studies the joint inference on conditional volatility parameters and the innovation moments by means of bootstrap to test for the existence of moments for GARCH(p,q) processes. We propose a residual bootstrap to mimic the joint distribution of the quasi-maximum likelihood estimators and the empirical moments of the residuals and also prove its validity. A bootstrap-based test for the existence of moments is proposed, which provides asymptotically correctly-sized tests without losing its consistency property. It is simple to implement and extends to other GARCH-type settings. A simulation study demonstrates the test's size and power properties in finite samples and an empirical application illustrates the testing approach.",econ.EM,Econometrics
A General Framework for Prediction in Time Series Models,"In this paper we propose a general framework to analyze prediction in time series models and show how a wide class of popular time series models satisfies this framework. We postulate a set of high-level assumptions, and formally verify these assumptions for the aforementioned time series models. Our framework coincides with that of Beutner et al. (2019, arXiv:1710.00643) who establish the validity of conditional confidence intervals for predictions made in this framework. The current paper therefore complements the results in Beutner et al. (2019, arXiv:1710.00643) by providing practically relevant applications of their theory.",econ.EM,Econometrics
Approximation Properties of Variational Bayes for Vector Autoregressions,"Variational Bayes (VB) is a recent approximate method for Bayesian inference. It has the merit of being a fast and scalable alternative to Markov Chain Monte Carlo (MCMC) but its approximation error is often unknown. In this paper, we derive the approximation error of VB in terms of mean, mode, variance, predictive density and KL divergence for the linear Gaussian multi-equation regression. Our results indicate that VB approximates the posterior mean perfectly. Factors affecting the magnitude of underestimation in posterior variance and mode are revealed. Importantly, We demonstrate that VB estimates predictive densities accurately.",econ.EM,Econometrics
On testing substitutability,"The papers~\cite{hatfimmokomi11} and~\cite{azizbrilharr13} propose algorithms for testing whether the choice function induced by a (strict) preference list of length $N$ over a universe $U$ is substitutable. The running time of these algorithms is $O(|U|^3\cdot N^3)$, respectively $O(|U|^2\cdot N^3)$. In this note we present an algorithm with running time $O(|U|^2\cdot N^2)$. Note that $N$ may be exponential in the size $|U|$ of the universe.",econ.EM,Econometrics
Capital Structure and Speed of Adjustment in U.S. Firms. A Comparative Study in Microeconomic and Macroeconomic Conditions - A Quantille Regression Approach,"The major perspective of this paper is to provide more evidence regarding how ""quickly"", in different macroeconomic states, companies adjust their capital structure to their leverage targets. This study extends the empirical research on the topic of capital structure by focusing on a quantile regression method to investigate the behavior of firm-specific characteristics and macroeconomic factors across all quantiles of distribution of leverage (book leverage and market leverage). Therefore, depending on a partial adjustment model, we find that the adjustment speed fluctuated in different stages of book versus market leverage. Furthermore, while macroeconomic states change, we detect clear differentiations of the contribution and the effects of the firm-specific and the macroeconomic variables between market leverage and book leverage debt ratios. Consequently, we deduce that across different macroeconomic states the nature and maturity of borrowing influence the persistence and endurance of the relation between determinants and borrowing.",econ.EM,Econometrics
Using generalized estimating equations to estimate nonlinear models with spatial data,"In this paper, we study estimation of nonlinear models with cross sectional data using two-step generalized estimating equations (GEE) in the quasi-maximum likelihood estimation (QMLE) framework. In the interest of improving efficiency, we propose a grouping estimator to account for the potential spatial correlation in the underlying innovations. We use a Poisson model and a Negative Binomial II model for count data and a Probit model for binary response data to demonstrate the GEE procedure. Under mild weak dependency assumptions, results on estimation consistency and asymptotic normality are provided. Monte Carlo simulations show efficiency gain of our approach in comparison of different estimation methods for count data and binary response data. Finally we apply the GEE approach to study the determinants of the inflow foreign direct investment (FDI) to China.",econ.EM,Econometrics
Seemingly Unrelated Regression with Measurement Error: Estimation via Markov chain Monte Carlo and Mean Field Variational Bayes Approximation,"Linear regression with measurement error in the covariates is a heavily studied topic, however, the statistics/econometrics literature is almost silent to estimating a multi-equation model with measurement error. This paper considers a seemingly unrelated regression model with measurement error in the covariates and introduces two novel estimation methods: a pure Bayesian algorithm (based on Markov chain Monte Carlo techniques) and its mean field variational Bayes (MFVB) approximation. The MFVB method has the added advantage of being computationally fast and can handle big data. An issue pertinent to measurement error models is parameter identification, and this is resolved by employing a prior distribution on the measurement error variance. The methods are shown to perform well in multiple simulation studies, where we analyze the impact on posterior estimates arising due to different values of reliability ratio or variance of the true unobserved quantity used in the data generating process. The paper further implements the proposed algorithms in an application drawn from the health literature and shows that modeling measurement error in the data can improve model fitting.",econ.EM,Econometrics
Valid Causal Inference with (Some) Invalid Instruments,"Instrumental variable methods provide a powerful approach to estimating causal effects in the presence of unobserved confounding. But a key challenge when applying them is the reliance on untestable ""exclusion"" assumptions that rule out any relationship between the instrument variable and the response that is not mediated by the treatment. In this paper, we show how to perform consistent IV estimation despite violations of the exclusion assumption. In particular, we show that when one has multiple candidate instruments, only a majority of these candidates---or, more generally, the modal candidate-response relationship---needs to be valid to estimate the causal effect. Our approach uses an estimate of the modal prediction from an ensemble of instrumental variable estimators. The technique is simple to apply and is ""black-box"" in the sense that it may be used with any instrumental variable estimator as long as the treatment effect is identified for each valid instrument independently. As such, it is compatible with recent machine-learning based estimators that allow for the estimation of conditional average treatment effects (CATE) on complex, high dimensional data. Experimentally, we achieve accurate estimates of conditional average treatment effects using an ensemble of deep network-based estimators, including on a challenging simulated Mendelian Randomization problem.",econ.EM,Econometrics
Matching Multidimensional Types: Theory and Application,"Becker (1973) presents a bilateral matching model in which scalar types describe agents. For this framework, he establishes the conditions under which positive sorting between agents' attributes is the unique market outcome. Becker's celebrated sorting result has been applied to address many economic questions. However, recent empirical studies in the fields of health, household, and labor economics suggest that agents have multiple outcome-relevant attributes. In this paper, I study a matching model with multidimensional types. I offer multidimensional generalizations of concordance and supermodularity to construct three multidimensional sorting patterns and two classes of multidimensional complementarities. For each of these sorting patterns, I identify the sufficient conditions which guarantee its optimality. In practice, we observe sorting patterns between observed attributes that are aggregated over unobserved characteristics. To reconcile theory with practice, I establish the link between production complementarities and the aggregated sorting patterns. Finally, I examine the relationship between agents' health status and their spouses' education levels among U.S. households within the framework for multidimensional matching markets. Preliminary analysis reveals a weak positive association between agents' health status and their spouses' education levels. This weak positive association is estimated to be a product of three factors: (a) an attraction between better-educated individuals, (b) an attraction between healthier individuals, and (c) a weak positive association between agents' health status and their education levels. The attraction channel suggests that the insurance risk associated with a two-person family plan is higher than the aggregate risk associated with two individual policies.",econ.EM,Econometrics
Empirical MSE Minimization to Estimate a Scalar Parameter,"We consider the estimation of a scalar parameter, when two estimators are available. The first is always consistent. The second is inconsistent in general, but has a smaller asymptotic variance than the first, and may be consistent if an assumption is satisfied. We propose to use the weighted sum of the two estimators with the lowest estimated mean-squared error (MSE). We show that this third estimator dominates the other two from a minimax-regret perspective: the maximum asymptotic-MSE-gain one may incur by using this estimator rather than one of the other estimators is larger than the maximum asymptotic-MSE-loss.",econ.EM,Econometrics
Principal Component Analysis: A Generalized Gini Approach,"A principal component analysis based on the generalized Gini correlation index is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based on the variance. It is shown, in the Gaussian case, that the standard PCA is equivalent to the Gini PCA. It is also proven that the dimensionality reduction based on the generalized Gini correlation matrix, that relies on city-block distances, is robust to outliers. Monte Carlo simulations and an application on cars data (with outliers) show the robustness of the Gini PCA and provide different interpretations of the results compared with the variance PCA.",econ.EM,Econometrics
Nonparametric Quantile Regressions for Panel Data Models with Large T,"This paper considers panel data models where the conditional quantiles of the dependent variables are additively separable as unknown functions of the regressors and the individual effects. We propose two estimators of the quantile partial effects while controlling for the individual heterogeneity. The first estimator is based on local linear quantile regressions, and the second is based on local linear smoothed quantile regressions, both of which are easy to compute in practice. Within the large T framework, we provide sufficient conditions under which the two estimators are shown to be asymptotically normally distributed. In particular, for the first estimator, it is shown that $N<<T^{2/(d+4)}$ is needed to ignore the incidental parameter biases, where $d$ is the dimension of the regressors. For the second estimator, we are able to derive the analytical expression of the asymptotic biases under the assumption that $N\approx Th^{d}$, where $h$ is the bandwidth parameter in local linear approximations. Our theoretical results provide the basis of using split-panel jackknife for bias corrections. A Monte Carlo simulation shows that the proposed estimators and the bias-correction method perform well in finite samples.",econ.EM,Econometrics
Mean-shift least squares model averaging,"This paper proposes a new estimator for selecting weights to average over least squares estimates obtained from a set of models. Our proposed estimator builds on the Mallows model average (MMA) estimator of Hansen (2007), but, unlike MMA, simultaneously controls for location bias and regression error through a common constant. We show that our proposed estimator-- the mean-shift Mallows model average (MSA) estimator-- is asymptotically optimal to the original MMA estimator in terms of mean squared error. A simulation study is presented, where we show that our proposed estimator uniformly outperforms the MMA estimator.",econ.EM,Econometrics
A Flexible Mixed-Frequency Vector Autoregression with a Steady-State Prior,"We propose a Bayesian vector autoregressive (VAR) model for mixed-frequency data. Our model is based on the mean-adjusted parametrization of the VAR and allows for an explicit prior on the 'steady states' (unconditional means) of the included variables. Based on recent developments in the literature, we discuss extensions of the model that improve the flexibility of the modeling approach. These extensions include a hierarchical shrinkage prior for the steady-state parameters, and the use of stochastic volatility to model heteroskedasticity. We put the proposed model to use in a forecast evaluation using US data consisting of 10 monthly and 3 quarterly variables. The results show that the predictive ability typically benefits from using mixed-frequency data, and that improvements can be obtained for both monthly and quarterly variables. We also find that the steady-state prior generally enhances the accuracy of the forecasts, and that accounting for heteroskedasticity by means of stochastic volatility usually provides additional improvements, although not for all variables.",econ.EM,Econometrics
Regression Discontinuity Design under Self-selection,"In Regression Discontinuity (RD) design, self-selection leads to different distributions of covariates on two sides of the policy intervention, which essentially violates the continuity of potential outcome assumption. The standard RD estimand becomes difficult to interpret due to the existence of some indirect effect, i.e. the effect due to self selection. We show that the direct causal effect of interest can still be recovered under a class of estimands. Specifically, we consider a class of weighted average treatment effects tailored for potentially different target populations. We show that a special case of our estimands can recover the average treatment effect under the conditional independence assumption per Angrist and Rokkanen (2015), and another example is the estimand recently proposed in Frlich and Huber (2018). We propose a set of estimators through a weighted local linear regression framework and prove the consistency and asymptotic normality of the estimators. Our approach can be further extended to the fuzzy RD case. In simulation exercises, we compare the performance of our estimator with the standard RD estimator. Finally, we apply our method to two empirical data sets: the U.S. House elections data in Lee (2008) and a novel data set from Microsoft Bing on Generalized Second Price (GSP) auction.",econ.EM,Econometrics
Competition of noise and collectivity in global cryptocurrency trading: route to a self-contained market,"Cross-correlations in fluctuations of the daily exchange rates within the basket of the 100 highest-capitalization cryptocurrencies over the period October 1, 2015, through March 31, 2019, are studied. The corresponding dynamics predominantly involve one leading eigenvalue of the correlation matrix, while the others largely coincide with those of Wishart random matrices. However, the magnitude of the principal eigenvalue, and thus the degree of collectivity, strongly depends on which cryptocurrency is used as a base. It is largest when the base is the most peripheral cryptocurrency; when more significant ones are taken into consideration, its magnitude systematically decreases, nevertheless preserving a sizable gap with respect to the random bulk, which in turn indicates that the organization of correlations becomes more heterogeneous. This finding provides a criterion for recognizing which currencies or cryptocurrencies play a dominant role in the global crypto-market. The present study shows that over the period under consideration, the Bitcoin (BTC) predominates, hallmarking exchange rate dynamics at least as influential as the US dollar. The BTC started dominating around the year 2017, while further cryptocurrencies, like the Ethereum (ETH) and even Ripple (XRP), assumed similar trends. At the same time, the USD, an original value determinant for the cryptocurrency market, became increasingly disconnected, its related characteristics eventually approaching those of a fictitious currency. These results are strong indicators of incipient independence of the global cryptocurrency market, delineating a self-contained trade resembling the Forex.",econ.EM,Econometrics
Semiparametric Estimation of Correlated Random Coefficient Models without Instrumental Variables,"We study a linear random coefficient model where slope parameters may be correlated with some continuous covariates. Such a model specification may occur in empirical research, for instance, when quantifying the effect of a continuous treatment observed at two time periods. We show one can carry identification and estimation without instruments. We propose a semiparametric estimator of average partial effects and of average treatment effects on the treated. We showcase the small sample properties of our estimator in an extensive simulation study. Among other things, we reveal that it compares favorably with a control function estimator. We conclude with an application to the effect of malaria eradication on economic development in Colombia.",econ.EM,Econometrics
Testing the Order of Multivariate Normal Mixture Models,"Finite mixtures of multivariate normal distributions have been widely used in empirical applications in diverse fields such as statistical genetics and statistical finance. Testing the number of components in multivariate normal mixture models is a long-standing challenge even in the most important case of testing homogeneity. This paper develops likelihood-based tests of the null hypothesis of $M_0$ components against the alternative hypothesis of $M_0 + 1$ components for a general $M_0 \geq 1$. For heteroscedastic normal mixtures, we propose an EM test and derive the asymptotic distribution of the EM test statistic. For homoscedastic normal mixtures, we derive the asymptotic distribution of the likelihood ratio test statistic. We also derive the asymptotic distribution of the likelihood ratio test statistic and EM test statistic under local alternatives and show the validity of parametric bootstrap. The simulations show that the proposed test has good finite sample size and power properties.",econ.EM,Econometrics
Deconvolution from two order statistics,"Economic data are often contaminated by measurement errors and truncated by ranking. This paper shows that the classical measurement error model with independent and additive measurement errors is identified nonparametrically using only two order statistics of repeated measurements. The identification result confirms a hypothesis by Athey and Haile (2002) for a symmetric ascending auction model with unobserved heterogeneity. Extensions allow for heterogeneous measurement errors, broadening the applicability to additional empirical settings, including asymmetric auctions and wage offer models. We adapt an existing simulated sieve estimator and illustrate its performance in finite samples.",econ.EM,Econometrics
"Dynamic Correlation of Market Connectivity, Risk Spillover and Abnormal Volatility in Stock Price","The connectivity of stock markets reflects the information efficiency of capital markets and contributes to interior risk contagion and spillover effects. We compare Shanghai Stock Exchange A-shares (SSE A-shares) during tranquil periods, with high leverage periods associated with the 2015 subprime mortgage crisis. We use Pearson correlations of returns, the maximum strongly connected subgraph, and $3$ principle to iteratively determine the threshold value for building a dynamic correlation network of SSE A-shares. Analyses are carried out based on the networking structure, intra-sector connectivity, and node status, identifying several contributions. First, compared with tranquil periods, the SSE A-shares network experiences a more significant small-world and connective effect during the subprime mortgage crisis and the high leverage period in 2015. Second, the finance, energy and utilities sectors have a stronger intra-industry connectivity than other sectors. Third, HUB nodes drive the growth of the SSE A-shares market during bull periods, while stocks have a think-tail degree distribution in bear periods and show distinct characteristics in terms of market value and finance. Granger linear and non-linear causality networks are also considered for the comparison purpose. Studies on the evolution of inter-cycle connectivity in the SSE A-share market may help investors improve portfolios and develop more robust risk management policies.",econ.EM,Econometrics
Exploring Distributions of House Prices and House Price Indices,"We use house prices (HP) and house price indices (HPI) as a proxy to income distribution. Specifically, we analyze sale prices in the 1970-2010 window of over 116,000 single-family homes in Hamilton County, Ohio, including Cincinnati metro area of about 2.2 million people. We also analyze HPI, published by Federal Housing Finance Agency (FHFA), for nearly 18,000 US ZIP codes that cover a period of over 40 years starting in 1980's. If HP can be viewed as a first derivative of income, HPI can be viewed as its second derivative. We use generalized beta (GB) family of functions to fit distributions of HP and HPI since GB naturally arises from the models of economic exchange described by stochastic differential equations. Our main finding is that HP and multi-year HPI exhibit a negative Dragon King (nDK) behavior, wherein power-law distribution tail gives way to an abrupt decay to a finite upper limit value, which is similar to our recent findings for realized volatility of S\&P500 index in the US stock market. This type of tail behavior is best fitted by a modified GB (mGB) distribution. Tails of single-year HPI appear to show more consistency with power-law behavior, which is better described by a GB Prime (GB2) distribution. We supplement full distribution fits by mGB and GB2 with direct linear fits (LF) of the tails. Our numerical procedure relies on evaluation of confidence intervals (CI) of the fits, as well as of p-values that give the likelihood that data come from the fitted distributions.",econ.EM,Econometrics
Binary Endogenous Treatment in Stochastic Frontier Models with an Application to Soil Conservation in El Salvador,"Improving the productivity of the agricultural sector is part of one of the Sustainable Development Goals set by the United Nations. To this end, many international organizations have funded training and technology transfer programs that aim to promote productivity and income growth, fight poverty and enhance food security among smallholder farmers in developing countries. Stochastic production frontier analysis can be a useful tool when evaluating the effectiveness of these programs. However, accounting for treatment endogeneity, often intrinsic to these interventions, only recently has received any attention in the stochastic frontier literature. In this work, we extend the classical maximum likelihood estimation of stochastic production frontier models by allowing both the production frontier and inefficiency to depend on a potentially endogenous binary treatment. We use instrumental variables to define an assignment mechanism for the treatment, and we explicitly model the density of the first and second-stage composite error terms. We provide empirical evidence of the importance of controlling for endogeneity in this setting using farm-level data from a soil conservation program in El Salvador.",econ.EM,Econometrics
Graph Neural Networks: Theory for Estimation with Application on Network Heterogeneity,"This paper presents a novel application of graph neural networks for modeling and estimating network heterogeneity. Network heterogeneity is characterized by variations in unit's decisions or outcomes that depend not only on its own attributes but also on the conditions of its surrounding neighborhood. We delineate the convergence rate of the graph neural networks estimator, as well as its applicability in semiparametric causal inference with heterogeneous treatment effects. The finite-sample performance of our estimator is evaluated through Monte Carlo simulations. In an empirical setting related to microfinance program participation, we apply the new estimator to examine the average treatment effects and outcomes of counterfactual policies, and to propose an enhanced strategy for selecting the initial recipients of program information in social networks.",econ.EM,Econometrics
Covariance Function Estimation for High-Dimensional Functional Time Series with Dual Factor Structures,"We propose a flexible dual functional factor model for modelling high-dimensional functional time series. In this model, a high-dimensional fully functional factor parametrisation is imposed on the observed functional processes, whereas a low-dimensional version (via series approximation) is assumed for the latent functional factors. We extend the classic principal component analysis technique for the estimation of a low-rank structure to the estimation of a large covariance matrix of random functions that satisfies a notion of (approximate) functional ""low-rank plus sparse"" structure; and generalise the matrix shrinkage method to functional shrinkage in order to estimate the sparse structure of functional idiosyncratic components. Under appropriate regularity conditions, we derive the large sample theory of the developed estimators, including the consistency of the estimated factors and functional factor loadings and the convergence rates of the estimated matrices of covariance functions measured by various (functional) matrix norms. Consistent selection of the number of factors and a data-driven rule to choose the shrinkage parameter are discussed. Simulation and empirical studies are provided to demonstrate the finite-sample performance of the developed model and estimation methodology.",econ.EM,Econometrics
Deep Learning With DAGs,"Social science theories often postulate causal relationships among a set of variables or events. Although directed acyclic graphs (DAGs) are increasingly used to represent these theories, their full potential has not yet been realized in practice. As non-parametric causal models, DAGs require no assumptions about the functional form of the hypothesized relationships. Nevertheless, to simplify the task of empirical evaluation, researchers tend to invoke such assumptions anyway, even though they are typically arbitrary and do not reflect any theoretical content or prior knowledge. Moreover, functional form assumptions can engender bias, whenever they fail to accurately capture the complexity of the causal system under investigation. In this article, we introduce causal-graphical normalizing flows (cGNFs), a novel approach to causal inference that leverages deep neural networks to empirically evaluate theories represented as DAGs. Unlike conventional approaches, cGNFs model the full joint distribution of the data according to a DAG supplied by the analyst, without relying on stringent assumptions about functional form. In this way, the method allows for flexible, semi-parametric estimation of any causal estimand that can be identified from the DAG, including total effects, conditional effects, direct and indirect effects, and path-specific effects. We illustrate the method with a reanalysis of Blau and Duncan's (1967) model of status attainment and Zhou's (2019) model of conditional versus controlled mobility. To facilitate adoption, we provide open-source software together with a series of online tutorials for implementing cGNFs. The article concludes with a discussion of current limitations and directions for future development.",econ.EM,Econometrics
Inference under partial identification with minimax test statistics,"We provide a means of computing and estimating the asymptotic distributions of statistics based on an outer minimization of an inner maximization. Such test statistics, which arise frequently in moment models, are of special interest in providing hypothesis tests under partial identification. Under general conditions, we provide an asymptotic characterization of such test statistics using the minimax theorem, and a means of computing critical values using the bootstrap. Making some light regularity assumptions, our results augment several asymptotic approximations that have been provided for partially identified hypothesis tests, and extend them by mitigating their dependence on local linear approximations of the parameter space. These asymptotic results are generally simple to state and straightforward to compute (esp.\ adversarially).",econ.EM,Econometrics
Counterfactuals in factor models,"We study a new model where the potential outcomes, corresponding to the values of a (possibly continuous) treatment, are linked through common factors. The factors can be estimated using a panel of regressors. We propose a procedure to estimate time-specific and unit-specific average marginal effects in this context. Our approach can be used either with high-dimensional time series or with large panels. It allows for treatment effects heterogenous across time and units and is straightforward to implement since it only relies on principal components analysis and elementary computations. We derive the asymptotic distribution of our estimator of the average marginal effect and highlight its solid finite sample performance through a simulation exercise. The approach can also be used to estimate average counterfactuals or adapted to an instrumental variables setting and we discuss these extensions. Finally, we illustrate our novel methodology through an empirical application on income inequality.",econ.EM,Econometrics
Estimating granular house price distributions in the Australian market using Gaussian mixtures,"A new methodology is proposed to approximate the time-dependent house price distribution at a fine regional scale using Gaussian mixtures. The means, variances and weights of the mixture components are related to time, location and dwelling type through a non linear function trained by a deep functional approximator. Price indices are derived as means, medians, quantiles or other functions of the estimated distributions. Price densities for larger regions, such as a city, are calculated via a weighted sum of the component density functions. The method is applied to a data set covering all of Australia at a fine spatial and temporal resolution. In addition to enabling a detailed exploration of the data, the proposed index yields lower prediction errors in the practical task of individual dwelling price projection from previous sales values within the three major Australian cities. The estimated quantiles are also found to be well calibrated empirically, capturing the complexity of house price distributions.",econ.EM,Econometrics
Modeling economies of scope in joint production: Convex regression of input distance function,"Modeling of joint production has proved a vexing problem. This paper develops a radial convex nonparametric least squares (CNLS) approach to estimate the input distance function with multiple outputs. We document the correct input distance function transformation and prove that the necessary orthogonality conditions can be satisfied in radial CNLS. A Monte Carlo study is performed to compare the finite sample performance of radial CNLS and other deterministic and stochastic frontier approaches in terms of the input distance function estimation. We apply our novel approach to the Finnish electricity distribution network regulation and empirically confirm that the input isoquants become more curved. In addition, we introduce the weight restriction to radial CNLS to mitigate the potential overfitting and increase the out-of-sample performance in energy regulation.",econ.EM,Econometrics
Predictive Density Combination Using a Tree-Based Synthesis Function,"Bayesian predictive synthesis (BPS) provides a method for combining multiple predictive distributions based on agent/expert opinion analysis theory and encompasses a range of existing density forecast pooling methods. The key ingredient in BPS is a ``synthesis'' function. This is typically specified parametrically as a dynamic linear regression. In this paper, we develop a nonparametric treatment of the synthesis function using regression trees. We show the advantages of our tree-based approach in two macroeconomic forecasting applications. The first uses density forecasts for GDP growth from the euro area's Survey of Professional Forecasters. The second combines density forecasts of US inflation produced by many regression models involving different predictors. Both applications demonstrate the benefits -- in terms of improved forecast accuracy and interpretability -- of modeling the synthesis function nonparametrically.",econ.EM,Econometrics
On the Limits of Regression Adjustment,"Regression adjustment, sometimes known as Controlled-experiment Using Pre-Experiment Data (CUPED), is an important technique in internet experimentation. It decreases the variance of effect size estimates, often cutting confidence interval widths in half or more while never making them worse. It does so by carefully regressing the goal metric against pre-experiment features to reduce the variance. The tremendous gains of regression adjustment begs the question: How much better can we do by engineering better features from pre-experiment data, for example by using machine learning techniques or synthetic controls? Could we even reduce the variance in our effect sizes arbitrarily close to zero with the right predictors? Unfortunately, our answer is negative. A simple form of regression adjustment, which uses just the pre-experiment values of the goal metric, captures most of the benefit. Specifically, under a mild assumption that observations closer in time are easier to predict that ones further away in time, we upper bound the potential gains of more sophisticated feature engineering, with respect to the gains of this simple form of regression adjustment. The maximum reduction in variance is $50\%$ in Theorem 1, or equivalently, the confidence interval width can be reduced by at most an additional $29\%$.",econ.EM,Econometrics
Development of Choice Model for Brand Evaluation,"Consumer choice modeling takes center stage as we delve into understanding how personal preferences of decision makers (customers) for products influence demand at the level of the individual. The contemporary choice theory is built upon the characteristics of the decision maker, alternatives available for the choice of the decision maker, the attributes of the available alternatives and decision rules that the decision maker uses to make a choice. The choice set in our research is represented by six major brands (products) of laundry detergents in the Japanese market. We use the panel data of the purchases of 98 households to which we apply the hierarchical probit model, facilitated by a Markov Chain Monte Carlo simulation (MCMC) in order to evaluate the brand values of six brands. The applied model also allows us to evaluate the tangible and intangible brand values. These evaluated metrics help us to assess the brands based on their tangible and intangible characteristics. Moreover, consumer choice modeling also provides a framework for assessing the environmental performance of laundry detergent brands as the model uses the information on components (physical attributes) of laundry detergents.",econ.EM,Econometrics
Direct Multi-Step Forecast based Comparison of Nested Models via an Encompassing Test,"We introduce a novel approach for comparing out-of-sample multi-step forecasts obtained from a pair of nested models that is based on the forecast encompassing principle. Our proposed approach relies on an alternative way of testing the population moment restriction implied by the forecast encompassing principle and that links the forecast errors from the two competing models in a particular way. Its key advantage is that it is able to bypass the variance degeneracy problem afflicting model based forecast comparisons across nested models. It results in a test statistic whose limiting distribution is standard normal and which is particularly simple to construct and can accommodate both single period and longer-horizon prediction comparisons. Inferences are also shown to be robust to different predictor types, including stationary, highly-persistent and purely deterministic processes. Finally, we illustrate the use of our proposed approach through an empirical application that explores the role of global inflation in enhancing individual country specific inflation forecasts.",econ.EM,Econometrics
Best-of-Both-Worlds Linear Contextual Bandits,"This study investigates the problem of $K$-armed linear contextual bandits, an instance of the multi-armed bandit problem, under an adversarial corruption. At each round, a decision-maker observes an independent and identically distributed context and then selects an arm based on the context and past observations. After selecting an arm, the decision-maker incurs a loss corresponding to the selected arm. The decision-maker aims to minimize the cumulative loss over the trial. The goal of this study is to develop a strategy that is effective in both stochastic and adversarial environments, with theoretical guarantees. We first formulate the problem by introducing a novel setting of bandits with adversarial corruption, referred to as the contextual adversarial regime with a self-bounding constraint. We assume linear models for the relationship between the loss and the context. Then, we propose a strategy that extends the RealLinExp3 by Neu & Olkhovskaya (2020) and the Follow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is shown to be upper-bounded by $O\left(\min\left\{\frac{(\log(T))^3}{_{*}} + \sqrt{\frac{C(\log(T))^3}{_{*}}},\ \ \sqrt{T}(\log(T))^2\right\}\right)$, where $T \in\mathbb{N}$ is the number of rounds, $_{*} > 0$ is the constant minimum gap between the best and suboptimal arms for any context, and $C\in[0, T] $ is an adversarial corruption parameter. This regret upper bound implies $O\left(\frac{(\log(T))^3}{_{*}}\right)$ in a stochastic environment and by $O\left( \sqrt{T}(\log(T))^2\right)$ in an adversarial environment. We refer to our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its theoretical guarantees in both stochastic and adversarial regimes.",econ.EM,Econometrics
Classification and Treatment Learning with Constraints via Composite Heaviside Optimization: a Progressive MIP Method,"This paper proposes a Heaviside composite optimization approach and presents a progressive (mixed) integer programming (PIP) method for solving multi-class classification and multi-action treatment problems with constraints. A Heaviside composite function is a composite of a Heaviside function (i.e., the indicator function of either the open $( \, 0,\infty )$ or closed $[ \, 0,\infty \, )$ interval) with a possibly nondifferentiable function. Modeling-wise, we show how Heaviside composite optimization provides a unified formulation for learning the optimal multi-class classification and multi-action treatment rules, subject to rule-dependent constraints stipulating a variety of domain restrictions. A Heaviside composite function has an equivalent discrete formulation, and the resulting optimization problem can in principle be solved by integer programming (IP) methods. Nevertheless, for constrained learning problems with large data sets, a straightforward application of off-the-shelf IP solvers is usually ineffective in achieving global optimality. To alleviate such a computational burden, our major contribution is the proposal of the PIP method by leveraging the effectiveness of state-of-the-art IP solvers for problems of modest sizes. We provide the theoretical advantage of the PIP method with the connection to continuous optimization and show that the computed solution is locally optimal for a broad class of Heaviside composite optimization problems. The numerical performance of the PIP method is demonstrated by extensive computational experimentation.",econ.EM,Econometrics
Learning Causal Representations from General Environments: Identifiability and Intrinsic Ambiguity,"We study causal representation learning, the task of recovering high-level latent variables and their causal relationships in the form of a causal graph from low-level observed data (such as text and images), assuming access to observations generated from multiple environments. Prior results on the identifiability of causal representations typically assume access to single-node interventions which is rather unrealistic in practice, since the latent variables are unknown in the first place. In this work, we provide the first identifiability results based on data that stem from general environments. We show that for linear causal models, while the causal graph can be fully recovered, the latent variables are only identified up to the surrounded-node ambiguity (SNA) \citep{varici2023score}. We provide a counterpart of our guarantee, showing that SNA is basically unavoidable in our setting. We also propose an algorithm, \texttt{LiNGCReL} which provably recovers the ground-truth model up to SNA, and we demonstrate its effectiveness via numerical experiments. Finally, we consider general non-parametric causal models and show that the same identification barrier holds when assuming access to groups of soft single-node interventions.",econ.EM,Econometrics
A Big Data Approach to Understand Sub-national Determinants of FDI in Africa,"Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based on text mining and social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in determining foreign ownership.",econ.EM,Econometrics
Dynamic Spatiotemporal ARCH Models: Small and Large Sample Results,"This paper explores the estimation of a dynamic spatiotemporal autoregressive conditional heteroscedasticity (ARCH) model. The log-volatility term in this model can depend on (i) the spatial lag of the log-squared outcome variable, (ii) the time-lag of the log-squared outcome variable, (iii) the spatiotemporal lag of the log-squared outcome variable, (iv) exogenous variables, and (v) the unobserved heterogeneity across regions and time, i.e., the regional and time fixed effects. We examine the small and large sample properties of two quasi-maximum likelihood estimators and a generalized method of moments estimator for this model. We first summarize the theoretical properties of these estimators and then compare their finite sample properties through Monte Carlo simulations.",econ.EM,Econometrics
On Efficient Inference of Causal Effects with Multiple Mediators,"This paper provides robust estimators and efficient inference of causal effects involving multiple interacting mediators. Most existing works either impose a linear model assumption among the mediators or are restricted to handle conditionally independent mediators given the exposure. To overcome these limitations, we define causal and individual mediation effects in a general setting, and employ a semiparametric framework to develop quadruply robust estimators for these causal effects. We further establish the asymptotic normality of the proposed estimators and prove their local semiparametric efficiencies. The proposed method is empirically validated via simulated and real datasets concerning psychiatric disorders in trauma survivors.",econ.EM,Econometrics
Finding Moving-Band Statistical Arbitrages via Convex-Concave Optimization,"We propose a new method for finding statistical arbitrages that can contain more assets than just the traditional pair. We formulate the problem as seeking a portfolio with the highest volatility, subject to its price remaining in a band and a leverage limit. This optimization problem is not convex, but can be approximately solved using the convex-concave procedure, a specific sequential convex programming method. We show how the method generalizes to finding moving-band statistical arbitrages, where the price band midpoint varies over time.",econ.EM,Econometrics
Research on the multi-stage impact of digital economy on rural revitalization in Hainan Province based on GPM model,"The rapid development of the digital economy has had a profound impact on the implementation of the rural revitalization strategy. Based on this, this study takes Hainan Province as the research object to deeply explore the impact of digital economic development on rural revitalization. The study collected panel data from 2003 to 2022 to construct an evaluation index system for the digital economy and rural revitalization and used panel regression analysis and other methods to explore the promotion effect of the digital economy on rural revitalization. Research results show that the digital economy has a significant positive impact on rural revitalization, and this impact increases as the level of fiscal expenditure increases. The issuance of digital RMB has further exerted a regulatory effect and promoted the development of the digital economy and the process of rural revitalization. At the same time, the establishment of the Hainan Free Trade Port has also played a positive role in promoting the development of the digital economy and rural revitalization. In the prediction of the optimal strategy for rural revitalization based on the development levels of the primary, secondary, and tertiary industries (Rate1, Rate2, and Rate3), it was found that rate1 can encourage Hainan Province to implement digital economic innovation, encourage rate3 to implement promotion behaviors, and increase rate2 can At the level of sustainable development when rate3 promotes rate2's digital economic innovation behavior, it can standardize rate2's production behavior to the greatest extent, accelerate the faster application of the digital economy to the rural revitalization industry, and promote the technological advancement of enterprises.",econ.EM,Econometrics
The prices of renewable commodities: A robust stationarity analysis,"This paper addresses the problem of testing for persistence in the effects of the shocks affecting the prices of renewable commodities, which have potential implications on stabilization policies and economic forecasting, among other areas. A robust methodology is employed that enables the determination of the potential presence and number of instant/gradual structural changes in the series, stationarity testing conditional on the number of changes detected, and the detection of change points. This procedure is applied to the annual real prices of eighteen renewable commodities over the period of 1900-2018. Results indicate that most of the series display non-linear features, including quadratic patterns and regime transitions that often coincide with well-known political and economic episodes. The conclusions of stationarity testing suggest that roughly half of the series are integrated. Stationarity fails to be rejected for grains, whereas most livestock and textile commodities do reject stationarity. Evidence is mixed in all soft commodities and tropical crops, where stationarity can be rejected in approximately half of the cases. The implication would be that for these commodities, stabilization schemes would not be recommended.",econ.EM,Econometrics
Monthly GDP nowcasting with Machine Learning and Unstructured Data,"In the dynamic landscape of continuous change, Machine Learning (ML) ""nowcasting"" models offer a distinct advantage for informed decision-making in both public and private sectors. This study introduces ML-based GDP growth projection models for monthly rates in Peru, integrating structured macroeconomic indicators with high-frequency unstructured sentiment variables. Analyzing data from January 2007 to May 2023, encompassing 91 leading economic indicators, the study evaluates six ML algorithms to identify optimal predictors. Findings highlight the superior predictive capability of ML models using unstructured data, particularly Gradient Boosting Machine, LASSO, and Elastic Net, exhibiting a 20% to 25% reduction in prediction errors compared to traditional AR and Dynamic Factor Models (DFM). This enhanced performance is attributed to better handling of data of ML models in high-uncertainty periods, such as economic crises.",econ.EM,Econometrics
Selective linear segmentation for detecting relevant parameter changes,"Change-point processes are one flexible approach to model long time series. We propose a method to uncover which model parameter truly vary when a change-point is detected. Given a set of breakpoints, we use a penalized likelihood approach to select the best set of parameters that changes over time and we prove that the penalty function leads to a consistent selection of the true model. Estimation is carried out via the deterministic annealing expectation-maximization algorithm. Our method accounts for model selection uncertainty and associates a probability to all the possible time-varying parameter specifications. Monte Carlo simulations highlight that the method works well for many time series models including heteroskedastic processes. For a sample of 14 Hedge funds (HF) strategies, using an asset based style pricing model, we shed light on the promising ability of our method to detect the time-varying dynamics of risk exposures as well as to forecast HF returns.",econ.EM,Econometrics
Applied Causal Inference Powered by ML and AI,"An introduction to the emerging fusion of machine learning and causal inference. The book presents ideas from classical structural equation models (SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and structural causal models (SCMs), and covers Double/Debiased Machine Learning methods to do inference in such models using modern predictive tools.",econ.EM,Econometrics
One Factor to Bind the Cross-Section of Returns,"We propose a new non-linear single-factor asset pricing model $r_{it}=h(f_{t}_{i})+_{it}$. Despite its parsimony, this model represents exactly any non-linear model with an arbitrary number of factors and loadings -- a consequence of the Kolmogorov-Arnold representation theorem. It features only one pricing component $h(f_{t}_{I})$, comprising a nonparametric link function of the time-dependent factor and factor loading that we jointly estimate with sieve-based estimators. Using 171 assets across major classes, our model delivers superior cross-sectional performance with a low-dimensional approximation of the link function. Most known finance and macro factors become insignificant controlling for our single-factor.",econ.EM,Econometrics
(Empirical) Bayes Approaches to Parallel Trends,"We consider Bayes and Empirical Bayes (EB) approaches for dealing with violations of parallel trends. In the Bayes approach, the researcher specifies a prior over both the pre-treatment violations of parallel trends $_{pre}$ and the post-treatment violations $_{post}$. The researcher then updates their posterior about the post-treatment bias $_{post}$ given an estimate of the pre-trends $_{pre}$. This allows them to form posterior means and credible sets for the treatment effect of interest, $_{post}$. In the EB approach, the prior on the violations of parallel trends is learned from the pre-treatment observations. We illustrate these approaches in two empirical applications.",econ.EM,Econometrics
Robust Impulse Responses using External Instruments: the Role of Information,"External-instrument identification leads to biased responses when the shock is not invertible and the measurement error is present. We propose to use this identification strategy in a structural Dynamic Factor Model, which we call Proxy DFM. In a simulation analysis, we show that the Proxy DFM always successfully retrieves the true impulse responses, while the Proxy SVAR systematically fails to do so when the model is either misspecified, does not include all relevant information, or the measurement error is present. In an application to US monetary policy, the Proxy DFM shows that a tightening shock is unequivocally contractionary, with deteriorations in domestic demand, labor, credit, housing, exchange, and financial markets. This holds true for all raw instruments available in the literature. The variance decomposition analysis highlights the importance of monetary policy shocks in explaining economic fluctuations, albeit at different horizons.",econ.EM,Econometrics
IV Regressions without Exclusion Restrictions,"We study identification and estimation of endogenous linear and nonlinear regression models without excluded instrumental variables, based on the standard mean independence condition and a nonlinear relevance condition. Based on the identification results, we propose two semiparametric estimators as well as a discretization-based estimator that does not require any nonparametric regressions. We establish their asymptotic normality and demonstrate via simulations their robust finite-sample performances with respect to exclusion restrictions violations and endogeneity. Our approach is applied to study the returns to education, and to test the direct effects of college proximity indicators as well as family background variables on the outcome.",econ.EM,Econometrics
Scenario Sampling for Large Supermodular Games,"This paper introduces a simulation algorithm for evaluating the log-likelihood function of a large supermodular binary-action game. Covered examples include (certain types of) peer effect, technology adoption, strategic network formation, and multi-market entry games. More generally, the algorithm facilitates simulated maximum likelihood (SML) estimation of games with large numbers of players, $T$, and/or many binary actions per player, $M$ (e.g., games with tens of thousands of strategic actions, $TM=O(10^4)$). In such cases the likelihood of the observed pure strategy combination is typically (i) very small and (ii) a $TM$-fold integral who region of integration has a complicated geometry. Direct numerical integration, as well as accept-reject Monte Carlo integration, are computationally impractical in such settings. In contrast, we introduce a novel importance sampling algorithm which allows for accurate likelihood simulation with modest numbers of simulation draws.",econ.EM,Econometrics
Using Probabilistic Stated Preference Analyses to Understand Actual Choices,"Can stated preferences help in counterfactual analyses of actual choice? This research proposes a novel approach to researchers who have access to both stated choices in hypothetical scenarios and actual choices. The key idea is to use probabilistic stated choices to identify the distribution of individual unobserved heterogeneity, even in the presence of measurement error. If this unobserved heterogeneity is the source of endogeneity, the researcher can correct for its influence in a demand function estimation using actual choices, and recover causal effects. Estimation is possible with an off-the-shelf Group Fixed Effects estimator.",econ.EM,Econometrics
Tackling Interference Induced by Data Training Loops in A/B Tests: A Weighted Training Approach,"In modern recommendation systems, the standard pipeline involves training machine learning models on historical data to predict user behaviors and improve recommendations continuously. However, these data training loops can introduce interference in A/B tests, where data generated by control and treatment algorithms, potentially with different distributions, are combined. To address these challenges, we introduce a novel approach called weighted training. This approach entails training a model to predict the probability of each data point appearing in either the treatment or control data and subsequently applying weighted losses during model training. We demonstrate that this approach achieves the least variance among all estimators that do not cause shifts in the training distributions. Through simulation studies, we demonstrate the lower bias and variance of our approach compared to other methods.",econ.EM,Econometrics
Optimal Estimation Methodologies for Panel Data Regression Models,"This survey study discusses main aspects to optimal estimation methodologies for panel data regression models. In particular, we present current methodological developments for modeling stationary panel data as well as robust methods for estimation and inference in nonstationary panel data regression models. Some applications from the network econometrics and high dimensional statistics literature are also discussed within a stationary time series environment.",econ.EM,Econometrics
A Bayesian Markov-switching SAR model for time-varying cross-price spillovers,"The spatial autoregressive (SAR) model is extended by introducing a Markov switching dynamics for the weight matrix and spatial autoregressive parameter. The framework enables the identification of regime-specific connectivity patterns and strengths and the study of the spatiotemporal propagation of shocks in a system with a time-varying spatial multiplier matrix. The proposed model is applied to disaggregated CPI data from 15 EU countries to examine cross-price dependencies. The analysis identifies distinct connectivity structures and spatial weights across the states, which capture shifts in consumer behaviour, with marked cross-country differences in the spillover from one price category to another.",econ.EM,Econometrics
Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a Fixed Budget,"This study investigates the experimental design problem for identifying the arm with the highest expected outcome, referred to as best arm identification (BAI). In our experiments, the number of treatment-allocation rounds is fixed. During each round, a decision-maker allocates an arm and observes a corresponding outcome, which follows a Gaussian distribution with variances that can differ among the arms. At the end of the experiment, the decision-maker recommends one of the arms as an estimate of the best arm. To design an experiment, we first discuss lower bounds for the probability of misidentification. Our analysis highlights that the available information on the outcome distribution, such as means (expected outcomes), variances, and the choice of the best arm, significantly influences the lower bounds. Because available information is limited in actual experiments, we develop a lower bound that is valid under the unknown means and the unknown choice of the best arm, which are referred to as the worst-case lower bound. We demonstrate that the worst-case lower bound depends solely on the variances of the outcomes. Then, under the assumption that the variances are known, we propose the Generalized-Neyman-Allocation (GNA)-empirical-best-arm (EBA) strategy, an extension of the Neyman allocation proposed by Neyman (1934). We show that the GNA-EBA strategy is asymptotically optimal in the sense that its probability of misidentification aligns with the lower bounds as the sample size increases infinitely and the differences between the expected outcomes of the best and other suboptimal arms converge to the same values across arms. We refer to such strategies as asymptotically worst-case optimal.",econ.EM,Econometrics
CausalGPS: An R Package for Causal Inference With Continuous Exposures,"Quantifying the causal effects of continuous exposures on outcomes of interest is critical for social, economic, health, and medical research. However, most existing software packages focus on binary exposures. We develop the CausalGPS R package that implements a collection of algorithms to provide algorithmic solutions for causal inference with continuous exposures. CausalGPS implements a causal inference workflow, with algorithms based on generalized propensity scores (GPS) as the core, extending propensity scores (the probability of a unit being exposed given pre-exposure covariates) from binary to continuous exposures. As the first step, the package implements efficient and flexible estimations of the GPS, allowing multiple user-specified modeling options. As the second step, the package provides two ways to adjust for confounding: weighting and matching, generating weighted and matched data sets, respectively. Lastly, the package provides built-in functions to fit flexible parametric, semi-parametric, or non-parametric regression models on the weighted or matched data to estimate the exposure-response function relating the outcome with the exposures. The computationally intensive tasks are implemented in C++, and efficient shared-memory parallelization is achieved by OpenMP API. This paper outlines the main components of the CausalGPS R package and demonstrates its application to assess the effect of long-term exposure to PM2.5 on educational attainment using zip code-level data from the contiguous United States from 2000-2016.",econ.EM,Econometrics
Nonparametric Regression with Dyadic Data,This paper studies the identification and estimation of a nonparametric nonseparable dyadic model where the structural function and the distribution of the unobservable random terms are assumed to be unknown. The identification and the estimation of the distribution of the unobservable random term are also proposed. I assume that the structural function is continuous and strictly increasing in the unobservable heterogeneity. I propose suitable normalization for the identification by allowing the structural function to have some desirable properties such as homogeneity of degree one in the unobservable random term and some of its observables. The consistency and the asymptotic distribution of the estimators are proposed. The finite sample properties of the proposed estimators in a Monte-Carlo simulation are assessed.,econ.EM,Econometrics
A remark on moment-dependent phase transitions in high-dimensional Gaussian approximations,"In this article, we study the critical growth rates of dimension below which Gaussian critical values can be used for hypothesis testing but beyond which they cannot. We are particularly interested in how these growth rates depend on the number of moments that the observations possess.",econ.EM,Econometrics
BVARs and Stochastic Volatility,"Bayesian vector autoregressions (BVARs) are the workhorse in macroeconomic forecasting. Research in the last decade has established the importance of allowing time-varying volatility to capture both secular and cyclical variations in macroeconomic uncertainty. This recognition, together with the growing availability of large datasets, has propelled a surge in recent research in building stochastic volatility models suitable for large BVARs. Some of these new models are also equipped with additional features that are especially desirable for large systems, such as order invariance -- i.e., estimates are not dependent on how the variables are ordered in the BVAR -- and robustness against COVID-19 outliers. Estimation of these large, flexible models is made possible by the recently developed equation-by-equation approach that drastically reduces the computational cost of estimating large systems. Despite these recent advances, there remains much ongoing work, such as the development of parsimonious approaches for time-varying coefficients and other types of nonlinearities in large BVARs.",econ.EM,Econometrics
"Causal Inference with Corrupted Data: Measurement Error, Missing Values, Discretization, and Differential Privacy","The US Census Bureau will deliberately corrupt data sets derived from the 2020 US Census, enhancing the privacy of respondents while potentially reducing the precision of economic analysis. To investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. We propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency and Gaussian approximation by finite sample arguments, with a rate of $n^{ 1/2}$ for semiparametric estimands that degrades gracefully for nonparametric estimands. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and empirically validate. Our analysis provides nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. Calibrated simulations verify the coverage of our data cleaning adjusted confidence intervals and demonstrate the relevance of our results for Census-derived data.",econ.EM,Econometrics
Some Finite Sample Properties of the Sign Test,"This paper contains two finite-sample results concerning the sign test. First, we show that the sign-test is unbiased with independent, non-identically distributed data for both one-sided and two-sided hypotheses. The proof for the two-sided case is based on a novel argument that relates the derivatives of the power function to a regular bipartite graph. Unbiasedness then follows from the existence of perfect matchings on such graphs. Second, we provide a simple theoretical counterexample to show that the sign test over-rejects when the data exhibits correlation. Our results can be useful for understanding the properties of approximate randomization tests in settings with few clusters.",econ.EM,Econometrics
Consistent Specification Test of the Quantile Autoregression,"This paper proposes a test for the joint hypothesis of correct dynamic specification and no omitted latent factors for the Quantile Autoregression. If the composite null is rejected we proceed to disentangle the cause of rejection, i.e., dynamic misspecification or an omitted variable. We establish the asymptotic distribution of the test statistics under fairly weak conditions and show that factor estimation error is negligible. A Monte Carlo study shows that the suggested tests have good finite sample properties. Finally, we undertake an empirical illustration of modelling GDP growth and CPI inflation in the United Kingdom, where we find evidence that factor augmented models are correctly specified in contrast with their non-augmented counterparts when it comes to GDP growth, while also exploring the asymmetric behaviour of the growth and inflation distributions.",econ.EM,Econometrics
Sustainable Investing and the Cross-Section of Returns and Maximum Drawdown,"We use supervised learning to identify factors that predict the cross-section of returns and maximum drawdown for stocks in the US equity market. Our data run from January 1970 to December 2019 and our analysis includes ordinary least squares, penalized linear regressions, tree-based models, and neural networks. We find that the most important predictors tended to be consistent across models, and that non-linear models had better predictive power than linear models. Predictive power was higher in calm periods than in stressed periods. Environmental, social, and governance indicators marginally impacted the predictive power of non-linear models in our data, despite their negative correlation with maximum drawdown and positive correlation with returns. Upon exploring whether ESG variables are captured by some models, we find that ESG data contribute to the prediction nonetheless.",econ.EM,Econometrics
Bias-Aware Inference in Fuzzy Regression Discontinuity Designs,"We propose new confidence sets (CSs) for the regression discontinuity parameter in fuzzy designs. Our CSs are based on local linear regression, and are bias-aware, in the sense that they take possible bias explicitly into account. Their construction shares similarities with that of Anderson-Rubin CSs in exactly identified instrumental variable models, and thereby avoids issues with ""delta method"" approximations that underlie most commonly used existing inference methods for fuzzy regression discontinuity analysis. Our CSs are asymptotically equivalent to existing procedures in canonical settings with strong identification and a continuous running variable. However, due to their particular construction they are also valid under a wide range of empirically relevant conditions in which existing methods can fail, such as setups with discrete running variables, donut designs, and weak identification.",econ.EM,Econometrics
Markov Switching,"Markov switching models are a popular family of models that introduces time-variation in the parameters in the form of their state- or regime-specific values. Importantly, this time-variation is governed by a discrete-valued latent stochastic process with limited memory. More specifically, the current value of the state indicator is determined only by the value of the state indicator from the previous period, thus the Markov property, and the transition matrix. The latter characterizes the properties of the Markov process by determining with what probability each of the states can be visited next period, given the state in the current period. This setup decides on the two main advantages of the Markov switching models. Namely, the estimation of the probability of state occurrences in each of the sample periods by using filtering and smoothing methods and the estimation of the state-specific parameters. These two features open the possibility for improved interpretations of the parameters associated with specific regimes combined with the corresponding regime probabilities, as well as for improved forecasting performance based on persistent regimes and parameters characterizing them.",econ.EM,Econometrics
Adaptive Estimation and Uniform Confidence Bands for Nonparametric Structural Functions and Elasticities,"We introduce two data-driven procedures for optimal estimation and inference in nonparametric models using instrumental variables. The first is a data-driven choice of sieve dimension for a popular class of sieve two-stage least squares estimators. When implemented with this choice, estimators of both the structural function $h_0$ and its derivatives (such as elasticities) converge at the fastest possible (i.e., minimax) rates in sup-norm. The second is for constructing uniform confidence bands (UCBs) for $h_0$ and its derivatives. Our UCBs guarantee coverage over a generic class of data-generating processes and contract at the minimax rate, possibly up to a logarithmic factor. As such, our UCBs are asymptotically more efficient than UCBs based on the usual approach of undersmoothing. As an application, we estimate the elasticity of the intensive margin of firm exports in a monopolistic competition model of international trade. Simulations illustrate the good performance of our procedures in empirically calibrated designs. Our results provide evidence against common parameterizations of the distribution of unobserved firm heterogeneity.",econ.EM,Econometrics
An Adversarial Approach to Structural Estimation,"We propose a new simulation-based estimation method, adversarial estimation, for structural models. The estimator is formulated as the solution to a minimax problem between a generator (which generates simulated observations using the structural model) and a discriminator (which classifies whether an observation is simulated). The discriminator maximizes the accuracy of its classification while the generator minimizes it. We show that, with a sufficiently rich discriminator, the adversarial estimator attains parametric efficiency under correct specification and the parametric rate under misspecification. We advocate the use of a neural network as a discriminator that can exploit adaptivity properties and attain fast rates of convergence. We apply our method to the elderly's saving decision model and show that our estimator uncovers the bequest motive as an important source of saving across the wealth distribution, not only for the rich.",econ.EM,Econometrics
"Algorithm as Experiment: Machine Learning, Market Design, and Policy Eligibility Rules","Algorithms make a growing portion of policy and business decisions. We develop a treatment-effect estimator using algorithmic decisions as instruments for a class of stochastic and deterministic algorithms. Our estimator is consistent and asymptotically normal for well-defined causal effects. A special case of our setup is multidimensional regression discontinuity designs with complex boundaries. We apply our estimator to evaluate the Coronavirus Aid, Relief, and Economic Security Act, which allocated many billions of dollars worth of relief funding to hospitals via an algorithmic rule. The funding is shown to have little effect on COVID-19-related hospital activities. Naive estimates exhibit selection bias.",econ.EM,Econometrics
Decoupling Shrinkage and Selection for the Bayesian Quantile Regression,"This paper extends the idea of decoupling shrinkage and sparsity for continuous priors to Bayesian Quantile Regression (BQR). The procedure follows two steps: In the first step, we shrink the quantile regression posterior through state of the art continuous priors and in the second step, we sparsify the posterior through an efficient variant of the adaptive lasso, the signal adaptive variable selection (SAVS) algorithm. We propose a new variant of the SAVS which automates the choice of penalisation through quantile specific loss-functions that are valid in high dimensions. We show in large scale simulations that our selection procedure decreases bias irrespective of the true underlying degree of sparsity in the data, compared to the un-sparsified regression posterior. We apply our two-step approach to a high dimensional growth-at-risk (GaR) exercise. The prediction accuracy of the un-sparsified posterior is retained while yielding interpretable quantile specific variable selection results. Our procedure can be used to communicate to policymakers which variables drive downside risk to the macro economy.",econ.EM,Econometrics
The Law of Large Numbers for Large Stable Matchings,"In many empirical studies of a large two-sided matching market (such as in a college admissions problem), the researcher performs statistical inference under the assumption that they observe a random sample from a large matching market. In this paper, we consider a setting in which the researcher observes either all or a nontrivial fraction of outcomes from a stable matching. We establish a concentration inequality for empirical matching probabilities assuming strong correlation among the colleges' preferences while allowing students' preferences to be fully heterogeneous. Our concentration inequality yields laws of large numbers for the empirical matching probabilities and other statistics commonly used in empirical analyses of a large matching market. To illustrate the usefulness of our concentration inequality, we prove consistency for estimators of conditional matching probabilities and measures of positive assortative matching.",econ.EM,Econometrics
Detecting Grouped Local Average Treatment Effects and Selecting True Instruments,"Under an endogenous binary treatment with heterogeneous effects and multiple instruments, we propose a two-step procedure for identifying complier groups with identical local average treatment effects (LATE) despite relying on distinct instruments, even if several instruments violate the identifying assumptions. We use the fact that the LATE is homogeneous for instruments which (i) satisfy the LATE assumptions (instrument validity and treatment monotonicity in the instrument) and (ii) generate identical complier groups in terms of treatment propensities given the respective instruments. We propose a two-step procedure, where we first cluster the propensity scores in the first step and find groups of IVs with the same reduced form parameters in the second step. Under the plurality assumption that within each set of instruments with identical treatment propensities, instruments truly satisfying the LATE assumptions are the largest group, our procedure permits identifying these true instruments in a data driven way. We show that our procedure is consistent and provides consistent and asymptotically normal estimators of underlying LATEs. We also provide a simulation study investigating the finite sample properties of our approach and an empirical application investigating the effect of incarceration on recidivism in the US with judge assignments serving as instruments.",econ.EM,Econometrics
Time-Varying Poisson Autoregression,"In this paper we propose a new time-varying econometric model, called Time-Varying Poisson AutoRegressive with eXogenous covariates (TV-PARX), suited to model and forecast time series of counts. {We show that the score-driven framework is particularly suitable to recover the evolution of time-varying parameters and provides the required flexibility to model and forecast time series of counts characterized by convoluted nonlinear dynamics and structural breaks.} We study the asymptotic properties of the TV-PARX model and prove that, under mild conditions, maximum likelihood estimation (MLE) yields strongly consistent and asymptotically normal parameter estimates. Finite-sample performance and forecasting accuracy are evaluated through Monte Carlo simulations. The empirical usefulness of the time-varying specification of the proposed TV-PARX model is shown by analyzing the number of new daily COVID-19 infections in Italy and the number of corporate defaults in the US.",econ.EM,Econometrics
Forecasting euro area inflation using a huge panel of survey expectations,"In this paper, we forecast euro area inflation and its main components using an econometric model which exploits a massive number of time series on survey expectations for the European Commission's Business and Consumer Survey. To make estimation of such a huge model tractable, we use recent advances in computational statistics to carry out posterior simulation and inference. Our findings suggest that the inclusion of a wide range of firms and consumers' opinions about future economic developments offers useful information to forecast prices and assess tail risks to inflation. These predictive improvements do not only arise from surveys related to expected inflation but also from other questions related to the general economic environment. Finally, we find that firms' expectations about the future seem to have more predictive content than consumer expectations.",econ.EM,Econometrics
A Structural Equation Modeling Approach to Understand User's Perceptions of Acceptance of Ride-Sharing Services in Dhaka City,"This research aims at building a multivariate statistical model for assessing users' perceptions of acceptance of ride-sharing services in Dhaka City. A structured questionnaire is developed based on the users' reported attitudes and perceived risks. A total of 350 normally distributed responses are collected from ride-sharing service users and stakeholders of Dhaka City. Respondents are interviewed to express their experience and opinions on ride-sharing services through the stated preference questionnaire. Structural Equation Modeling (SEM) is used to validate the research hypotheses. Statistical parameters and several trials are used to choose the best SEM. The responses are also analyzed using the Relative Importance Index (RII) method, validating the chosen SEM. Inside SEM, the quality of ride-sharing services is measured by two latent and eighteen observed variables. The latent variable 'safety & security' is more influential than 'service performance' on the overall quality of service index. Under 'safety & security' the other two variables, i.e., 'account information' and 'personal information' are found to be the most significant that impact the decision to share rides with others. In addition, 'risk of conflict' and 'possibility of accident' are identified using the perception model as the lowest contributing variables. Factor analysis reveals the suitability and reliability of the proposed SEM. Identifying the influential parameters in this will help the service providers understand and improve the quality of ride-sharing service for users.",econ.EM,Econometrics
Instrumental variable quantile regression under random right censoring,This paper studies a semiparametric quantile regression model with endogenous variables and random right censoring. The endogeneity issue is solved using instrumental variables. It is assumed that the structural quantile of the logarithm of the outcome variable is linear in the covariates and censoring is independent. The regressors and instruments can be either continuous or discrete. The specification generates a continuum of equations of which the quantile regression coefficients are a solution. Identification is obtained when this system of equations has a unique solution. Our estimation procedure solves an empirical analogue of the system of equations. We derive conditions under which the estimator is asymptotically normal and prove the validity of a bootstrap procedure for inference. The finite sample performance of the approach is evaluated through numerical simulations. An application to the national Job Training Partnership Act study illustrates the method.,econ.EM,Econometrics
Structural Estimation of Markov Decision Processes in High-Dimensional State Space with Finite-Time Guarantees,"We consider the task of estimating a structural model of dynamic decisions by a human agent based upon the observable history of implemented actions and visited states. This problem has an inherent nested structure: in the inner problem, an optimal policy for a given reward function is identified while in the outer problem, a measure of fit is maximized. Several approaches have been proposed to alleviate the computational burden of this nested-loop structure, but these methods still suffer from high complexity when the state space is either discrete with large cardinality or continuous in high dimensions. Other approaches in the inverse reinforcement learning (IRL) literature emphasize policy estimation at the expense of reduced reward estimation accuracy. In this paper we propose a single-loop estimation algorithm with finite time guarantees that is equipped to deal with high-dimensional state spaces without compromising reward estimation accuracy. In the proposed algorithm, each policy improvement step is followed by a stochastic gradient step for likelihood maximization. We show that the proposed algorithm converges to a stationary solution with a finite-time guarantee. Further, if the reward is parameterized linearly, we show that the algorithm approximates the maximum likelihood estimator sublinearly. Finally, by using robotics control problems in MuJoCo and their transfer settings, we show that the proposed algorithm achieves superior performance compared with other IRL and imitation learning benchmarks.",econ.EM,Econometrics
Estimation of Average Derivatives of Latent Regressors: With an Application to Inference on Buffer-Stock Saving,"This paper proposes a density-weighted average derivative estimator based on two noisy measures of a latent regressor. Both measures have classical errors with possibly asymmetric distributions. We show that the proposed estimator achieves the root-n rate of convergence, and derive its asymptotic normal distribution for statistical inference. Simulation studies demonstrate excellent small-sample performance supporting the root-n asymptotic normality. Based on the proposed estimator, we construct a formal test on the sub-unity of the marginal propensity to consume out of permanent income (MPCP) under a nonparametric consumption model and a permanent-transitory model of income dynamics with nonparametric distribution. Applying the test to four recent waves of U.S. Panel Study of Income Dynamics (PSID), we reject the null hypothesis of the unit MPCP in favor of a sub-unit MPCP, supporting the buffer-stock model of saving.",econ.EM,Econometrics
Interpreting and predicting the economy flows: A time-varying parameter global vector autoregressive integrated the machine learning model,"The paper proposes a time-varying parameter global vector autoregressive (TVP-GVAR) framework for predicting and analysing developed region economic variables. We want to provide an easily accessible approach for the economy application settings, where a variety of machine learning models can be incorporated for out-of-sample prediction. The LASSO-type technique for numerically efficient model selection of mean squared errors (MSEs) is selected. We show the convincing in-sample performance of our proposed model in all economic variables and relatively high precision out-of-sample predictions with different-frequency economic inputs. Furthermore, the time-varying orthogonal impulse responses provide novel insights into the connectedness of economic variables at critical time points across developed regions. We also derive the corresponding asymptotic bands (the confidence intervals) for orthogonal impulse responses function under standard assumptions.",econ.EM,Econometrics
Bikeability and the induced demand for cycling,"To what extent is the volume of urban bicycle traffic affected by the provision of bicycle infrastructure? In this study, we exploit a large dataset of observed bicycle trajectories in combination with a fine-grained representation of the Copenhagen bicycle-relevant network. We apply a novel model for bicyclists' choice of route from origin to destination that takes the complete network into account. This enables us to determine bicyclists' preferences for a range of infrastructure and land-use types. We use the estimated preferences to compute a subjective cost of bicycle travel, which we correlate with the number of bicycle trips across a large number of origin-destination pairs. Simulations suggest that the extensive Copenhagen bicycle lane network has caused the number of bicycle trips and the bicycle kilometers traveled to increase by 60% and 90%, respectively, compared with a counterfactual without the bicycle lane network. This translates into an annual benefit of EUR 0.4M per km of bicycle lane owing to changes in subjective travel cost, health, and accidents. Our results thus strongly support the provision of bicycle infrastructure.",econ.EM,Econometrics
Phase transitions in nonparametric regressions,"When the unknown regression function of a single variable is known to have derivatives up to the $(+1)$th order bounded in absolute values by a common constant everywhere or a.e. (i.e., $(+1)$th degree of smoothness), the minimax optimal rate of the mean integrated squared error (MISE) is stated as $\left(\frac{1}{n}\right)^{\frac{2+2}{2+3}}$ in the literature. This paper shows that: (i) if $n\leq\left(+1\right)^{2+3}$, the minimax optimal MISE rate is $\frac{\log n}{n\log(\log n)}$ and the optimal degree of smoothness to exploit is roughly $\max\left\{ \left\lfloor \frac{\log n}{2\log\left(\log n\right)}\right\rfloor ,\,1\right\} $; (ii) if $n>\left(+1\right)^{2+3}$, the minimax optimal MISE rate is $\left(\frac{1}{n}\right)^{\frac{2+2}{2+3}}$ and the optimal degree of smoothness to exploit is $+1$. The fundamental contribution of this paper is a set of metric entropy bounds we develop for smooth function classes. Some of our bounds are original, and some of them improve and/or generalize the ones in the literature (e.g., Kolmogorov and Tikhomirov, 1959). Our metric entropy bounds allow us to show phase transitions in the minimax optimal MISE rates associated with some commonly seen smoothness classes as well as non-standard smoothness classes, and can also be of independent interest outside the nonparametric regression problems.",econ.EM,Econometrics
"Efficient counterfactual estimation in semiparametric discrete choice models: a note on Chiong, Hsieh, and Shum (2017)","I suggest an enhancement of the procedure of Chiong, Hsieh, and Shum (2017) for calculating bounds on counterfactual demand in semiparametric discrete choice models. Their algorithm relies on a system of inequalities indexed by cycles of a large number $M$ of observed markets and hence seems to require computationally infeasible enumeration of all such cycles. I show that such enumeration is unnecessary because solving the ""fully efficient"" inequality system exploiting cycles of all possible lengths $K=1,\dots,M$ can be reduced to finding the length of the shortest path between every pair of vertices in a complete bidirected weighted graph on $M$ vertices. The latter problem can be solved using the Floyd--Warshall algorithm with computational complexity $O\left(M^3\right)$, which takes only seconds to run even for thousands of markets. Monte Carlo simulations illustrate the efficiency gain from using cycles of all lengths, which turns out to be positive, but small.",econ.EM,Econometrics
Covariate Balancing Sensitivity Analysis for Extrapolating Randomized Trials across Locations,"The ability to generalize experimental results from randomized control trials (RCTs) across locations is crucial for informing policy decisions in targeted regions. Such generalization is often hindered by the lack of identifiability due to unmeasured effect modifiers that compromise direct transport of treatment effect estimates from one location to another. We build upon sensitivity analysis in observational studies and propose an optimization procedure that allows us to get bounds on the treatment effects in targeted regions. Furthermore, we construct more informative bounds by balancing on the moments of covariates. In simulation experiments, we show that the covariate balancing approach is promising in getting sharper identification intervals.",econ.EM,Econometrics
Housing Price Prediction Model Selection Based on Lorenz and Concentration Curves: Empirical Evidence from Tehran Housing Market,"This study contributes a house price prediction model selection in Tehran City based on the area between Lorenz curve (LC) and concentration curve (CC) of the predicted price by using 206,556 observed transaction data over the period from March 21, 2018, to February 19, 2021. Several different methods such as generalized linear models (GLM) and recursive partitioning and regression trees (RPART), random forests (RF) regression models, and neural network (NN) models were examined house price prediction. We used 90% of all data samples which were chosen randomly to estimate the parameters of pricing models and 10% of remaining datasets to test the accuracy of prediction. Results showed that the area between the LC and CC curves (which are known as ABC criterion) of real and predicted prices in the test data sample of the random forest regression model was less than by other models under study. The comparison of the calculated ABC criteria leads us to conclude that the nonlinear regression models such as RF regression models give an accurate prediction of house prices in Tehran City.",econ.EM,Econometrics
Deep Quantile and Deep Composite Model Regression,"A main difficulty in actuarial claim size modeling is that there is no simple off-the-shelf distribution that simultaneously provides a good distributional model for the main body and the tail of the data. In particular, covariates may have different effects for small and for large claim sizes. To cope with this problem, we introduce a deep composite regression model whose splicing point is given in terms of a quantile of the conditional claim size distribution rather than a constant. To facilitate M-estimation for such models, we introduce and characterize the class of strictly consistent scoring functions for the triplet consisting a quantile, as well as the lower and upper expected shortfall beyond that quantile. In a second step, this elicitability result is applied to fit deep neural network regression models. We demonstrate the applicability of our approach and its superiority over classical approaches on a real accident insurance data set.",econ.EM,Econometrics
Factor Models with Sparse VAR Idiosyncratic Components,"We reconcile the two worlds of dense and sparse modeling by exploiting the positive aspects of both. We employ a factor model and assume {the dynamic of the factors is non-pervasive while} the idiosyncratic term follows a sparse vector autoregressive model (VAR) {which allows} for cross-sectional and time dependence. The estimation is articulated in two steps: first, the factors and their loadings are estimated via principal component analysis and second, the sparse VAR is estimated by regularized regression on the estimated idiosyncratic components. We prove the consistency of the proposed estimation approach as the time and cross-sectional dimension diverge. In the second step, the estimation error of the first step needs to be accounted for. Here, we do not follow the naive approach of simply plugging in the standard rates derived for the factor estimation. Instead, we derive a more refined expression of the error. This enables us to derive tighter rates. We discuss the implications of our model for forecasting, factor augmented regression, bootstrap of factor models, and time series dependence networks via semi-parametric estimation of the inverse of the spectral density matrix.",econ.EM,Econometrics
Behavioral Foundations of Nested Stochastic Choice and Nested Logit,"We provide the first behavioral characterization of nested logit, a foundational and widely applied discrete choice model, through the introduction of a non-parametric version of nested logit that we call Nested Stochastic Choice (NSC). NSC is characterized by a single axiom that weakens Independence of Irrelevant Alternatives based on revealed similarity to allow for the similarity effect. Nested logit is characterized by an additional menu-independence axiom. Our axiomatic characterization leads to a practical, data-driven algorithm that identifies the true nest structure from choice data. We also discuss limitations of generalizing nested logit by studying the testable implications of cross-nested logit.",econ.EM,Econometrics
On Well-posedness and Minimax Optimal Rates of Nonparametric Q-function Estimation in Off-policy Evaluation,"We study the off-policy evaluation (OPE) problem in an infinite-horizon Markov decision process with continuous states and actions. We recast the $Q$-function estimation into a special form of the nonparametric instrumental variables (NPIV) estimation problem. We first show that under one mild condition the NPIV formulation of $Q$-function estimation is well-posed in the sense of $L^2$-measure of ill-posedness with respect to the data generating distribution, bypassing a strong assumption on the discount factor $$ imposed in the recent literature for obtaining the $L^2$ convergence rates of various $Q$-function estimators. Thanks to this new well-posed property, we derive the first minimax lower bounds for the convergence rates of nonparametric estimation of $Q$-function and its derivatives in both sup-norm and $L^2$-norm, which are shown to be the same as those for the classical nonparametric regression (Stone, 1982). We then propose a sieve two-stage least squares estimator and establish its rate-optimality in both norms under some mild conditions. Our general results on the well-posedness and the minimax lower bounds are of independent interest to study not only other nonparametric estimators for $Q$-function but also efficient estimation on the value of any target policy in off-policy settings.",econ.EM,Econometrics
The Past as a Stochastic Process,"Historical processes manifest remarkable diversity. Nevertheless, scholars have long attempted to identify patterns and categorize historical actors and influences with some success. A stochastic process framework provides a structured approach for the analysis of large historical datasets that allows for detection of sometimes surprising patterns, identification of relevant causal actors both endogenous and exogenous to the process, and comparison between different historical cases. The combination of data, analytical tools and the organizing theoretical framework of stochastic processes complements traditional narrative approaches in history and archaeology.",econ.EM,Econometrics
Sequential Cauchy Combination Test for Multiple Testing Problems with Financial Applications,"We introduce a simple tool to control for false discoveries and identify individual signals in scenarios involving many tests, dependent test statistics, and potentially sparse signals. The tool applies the Cauchy combination test recursively on a sequence of expanding subsets of $p$-values and is referred to as the sequential Cauchy combination test. While the original Cauchy combination test aims to make a global statement about a set of null hypotheses by summing transformed $p$-values, our sequential version determines which $p$-values trigger the rejection of the global null. The sequential test achieves strong familywise error rate control, exhibits less conservatism compared to existing controlling procedures when dealing with dependent test statistics, and provides a power boost. As illustrations, we revisit two well-known large-scale multiple testing problems in finance for which the test statistics have either serial dependence or cross-sectional dependence, namely monitoring drift bursts in asset prices and searching for assets with a nonzero alpha. In both applications, the sequential Cauchy combination test proves to be a preferable alternative. It overcomes many of the drawbacks inherent to inequality-based controlling procedures, extreme value approaches, resampling and screening methods, and it improves the power in simulations, leading to distinct empirical outcomes.",econ.EM,Econometrics
Identification of Unobservables in Observations,"In empirical studies, the data usually don't include all the variables of interest in an economic model. This paper shows the identification of unobserved variables in observations at the population level. When the observables are distinct in each observation, there exists a function mapping from the observables to the unobservables. Such a function guarantees the uniqueness of the latent value in each observation. The key lies in the identification of the joint distribution of observables and unobservables from the distribution of observables. The joint distribution of observables and unobservables then reveal the latent value in each observation. Three examples of this result are discussed.",econ.EM,Econometrics
Robust Inference in High Dimensional Linear Model with Cluster Dependence,"Cluster standard error (Liang and Zeger, 1986) is widely used by empirical researchers to account for cluster dependence in linear model. It is well known that this standard error is biased. We show that the bias does not vanish under high dimensional asymptotics by revisiting Chesher and Jewitt (1987)'s approach. An alternative leave-cluster-out crossfit (LCOC) estimator that is unbiased, consistent and robust to cluster dependence is provided under high dimensional setting introduced by Cattaneo, Jansson and Newey (2018). Since LCOC estimator nests the leave-one-out crossfit estimator of Kline, Saggio and Solvsten (2019), the two papers are unified. Monte Carlo comparisons are provided to give insights on its finite sample properties. The LCOC estimator is then applied to Angrist and Lavy's (2009) study of the effects of high school achievement award and Donohue III and Levitt's (2001) study of the impact of abortion on crime.",econ.EM,Econometrics
Dominant Drivers of National Inflation,"For western economies a long-forgotten phenomenon is on the horizon: rising inflation rates. We propose a novel approach christened D2ML to identify drivers of national inflation. D2ML combines machine learning for model selection with time dependent data and graphical models to estimate the inverse of the covariance matrix, which is then used to identify dominant drivers. Using a dataset of 33 countries, we find that the US inflation rate and oil prices are dominant drivers of national inflation rates. For a more general framework, we carry out Monte Carlo simulations to show that our estimator correctly identifies dominant drivers.",econ.EM,Econometrics
Logs with zeros? Some problems and solutions,"When studying an outcome $Y$ that is weakly-positive but can equal zero (e.g. earnings), researchers frequently estimate an average treatment effect (ATE) for a ""log-like"" transformation that behaves like $\log(Y)$ for large $Y$ but is defined at zero (e.g. $\log(1+Y)$, $\mathrm{arcsinh}(Y)$). We argue that ATEs for log-like transformations should not be interpreted as approximating percentage effects, since unlike a percentage, they depend on the units of the outcome. In fact, we show that if the treatment affects the extensive margin, one can obtain a treatment effect of any magnitude simply by re-scaling the units of $Y$ before taking the log-like transformation. This arbitrary unit-dependence arises because an individual-level percentage effect is not well-defined for individuals whose outcome changes from zero to non-zero when receiving treatment, and the units of the outcome implicitly determine how much weight the ATE for a log-like transformation places on the extensive margin. We further establish a trilemma: when the outcome can equal zero, there is no treatment effect parameter that is an average of individual-level treatment effects, unit-invariant, and point-identified. We discuss several alternative approaches that may be sensible in settings with an intensive and extensive margin, including (i) expressing the ATE in levels as a percentage (e.g. using Poisson regression), (ii) explicitly calibrating the value placed on the intensive and extensive margins, and (iii) estimating separate effects for the two margins (e.g. using Lee bounds). We illustrate these approaches in three empirical applications.",econ.EM,Econometrics
The Falsification Adaptive Set in Linear Models with Instrumental Variables that Violate the Exclusion or Conditional Exogeneity Restriction,"Masten and Poirier (2021) introduced the falsification adaptive set (FAS) in linear models with a single endogenous variable estimated with multiple correlated instrumental variables (IVs). The FAS reflects the model uncertainty that arises from falsification of the baseline model. We show that it applies to cases where a conditional exogeneity assumption holds and invalid instruments violate the exclusion assumption only. We propose a generalized FAS that reflects the model uncertainty when some instruments violate the exclusion assumption and/or some instruments violate the conditional exogeneity assumption. Under the assumption that invalid instruments are not themselves endogenous explanatory variables, if there is at least one relevant instrument that satisfies both the exclusion and conditional exogeneity assumptions then this generalized FAS is guaranteed to contain the parameter of interest.",econ.EM,Econometrics
Optimal Model Selection in RDD and Related Settings Using Placebo Zones,"We propose a new model-selection algorithm for Regression Discontinuity Design, Regression Kink Design, and related IV estimators. Candidate models are assessed within a 'placebo zone' of the running variable, where the true effects are known to be zero. The approach yields an optimal combination of bandwidth, polynomial, and any other choice parameters. It can also inform choices between classes of models (e.g. RDD versus cohort-IV) and any other choices, such as covariates, kernel, or other weights. We outline sufficient conditions under which the approach is asymptotically optimal. The approach also performs favorably under more general conditions in a series of Monte Carlo simulations. We demonstrate the approach in an evaluation of changes to Minimum Supervised Driving Hours in the Australian state of New South Wales. We also re-evaluate evidence on the effects of Head Start and Minimum Legal Drinking Age. Our Stata commands implement the procedure and compare its performance to other approaches.",econ.EM,Econometrics
Labor Income Risk and the Cross-Section of Expected Returns,"This paper explores asset pricing implications of unemployment risk from sectoral shifts. I proxy for this risk using cross-industry dispersion (CID), defined as a mean absolute deviation of returns of 49 industry portfolios. CID peaks during periods of accelerated sectoral reallocation and heightened uncertainty. I find that expected stock returns are related cross-sectionally to the sensitivities of returns to innovations in CID. Annualized returns of the stocks with high sensitivity to CID are 5.9% lower than the returns of the stocks with low sensitivity. Abnormal returns with respect to the best factor model are 3.5%, suggesting that common factors can not explain this return spread. Stocks with high sensitivity to CID are likely to be the stocks, which benefited from sectoral shifts. CID positively predicts unemployment through its long-term component, consistent with the hypothesis that CID is a proxy for unemployment risk from sectoral shifts.",econ.EM,Econometrics
Identifying an Earnings Process With Dependent Contemporaneous Income Shocks,"This paper proposes a novel approach for identifying coefficients in an earnings dynamics model with arbitrarily dependent contemporaneous income shocks. Traditional methods relying on second moments fail to identify these coefficients, emphasizing the need for nongaussianity assumptions that capture information from higher moments. Our results contribute to the literature on earnings dynamics by allowing models of earnings to have, for example, the permanent income shock of a job change to be linked to the contemporaneous transitory income shock of a relocation bonus.",econ.EM,Econometrics
Estimation of Grouped Time-Varying Network Vector Autoregression Models,"This paper introduces a flexible time-varying network vector autoregressive model framework for large-scale time series. A latent group structure is imposed on the heterogeneous and node-specific time-varying momentum and network spillover effects so that the number of unknown time-varying coefficients to be estimated can be reduced considerably. A classic agglomerative clustering algorithm with nonparametrically estimated distance matrix is combined with a ratio criterion to consistently estimate the latent group number and membership. A post-grouping local linear smoothing method is proposed to estimate the group-specific time-varying momentum and network effects, substantially improving the convergence rates of the preliminary estimates which ignore the latent structure. We further modify the methodology and theory to allow for structural breaks in either the group membership, group number or group-specific coefficient functions. Numerical studies including Monte-Carlo simulation and an empirical application are presented to examine the finite-sample performance of the developed model and methodology.",econ.EM,Econometrics
Target PCA: Transfer Learning Large Dimensional Panel Data,"This paper develops a novel method to estimate a latent factor model for a large target panel with missing observations by optimally using the information from auxiliary panel data sets. We refer to our estimator as target-PCA. Transfer learning from auxiliary panel data allows us to deal with a large fraction of missing observations and weak signals in the target panel. We show that our estimator is more efficient and can consistently estimate weak factors, which are not identifiable with conventional methods. We provide the asymptotic inferential theory for target-PCA under very general assumptions on the approximate factor model and missing patterns. In an empirical study of imputing data in a mixed-frequency macroeconomic panel, we demonstrate that target-PCA significantly outperforms all benchmark methods.",econ.EM,Econometrics
On the use of U-statistics for linear dyadic interaction models,"Even though dyadic regressions are widely used in empirical applications, the (asymptotic) properties of estimation methods only began to be studied recently in the literature. This paper aims to provide in a step-by-step manner how U-statistics tools can be applied to obtain the asymptotic properties of pairwise differences estimators for a two-way fixed effects model of dyadic interactions. More specifically, we first propose an estimator for the model that relies on pairwise differencing such that the fixed effects are differenced out. As a result, the summands of the influence function will not be independent anymore, showing dependence on the individual level and translating to the fact that the usual law of large numbers and central limit theorems do not straightforwardly apply. To overcome such obstacles, we show how to generalize tools of U-statistics for single-index variables to the double-indices context of dyadic datasets. A key result is that there can be different ways of defining the Hajek projection for a directed dyadic structure, which will lead to distinct, but equivalent, consistent estimators for the asymptotic variances. The results presented in this paper are easily extended to non-linear models.",econ.EM,Econometrics
iCOS: Option-Implied COS Method,"This paper proposes the option-implied Fourier-cosine method, iCOS, for non-parametric estimation of risk-neutral densities, option prices, and option sensitivities. The iCOS method leverages the Fourier-based COS technique, proposed by Fang and Oosterlee (2008), by utilizing the option-implied cosine series coefficients. Notably, this procedure does not rely on any model assumptions about the underlying asset price dynamics, it is fully non-parametric, and it does not involve any numerical optimization. These features make it rather general and computationally appealing. Furthermore, we derive the asymptotic properties of the proposed non-parametric estimators and study their finite-sample behavior in Monte Carlo simulations. Our empirical analysis using S&P 500 index options and Amazon equity options illustrates the effectiveness of the iCOS method in extracting valuable information from option prices under different market conditions. Additionally, we apply our methodology to dissect and quantify observation and discretization errors in the VIX index.",econ.EM,Econometrics
Combining predictive distributions of electricity prices: Does minimizing the CRPS lead to optimal decisions in day-ahead bidding?,"Probabilistic price forecasting has recently gained attention in power trading because decisions based on such predictions can yield significantly higher profits than those made with point forecasts alone. At the same time, methods are being developed to combine predictive distributions, since no model is perfect and averaging generally improves forecasting performance. In this article we address the question of whether using CRPS learning, a novel weighting technique minimizing the continuous ranked probability score (CRPS), leads to optimal decisions in day-ahead bidding. To this end, we conduct an empirical study using hourly day-ahead electricity prices from the German EPEX market. We find that increasing the diversity of an ensemble can have a positive impact on accuracy. At the same time, the higher computational cost of using CRPS learning compared to an equal-weighted aggregation of distributions is not offset by higher profits, despite significantly more accurate predictions.",econ.EM,Econometrics
Transformers versus LSTMs for electronic trading,"With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.
  Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.
  To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture for the Transformer-based model is designed to adapt for financial prediction. The experiment result reflects that the Transformer-based model only has the limited advantage in absolute price sequence prediction. The LSTM-based models show better and more robust performance on difference sequence prediction, such as price difference and price movement.",econ.EM,Econometrics
Smoothness-Adaptive Dynamic Pricing with Nonparametric Demand Learning,"We study the dynamic pricing problem where the demand function is nonparametric and Hlder smooth, and we focus on adaptivity to the unknown Hlder smoothness parameter $$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{+1}{2+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $(T^{\frac{+1}{2+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves this minimax optimal regret bound without the prior knowledge $$.",econ.EM,Econometrics
"Marital Sorting, Household Inequality and Selection","Using CPS data for 1976 to 2022 we explore how wage inequality has evolved for married couples with both spouses working full time full year, and its impact on household income inequality. We also investigate how marriage sorting patterns have changed over this period. To determine the factors driving income inequality we estimate a model explaining the joint distribution of wages which accounts for the spouses' employment decisions. We find that income inequality has increased for these households and increased assortative matching of wages has exacerbated the inequality resulting from individual wage growth. We find that positive sorting partially reflects the correlation across unobservables influencing both members' of the marriage wages. We decompose the changes in sorting patterns over the 47 years comprising our sample into structural, composition and selection effects and find that the increase in positive sorting primarily reflects the increased skill premia for both observed and unobserved characteristics.",econ.EM,Econometrics
Robust Minimum Distance Inference in Structural Models,"This paper proposes minimum distance inference for a structural parameter of interest, which is robust to the lack of identification of other structural nuisance parameters. Some choices of the weighting matrix lead to asymptotic chi-squared distributions with degrees of freedom that can be consistently estimated from the data, even under partial identification. In any case, knowledge of the level of under-identification is not required. We study the power of our robust test. Several examples show the wide applicability of the procedure and a Monte Carlo investigates its finite sample performance. Our identification-robust inference method can be applied to make inferences on both calibrated (fixed) parameters and any other structural parameter of interest. We illustrate the method's usefulness by applying it to a structural model on the non-neutrality of monetary policy, as in \cite{nakamura2018high}, where we empirically evaluate the validity of the calibrated parameters and we carry out robust inference on the slope of the Phillips curve and the information effect.",econ.EM,Econometrics
Weak Identification with Many Instruments,"Linear instrumental variable regressions are widely used to estimate causal effects. Many instruments arise from the use of ``technical'' instruments and more recently from the empirical strategy of ``judge design''. This paper surveys and summarizes ideas from recent literature on estimation and statistical inferences with many instruments for a single endogenous regressor. We discuss how to assess the strength of the instruments and how to conduct weak identification-robust inference under heteroskedasticity. We establish new results for a jack-knifed version of the Lagrange Multiplier (LM) test statistic. Furthermore, we extend the weak-identification-robust tests to settings with both many exogenous regressors and many instruments. We propose a test that properly partials out many exogenous regressors while preserving the re-centering property of the jack-knife. The proposed tests have correct size and good power properties.",econ.EM,Econometrics
Characterizing Correlation Matrices that Admit a Clustered Factor Representation,"The Clustered Factor (CF) model induces a block structure on the correlation matrix and is commonly used to parameterize correlation matrices. Our results reveal that the CF model imposes superfluous restrictions on the correlation matrix. This can be avoided by a different parametrization, involving the logarithmic transformation of the block correlation matrix.",econ.EM,Econometrics
Identifying spatial interdependence in panel data with large N and small T,"This paper develops a simple two-stage variational Bayesian algorithm to estimate panel spatial autoregressive models, where N, the number of cross-sectional units, is much larger than T, the number of time periods without restricting the spatial effects using a predetermined weighting matrix. We use Dirichlet-Laplace priors for variable selection and parameter shrinkage. Without imposing any a priori structures on the spatial linkages between variables, we let the data speak for themselves. Extensive Monte Carlo studies show that our method is super-fast and our estimated spatial weights matrices strongly resemble the true spatial weights matrices. As an illustration, we investigate the spatial interdependence of European Union regional gross value added growth rates. In addition to a clear pattern of predominant country clusters, we have uncovered a number of important between-country spatial linkages which are yet to be documented in the literature. This new procedure for estimating spatial effects is of particular relevance for researchers and policy makers alike.",econ.EM,Econometrics
Stochastic volatility models with skewness selection,"This paper expands traditional stochastic volatility models by allowing for time-varying skewness without imposing it. While dynamic asymmetry may capture the likely direction of future asset returns, it comes at the risk of leading to overparameterization. Our proposed approach mitigates this concern by leveraging sparsity-inducing priors to automatically selects the skewness parameter as being dynamic, static or zero in a data-driven framework. We consider two empirical applications. First, in a bond yield application, dynamic skewness captures interest rate cycles of monetary easing and tightening being partially explained by central banks' mandates. In an currency modeling framework, our model indicates no skewness in the carry factor after accounting for stochastic volatility which supports the idea of carry crashes being the result of volatility surges instead of dynamic skewness.",econ.EM,Econometrics
"Individual Updating of Subjective Probability of Homicide Victimization: a ""Natural Experiment'' on Risk Communication","We investigate the dynamics of the update of subjective homicide victimization risk after an informational shock by developing two econometric models able to accommodate both optimal decisions of changing prior expectations which enable us to rationalize skeptical Bayesian agents with their disregard to new information. We apply our models to a unique household data (N = 4,030) that consists of socioeconomic and victimization expectation variables in Brazil, coupled with an informational ``natural experiment'' brought by the sample design methodology, which randomized interviewers to interviewees. The higher priors about their own subjective homicide victimization risk are set, the more likely individuals are to change their initial perceptions. In case of an update, we find that elders and females are more reluctant to change priors and choose the new response level. In addition, even though the respondents' level of education is not significant, the interviewers' level of education has a key role in changing and updating decisions. The results show that our econometric approach fits reasonable well the available empirical evidence, stressing the salient role heterogeneity represented by individual characteristics of interviewees and interviewers have on belief updating and lack of it, say, skepticism. Furthermore, we can rationalize skeptics through an informational quality/credibility argument.",econ.EM,Econometrics
Structural Modelling of Dynamic Networks and Identifying Maximum Likelihood,"This paper considers nonlinear dynamic models where the main parameter of interest is a nonnegative matrix characterizing the network (contagion) effects. This network matrix is usually constrained either by assuming a limited number of nonzero elements (sparsity), or by considering a reduced rank approach for nonnegative matrix factorization (NMF). We follow the latter approach and develop a new probabilistic NMF method. We introduce a new Identifying Maximum Likelihood (IML) method for consistent estimation of the identified set of admissible NMF's and derive its asymptotic distribution. Moreover, we propose a maximum likelihood estimator of the parameter matrix for a given non-negative rank, derive its asymptotic distribution and the associated efficiency bound.",econ.EM,Econometrics
Strategyproof Decision-Making in Panel Data Settings and Beyond,"We consider the problem of decision-making using panel data, in which a decision-maker gets noisy, repeated measurements of multiple units (or agents). We consider a setup where there is a pre-intervention period, when the principal observes the outcomes of each unit, after which the principal uses these observations to assign a treatment to each unit. Unlike this classical setting, we permit the units generating the panel data to be strategic, i.e. units may modify their pre-intervention outcomes in order to receive a more desirable intervention. The principal's goal is to design a strategyproof intervention policy, i.e. a policy that assigns units to their utility-maximizing interventions despite their potential strategizing. We first identify a necessary and sufficient condition under which a strategyproof intervention policy exists, and provide a strategyproof mechanism with a simple closed form when one does exist. Along the way, we prove impossibility results for strategic multiclass classification, which may be of independent interest. When there are two interventions, we establish that there always exists a strategyproof mechanism, and provide an algorithm for learning such a mechanism. For three or more interventions, we provide an algorithm for learning a strategyproof mechanism if there exists a sufficiently large gap in the principal's rewards between different interventions. Finally, we empirically evaluate our model using real-world panel data collected from product sales over 18 months. We find that our methods compare favorably to baselines which do not take strategic interactions into consideration, even in the presence of model misspecification.",econ.EM,Econometrics
Synthetic Principal Component Design: Fast Covariate Balancing with Synthetic Controls,"The optimal design of experiments typically involves solving an NP-hard combinatorial optimization problem. In this paper, we aim to develop a globally convergent and practically efficient optimization algorithm. Specifically, we consider a setting where the pre-treatment outcome data is available and the synthetic control estimator is invoked. The average treatment effect is estimated via the difference between the weighted average outcomes of the treated and control units, where the weights are learned from the observed data. {Under this setting, we surprisingly observed that the optimal experimental design problem could be reduced to a so-called \textit{phase synchronization} problem.} We solve this problem via a normalized variant of the generalized power method with spectral initialization. On the theoretical side, we establish the first global optimality guarantee for experiment design when pre-treatment data is sampled from certain data-generating processes. Empirically, we conduct extensive experiments to demonstrate the effectiveness of our method on both the US Bureau of Labor Statistics and the Abadie-Diemond-Hainmueller California Smoking Data. In terms of the root mean square error, our algorithm surpasses the random design by a large margin.",econ.EM,Econometrics
mCube: Multinomial Micro-level reserving Model,"This paper presents a multinomial multi-state micro-level reserving model, denoted mCube. We propose a unified framework for modelling the time and the payment process for IBNR and RBNS claims and for modeling IBNR claim counts. We use multinomial distributions for the time process and spliced mixture models for the payment process. We illustrate the excellent performance of the proposed model on a real data set of a major insurance company consisting of bodily injury claims. It is shown that the proposed model produces a best estimate distribution that is centered around the true reserve.",econ.EM,Econometrics
GLS under Monotone Heteroskedasticity,"The generalized least square (GLS) is one of the most basic tools in regression analyses. A major issue in implementing the GLS is estimation of the conditional variance function of the error term, which typically requires a restrictive functional form assumption for parametric estimation or smoothing parameters for nonparametric estimation. In this paper, we propose an alternative approach to estimate the conditional variance function under nonparametric monotonicity constraints by utilizing the isotonic regression method. Our GLS estimator is shown to be asymptotically equivalent to the infeasible GLS estimator with knowledge of the conditional error variance, and involves only some tuning to trim boundary observations, not only for point estimation but also for interval estimation or hypothesis testing. Our analysis extends the scope of the isotonic regression method by showing that the isotonic estimates, possibly with generated variables, can be employed as first stage estimates to be plugged in for semiparametric objects. Simulation studies illustrate excellent finite sample performances of the proposed method. As an empirical example, we revisit Acemoglu and Restrepo's (2017) study on the relationship between an aging population and economic growth to illustrate how our GLS estimator effectively reduces estimation errors.",econ.EM,Econometrics
Contextual Bandits in a Survey Experiment on Charitable Giving: Within-Experiment Outcomes versus Policy Learning,"We design and implement an adaptive experiment (a ``contextual bandit'') to learn a targeted treatment assignment policy, where the goal is to use a participant's survey responses to determine which charity to expose them to in a donation solicitation. The design balances two competing objectives: optimizing the outcomes for the subjects in the experiment (``cumulative regret minimization'') and gathering data that will be most useful for policy learning, that is, for learning an assignment rule that will maximize welfare if used after the experiment (``simple regret minimization''). We evaluate alternative experimental designs by collecting pilot data and then conducting a simulation study. Next, we implement our selected algorithm. Finally, we perform a second simulation study anchored to the collected data that evaluates the benefits of the algorithm we chose. Our first result is that the value of a learned policy in this setting is higher when data is collected via a uniform randomization rather than collected adaptively using standard cumulative regret minimization or policy learning algorithms. We propose a simple heuristic for adaptive experimentation that improves upon uniform randomization from the perspective of policy learning at the expense of increasing cumulative regret relative to alternative bandit algorithms. The heuristic modifies an existing contextual bandit algorithm by (i) imposing a lower bound on assignment probabilities that decay slowly so that no arm is discarded too quickly, and (ii) after adaptively collecting data, restricting policy learning to select from arms where sufficient data has been gathered.",econ.EM,Econometrics
Boosted p-Values for High-Dimensional Vector Autoregression,"Assessing the statistical significance of parameter estimates is an important step in high-dimensional vector autoregression modeling. Using the least-squares boosting method, we compute the p-value for each selected parameter at every boosting step in a linear model. The p-values are asymptotically valid and also adapt to the iterative nature of the boosting procedure. Our simulation experiment shows that the p-values can keep false positive rate under control in high-dimensional vector autoregressions. In an application with more than 100 macroeconomic time series, we further show that the p-values can not only select a sparser model with good prediction performance but also help control model stability. A companion R package boostvar is developed.",econ.EM,Econometrics
Are Synthetic Control Weights Balancing Score?,"In this short note, I outline conditions under which conditioning on Synthetic Control (SC) weights emulates a randomized control trial where the treatment status is independent of potential outcomes. Specifically, I demonstrate that if there exist SC weights such that (i) the treatment effects are exactly identified and (ii) these weights are uniformly and cumulatively bounded, then SC weights are balancing scores.",econ.EM,Econometrics
Eigenvalue tests for the number of latent factors in short panels,"This paper studies new tests for the number of latent factors in a large cross-sectional factor model with small time dimension. These tests are based on the eigenvalues of variance-covariance matrices of (possibly weighted) asset returns, and rely on either the assumption of spherical errors, or instrumental variables for factor betas. We establish the asymptotic distributional results using expansion theorems based on perturbation theory for symmetric matrices. Our framework accommodates semi-strong factors in the systematic components. We propose a novel statistical test for weak factors against strong or semi-strong factors. We provide an empirical application to US equity data. Evidence for a different number of latent factors according to market downturns and market upturns, is statistically ambiguous in the considered subperiods. In particular, our results contradicts the common wisdom of a single factor model in bear markets.",econ.EM,Econometrics
Bayesian Neural Networks for Macroeconomic Analysis,"Macroeconomic data is characterized by a limited number of observations (small T), many time series (big K) but also by featuring temporal dependence. Neural networks, by contrast, are designed for datasets with millions of observations and covariates. In this paper, we develop Bayesian neural networks (BNNs) that are well-suited for handling datasets commonly used for macroeconomic analysis in policy institutions. Our approach avoids extensive specification searches through a novel mixture specification for the activation function that appropriately selects the form of nonlinearities. Shrinkage priors are used to prune the network and force irrelevant neurons to zero. To cope with heteroskedasticity, the BNN is augmented with a stochastic volatility model for the error term. We illustrate how the model can be used in a policy institution by first showing that our different BNNs produce precise density forecasts, typically better than those from other machine learning methods. Finally, we showcase how our model can be used to recover nonlinearities in the reaction of macroeconomic aggregates to financial shocks.",econ.EM,Econometrics
Causal Bandits: Online Decision-Making in Endogenous Settings,"The deployment of Multi-Armed Bandits (MAB) has become commonplace in many economic applications. However, regret guarantees for even state-of-the-art linear bandit algorithms (such as Optimism in the Face of Uncertainty Linear bandit (OFUL)) make strong exogeneity assumptions w.r.t. arm covariates. This assumption is very often violated in many economic contexts and using such algorithms can lead to sub-optimal decisions. Further, in social science analysis, it is also important to understand the asymptotic distribution of estimated parameters. To this end, in this paper, we consider the problem of online learning in linear stochastic contextual bandit problems with endogenous covariates. We propose an algorithm we term $$-BanditIV, that uses instrumental variables to correct for this bias, and prove an $\tilde{\mathcal{O}}(k\sqrt{T})$ upper bound for the expected regret of the algorithm. Further, we demonstrate the asymptotic consistency and normality of the $$-BanditIV estimator. We carry out extensive Monte Carlo simulations to demonstrate the performance of our algorithms compared to other methods. We show that $$-BanditIV significantly outperforms other existing methods in endogenous settings. Finally, we use data from real-time bidding (RTB) system to demonstrate how $$-BanditIV can be used to estimate the causal impact of advertising in such settings and compare its performance with other existing methods.",econ.EM,Econometrics
Same Root Different Leaves: Time Series and Cross-Sectional Methods in Panel Data,"A central goal in social science is to evaluate the causal effect of a policy. One dominant approach is through panel data analysis in which the behaviors of multiple units are observed over time. The information across time and space motivates two general approaches: (i) horizontal regression (i.e., unconfoundedness), which exploits time series patterns, and (ii) vertical regression (e.g., synthetic controls), which exploits cross-sectional patterns. Conventional wisdom states that the two approaches are fundamentally different. We establish this position to be partly false for estimation but generally true for inference. In particular, we prove that both approaches yield identical point estimates under several standard settings. For the same point estimate, however, each approach quantifies uncertainty with respect to a distinct estimand. In turn, the confidence interval developed for one estimand may have incorrect coverage for another. This emphasizes that the source of randomness that researchers assume has direct implications for the accuracy of inference.",econ.EM,Econometrics
On the Estimation of Peer Effects for Sampled Networks,"This paper deals with the estimation of exogeneous peer effects for partially observed networks under the new inferential paradigm of design identification, which characterizes the missing data challenge arising with sampled networks with the central idea that two full data versions which are topologically compatible with the observed data may give rise to two different probability distributions. We show that peer effects cannot be identified by design when network links between sampled and unsampled units are not observed. Under realistic modeling conditions, and under the assumption that sampled units report on the size of their network of contacts, the asymptotic bias arising from estimating peer effects with incomplete network data is characterized, and a bias-corrected estimator is proposed. The finite sample performance of our methodology is investigated via simulations.",econ.EM,Econometrics
From the historical Roman road network to modern infrastructure in Italy,"An integrated and widespread road system, like the one built during the Roman Empire in Italy, plays an important role today in facilitating the construction of new infrastructure. This paper investigates the historical path of Roman roads as main determinant of both motorways and railways in the country. The empirical analysis shows how the modern Italian transport infrastructure followed the path traced in ancient times by the Romans in constructing their roads. Being paved and connecting Italy from North to South, consular trajectories lasted in time, representing the starting physical capital for developing the new transport networks.",econ.EM,Econometrics
Stable Matching with Mistaken Agents,"Motivated by growing evidence of agents' mistakes in strategically simple environments, we propose a solution concept -- robust equilibrium -- that requires only an asymptotically optimal behavior. We use it to study large random matching markets operated by the applicant-proposing Deferred Acceptance (DA). Although truth-telling is a dominant strategy, almost all applicants may be non-truthful in robust equilibrium; however, the outcome must be arbitrarily close to the stable matching. Our results imply that one can assume truthful agents to study DA outcomes, theoretically or counterfactually. However, to estimate the preferences of mistaken agents, one should assume stable matching but not truth-telling.",econ.EM,Econometrics
"Weak Instruments, First-Stage Heteroskedasticity, the Robust F-Test and a GMM Estimator with the Weight Matrix Based on First-Stage Residuals","This paper is concerned with the findings related to the robust first-stage F-statistic in the Monte Carlo analysis of Andrews (2018), who found in a heteroskedastic grouped-data design that even for very large values of the robust F-statistic, the standard 2SLS confidence intervals had large coverage distortions. This finding appears to discredit the robust F-statistic as a test for underidentification. However, it is shown here that large values of the robust F-statistic do imply that there is first-stage information, but this may not be utilized well by the 2SLS estimator, or the standard GMM estimator. An estimator that corrects for this is a robust GMM estimator, denoted GMMf, with the robust weight matrix not based on the structural residuals, but on the first-stage residuals. For the grouped-data setting of Andrews (2018), this GMMf estimator gives the weights to the group specific estimators according to the group specific concentration parameters in the same way as 2SLS does under homoskedasticity, which is formally shown using weak instrument asymptotics. The GMMf estimator is much better behaved than the 2SLS estimator in the Andrews (2018) design, behaving well in terms of relative bias and Wald-test size distortion at more standard values of the robust F-statistic. We show that the same patterns can occur in a dynamic panel data model when the error variance is heteroskedastic over time. We further derive the conditions under which the Stock and Yogo (2005) weak instruments critical values apply to the robust F-statistic in relation to the behaviour of the GMMf estimator.",econ.EM,Econometrics
A penalized two-pass regression to predict stock returns with time-varying risk premia,"We develop a penalized two-pass regression with time-varying factor loadings. The penalization in the first pass enforces sparsity for the time-variation drivers while also maintaining compatibility with the no-arbitrage restrictions by regularizing appropriate groups of coefficients. The second pass delivers risk premia estimates to predict equity excess returns. Our Monte Carlo results and our empirical results on a large cross-sectional data set of US individual stocks show that penalization without grouping can yield to nearly all estimated time-varying models violating the no-arbitrage restrictions. Moreover, our results demonstrate that the proposed method reduces the prediction errors compared to a penalized approach without appropriate grouping or a time-invariant factor model.",econ.EM,Econometrics
Weak convergence to derivatives of fractional Brownian motion,"It is well known that, under suitable regularity conditions, the normalized fractional process with fractional parameter $d$ converges weakly to fractional Brownian motion for $d>1/2$. We show that, for any non-negative integer $M$, derivatives of order $m=0,1,\dots,M$ of the normalized fractional process with respect to the fractional parameter $d$, jointly converge weakly to the corresponding derivatives of fractional Brownian motion. As an illustration we apply the results to the asymptotic distribution of the score vectors in the multifractional vector autoregressive model.",econ.EM,Econometrics
Strategic differences between regional investments into graphene technology and how corporations and universities manage patent portfolios,"Nowadays, patenting activities are essential in converting applied science to technology in the prevailing innovation model. To gain strategic advantages in the technological competitions between regions, nations need to leverage the investments of public and private funds to diversify over all technologies or specialize in a small number of technologies. In this paper, we investigated who the leaders are at the regional and assignee levels, how they attained their leadership positions, and whether they adopted diversification or specialization strategies, using a dataset of 176,193 patent records on graphene between 1986 and 2017 downloaded from Derwent Innovation. By applying a co-clustering method to the IPC subclasses in the patents and using a z-score method to extract keywords from their titles and abstracts, we identified seven graphene technology areas emerging in the sequence synthesis - composites - sensors - devices - catalyst - batteries - water treatment. We then examined the top regions in their investment preferences and their changes in rankings over time and found that they invested in all seven technology areas. In contrast, at the assignee level, some were diversified while others were specialized. We found that large entities diversified their portfolios across multiple technology areas, while small entities specialized around their core competencies. In addition, we found that universities had higher entropy values than corporations on average, leading us to the hypothesis that corporations file, buy, or sell patents to enable product development. In contrast, universities focus only on licensing their patents. We validated this hypothesis through an aggregate analysis of reassignment and licensing and a more detailed analysis of three case studies - SAMSUNG, RICE UNIVERSITY, and DYSON.",econ.EM,Econometrics
Time is limited on the road to asymptopia,"One challenge in the estimation of financial market agent-based models (FABMs) is to infer reliable insights using numerical simulations validated by only a single observed time series. Ergodicity (besides stationarity) is a strong precondition for any estimation, however it has not been systematically explored and is often simply presumed. For finite-sample lengths and limited computational resources empirical estimation always takes place in pre-asymptopia. Thus broken ergodicity must be considered the rule, but it remains largely unclear how to deal with the remaining uncertainty in non-ergodic observables. Here we show how an understanding of the ergodic properties of moment functions can help to improve the estimation of (F)ABMs. We run Monte Carlo experiments and study the convergence behaviour of moment functions of two prototype models. We find infeasibly-long convergence times for most. Choosing an efficient mix of ensemble size and simulated time length guided our estimation and might help in general.",econ.EM,Econometrics
Inference on Strongly Identified Functionals of Weakly Identified Functions,"In a variety of applications, including nonparametric instrumental variable (NPIV) analysis, proximal causal inference under unmeasured confounding, and missing-not-at-random data with shadow variables, we are interested in inference on a continuous linear functional (e.g., average causal effects) of nuisance function (e.g., NPIV regression) defined by conditional moment restrictions. These nuisance functions are generally weakly identified, in that the conditional moment restrictions can be severely ill-posed as well as admit multiple solutions. This is sometimes resolved by imposing strong conditions that imply the function can be estimated at rates that make inference on the functional possible. In this paper, we study a novel condition for the functional to be strongly identified even when the nuisance function is not; that is, the functional is amenable to asymptotically-normal estimation at $\sqrt{n}$-rates. The condition implies the existence of debiasing nuisance functions, and we propose penalized minimax estimators for both the primary and debiasing nuisance functions. The proposed nuisance estimators can accommodate flexible function classes, and importantly they can converge to fixed limits determined by the penalization regardless of the identifiability of the nuisances. We use the penalized nuisance estimators to form a debiased estimator for the functional of interest and prove its asymptotic normality under generic high-level conditions, which provide for asymptotically valid confidence intervals. We also illustrate our method in a novel partially linear proximal causal inference problem and a partially linear instrumental variable regression problem.",econ.EM,Econometrics
Party On: The Labor Market Returns to Social Networks in Adolescence,"We investigate the returns to adolescent friendships on earnings in adulthood using data from the National Longitudinal Study of Adolescent to Adult Health. Because both education and friendships are jointly determined in adolescence, OLS estimates of their returns are likely biased. We implement a novel procedure to obtain bounds on the causal returns to friendships: we assume that the returns to schooling range from 5 to 15% (based on prior literature), and instrument for friendships using similarity in age among peers. Having one more friend in adolescence increases earnings between 7 and 14%, substantially more than OLS estimates would suggest.",econ.EM,Econometrics
A New Method for Generating Random Correlation Matrices,"We propose a new method for generating random correlation matrices that makes it simple to control both location and dispersion. The method is based on a vector parameterization, gamma = g(C), which maps any distribution on R^d, d = n(n-1)/2 to a distribution on the space of non-singular nxn correlation matrices. Correlation matrices with certain properties, such as being well-conditioned, having block structures, and having strictly positive elements, are simple to generate. We compare the new method with existing methods.",econ.EM,Econometrics
Efficient Integrated Volatility Estimation in the Presence of Infinite Variation Jumps via Debiased Truncated Realized Variations,"Statistical inference for stochastic processes based on high-frequency observations has been an active research area for more than two decades. One of the most well-known and widely studied problems has been the estimation of the quadratic variation of the continuous component of an It semimartingale with jumps. Several rate- and variance-efficient estimators have been proposed in the literature when the jump component is of bounded variation. However, to date, very few methods can deal with jumps of unbounded variation. By developing new high-order expansions of the truncated moments of a locally stable Lvy process, we propose a new rate- and variance-efficient volatility estimator for a class of It semimartingales whose jumps behave locally like those of a stable Lvy process with Blumenthal-Getoor index $Y\in (1,8/5)$ (hence, of unbounded variation). The proposed method is based on a two-step debiasing procedure for the truncated realized quadratic variation of the process and can also cover the case $Y<1$. Our Monte Carlo experiments indicate that the method outperforms other efficient alternatives in the literature in the setting covered by our theoretical framework.",econ.EM,Econometrics
Sample Constrained Treatment Effect Estimation,"Treatment effect estimation is a fundamental problem in causal inference. We focus on designing efficient randomized controlled trials, to accurately estimate the effect of some treatment on a population of $n$ individuals. In particular, we study sample-constrained treatment effect estimation, where we must select a subset of $s \ll n$ individuals from the population to experiment on. This subset must be further partitioned into treatment and control groups. Algorithms for partitioning the entire population into treatment and control groups, or for choosing a single representative subset, have been well-studied. The key challenge in our setting is jointly choosing a representative subset and a partition for that set.
  We focus on both individual and average treatment effect estimation, under a linear effects model. We give provably efficient experimental designs and corresponding estimators, by identifying connections to discrepancy minimization and leverage-score-based sampling used in randomized numerical linear algebra. Our theoretical results obtain a smooth transition to known guarantees when $s$ equals the population size. We also empirically demonstrate the performance of our algorithms.",econ.EM,Econometrics
Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference,"We propose a generalization of the synthetic controls and synthetic interventions methodology to incorporate network interference. We consider the estimation of unit-specific potential outcomes from panel data in the presence of spillover across units and unobserved confounding. Key to our approach is a novel latent factor model that takes into account network interference and generalizes the factor models typically used in panel data settings. We propose an estimator, Network Synthetic Interventions (NSI), and show that it consistently estimates the mean outcomes for a unit under an arbitrary set of counterfactual treatments for the network. We further establish that the estimator is asymptotically normal. We furnish two validity tests for whether the NSI estimator reliably generalizes to produce accurate counterfactual estimates. We provide a novel graph-based experiment design that guarantees the NSI estimator produces accurate counterfactual estimates, and also analyze the sample complexity of the proposed design. We conclude with simulations that corroborate our theoretical findings.",econ.EM,Econometrics
Robust Causal Learning for the Estimation of Average Treatment Effects,"Many practical decision-making problems in economics and healthcare seek to estimate the average treatment effect (ATE) from observational data. The Double/Debiased Machine Learning (DML) is one of the prevalent methods to estimate ATE in the observational study. However, the DML estimators can suffer an error-compounding issue and even give an extreme estimate when the propensity scores are misspecified or very close to 0 or 1. Previous studies have overcome this issue through some empirical tricks such as propensity score trimming, yet none of the existing literature solves this problem from a theoretical standpoint. In this paper, we propose a Robust Causal Learning (RCL) method to offset the deficiencies of the DML estimators. Theoretically, the RCL estimators i) are as consistent and doubly robust as the DML estimators, and ii) can get rid of the error-compounding issue. Empirically, the comprehensive experiments show that i) the RCL estimators give more stable estimations of the causal parameters than the DML estimators, and ii) the RCL estimators outperform the traditional estimators and their variants when applying different machine learning models on both simulation and benchmark datasets.",econ.EM,Econometrics
Testing the martingale difference hypothesis in high dimension,"In this paper, we consider testing the martingale difference hypothesis for high-dimensional time series. Our test is built on the sum of squares of the element-wise max-norm of the proposed matrix-valued nonlinear dependence measure at different lags. To conduct the inference, we approximate the null distribution of our test statistic by Gaussian approximation and provide a simulation-based approach to generate critical values. The asymptotic behavior of the test statistic under the alternative is also studied. Our approach is nonparametric as the null hypothesis only assumes the time series concerned is martingale difference without specifying any parametric forms of its conditional moments. As an advantage of Gaussian approximation, our test is robust to the cross-series dependence of unknown magnitude. To the best of our knowledge, this is the first valid test for the martingale difference hypothesis that not only allows for large dimension but also captures nonlinear serial dependence. The practical usefulness of our test is illustrated via simulation and a real data analysis. The test is implemented in a user-friendly R-function.",econ.EM,Econometrics
Moate Simulation of Stochastic Processes,"A novel approach called Moate Simulation is presented to provide an accurate numerical evolution of probability distribution functions represented on grids arising from stochastic differential processes where initial conditions are specified. Where the variables of stochastic differential equations may be transformed via It-Doeblin calculus into stochastic differentials with a constant diffusion term, the probability distribution function for these variables can be simulated in discrete time steps. The drift is applied directly to a volume element of the distribution while the stochastic diffusion term is applied through the use of convolution techniques such as Fast or Discrete Fourier Transforms. This allows for highly accurate distributions to be efficiently simulated to a given time horizon and may be employed in one, two or higher dimensional expectation integrals, e.g. for pricing of financial derivatives. The Moate Simulation approach forms a more accurate and considerably faster alternative to Monte Carlo Simulation for many applications while retaining the opportunity to alter the distribution in mid-simulation.",econ.EM,Econometrics
Stable Probability Weighting: Large-Sample and Finite-Sample Estimation and Inference Methods for Heterogeneous Causal Effects of Multivalued Treatments Under Limited Overlap,"In this paper, I try to tame ""Basu's elephants"" (data with extreme selection on observables). I propose new practical large-sample and finite-sample methods for estimating and inferring heterogeneous causal effects (under unconfoundedness) in the empirically relevant context of limited overlap. I develop a general principle called ""Stable Probability Weighting"" (SPW) that can be used as an alternative to the widely used Inverse Probability Weighting (IPW) technique, which relies on strong overlap. I show that IPW (or its augmented version), when valid, is a special case of the more general SPW (or its doubly robust version), which adjusts for the extremeness of the conditional probabilities of the treatment states. The SPW principle can be implemented using several existing large-sample parametric, semiparametric, and nonparametric procedures for conditional moment models. In addition, I provide new finite-sample results that apply when unconfoundedness is plausible within fine strata. Since IPW estimation relies on the problematic reciprocal of the estimated propensity score, I develop a ""Finite-Sample Stable Probability Weighting"" (FPW) set-estimator that is unbiased in a sense. I also propose new finite-sample inference methods for testing a general class of weak null hypotheses. The associated computationally convenient methods, which can be used to construct valid confidence sets and to bound the finite-sample confidence distribution, are of independent interest. My large-sample and finite-sample frameworks extend to the setting of multivalued treatments.",econ.EM,Econometrics
Statistical inference for the logarithmic spatial heteroskedasticity model with exogenous variables,"The spatial dependence in mean has been well studied by plenty of models in a large strand of literature, however, the investigation of spatial dependence in variance is lagging significantly behind. The existing models for the spatial dependence in variance are scarce, with neither probabilistic structure nor statistical inference procedure being explored. To circumvent this deficiency, this paper proposes a new generalized logarithmic spatial heteroscedasticity model with exogenous variables (denoted by the log-SHE model) to study the spatial dependence in variance. For the log-SHE model, its spatial near-epoch dependence (NED) property is investigated, and a systematic statistical inference procedure is provided, including the maximum likelihood and generalized method of moments estimators, the Wald, Lagrange multiplier and likelihood-ratio-type D tests for model parameter constraints, and the overidentification test for the model diagnostic checking. Using the tool of spatial NED, the asymptotics of all proposed estimators and tests are established under regular conditions. The usefulness of the proposed methodology is illustrated by simulation results and a real data example on the house selling price.",econ.EM,Econometrics
Quantile Autoregression-based Non-causality Testing,"Non-causal processes have been drawing attention recently in Macroeconomics and Finance for their ability to display nonlinear behaviors such as asymmetric dynamics, clustering volatility, and local explosiveness. In this paper, we investigate the statistical properties of empirical conditional quantiles of non-causal processes. Specifically, we show that the quantile autoregression (QAR) estimates for non-causal processes do not remain constant across different quantiles in contrast to their causal counterparts. Furthermore, we demonstrate that non-causal autoregressive processes admit nonlinear representations for conditional quantiles given past observations. Exploiting these properties, we propose three novel testing strategies of non-causality for non-Gaussian processes within the QAR framework. The tests are constructed either by verifying the constancy of the slope coefficients or by applying a misspecification test of the linear QAR model over different quantiles of the process. Some numerical experiments are included to examine the finite sample performance of the testing strategies, where we compare different specification tests for dynamic quantiles with the Kolmogorov-Smirnov constancy test. The new methodology is applied to some time series from financial markets to investigate the presence of speculative bubbles. The extension of the approach based on the specification tests to AR processes driven by innovations with heteroskedasticity is studied through simulations. The performance of QAR estimates of non-causal processes at extreme quantiles is also explored.",econ.EM,Econometrics
Relaxing Instrument Exogeneity with Common Confounders,"Instruments can be used to identify causal effects in the presence of unobserved confounding, under the famous relevance and exogeneity (unconfoundedness and exclusion) assumptions. As exogeneity is difficult to justify and to some degree untestable, it often invites criticism in applications. Hoping to alleviate this problem, we propose a novel identification approach, which relaxes traditional IV exogeneity to exogeneity conditional on some unobserved common confounders. We assume there exist some relevant proxies for the unobserved common confounders. Unlike typical proxies, our proxies can have a direct effect on the endogenous regressor and the outcome. We provide point identification results with a linearly separable outcome model in the disturbance, and alternatively with strict monotonicity in the first stage. General doubly robust and Neyman orthogonal moments are derived consecutively to enable the straightforward root-n estimation of low-dimensional parameters despite the high-dimensionality of nuisances, themselves non-uniquely defined by Fredholm integral equations. Using this novel method with NLS97 data, we separate ability bias from general selection bias in the economic returns to education problem.",econ.EM,Econometrics
Inference for Model Misspecification in Interest Rate Term Structure using Functional Principal Component Analysis,"Level, slope, and curvature are three commonly-believed principal components in interest rate term structure and are thus widely used in modeling. This paper characterizes the heterogeneity of how misspecified such models are through time. Presenting the orthonormal basis in the Nelson-Siegel model interpretable as the three factors, we design two nonparametric tests for whether the basis is equivalent to the data-driven functional principal component basis underlying the yield curve dynamics, considering the ordering of eigenfunctions or not, respectively. Eventually, we discover high dispersion between the two bases when rare events occur, suggesting occasional misspecification even if the model is overall expressive.",econ.EM,Econometrics
An Effective Treatment Approach to Difference-in-Differences with General Treatment Patterns,"We consider a general difference-in-differences model in which the treatment variable of interest may be non-binary and its value may change in each period. It is generally difficult to estimate treatment parameters defined with the potential outcome given the entire path of treatment adoption, because each treatment path may be experienced by only a small number of observations. We propose an alternative approach using the concept of effective treatment, which summarizes the treatment path into an empirically tractable low-dimensional variable, and develop doubly robust identification, estimation, and inference methods. We also provide a companion R software package.",econ.EM,Econometrics
Spectral and post-spectral estimators for grouped panel data models,"In this paper, we develop spectral and post-spectral estimators for grouped panel data models. Both estimators are consistent in the asymptotics where the number of observations $N$ and the number of time periods $T$ simultaneously grow large. In addition, the post-spectral estimator is $\sqrt{NT}$-consistent and asymptotically normal with mean zero under the assumption of well-separated groups even if $T$ is growing much slower than $N$. The post-spectral estimator has, therefore, theoretical properties that are comparable to those of the grouped fixed-effect estimator developed by Bonhomme and Manresa (2015). In contrast to the grouped fixed-effect estimator, however, our post-spectral estimator is computationally straightforward.",econ.EM,Econometrics
Inference on Time Series Nonparametric Conditional Moment Restrictions Using General Sieves,"General nonlinear sieve learnings are classes of nonlinear sieves that can approximate nonlinear functions of high dimensional variables much more flexibly than various linear sieves (or series). This paper considers general nonlinear sieve quasi-likelihood ratio (GN-QLR) based inference on expectation functionals of time series data, where the functionals of interest are based on some nonparametric function that satisfy conditional moment restrictions and are learned using multilayer neural networks. While the asymptotic normality of the estimated functionals depends on some unknown Riesz representer of the functional space, we show that the optimally weighted GN-QLR statistic is asymptotically Chi-square distributed, regardless whether the expectation functional is regular (root-$n$ estimable) or not. This holds when the data are weakly dependent beta-mixing condition. We apply our method to the off-policy evaluation in reinforcement learning, by formulating the Bellman equation into the conditional moment restriction framework, so that we can make inference about the state-specific value functional using the proposed GN-QLR method with time series data. In addition, estimating the averaged partial means and averaged partial derivatives of nonparametric instrumental variables and quantile IV models are also presented as leading examples. Finally, a Monte Carlo study shows the finite sample performance of the procedure",econ.EM,Econometrics
Uniform Inference in Linear Error-in-Variables Models: Divide-and-Conquer,"It is customary to estimate error-in-variables models using higher-order moments of observables. This moments-based estimator is consistent only when the coefficient of the latent regressor is assumed to be non-zero. We develop a new estimator based on the divide-and-conquer principle that is consistent for any value of the coefficient of the latent regressor. In an application on the relation between investment, (mismeasured) Tobin's $q$ and cash flow, we find time periods in which the effect of Tobin's $q$ is not statistically different from zero. The implausibly large higher-order moment estimates in these periods disappear when using the proposed estimator.",econ.EM,Econometrics
Time-Varying Coefficient DAR Model and Stability Measures for Stablecoin Prices: An Application to Tether,"This paper examines the dynamics of Tether, the stablecoin with the largest market capitalization. We show that the distributional and dynamic properties of Tether/USD rates have been evolving from 2017 to 2021. We use local analysis methods to detect and describe the local patterns, such as short-lived trends, time-varying volatility and persistence. To accommodate these patterns, we consider a time varying parameter Double Autoregressive tvDAR(1) model under the assumption of local stationarity of Tether/USD rates. We estimate the tvDAR model non-parametrically and test hypotheses on the functional parameters. In the application to Tether, the model provides a good fit and reliable out-of-sample forecasts at short horizons, while being robust to time-varying persistence and volatility. In addition, the model yields a simple plug-in measure of stability for Tether and other stablecoins for assessing and comparing their stability.",econ.EM,Econometrics
Inference for Large Panel Data with Many Covariates,"This paper proposes a novel testing procedure for selecting a sparse set of covariates that explains a large dimensional panel. Our selection method provides correct false detection control while having higher power than existing approaches. We develop the inferential theory for large panels with many covariates by combining post-selection inference with a novel multiple testing adjustment. Our data-driven hypotheses are conditional on the sparse covariate selection. We control for family-wise error rates for covariate discovery for large cross-sections. As an easy-to-use and practically relevant procedure, we propose Panel-PoSI, which combines the data-driven adjustment for panel multiple testing with valid post-selection p-values of a generalized LASSO, that allows us to incorporate priors. In an empirical study, we select a small number of asset pricing factors that explain a large cross-section of investment strategies. Our method dominates the benchmarks out-of-sample due to its better size and power.",econ.EM,Econometrics
A Structural Model for Detecting Communities in Networks,"The objective of this paper is to identify and analyze the response actions of a set of players embedded in sub-networks in the context of interaction and learning. We characterize strategic network formation as a static game of interactions where players maximize their utility depending on the connections they establish and multiple interdependent actions that permit group-specific parameters of players. It is challenging to apply this type of model to real-life scenarios for two reasons: The computation of the Bayesian Nash Equilibrium is highly demanding and the identification of social influence requires the use of excluded variables that are oftentimes unavailable. Based on the theoretical proposal, we propose a set of simulant equations and discuss the identification of the social interaction effect employing multi-modal network autoregressive.",econ.EM,Econometrics
Marijuana on Main Streets? The Story Continues in Colombia: An Endogenous Three-part Model,"Cannabis is the most common illicit drug, and understanding its demand is relevant to analyze the potential implications of its legalization. This paper proposes an endogenous three-part model taking into account incidental truncation and access restrictions to study demand for marijuana in Colombia, and analyze the potential effects of its legalization. Our application suggests that modeling simultaneously access, intensive and extensive margin is relevant, and that selection into access is important for the intensive margin. We find that younger men that have consumed alcohol and cigarettes, living in a neighborhood with drug suppliers, and friends that consume marijuana face higher probability of having access and using this drug. In addition, we find that marijuana is an inelastic good (-0.45 elasticity). Our results are robust to different specifications and definitions. If marijuana were legalized, younger individuals with a medium or low risk perception about marijuana use would increase the probability of use in 3.8 percentage points, from 13.6% to 17.4%. Overall, legalization would increase the probability of consumption in 0.7 p.p. (2.3% to 3.0%). Different price settings suggest that annual tax revenues fluctuate between USD 11.0 million and USD 54.2 million, a potential benchmark is USD 32 million.",econ.EM,Econometrics
Modeling Interference Using Experiment Roll-out,"Experiments on online marketplaces and social networks suffer from interference, where the outcome of a unit is impacted by the treatment status of other units. We propose a framework for modeling interference using a ubiquitous deployment mechanism for experiments, staggered roll-out designs, which slowly increase the fraction of units exposed to the treatment to mitigate any unanticipated adverse side effects. Our main idea is to leverage the temporal variations in treatment assignments introduced by roll-outs to model the interference structure. Since there are often multiple competing models of interference in practice we first develop a model selection method that evaluates models based on their ability to explain outcome variation observed along the roll-out. Through simulations, we show that our heuristic model selection method, Leave-One-Period-Out, outperforms other baselines. Next, we present a set of model identification conditions under which the estimation of common estimands is possible and show how these conditions are aided by roll-out designs. We conclude with a set of considerations, robustness checks, and potential limitations for practitioners wishing to use our framework.",econ.EM,Econometrics
On the Time-Varying Structure of the Arbitrage Pricing Theory using the Japanese Sector Indices,"This paper is the first study to examine the time instability of the APT in the Japanese stock market. In particular, we measure how changes in each risk factor affect the stock risk premiums to investigate the validity of the APT over time, applying the rolling window method to Fama and MacBeth's (1973) two-step regression and Kamstra and Shi's (2023) generalized GRS test. We summarize our empirical results as follows: (1) the changes in monetary policy by major central banks greatly affect the validity of the APT in Japan, and (2) the time-varying estimates of the risk premiums for each factor are also unstable over time, and they are affected by the business cycle and economic crises. Therefore, we conclude that the validity of the APT as an appropriate model to explain the Japanese sector index is not stable over time.",econ.EM,Econometrics
Statistical Estimation for Covariance Structures with Tail Estimates using Nodewise Quantile Predictive Regression Models,"This paper considers the specification of covariance structures with tail estimates. We focus on two aspects: (i) the estimation of the VaR-CoVaR risk matrix in the case of larger number of time series observations than assets in a portfolio using quantile predictive regression models without assuming the presence of nonstationary regressors and; (ii) the construction of a novel variable selection algorithm, so-called, Feature Ordering by Centrality Exclusion (FOCE), which is based on an assumption-lean regression framework, has no tuning parameters and is proved to be consistent under general sparsity assumptions. We illustrate the usefulness of our proposed methodology with numerical studies of real and simulated datasets when modelling systemic risk in a network.",econ.EM,Econometrics
Generalized Autoregressive Score Trees and Forests,"We propose methods to improve the forecasts from generalized autoregressive score (GAS) models (Creal et. al, 2013; Harvey, 2013) by localizing their parameters using decision trees and random forests. These methods avoid the curse of dimensionality faced by kernel-based approaches, and allow one to draw on information from multiple state variables simultaneously. We apply the new models to four distinct empirical analyses, and in all applications the proposed new methods significantly outperform the baseline GAS model. In our applications to stock return volatility and density prediction, the optimal GAS tree model reveals a leverage effect and a variance risk premium effect. Our study of stock-bond dependence finds evidence of a flight-to-quality effect in the optimal GAS forest forecasts, while our analysis of high-frequency trade durations uncovers a volume-volatility effect.",econ.EM,Econometrics
